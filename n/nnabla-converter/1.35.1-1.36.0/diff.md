# Comparing `tmp/nnabla_converter-1.35.1-py3-none-any.whl.zip` & `tmp/nnabla_converter-1.36.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 3228166 bytes, number of entries: 23
--rw-r--r--  2.0 unx      667 b- defN 23-Apr-19 09:14 nnabla/utils/converter/onnx/__init__.py
--rw-r--r--  2.0 unx   164298 b- defN 23-Apr-19 09:14 nnabla/utils/converter/onnx/exporter.py
--rw-r--r--  2.0 unx   191397 b- defN 23-Apr-19 09:14 nnabla/utils/converter/onnx/importer.py
--rw-r--r--  2.0 unx      789 b- defN 23-Apr-19 09:14 nnabla/utils/converter/onnx/utils.py
--rw-r--r--  2.0 unx      715 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/__init__.py
--rw-r--r--  2.0 unx     4248 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/common.py
--rw-r--r--  2.0 unx     3605 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/exporter.py
--rw-r--r--  2.0 unx     4464 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/importer.py
--rw-r--r--  2.0 unx    30997 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/parsetab.py
--rw-r--r--  2.0 unx    19583 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/refine_graph.py
--rw-r--r--  2.0 unx     6511 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tensorflow/refine_parser.py
--rw-r--r--  2.0 unx      662 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tflite/__init__.py
--rw-r--r--  2.0 unx   112162 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tflite/exporter.py
--rwxr-xr-x  2.0 unx  3298328 b- defN 23-Apr-19 09:26 nnabla/utils/converter/tflite/flatc_linux
--rwxr-xr-x  2.0 unx  2913472 b- defN 23-Apr-19 09:26 nnabla/utils/converter/tflite/flatc_mac
--rwxr-xr-x  2.0 unx  2649088 b- defN 23-Apr-19 09:26 nnabla/utils/converter/tflite/flatc_windows.exe
--rw-r--r--  2.0 unx     1194 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tflite/importer.py
--rw-r--r--  2.0 unx    28477 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tflite/quantized_converter.py
--rw-rw-r--  2.0 unx    26803 b- defN 23-Apr-19 09:14 nnabla/utils/converter/tflite/schema.fbs
--rw-r--r--  2.0 unx     2266 b- defN 23-Apr-19 09:26 nnabla_converter-1.35.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-19 09:26 nnabla_converter-1.35.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-Apr-19 09:26 nnabla_converter-1.35.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2252 b- defN 23-Apr-19 09:26 nnabla_converter-1.35.1.dist-info/RECORD
-23 files, 9462077 bytes uncompressed, 3224426 bytes compressed:  65.9%
+Zip file size: 3228388 bytes, number of entries: 23
+-rw-r--r--  2.0 unx      667 b- defN 23-Jul-06 06:22 nnabla/utils/converter/onnx/__init__.py
+-rw-r--r--  2.0 unx   164653 b- defN 23-Jul-06 06:22 nnabla/utils/converter/onnx/exporter.py
+-rw-r--r--  2.0 unx   191470 b- defN 23-Jul-06 06:22 nnabla/utils/converter/onnx/importer.py
+-rw-r--r--  2.0 unx     1013 b- defN 23-Jul-06 06:22 nnabla/utils/converter/onnx/utils.py
+-rw-r--r--  2.0 unx      715 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     4248 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/common.py
+-rw-r--r--  2.0 unx     3673 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/exporter.py
+-rw-r--r--  2.0 unx     4464 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/importer.py
+-rw-r--r--  2.0 unx    30997 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/parsetab.py
+-rw-r--r--  2.0 unx    19583 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/refine_graph.py
+-rw-r--r--  2.0 unx     6511 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tensorflow/refine_parser.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tflite/__init__.py
+-rw-r--r--  2.0 unx   112163 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tflite/exporter.py
+-rwxr-xr-x  2.0 unx  3298328 b- defN 23-Jul-06 06:42 nnabla/utils/converter/tflite/flatc_linux
+-rwxr-xr-x  2.0 unx  2913472 b- defN 23-Jul-06 06:42 nnabla/utils/converter/tflite/flatc_mac
+-rwxr-xr-x  2.0 unx  2649088 b- defN 23-Jul-06 06:42 nnabla/utils/converter/tflite/flatc_windows.exe
+-rw-r--r--  2.0 unx     1194 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tflite/importer.py
+-rw-r--r--  2.0 unx    28477 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tflite/quantized_converter.py
+-rw-rw-r--  2.0 unx    26803 b- defN 23-Jul-06 06:22 nnabla/utils/converter/tflite/schema.fbs
+-rw-r--r--  2.0 unx     2300 b- defN 23-Jul-06 06:42 nnabla_converter-1.36.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-06 06:42 nnabla_converter-1.36.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-Jul-06 06:42 nnabla_converter-1.36.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2253 b- defN 23-Jul-06 06:42 nnabla_converter-1.36.0.dist-info/RECORD
+23 files, 9462833 bytes uncompressed, 3224648 bytes compressed:  65.9%
```

## zipnote {}

```diff
@@ -51,20 +51,20 @@
 
 Filename: nnabla/utils/converter/tflite/quantized_converter.py
 Comment: 
 
 Filename: nnabla/utils/converter/tflite/schema.fbs
 Comment: 
 
-Filename: nnabla_converter-1.35.1.dist-info/METADATA
+Filename: nnabla_converter-1.36.0.dist-info/METADATA
 Comment: 
 
-Filename: nnabla_converter-1.35.1.dist-info/WHEEL
+Filename: nnabla_converter-1.36.0.dist-info/WHEEL
 Comment: 
 
-Filename: nnabla_converter-1.35.1.dist-info/top_level.txt
+Filename: nnabla_converter-1.36.0.dist-info/top_level.txt
 Comment: 
 
-Filename: nnabla_converter-1.35.1.dist-info/RECORD
+Filename: nnabla_converter-1.36.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nnabla/utils/converter/onnx/exporter.py

```diff
@@ -34,17 +34,17 @@
     TensorProto.UINT8: np.uint8,
     TensorProto.INT8: np.int8,
     TensorProto.UINT32: np.uint32,
     TensorProto.INT32: np.int32,
     TensorProto.INT64: np.int64,
 }
 
-
 random_seed = 0
 
+
 # Helper functions
 
 
 def generate_scalar_constant(output_name, tensor_name, scalar):
     """Convert a scalar value to a Constant buffer.
     This is mainly used for xxScalar operators."""
     t = onnx.helper.make_tensor(tensor_name,
@@ -115,15 +115,15 @@
         n = onnx.helper.make_node(
             "Pad",
             [input_name],
             [output_name],
             mode=pad_mode,
             pads=pads,
             value=value,
-            )
+        )
         nl.append(n)
 
     return nl
 
 
 def generate_value(type, dims, data_type, multiplier):
     d = TENSOR_TYPE_TO_DTYPE[data_type]
@@ -159,15 +159,15 @@
         [mul_out]
     )
     nl.append(n)
 
     # ReduceSum
     sum_out = fork_name(func_input + "_sum")
     if opset == '13':
-        axes_shape = (len(axes), )
+        axes_shape = (len(axes),)
         axes_param_name = fork_name("ReduceSumAxes")
         add_param(graph, axes_param_name,
                   TensorProto.INT64, axes_shape,
                   np.array(axes).astype(np.int64).tostring())
         n = onnx.helper.make_node(
             'ReduceSum',
             [mul_out, axes_param_name],
@@ -323,17 +323,19 @@
 
 class ParameterState:
     TRANSPOSED = 1
 
 
 # OnnxExporter class
 class OnnxExporter:
-    def __init__(self, nnp, batch_size, opset="11"):
+    def __init__(self, nnp, batch_size, ir_version, opset="11"):
         self._nnp = nnp.protobuf
         self._batch_size = batch_size
+        # If ir_version is negative, it defaults to ONNX_IR_VERSION for the current ONNX version.
+        self._ir_version = ir_version if ir_version > 0 else ONNX_IR_VERSION
         self._model_proto = None
         self._net = None
         self._onehot_table = {}
         self._var_dict = {}
         self._parameters = {}
         self._parameters_state = {}
         self._input_types = {}
@@ -652,15 +654,15 @@
 
         if func_name == "Less" or func_name == "Greater":
             self._output_types[func.output[0]] = TensorProto.BOOL
 
         if func_name == "Equal":
             self._output_types[func.output[0]] = TensorProto.BOOL
 
-        # Check if the second input is a brodcast target.
+        # Check if the second input is a broadcast target.
         bt = func.input[1]
         nl = []
         if bt in self._broadcast_target:
             # Set the broadcast attribute to the operator
             # so we can combine BroadcastTo with this operator.
             param = self._broadcast_target[bt]
             second_input = param[0]
@@ -676,17 +678,17 @@
                 )
                 nl.append(n)
             else:  # opset >= 7
                 input0_shape_len = len(self._var_dict[func.input[0]].dim)
                 input1_shape_len = len(self._var_dict[second_input].dim)
                 unsqueeze_output = fork_name("broadcast_unsqueeze")
                 trailing = list(
-                    range(input1_shape_len+1, input0_shape_len))
+                    range(input1_shape_len + 1, input0_shape_len))
                 axes = list(range(axis)) + trailing
-                axes_shape = (len(axes), )
+                axes_shape = (len(axes),)
                 if opset == '13':
                     axes_param_name = fork_name("UnsqueezeAxes")
                     add_param(self._model_proto.graph, axes_param_name,
                               TensorProto.INT64, axes_shape,
                               np.array(axes).astype(np.int64).tostring())
                     unsqueeze = onnx.helper.make_node(
                         "Unsqueeze",
@@ -946,15 +948,15 @@
                                  new_input_shape)
             nl.append(n)
             inputs[0] = output_x_reshape_name
 
         # Conv
         output_conv = fork_name('output_conv')
         conv_output_shape = np.array(np.concatenate(
-                ([np.prod(output_y_shape[:cp.base_axis])], output_y_shape[cp.base_axis:])))
+            ([np.prod(output_y_shape[:cp.base_axis])], output_y_shape[cp.base_axis:])))
         n = onnx.helper.make_node(
             'Conv',
             inputs[:2],
             [output_conv],
             kernel_shape=kernel_shape,
             dilations=dilations,
             strides=strides,
@@ -962,15 +964,16 @@
             group=cp.group
         )
         nl.append(n)
 
         input_alpha_shape = np.array(
             [d for d in self._var_dict[inputs[2]].dim])
         input_alpha_shape = np.array([input_alpha_shape[0] if input_alpha_shape[0]
-                                      == conv_output_shape[i] else 1 for i in range(len(conv_output_shape))])
+                                      == conv_output_shape[i] else 1 for i in
+                                      range(len(conv_output_shape))])
         proto_alpha_shape = self._var_dict[inputs[2]]
         del proto_alpha_shape.dim[:]
         proto_alpha_shape.dim.extend(input_alpha_shape)
 
         # Mul
         output_mul = fork_name('output_mul')
         n = onnx.helper.make_node(
@@ -984,15 +987,16 @@
         nl.append(n)
         output = output_mul
 
         if len(inputs) > 3:
             input_bias_shape = np.array(
                 [d for d in self._var_dict[inputs[3]].dim])
             input_bias_shape = np.array([input_bias_shape[0] if input_bias_shape[0]
-                                         == conv_output_shape[i] else 1 for i in range(len(conv_output_shape))])
+                                         == conv_output_shape[i] else 1 for i in
+                                         range(len(conv_output_shape))])
             proto_bias_shape = self._var_dict[inputs[3]]
             del proto_bias_shape.dim[:]
             proto_bias_shape.dim.extend(input_bias_shape)
 
             # Add
             output_add = fork_name('output_add')
             n = onnx.helper.make_node(
@@ -1171,18 +1175,18 @@
             b_shape.dim.extend([d])
             self._var_dict[p] = b_shape
 
         input_shape = [d for d in self._var_dict[inputs[0]].dim]
         input_shape_reshape = input_shape[:]
         if axes > 1:
             input_shape_reshape = [
-                np.prod(input_shape_reshape[:axes])] + input_shape_reshape[axes:]
+                                      np.prod(input_shape_reshape[:axes])] + input_shape_reshape[axes:]
         if len(input_shape_reshape) < 4:
             input_shape_reshape = input_shape_reshape + \
-                [1] * (4 - len(input_shape_reshape))
+                                  [1] * (4 - len(input_shape_reshape))
         if input_shape_reshape != input_shape:
             output_x_reshape = fork_name("output_x_reshape")
             n = generate_reshape(self._model_proto.graph, inputs[0], output_x_reshape,
                                  np.array(input_shape_reshape))
             nl.append(n)
             inputs[0] = output_x_reshape
             outputs[0] = fork_name("reshape_output")
@@ -1200,15 +1204,15 @@
                 keepdims=True
             )
             nl.append(n)
 
             # Squeeze
             squeeze_out = fork_name(func.input[3]) + "_squeeze"
             if opset == '13':
-                reduc_axes_shape = (len(reduc_axes), )
+                reduc_axes_shape = (len(reduc_axes),)
                 reduc_axes_param_name = fork_name("BnSqueezeAxes")
                 add_param(self._model_proto.graph, reduc_axes_param_name,
                           TensorProto.INT64, reduc_axes_shape,
                           np.array(reduc_axes).astype(np.int64).tostring())
                 n = onnx.helper.make_node(
                     'Squeeze',
                     [mean_out, reduc_axes_param_name],
@@ -1241,15 +1245,15 @@
                 [mul_out],
             )
             nl.append(n)
 
             # ReduceSum
             sum_out = fork_name(func.input[4]) + "_sum"
             if opset == '13':
-                reduc_axes_shape = (len(reduc_axes), )
+                reduc_axes_shape = (len(reduc_axes),)
                 reduc_axes_param_name = fork_name("BnReduceSumAxes")
                 add_param(self._model_proto.graph, reduc_axes_param_name,
                           TensorProto.INT64, reduc_axes_shape,
                           np.array(reduc_axes).astype(np.int64).tostring())
                 n = onnx.helper.make_node(
                     'ReduceSum',
                     [mul_out, reduc_axes_param_name],
@@ -1410,15 +1414,15 @@
             rout = fork_name(input) + "_reshape"
             n = generate_reshape(self._model_proto.graph, input, rout,
                                  input_shape_reshape)
             nl.append(n)
             input = rout
             output = fork_name(func.output[0]) + "_reshape"
         scales = np.array([1.0, 1.0] + func.unpooling_param.kernel.dim[:])
-        scale_shape = (len(scales), )
+        scale_shape = (len(scales),)
         scale_param_name = fork_name("UpsampleScales")
         add_param(self._model_proto.graph, scale_param_name,
                   TensorProto.FLOAT, scale_shape,
                   scales.astype(np.float32).tostring())
 
         n = onnx.helper.make_node(
             'Upsample',
@@ -1549,15 +1553,15 @@
             )
             nl.append(n)
 
             # Step 2: gather
             index_name = fork_name('GatherFlip')
             gather_name = fork_name('GatherFlipOutput')
             raw_data = np.arange(input_shape[axis])[
-                ::-1].astype(np.int32).tostring()
+                       ::-1].astype(np.int32).tostring()
             index_shape = [input_shape[axis]]
             add_param(self._model_proto.graph, index_name,
                       TensorProto.INT32, index_shape, raw_data)
             n = onnx.helper.make_node(
                 'Gather',
                 [o, index_name],
                 [gather_name],
@@ -1756,15 +1760,15 @@
         return nl
 
     def Stack(self, opset, func):
         nl = []
         outputs = []
         if opset == '13':
             axes = [func.stack_param.axis]
-            axes_shape = (len(axes), )
+            axes_shape = (len(axes),)
             for i, x in enumerate(func.input):
                 axes_param_name = fork_name("StackAxes")
                 add_param(self._model_proto.graph, axes_param_name,
                           TensorProto.INT64, axes_shape,
                           np.array(axes).astype(np.int64).tostring())
                 output_name = fork_name(x)
                 n = onnx.helper.make_node(
@@ -1808,15 +1812,15 @@
             name=fork_name("Split"))
         attr = onnx.helper.make_attribute("axis", func.split_param.axis)
         n.attribute.extend([attr])
         nl.append(n)
 
         if opset == '13':
             axes = [func.split_param.axis]
-            axes_shape = (len(axes), )
+            axes_shape = (len(axes),)
             for i, x in enumerate(outputs):
                 axes_param_name = fork_name("SplitAxes")
                 add_param(self._model_proto.graph, axes_param_name,
                           TensorProto.INT64, axes_shape,
                           np.array(axes).astype(np.int64).tostring())
 
                 n = onnx.helper.make_node(
@@ -2317,15 +2321,15 @@
             [expout],
         )
         nl.append(n)
 
         # ReduceSum
         sumout = fork_name(func.input[0]) + "_reducesum"
         if opset == '13':
-            axis_shape = (len([axis]), )
+            axis_shape = (len([axis]),)
             axis_param_name = fork_name("SoftmaxAxis")
             add_param(self._model_proto.graph, axis_param_name,
                       TensorProto.INT64, axis_shape,
                       np.array([axis]).astype(np.int64).tostring())
             n = onnx.helper.make_node(
                 'ReduceSum',
                 [expout, axis_param_name],
@@ -2464,15 +2468,15 @@
             [expout],
         )
         nl.append(n)
 
         # ReduceSum
         sumout = fork_name(func.input[0]) + "_reducesum"
         if opset == '13':
-            axis_shape = (len([axis]), )
+            axis_shape = (len([axis]),)
             axis_param_name = fork_name("LogSoftmaxAxis")
             add_param(self._model_proto.graph, axis_param_name,
                       TensorProto.INT64, axis_shape,
                       np.array([axis]).astype(np.int64).tostring())
             n = onnx.helper.make_node(
                 'ReduceSum',
                 [expout, axis_param_name],
@@ -2722,15 +2726,15 @@
                               [1])
         nl.append(c)
 
         # Constant
         constant4 = fork_name("constant")
         c = generate_constant(constant4, func.name + "_constant4",
                               TensorProto.FLOAT, [1],
-                              [2/np.pi])
+                              [2 / np.pi])
         nl.append(c)
 
         # Pow
         pow_out = fork_name(func.input[0]) + "_pow"
         n = onnx.helper.make_node("Pow",
                                   [func.input[0], constant0],
                                   [pow_out])
@@ -3196,15 +3200,15 @@
         return nl
 
     def ReduceSum(self, func):
         nl = []
         axes = func.sum_param.axes
         k = func.sum_param.keep_dims
 
-        axes_shape = (len(axes), )
+        axes_shape = (len(axes),)
         axes_param_name = fork_name("SumAxes")
         add_param(self._model_proto.graph, axes_param_name,
                   TensorProto.INT64, axes_shape,
                   np.array(axes).astype(np.int64).tostring())
         n = onnx.helper.make_node(
             'ReduceSum',
             [func.input[0], axes_param_name],
@@ -3303,15 +3307,15 @@
             scale = [float(output_shape[i]) / float(input_shape[i])
                      for i in range(4)]
         else:
             diff = len(input_shape) - 4
             if diff > 0:
                 diff += 1
                 new_input_shape = [
-                    np.prod(input_shape[:diff])] + input_shape[diff:]
+                                      np.prod(input_shape[:diff])] + input_shape[diff:]
                 new_output_shape = [new_input_shape[0]] + output_shape[diff:]
             else:
                 new_input_shape = [1] * -diff + input_shape
                 new_output_shape = [1] * -diff + output_shape
             scale = [float(new_output_shape[i]) /
                      float(new_input_shape[i]) for i in range(4)]
 
@@ -3365,15 +3369,15 @@
             scale = [float(output_shape[i]) / float(input_shape[i])
                      for i in range(4)]
         else:
             diff = len(input_shape) - 4
             if diff > 0:
                 diff += 1
                 new_input_shape = [
-                    np.prod(input_shape[:diff])] + input_shape[diff:]
+                                      np.prod(input_shape[:diff])] + input_shape[diff:]
                 new_output_shape = [new_input_shape[0]] + output_shape[diff:]
             else:
                 new_input_shape = [1] * -diff + input_shape
                 new_output_shape = [1] * -diff + output_shape
             scale = [float(new_output_shape[i]) /
                      float(new_input_shape[i]) for i in range(4)]
 
@@ -3496,23 +3500,23 @@
 
         if func.input[1] not in self._parameters or func.input[2] not in self._parameters:
             raise ValueError(
                 "The type of scale and zero_point must be parameter")
 
         if self._scale_cnt[func.input[1]] > 1:
             scale_name = fork_name(func.input[1])
-            scale = [1/d for d in self._parameters[func.input[1]].data]
+            scale = [1 / d for d in self._parameters[func.input[1]].data]
             p = self._nnp.parameter.add()
             p.variable_name = scale_name
             p.shape.dim.extend([len(scale)])
             p.data.extend(scale)
             self._var_dict[scale_name] = self._var_dict[func.input[1]]
             func.input[1] = scale_name
         else:
-            scale = [1/d for d in self._parameters[func.input[1]].data]
+            scale = [1 / d for d in self._parameters[func.input[1]].data]
             for p in self._nnp.parameter[:]:
                 if p.variable_name == func.input[1]:
                     self._nnp.parameter.remove(p)
             p = self._nnp.parameter.add()
             p.variable_name = func.input[1]
             p.shape.dim.extend([len(scale)])
             p.data.extend(scale)
@@ -3839,15 +3843,15 @@
                                   [func.input[0], constant0],
                                   [pow_out1])
         nl.append(n)
 
         # ReduceSum, np.sum(w ** 2, axes, keepdims=True)
         sum_out = fork_name(func.input[0]) + "_sum"
         if opset == '13':
-            axes_shape = (len(axes), )
+            axes_shape = (len(axes),)
             axes_param_name = fork_name("WnReduceSumAxes")
             add_param(self._model_proto.graph, axes_param_name,
                       TensorProto.INT64, axes_shape,
                       np.array(axes).astype(np.int64).tostring())
             n = onnx.helper.make_node(
                 'ReduceSum',
                 [pow_out1, axes_param_name],
@@ -4107,15 +4111,15 @@
         constant2 = fork_name("constant")
         c = generate_constant(constant2, func.name + "_constant2",
                               TensorProto.FLOAT, [1], [0])
         nl.append(c)
 
         if dim != 0:
             dims_transpose = [dim] + \
-                [i for i in range(len(w_shape)) if i != dim]
+                             [i for i in range(len(w_shape)) if i != dim]
             transpose_out = fork_name(func.input[0]) + "_transpose"
             n = onnx.helper.make_node(
                 'Transpose',
                 [func.input[0]],
                 [transpose_out],
                 perm=dims_transpose
             )
@@ -4494,15 +4498,15 @@
             # Less, Mul, Or, Pow, Sub, Xor
             bp = func.broadcast_to_param
             broadcast_target[func.output[0]] = (func.input[1], bp.axis)
             # we do not append node here because BroadcastTo should disappear
         elif func.type == "Constant":
             cp = func.constant_param
             shape = list(self._var_dict[func.output[0]].dim[:])
-            val = [cp.val]*np.prod(shape)
+            val = [cp.val] * np.prod(shape)
             t = onnx.helper.make_tensor("Constant",
                                         data_type=TensorProto.FLOAT,
                                         dims=shape, vals=val)
             p = onnx.helper.make_attribute("value", t)
             n.attribute.extend([p])
             nl.append(n)
         elif func.type == "IsNaN":
@@ -4532,15 +4536,15 @@
             # operator's inputs, we throw an error.
             raise ValueError("BroadcastTo targets must be used in conjunction"
                              " with certain operators in order to get converted to ONNX")
         self.set_variables()
 
     def create_model(self):
         mp = ModelProto()
-        mp.ir_version = ONNX_IR_VERSION
+        mp.ir_version = self._ir_version
         op = mp.opset_import.add()
         op.domain = ""  # empty string indicates ONNX domain
         op.version = self._opset
         # nn_opset = mp.opset_import.add()
         # nn_opset.domain = NNABLA_DOMAIN
         # nn_opset.version = NNABLA_OPSET_VERSION
         mp.producer_name = PRODUCER_NAME
```

## nnabla/utils/converter/onnx/importer.py

```diff
@@ -132,27 +132,27 @@
     for i in range(len(i_shape)):
         index = axes[i]
         o_shape.append(i_shape[index])
     return o_shape
 
 
 def generate_broadcast_to(node_name, x, y, out_name, axis, base_name, func_counter):
-    """Generate a BroadcastTo operator to brodcastto specified buffer"""
+    """Generate a BroadcastTo operator to broadcastto specified buffer"""
     bt = nnabla_pb2.Function()
     bt.type = "BroadcastTo"
     set_function_name(bt, node_name, base_name, func_counter)
     bt.input.extend([x, y])
     bt.output.extend([out_name])
     btp = bt.broadcast_to_param
     btp.axis = axis
     return bt
 
 
 def generate_broadcast(node_name, in_name, out_name, shape, base_name, func_counter):
-    """Generate a Broadcast operator to brodcast specified buffer"""
+    """Generate a Broadcast operator to broadcast specified buffer"""
     bt = nnabla_pb2.Function()
     bt.type = "Broadcast"
     set_function_name(bt, node_name, base_name, func_counter)
     bt.input.extend([in_name])
     bt.output.extend([out_name])
     btp = bt.broadcast_param
     btp.shape.dim.extend(shape)
@@ -979,15 +979,15 @@
                     raise ValueError("Only STRING is supported for auto_pad in {} op_type"
                                      .format(n.op_type))
                 auto_pad = attr.s.decode("utf-8")
             else:
                 raise ValueError("Unsupported attribute {} was specified at {}"
                                  .format(attr.name, n.op_type))
 
-        # NNabla requires for the dimensions of strides, pads, dilations to match.
+        # NNabla requires for the dimensions of strides, pads, dilation to match.
         # We align the dimensions for all three attributes to the shortest one
         cp.stride.dim.extend(strides[:])
         cp.dilation.dim.extend(dilations[:])
         if auto_pad != 'NOTSET':
             kernels = weight_shape[2:]
             pads_value = [0] * dims
             if auto_pad in ('SAME_UPPER', 'SAME_LOWER'):
@@ -2875,15 +2875,15 @@
                     raise ValueError(
                         "Only STRING is supported for auto_pad in ConvTranspose op_type")
                 auto_pad = attr.s.decode("utf-8")
             else:
                 raise ValueError("Unsupported attribute {} was specified at {}"
                                  .format(attr.name, n.op_type))
 
-        # NNabla requires for the dimensions of strides, pads, dilations to match.
+        # NNabla requires for the dimensions of strides, pads, dilation to match.
         # We align the dimensions for all three attributes to the shortest one
         if strides:
             cp.stride.dim.extend(strides[:])
         else:
             cp.stride.dim.extend([1]*dim)
         if dilations:
             cp.dilation.dim.extend(dilations[:])
@@ -3685,15 +3685,15 @@
                 ctm = attr.s.decode("utf-8")
             elif attr.name == "mode":
                 check_attr_string_type(attr, n)
                 mode = attr.s.decode("utf-8")
         input_len = len(n.input)
         sizes = n.input[3] if input_len == 4 else None
         scales = n.input[2] if input_len >= 3 else None
-        # roi input is not suppoted
+        # roi input is not supported
 
         # precheck
         if sizes and scales:
             raise ValueError(
                     "Only one of 'scales' and 'sizes' can be specified in Resize.")
         if not sizes and not scales:
             raise ValueError(
@@ -4608,14 +4608,16 @@
                         "6")
                 else:
                     self._onnx_optype_to_nnabla_function_type = self.opver_impl_map.get(
                         str(opset.version))
                 if not self._onnx_optype_to_nnabla_function_type:
                     raise ValueError("ONNX opset version is currently not supported: {}".format(
                         opset.version))
+            elif opset.domain == "ai.onnx.ml":
+                continue
             else:
                 raise ValueError(
                     "Unsupported opset from domain {}".format(opset.domain))
 
         # convert onnx model to nnabla protobuf
         # logger.log(99, "Converting ONNX made by {}.".format(model.producer_name))
```

## nnabla/utils/converter/onnx/utils.py

```diff
@@ -8,13 +8,18 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import onnx
+
 NNABLA_DOMAIN = "org.nnabla"
 NNABLA_OPSET_VERSION = 1
-ONNX_IR_VERSION = 7
+# Loop up to ONNX version table to find the supported IR version
+ONNX_VERSION = onnx.__version__
+ONNX_IR_VERSION = next(
+    (version_info[1] for version_info in onnx.helper.VERSION_TABLE if version_info[0] == ONNX_VERSION), None)
 ONNX_OPSET_VERSION = 6
 PRODUCER_NAME = "nnabla-onnx"
 PRODUCER_VERSION = "0.1"
```

## nnabla/utils/converter/tensorflow/exporter.py

```diff
@@ -19,17 +19,18 @@
 from onnx_tf.backend import prepare
 from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
 
 from ..onnx import OnnxExporter
 
 
 class TensorflowExporter:
-    def __init__(self, nnp, batch_size, model_format='TF_PB'):
+    def __init__(self, nnp, batch_size, ir_version, model_format='TF_PB'):
         self._nnp = nnp
         self._batch_size = batch_size
+        self._ir_version = ir_version
         self._model_format = model_format
         self.check_nnp_variable_name()
 
     def check_nnp_variable_name(self):
         def fix_variable_name(variable_name):
             if "[" in variable_name and "]" in variable_name:
                 variable_name = variable_name.replace("[", "_")
@@ -63,15 +64,15 @@
         for var in executor[0].data_variable:
             var.variable_name = fix_variable_name(var.variable_name)
         for var in executor[0].output_variable:
             var.variable_name = fix_variable_name(var.variable_name)
 
     def execute(self, output):
         onnx_model = OnnxExporter(
-            self._nnp, self._batch_size, opset="11").export_model_proto()
+            self._nnp, self._batch_size, self._ir_version, opset="11").export_model_proto()
         tf_rep = prepare(onnx_model)
         if self._model_format == 'TF_PB':
             output_path = os.path.dirname(output)
             tf_model = tf_rep.tf_module.__call__.get_concrete_function(
                 **tf_rep.signatures)
             frozen_func = convert_variables_to_constants_v2(
                 tf_model, lower_control_flow=False)
```

## nnabla/utils/converter/tflite/exporter.py

```diff
@@ -525,15 +525,15 @@
             self.variables[outputs[0]] = [
                 new_shape_b[i] if new_shape_a[i] == 1 else new_shape_a[i] for i in range(dims)]
             self.network.variables[outputs[0]].data_format = data_format0
         return resolve_inputs
 
     def check_resolve_input_semantic_divergence(self, inputs, outputs, is_transpose_conv=False):
         # Check for semantic divergence. if it exists:
-        # 1. resove the input tensor by adding transpose.
+        # 1. resolve the input tensor by adding transpose.
         # 2. parameter will be directly transposed and saved.
         # 3. modify outputs shape and set data_format to outputs variables
         resolve_inputs = []
         if self.global_data_format == DataFormat.channel_first:
             for inp in inputs[:]:
                 if inp in self.parameters:
                     # convert weight data_format
@@ -1617,16 +1617,16 @@
         value = pf.args.get('val', 1.0)
         scalar_name = fork_name(pf.inputs[0]) + "_scalar"
         self.create_parameter(scalar_name, [1], [value])
         if reverse:
             inputs = [scalar_name, pf.inputs[0]]
         else:
             inputs = [pf.inputs[0], scalar_name]
-        self.convert_generic_tflite_op(inputs, pf.outputs, tflite_operator)
         self.propagate_variable_semantic(pf)
+        self.convert_generic_tflite_op(inputs, pf.outputs, tflite_operator)
 
     def BatchNormalization(self, pf):
         inputs = pf.inputs[:]
         outputs = pf.outputs[:]
         eps = pf.args.get('eps', 1e-05)
         axes = pf.args.get('axes', (1,))[0]
         batch_stat = pf.args.get('batch_stat', True)
```

## Comparing `nnabla_converter-1.35.1.dist-info/METADATA` & `nnabla_converter-1.36.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nnabla-converter
-Version: 1.35.1
+Version: 1.36.0
 Summary: NNabla File Format Converter
 Home-page: https://github.com/sony/nnabla
 Author: Takuya Narihira, Sony Corporation
 Author-email: nnabla@googlegroups.com
 License: Apache License 2.0
 Keywords: NNabla File Format Converter
 Classifier: Development Status :: 5 - Production/Stable
@@ -19,21 +19,22 @@
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 Requires-Dist: tensorboard (<=2.9.0,>=2.6.0)
 Requires-Dist: tensorflow-probability (==0.16.0)
 Requires-Dist: onnx-tf
-Requires-Dist: tf2onnx (~=1.7.2)
+Requires-Dist: tf2onnx (~=1.14.0)
 Requires-Dist: tensorflow-addons
 Requires-Dist: onnx (~=1.12.0)
 Requires-Dist: tflite2onnx
 Requires-Dist: flatbuffers
 Requires-Dist: pyopenssl
 Requires-Dist: certifi
+Requires-Dist: nnabla (==1.36.0)
 Requires-Dist: tensorflow (~=2.8.0) ; platform_system != "Windows"
 Requires-Dist: tensorflow (<=2.8.1,>=2.8.0) ; platform_system == "Windows"
 
 # File Format Converter 
 
 nnabla-converter enables you to convert NN models. Currently we are supporting following models:
```

## Comparing `nnabla_converter-1.35.1.dist-info/RECORD` & `nnabla_converter-1.36.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 nnabla/utils/converter/onnx/__init__.py,sha256=TWC1rkZzNFv1JuESqMpjojOP6i80LZeEzS9epFEPb2c,667
-nnabla/utils/converter/onnx/exporter.py,sha256=MRhJCyWFdS4Zuel8JHbXQXFU2iKkIEQJMUJ23MnwEZ4,164298
-nnabla/utils/converter/onnx/importer.py,sha256=X16AZd9ypstmTl_J1GSEXVggspSFnSVJC8FLTxLz-zM,191397
-nnabla/utils/converter/onnx/utils.py,sha256=uPWMHXqJzgMbrjTfeIIAm7Ecu5RZ-Mkagfy6s_2QDyY,789
+nnabla/utils/converter/onnx/exporter.py,sha256=M_FApPjik1O6E4HXOEvGV1mGUT-sjw59fdIDhnh02c0,164653
+nnabla/utils/converter/onnx/importer.py,sha256=g1yqaVTdjJDxOO9zGW6vMlZv8-l9hL5wYO1e9Ynw2EA,191470
+nnabla/utils/converter/onnx/utils.py,sha256=dWskSgf1l0Gx-vvrTyIyXMZg0mcicaupanPQVLGDNHg,1013
 nnabla/utils/converter/tensorflow/__init__.py,sha256=yDXWHyFidC1mxzibIf89ABvYHp-GpFgWzeOphxQYwiQ,715
 nnabla/utils/converter/tensorflow/common.py,sha256=FfVPTalGEtVID_B6IvXpcQ9s2vCvp34h9RFaF-aEy2o,4248
-nnabla/utils/converter/tensorflow/exporter.py,sha256=lM6Vh-OOeRxmccQ-4jOSpVovd3i6ybm_p3F4VpbDzVw,3605
+nnabla/utils/converter/tensorflow/exporter.py,sha256=39l5qB8DAurRqTWKTvsUlKPQ-7CvcHQq9PkHOvhV7Fo,3673
 nnabla/utils/converter/tensorflow/importer.py,sha256=jiV_b-mzIl_uOv6UgRJnndHAzi1ki0U9fGdvdsjC6mQ,4464
 nnabla/utils/converter/tensorflow/parsetab.py,sha256=nKYztfq0lP_SwpBqXKOcRwIubS7qdDk45OrwMgndnQE,30997
 nnabla/utils/converter/tensorflow/refine_graph.py,sha256=Wk_sRevOxYLUBl2HhKePN1YmiggBPQUDgzivCwzovSU,19583
 nnabla/utils/converter/tensorflow/refine_parser.py,sha256=BWLIO4PJhXSH0XZ1UIvrTAvgGZTXtZ6WA4Qgv7jBzMc,6511
 nnabla/utils/converter/tflite/__init__.py,sha256=4a-9FcOCQiaWi3yO7gNMyQZ3yofGEyL9zBrby4UJhJw,662
-nnabla/utils/converter/tflite/exporter.py,sha256=FSBCe35QLxqguL_95vBGyHoHWC_1gvqsz1ji-okQWGI,112162
+nnabla/utils/converter/tflite/exporter.py,sha256=5ItHz_ikbn9W6luxc6vHM0qFECua9HUuDEUsu7-DiFs,112163
 nnabla/utils/converter/tflite/flatc_linux,sha256=_-dsIQyTaOti1kbuvZsX7dPm7aERNTwwVxQUlVYr67k,3298328
 nnabla/utils/converter/tflite/flatc_mac,sha256=4FAjoz6pwLiWHY-D1VpztzhLx9XVDzX1dB-Xw8Z9EKY,2913472
 nnabla/utils/converter/tflite/flatc_windows.exe,sha256=l0IoXgA5PrJwZswLbdkNLujA6XR6ooU2WO0OnEjeFcQ,2649088
 nnabla/utils/converter/tflite/importer.py,sha256=n1yp2NZ1OCGtx_atLtYvtucTuOo0HuEz4PoVS7iRFZ4,1194
 nnabla/utils/converter/tflite/quantized_converter.py,sha256=7o5XI4C1CJysvzQxzrDOn5uKeIGvDZCZfkt9giGyrhw,28477
 nnabla/utils/converter/tflite/schema.fbs,sha256=dEhbfjc36089INSK50iL8g2kdAmudYU4ZqkzA_h0hAM,26803
-nnabla_converter-1.35.1.dist-info/METADATA,sha256=364k3kscCuZnlCL3q8Nerub6ePKjiIahD2s1gGuETkM,2266
-nnabla_converter-1.35.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-nnabla_converter-1.35.1.dist-info/top_level.txt,sha256=03bBFAHlMadm8sHe234lCJWB549cFZHokyY7f0003p8,7
-nnabla_converter-1.35.1.dist-info/RECORD,,
+nnabla_converter-1.36.0.dist-info/METADATA,sha256=Qzh-2Xldx_81iNEEoy8NQlIXq65qtB6uBBz8zHU-Dhg,2300
+nnabla_converter-1.36.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+nnabla_converter-1.36.0.dist-info/top_level.txt,sha256=03bBFAHlMadm8sHe234lCJWB549cFZHokyY7f0003p8,7
+nnabla_converter-1.36.0.dist-info/RECORD,,
```

