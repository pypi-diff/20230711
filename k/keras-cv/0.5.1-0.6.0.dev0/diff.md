# Comparing `tmp/keras_cv-0.5.1-py3-none-any.whl.zip` & `tmp/keras_cv-0.6.0.dev0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,416 +1,423 @@
-Zip file size: 742026 bytes, number of entries: 414
--rw-r--r--  2.0 unx     1182 b- defN 23-Jul-07 12:41 keras_cv/__init__.py
--rw-r--r--  2.0 unx     2354 b- defN 23-Jul-07 12:41 keras_cv/conftest.py
--rw-r--r--  2.0 unx     1159 b- defN 23-Jul-07 12:41 keras_cv/version_check.py
--rw-r--r--  2.0 unx     1362 b- defN 23-Jul-07 12:41 keras_cv/version_check_test.py
--rw-r--r--  2.0 unx     1662 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/__init__.py
--rw-r--r--  2.0 unx    18483 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/converters.py
--rw-r--r--  2.0 unx     7134 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/converters_test.py
--rw-r--r--  2.0 unx      909 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/ensure_tensor.py
--rw-r--r--  2.0 unx     1443 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/ensure_tensor_test.py
--rw-r--r--  2.0 unx     4035 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/formats.py
--rw-r--r--  2.0 unx     9144 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/iou.py
--rw-r--r--  2.0 unx     6130 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/iou_test.py
--rw-r--r--  2.0 unx     3751 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/mask_invalid_detections.py
--rw-r--r--  2.0 unx     3901 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/mask_invalid_detections_test.py
--rw-r--r--  2.0 unx     3204 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_dense.py
--rw-r--r--  2.0 unx     1147 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_dense_test.py
--rw-r--r--  2.0 unx     3014 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_ragged.py
--rw-r--r--  2.0 unx     2713 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_ragged_test.py
--rw-r--r--  2.0 unx     7178 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/utils.py
--rw-r--r--  2.0 unx     5569 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/utils_test.py
--rw-r--r--  2.0 unx     3479 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/validate_format.py
--rw-r--r--  2.0 unx     1581 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/validate_format_test.py
--rw-r--r--  2.0 unx      652 b- defN 23-Jul-07 12:41 keras_cv/bounding_box_3d/__init__.py
--rw-r--r--  2.0 unx     1609 b- defN 23-Jul-07 12:41 keras_cv/bounding_box_3d/formats.py
--rw-r--r--  2.0 unx      727 b- defN 23-Jul-07 12:41 keras_cv/callbacks/__init__.py
--rw-r--r--  2.0 unx     4693 b- defN 23-Jul-07 12:41 keras_cv/callbacks/pycoco_callback.py
--rw-r--r--  2.0 unx     3323 b- defN 23-Jul-07 12:41 keras_cv/callbacks/pycoco_callback_test.py
--rw-r--r--  2.0 unx     6910 b- defN 23-Jul-07 12:41 keras_cv/callbacks/waymo_evaluation_callback.py
--rw-r--r--  2.0 unx     3413 b- defN 23-Jul-07 12:41 keras_cv/callbacks/waymo_evaluation_callback_test.py
--rw-r--r--  2.0 unx      936 b- defN 23-Jul-07 12:41 keras_cv/core/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/__init__.py
--rw-r--r--  2.0 unx     1667 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/constant_factor_sampler.py
--rw-r--r--  2.0 unx      964 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
--rw-r--r--  2.0 unx     1343 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/factor_sampler.py
--rw-r--r--  2.0 unx     2501 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/normal_factor_sampler.py
--rw-r--r--  2.0 unx     1120 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
--rw-r--r--  2.0 unx     2183 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/uniform_factor_sampler.py
--rw-r--r--  2.0 unx     1026 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/custom_ops/__init__.py
--rw-r--r--  2.0 unx      625 b- defN 23-Jul-07 12:41 keras_cv/datasets/__init__.py
--rw-r--r--  2.0 unx      633 b- defN 23-Jul-07 12:41 keras_cv/datasets/imagenet/__init__.py
--rw-r--r--  2.0 unx     4439 b- defN 23-Jul-07 12:41 keras_cv/datasets/imagenet/load.py
--rw-r--r--  2.0 unx      635 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/__init__.py
--rw-r--r--  2.0 unx     3642 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/load.py
--rw-r--r--  2.0 unx    18789 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/segmentation.py
--rw-r--r--  2.0 unx    12179 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/segmentation_test.py
--rw-r--r--  2.0 unx     1103 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/__init__.py
--rw-r--r--  2.0 unx     2935 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/load.py
--rw-r--r--  2.0 unx     2114 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/load_test.py
--rw-r--r--  2.0 unx     1980 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/struct.py
--rw-r--r--  2.0 unx    27253 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/transformer.py
--rw-r--r--  2.0 unx     6947 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/transformer_test.py
--rw-r--r--  2.0 unx      783 b- defN 23-Jul-07 12:41 keras_cv/keypoint/__init__.py
--rw-r--r--  2.0 unx     6955 b- defN 23-Jul-07 12:41 keras_cv/keypoint/converters.py
--rw-r--r--  2.0 unx     5181 b- defN 23-Jul-07 12:41 keras_cv/keypoint/converters_test.py
--rw-r--r--  2.0 unx     1725 b- defN 23-Jul-07 12:41 keras_cv/keypoint/formats.py
--rw-r--r--  2.0 unx     1597 b- defN 23-Jul-07 12:41 keras_cv/keypoint/utils.py
--rw-r--r--  2.0 unx     2000 b- defN 23-Jul-07 12:41 keras_cv/keypoint/utils_test.py
--rw-r--r--  2.0 unx     6103 b- defN 23-Jul-07 12:41 keras_cv/layers/__init__.py
--rw-r--r--  2.0 unx     8828 b- defN 23-Jul-07 12:41 keras_cv/layers/feature_pyramid.py
--rw-r--r--  2.0 unx     5125 b- defN 23-Jul-07 12:41 keras_cv/layers/feature_pyramid_test.py
--rw-r--r--  2.0 unx     8076 b- defN 23-Jul-07 12:41 keras_cv/layers/fusedmbconv.py
--rw-r--r--  2.0 unx     2273 b- defN 23-Jul-07 12:41 keras_cv/layers/fusedmbconv_test.py
--rw-r--r--  2.0 unx     8349 b- defN 23-Jul-07 12:41 keras_cv/layers/mbconv.py
--rw-r--r--  2.0 unx     2216 b- defN 23-Jul-07 12:41 keras_cv/layers/mbconv_test.py
--rw-r--r--  2.0 unx    12061 b- defN 23-Jul-07 12:41 keras_cv/layers/serialization_test.py
--rw-r--r--  2.0 unx     6281 b- defN 23-Jul-07 12:41 keras_cv/layers/spatial_pyramid.py
--rw-r--r--  2.0 unx     1290 b- defN 23-Jul-07 12:41 keras_cv/layers/spatial_pyramid_test.py
--rw-r--r--  2.0 unx     5249 b- defN 23-Jul-07 12:41 keras_cv/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     2135 b- defN 23-Jul-07 12:41 keras_cv/layers/transformer_encoder_test.py
--rw-r--r--  2.0 unx     7723 b- defN 23-Jul-07 12:41 keras_cv/layers/vit_layers.py
--rw-r--r--  2.0 unx     2886 b- defN 23-Jul-07 12:41 keras_cv/layers/vit_layers_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/__init__.py
--rw-r--r--  2.0 unx    11406 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/anchor_generator.py
--rw-r--r--  2.0 unx     6422 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/anchor_generator_test.py
--rw-r--r--  2.0 unx    11483 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/box_matcher.py
--rw-r--r--  2.0 unx     4939 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/box_matcher_test.py
--rw-r--r--  2.0 unx     5140 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
--rw-r--r--  2.0 unx     1610 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
--rw-r--r--  2.0 unx    16185 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_align.py
--rw-r--r--  2.0 unx     9833 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_generator.py
--rw-r--r--  2.0 unx     9256 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_generator_test.py
--rw-r--r--  2.0 unx     6427 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_pool.py
--rw-r--r--  2.0 unx     9712 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_pool_test.py
--rw-r--r--  2.0 unx     9064 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_sampler.py
--rw-r--r--  2.0 unx    11643 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_sampler_test.py
--rw-r--r--  2.0 unx     9272 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/rpn_label_encoder.py
--rw-r--r--  2.0 unx     5631 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/rpn_label_encoder_test.py
--rw-r--r--  2.0 unx     3426 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/sampling.py
--rw-r--r--  2.0 unx     7277 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/sampling_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx    16098 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/centernet_label_encoder.py
--rw-r--r--  2.0 unx     4743 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
--rw-r--r--  2.0 unx     8107 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/heatmap_decoder.py
--rw-r--r--  2.0 unx    10029 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxel_utils.py
--rw-r--r--  2.0 unx     2841 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxel_utils_test.py
--rw-r--r--  2.0 unx     9159 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxelization.py
--rw-r--r--  2.0 unx     4114 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxelization_test.py
--rw-r--r--  2.0 unx     3914 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/__init__.py
--rw-r--r--  2.0 unx    12754 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/aug_mix.py
--rw-r--r--  2.0 unx     2580 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/aug_mix_test.py
--rw-r--r--  2.0 unx     3509 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/auto_contrast.py
--rw-r--r--  2.0 unx     3331 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/auto_contrast_test.py
--rw-r--r--  2.0 unx    20504 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
--rw-r--r--  2.0 unx    10953 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
--rw-r--r--  2.0 unx     4559 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/channel_shuffle.py
--rw-r--r--  2.0 unx     4065 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/channel_shuffle_test.py
--rw-r--r--  2.0 unx     5992 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/cut_mix.py
--rw-r--r--  2.0 unx     5040 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/cut_mix_test.py
--rw-r--r--  2.0 unx     4929 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/equalization.py
--rw-r--r--  2.0 unx     2446 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/equalization_test.py
--rw-r--r--  2.0 unx     8025 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/fourier_mix.py
--rw-r--r--  2.0 unx     3447 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/fourier_mix_test.py
--rw-r--r--  2.0 unx     3841 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grayscale.py
--rw-r--r--  2.0 unx     2820 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grayscale_test.py
--rw-r--r--  2.0 unx     9733 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grid_mask.py
--rw-r--r--  2.0 unx     3823 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grid_mask_test.py
--rw-r--r--  2.0 unx    11349 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/jittered_resize.py
--rw-r--r--  2.0 unx     7360 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/jittered_resize_test.py
--rw-r--r--  2.0 unx     7461 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mix_up.py
--rw-r--r--  2.0 unx     6180 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mix_up_test.py
--rw-r--r--  2.0 unx    13424 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mosaic.py
--rw-r--r--  2.0 unx     3726 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mosaic_test.py
--rw-r--r--  2.0 unx     4237 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/posterization.py
--rw-r--r--  2.0 unx     3792 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/posterization_test.py
--rw-r--r--  2.0 unx     5101 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/ragged_image_test.py
--rw-r--r--  2.0 unx    10805 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rand_augment.py
--rw-r--r--  2.0 unx     3758 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rand_augment_test.py
--rw-r--r--  2.0 unx     5023 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_apply.py
--rw-r--r--  2.0 unx     4646 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_apply_test.py
--rw-r--r--  2.0 unx     4683 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_aspect_ratio.py
--rw-r--r--  2.0 unx     2277 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
--rw-r--r--  2.0 unx     4754 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
--rw-r--r--  2.0 unx     3340 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
--rw-r--r--  2.0 unx     5241 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_brightness.py
--rw-r--r--  2.0 unx     3337 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_brightness_test.py
--rw-r--r--  2.0 unx     4396 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_channel_shift.py
--rw-r--r--  2.0 unx     3964 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_channel_shift_test.py
--rw-r--r--  2.0 unx     4503 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_choice.py
--rw-r--r--  2.0 unx     3316 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_choice_test.py
--rw-r--r--  2.0 unx     3392 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_degeneration.py
--rw-r--r--  2.0 unx     2648 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_degeneration_test.py
--rw-r--r--  2.0 unx     6946 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_jitter.py
--rw-r--r--  2.0 unx     3902 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_jitter_test.py
--rw-r--r--  2.0 unx     4864 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_contrast.py
--rw-r--r--  2.0 unx     2213 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_contrast_test.py
--rw-r--r--  2.0 unx    10846 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop.py
--rw-r--r--  2.0 unx    11178 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_and_resize.py
--rw-r--r--  2.0 unx    10241 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
--rw-r--r--  2.0 unx     9856 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_test.py
--rw-r--r--  2.0 unx     7024 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_cutout.py
--rw-r--r--  2.0 unx     4978 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_cutout_test.py
--rw-r--r--  2.0 unx     9003 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_flip.py
--rw-r--r--  2.0 unx    10994 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_flip_test.py
--rw-r--r--  2.0 unx     4576 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_gaussian_blur.py
--rw-r--r--  2.0 unx     3245 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
--rw-r--r--  2.0 unx     5433 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_hue.py
--rw-r--r--  2.0 unx     4230 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_hue_test.py
--rw-r--r--  2.0 unx     3021 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_jpeg_quality.py
--rw-r--r--  2.0 unx     1984 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
--rw-r--r--  2.0 unx    12271 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_rotation.py
--rw-r--r--  2.0 unx     7395 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_rotation_test.py
--rw-r--r--  2.0 unx     4951 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_saturation.py
--rw-r--r--  2.0 unx     8248 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_saturation_test.py
--rw-r--r--  2.0 unx     5813 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_sharpness.py
--rw-r--r--  2.0 unx     2724 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_sharpness_test.py
--rw-r--r--  2.0 unx    13111 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_shear.py
--rw-r--r--  2.0 unx     9331 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_shear_test.py
--rw-r--r--  2.0 unx    11148 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_translation.py
--rw-r--r--  2.0 unx     8917 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_translation_test.py
--rw-r--r--  2.0 unx    10340 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_zoom.py
--rw-r--r--  2.0 unx     5859 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_zoom_test.py
--rw-r--r--  2.0 unx     4677 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/repeated_augmentation.py
--rw-r--r--  2.0 unx     1753 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/repeated_augmentation_test.py
--rw-r--r--  2.0 unx     2766 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rescaling.py
--rw-r--r--  2.0 unx     2123 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rescaling_test.py
--rw-r--r--  2.0 unx    15342 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/resizing.py
--rw-r--r--  2.0 unx    12313 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/resizing_test.py
--rw-r--r--  2.0 unx     6315 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/solarization.py
--rw-r--r--  2.0 unx     3152 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/solarization_test.py
--rw-r--r--  2.0 unx    19868 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
--rw-r--r--  2.0 unx    20977 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
--rw-r--r--  2.0 unx     4773 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_labels_test.py
--rw-r--r--  2.0 unx     5231 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_mixed_precision_test.py
--rw-r--r--  2.0 unx     4885 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
--rw-r--r--  2.0 unx     1904 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/__init__.py
--rw-r--r--  2.0 unx    10608 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
--rw-r--r--  2.0 unx     4789 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
--rw-r--r--  2.0 unx     3623 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/input_format_test.py
--rw-r--r--  2.0 unx      172 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/__init__.py
--rw-r--r--  2.0 unx     5860 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py
--rw-r--r--  2.0 unx     5063 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py
--rw-r--r--  2.0 unx     6556 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py
--rw-r--r--  2.0 unx     8474 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py
--rw-r--r--  2.0 unx     3403 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py
--rw-r--r--  2.0 unx     4709 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py
--rw-r--r--  2.0 unx     4046 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py
--rw-r--r--  2.0 unx     2879 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py
--rw-r--r--  2.0 unx     5604 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py
--rw-r--r--  2.0 unx     2753 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py
--rw-r--r--  2.0 unx     6678 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py
--rw-r--r--  2.0 unx     4205 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py
--rw-r--r--  2.0 unx     4321 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py
--rw-r--r--  2.0 unx     2530 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_translation_test.py
--rw-r--r--  2.0 unx    11112 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py
--rw-r--r--  2.0 unx     7069 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes_test.py
--rw-r--r--  2.0 unx    12254 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py
--rw-r--r--  2.0 unx     7952 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py
--rw-r--r--  2.0 unx     5006 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py
--rw-r--r--  2.0 unx    11932 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py
--rw-r--r--  2.0 unx     6970 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/swap_background.py
--rw-r--r--  2.0 unx    10691 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py
--rw-r--r--  2.0 unx      868 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/__init__.py
--rw-r--r--  2.0 unx     2545 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/drop_path.py
--rw-r--r--  2.0 unx     2460 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/drop_path_test.py
--rw-r--r--  2.0 unx     8831 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/dropblock_2d.py
--rw-r--r--  2.0 unx     3653 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/dropblock_2d_test.py
--rw-r--r--  2.0 unx     4906 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/squeeze_excite.py
--rw-r--r--  2.0 unx     1882 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/squeeze_excite_test.py
--rw-r--r--  2.0 unx     2738 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/stochastic_depth.py
--rw-r--r--  2.0 unx     1771 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/stochastic_depth_test.py
--rw-r--r--  2.0 unx     1036 b- defN 23-Jul-07 12:41 keras_cv/losses/__init__.py
--rw-r--r--  2.0 unx     4854 b- defN 23-Jul-07 12:41 keras_cv/losses/centernet_box_loss.py
--rw-r--r--  2.0 unx     1459 b- defN 23-Jul-07 12:41 keras_cv/losses/centernet_box_loss_test.py
--rw-r--r--  2.0 unx     3548 b- defN 23-Jul-07 12:41 keras_cv/losses/ciou_loss.py
--rw-r--r--  2.0 unx     3042 b- defN 23-Jul-07 12:41 keras_cv/losses/ciou_loss_test.py
--rw-r--r--  2.0 unx     4140 b- defN 23-Jul-07 12:41 keras_cv/losses/focal.py
--rw-r--r--  2.0 unx     2486 b- defN 23-Jul-07 12:41 keras_cv/losses/focal_test.py
--rw-r--r--  2.0 unx     7410 b- defN 23-Jul-07 12:41 keras_cv/losses/giou_loss.py
--rw-r--r--  2.0 unx     2541 b- defN 23-Jul-07 12:41 keras_cv/losses/giou_loss_test.py
--rw-r--r--  2.0 unx     4921 b- defN 23-Jul-07 12:41 keras_cv/losses/iou_loss.py
--rw-r--r--  2.0 unx     2521 b- defN 23-Jul-07 12:41 keras_cv/losses/iou_loss_test.py
--rw-r--r--  2.0 unx     4283 b- defN 23-Jul-07 12:41 keras_cv/losses/penalty_reduced_focal_loss.py
--rw-r--r--  2.0 unx     3744 b- defN 23-Jul-07 12:41 keras_cv/losses/penalty_reduced_focal_loss_test.py
--rw-r--r--  2.0 unx     2211 b- defN 23-Jul-07 12:41 keras_cv/losses/serialization_test.py
--rw-r--r--  2.0 unx     3468 b- defN 23-Jul-07 12:41 keras_cv/losses/simclr_loss.py
--rw-r--r--  2.0 unx     2145 b- defN 23-Jul-07 12:41 keras_cv/losses/simclr_loss_test.py
--rw-r--r--  2.0 unx     1885 b- defN 23-Jul-07 12:41 keras_cv/losses/smooth_l1.py
--rw-r--r--  2.0 unx     1225 b- defN 23-Jul-07 12:41 keras_cv/losses/smooth_l1_test.py
--rw-r--r--  2.0 unx      663 b- defN 23-Jul-07 12:41 keras_cv/metrics/__init__.py
--rw-r--r--  2.0 unx      719 b- defN 23-Jul-07 12:41 keras_cv/metrics/coco/__init__.py
--rw-r--r--  2.0 unx     8421 b- defN 23-Jul-07 12:41 keras_cv/metrics/coco/pycoco_wrapper.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/__init__.py
--rw-r--r--  2.0 unx    10152 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/box_coco_metrics.py
--rw-r--r--  2.0 unx     9331 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/box_coco_metrics_test.py
--rw-r--r--  2.0 unx     4472 b- defN 23-Jul-07 12:41 keras_cv/models/__init__.py
--rw-r--r--  2.0 unx     7295 b- defN 23-Jul-07 12:41 keras_cv/models/task.py
--rw-r--r--  2.0 unx     1855 b- defN 23-Jul-07 12:41 keras_cv/models/utils.py
--rw-r--r--  2.0 unx     1133 b- defN 23-Jul-07 12:41 keras_cv/models/utils_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/__init__.py
--rw-r--r--  2.0 unx     5966 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/unet.py
--rw-r--r--  2.0 unx     1693 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/unet_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/__init__.py
--rw-r--r--  2.0 unx     6535 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/backbone.py
--rw-r--r--  2.0 unx     2236 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/backbone_presets.py
--rw-r--r--  2.0 unx      821 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/test_backbone_presets.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/__init__.py
--rw-r--r--  2.0 unx    12495 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
--rw-r--r--  2.0 unx     6518 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
--rw-r--r--  2.0 unx     4028 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
--rw-r--r--  2.0 unx     5644 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
--rw-r--r--  2.0 unx    12186 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/__init__.py
--rw-r--r--  2.0 unx     4818 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_aliases.py
--rw-r--r--  2.0 unx     7928 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone.py
--rw-r--r--  2.0 unx     3952 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_presets.py
--rw-r--r--  2.0 unx     3681 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py
--rw-r--r--  2.0 unx     4853 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/__init__.py
--rw-r--r--  2.0 unx     9316 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
--rw-r--r--  2.0 unx    12987 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
--rw-r--r--  2.0 unx    19514 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
--rw-r--r--  2.0 unx     2299 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
--rw-r--r--  2.0 unx     7678 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/__init__.py
--rw-r--r--  2.0 unx     3931 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py
--rw-r--r--  2.0 unx    12382 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
--rw-r--r--  2.0 unx     6080 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
--rw-r--r--  2.0 unx     2580 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
--rw-r--r--  2.0 unx     3830 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/__init__.py
--rw-r--r--  2.0 unx     6506 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py
--rw-r--r--  2.0 unx    11614 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
--rw-r--r--  2.0 unx     5551 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
--rw-r--r--  2.0 unx     3550 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
--rw-r--r--  2.0 unx     5815 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/__init__.py
--rw-r--r--  2.0 unx     6666 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py
--rw-r--r--  2.0 unx    13017 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
--rw-r--r--  2.0 unx     5617 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
--rw-r--r--  2.0 unx     3723 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
--rw-r--r--  2.0 unx     5226 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/classification/__init__.py
--rw-r--r--  2.0 unx     4645 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier.py
--rw-r--r--  2.0 unx     8454 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier_presets.py
--rw-r--r--  2.0 unx     7220 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier_test.py
--rw-r--r--  2.0 unx     4346 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/__init__.py
--rw-r--r--  2.0 unx    14440 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convmixer.py
--rw-r--r--  2.0 unx     1835 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convmixer_test.py
--rw-r--r--  2.0 unx    20264 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convnext.py
--rw-r--r--  2.0 unx     2456 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convnext_test.py
--rw-r--r--  2.0 unx    11050 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/darknet.py
--rw-r--r--  2.0 unx     1823 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/darknet_test.py
--rw-r--r--  2.0 unx    22320 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_lite.py
--rw-r--r--  2.0 unx     2173 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_lite_test.py
--rw-r--r--  2.0 unx    29352 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_v1.py
--rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_v1_test.py
--rw-r--r--  2.0 unx    14405 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/mlp_mixer.py
--rw-r--r--  2.0 unx     2050 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/mlp_mixer_test.py
--rw-r--r--  2.0 unx     6602 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/models_test.py
--rw-r--r--  2.0 unx    45767 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnet.py
--rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnetx_test.py
--rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnety_test.py
--rw-r--r--  2.0 unx     3662 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/utils.py
--rw-r--r--  2.0 unx     2320 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/utils_test.py
--rw-r--r--  2.0 unx     8144 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg16.py
--rw-r--r--  2.0 unx     1779 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg16_test.py
--rw-r--r--  2.0 unx     7103 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg19.py
--rw-r--r--  2.0 unx     1892 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg19_test.py
--rw-r--r--  2.0 unx    26159 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vit.py
--rw-r--r--  2.0 unx     2410 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vit_test.py
--rw-r--r--  2.0 unx     8708 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/weights.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py
--rw-r--r--  2.0 unx    23865 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py
--rw-r--r--  2.0 unx     3914 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/__init__.py
--rw-r--r--  2.0 unx    12412 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/deeplab.py
--rw-r--r--  2.0 unx     5686 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/deeplab_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__init__.py
--rw-r--r--  2.0 unx     4329 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__internal__.py
--rw-r--r--  2.0 unx     1904 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__test_utils__.py
--rw-r--r--  2.0 unx     3694 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/predict_utils.py
--rw-r--r--  2.0 unx      898 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/__init__.py
--rw-r--r--  2.0 unx     2510 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/feature_pyramid.py
--rw-r--r--  2.0 unx     2841 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/prediction_head.py
--rw-r--r--  2.0 unx    24260 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet.py
--rw-r--r--  2.0 unx     9759 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
--rw-r--r--  2.0 unx     4594 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
--rw-r--r--  2.0 unx     1672 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_presets.py
--rw-r--r--  2.0 unx    11212 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_test.py
--rw-r--r--  2.0 unx      687 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/__init__.py
--rw-r--r--  2.0 unx     7027 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
--rw-r--r--  2.0 unx     6577 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
--rw-r--r--  2.0 unx    23732 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
--rw-r--r--  2.0 unx     1598 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
--rw-r--r--  2.0 unx     8675 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
--rw-r--r--  2.0 unx    16358 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
--rw-r--r--  2.0 unx     2884 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/__init__.py
--rw-r--r--  2.0 unx     3419 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/binary_crossentropy.py
--rw-r--r--  2.0 unx      954 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/__init__.py
--rw-r--r--  2.0 unx     6313 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
--rw-r--r--  2.0 unx     5423 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_head.py
--rw-r--r--  2.0 unx     1859 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
--rw-r--r--  2.0 unx     1923 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
--rw-r--r--  2.0 unx     2986 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
--rw-r--r--  2.0 unx     5198 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
--rw-r--r--  2.0 unx     1806 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx    10038 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/center_pillar.py
--rw-r--r--  2.0 unx     5577 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/center_pillar_test.py
--rw-r--r--  2.0 unx     1324 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__init__.py
--rw-r--r--  2.0 unx     7025 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/clip_tokenizer.py
--rw-r--r--  2.0 unx    17410 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/constants.py
--rw-r--r--  2.0 unx     2690 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/decoder.py
--rw-r--r--  2.0 unx    13271 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/diffusion_model.py
--rw-r--r--  2.0 unx     2757 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/image_encoder.py
--rw-r--r--  2.0 unx     7771 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/noise_scheduler.py
--rw-r--r--  2.0 unx    19440 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/stable_diffusion.py
--rw-r--r--  2.0 unx     2414 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/stable_diffusion_test.py
--rw-r--r--  2.0 unx     6680 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/text_encoder.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
--rw-r--r--  2.0 unx     1948 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
--rw-r--r--  2.0 unx     1005 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
--rw-r--r--  2.0 unx     1560 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
--rw-r--r--  2.0 unx      624 b- defN 23-Jul-07 12:41 keras_cv/ops/__init__.py
--rw-r--r--  2.0 unx     1589 b- defN 23-Jul-07 12:41 keras_cv/ops/iou_3d.py
--rw-r--r--  2.0 unx     2243 b- defN 23-Jul-07 12:41 keras_cv/ops/iou_3d_test.py
--rw-r--r--  2.0 unx     1522 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/__init__.py
--rw-r--r--  2.0 unx    18327 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/point_cloud.py
--rw-r--r--  2.0 unx    14038 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/point_cloud_test.py
--rw-r--r--  2.0 unx     7774 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/within_box_3d_test.py
--rw-r--r--  2.0 unx      810 b- defN 23-Jul-07 12:41 keras_cv/training/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/__init__.py
--rw-r--r--  2.0 unx     9871 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/contrastive_trainer.py
--rw-r--r--  2.0 unx     6332 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/contrastive_trainer_test.py
--rw-r--r--  2.0 unx     3199 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/simclr_trainer.py
--rw-r--r--  2.0 unx     1779 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/simclr_trainer_test.py
--rw-r--r--  2.0 unx     1408 b- defN 23-Jul-07 12:41 keras_cv/utils/__init__.py
--rw-r--r--  2.0 unx     2080 b- defN 23-Jul-07 12:41 keras_cv/utils/conditional_imports.py
--rw-r--r--  2.0 unx     2474 b- defN 23-Jul-07 12:41 keras_cv/utils/conv_utils.py
--rw-r--r--  2.0 unx     3105 b- defN 23-Jul-07 12:41 keras_cv/utils/fill_utils.py
--rw-r--r--  2.0 unx    11265 b- defN 23-Jul-07 12:41 keras_cv/utils/fill_utils_test.py
--rw-r--r--  2.0 unx    14465 b- defN 23-Jul-07 12:41 keras_cv/utils/preprocessing.py
--rw-r--r--  2.0 unx     2303 b- defN 23-Jul-07 12:41 keras_cv/utils/preprocessing_test.py
--rw-r--r--  2.0 unx     1803 b- defN 23-Jul-07 12:41 keras_cv/utils/python_utils.py
--rw-r--r--  2.0 unx     2843 b- defN 23-Jul-07 12:41 keras_cv/utils/resource_loader.py
--rw-r--r--  2.0 unx     4731 b- defN 23-Jul-07 12:41 keras_cv/utils/target_gather.py
--rw-r--r--  2.0 unx     5346 b- defN 23-Jul-07 12:41 keras_cv/utils/target_gather_test.py
--rw-r--r--  2.0 unx     3628 b- defN 23-Jul-07 12:41 keras_cv/utils/test_utils.py
--rw-r--r--  2.0 unx      991 b- defN 23-Jul-07 12:41 keras_cv/utils/to_numpy.py
--rw-r--r--  2.0 unx     2813 b- defN 23-Jul-07 12:41 keras_cv/utils/train.py
--rw-r--r--  2.0 unx      860 b- defN 23-Jul-07 12:41 keras_cv/visualization/__init__.py
--rw-r--r--  2.0 unx     5497 b- defN 23-Jul-07 12:41 keras_cv/visualization/draw_bounding_boxes.py
--rw-r--r--  2.0 unx     6120 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_bounding_box_gallery.py
--rw-r--r--  2.0 unx     5899 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_image_gallery.py
--rw-r--r--  2.0 unx     4718 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_segmentation_mask_gallery.py
--rw-r--r--  2.0 unx    11853 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    10791 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    43358 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/RECORD
-414 files, 2398381 bytes uncompressed, 670946 bytes compressed:  72.0%
+Zip file size: 756408 bytes, number of entries: 421
+-rw-r--r--  2.0 unx     1373 b- defN 23-Jul-11 01:24 keras_cv/__init__.py
+-rw-r--r--  2.0 unx     2740 b- defN 23-Jul-11 01:24 keras_cv/conftest.py
+-rw-r--r--  2.0 unx     1159 b- defN 23-Jul-11 01:24 keras_cv/version_check.py
+-rw-r--r--  2.0 unx     1363 b- defN 23-Jul-11 01:24 keras_cv/version_check_test.py
+-rw-r--r--  2.0 unx     2921 b- defN 23-Jul-11 01:24 keras_cv/backend/__init__.py
+-rw-r--r--  2.0 unx     2206 b- defN 23-Jul-11 01:24 keras_cv/backend/config.py
+-rw-r--r--  2.0 unx      953 b- defN 23-Jul-11 01:24 keras_cv/backend/ops.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-Jul-11 01:24 keras_cv/backend/scope.py
+-rw-r--r--  2.0 unx     1595 b- defN 23-Jul-11 01:24 keras_cv/backend/tf_ops.py
+-rw-r--r--  2.0 unx     1662 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/__init__.py
+-rw-r--r--  2.0 unx    18316 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/converters.py
+-rw-r--r--  2.0 unx     7244 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/converters_test.py
+-rw-r--r--  2.0 unx      909 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/ensure_tensor.py
+-rw-r--r--  2.0 unx     1356 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/ensure_tensor_test.py
+-rw-r--r--  2.0 unx     4035 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/formats.py
+-rw-r--r--  2.0 unx     9063 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/iou.py
+-rw-r--r--  2.0 unx     5692 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/iou_test.py
+-rw-r--r--  2.0 unx     3728 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/mask_invalid_detections.py
+-rw-r--r--  2.0 unx     3265 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/mask_invalid_detections_test.py
+-rw-r--r--  2.0 unx     3256 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/to_dense.py
+-rw-r--r--  2.0 unx     1192 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/to_dense_test.py
+-rw-r--r--  2.0 unx     3347 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/to_ragged.py
+-rw-r--r--  2.0 unx     3422 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/to_ragged_test.py
+-rw-r--r--  2.0 unx     6947 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/utils.py
+-rw-r--r--  2.0 unx     5326 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/utils_test.py
+-rw-r--r--  2.0 unx     3479 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/validate_format.py
+-rw-r--r--  2.0 unx     1581 b- defN 23-Jul-11 01:24 keras_cv/bounding_box/validate_format_test.py
+-rw-r--r--  2.0 unx      652 b- defN 23-Jul-11 01:24 keras_cv/bounding_box_3d/__init__.py
+-rw-r--r--  2.0 unx     1609 b- defN 23-Jul-11 01:24 keras_cv/bounding_box_3d/formats.py
+-rw-r--r--  2.0 unx      727 b- defN 23-Jul-11 01:24 keras_cv/callbacks/__init__.py
+-rw-r--r--  2.0 unx     5322 b- defN 23-Jul-11 01:24 keras_cv/callbacks/pycoco_callback.py
+-rw-r--r--  2.0 unx     3326 b- defN 23-Jul-11 01:24 keras_cv/callbacks/pycoco_callback_test.py
+-rw-r--r--  2.0 unx     6910 b- defN 23-Jul-11 01:24 keras_cv/callbacks/waymo_evaluation_callback.py
+-rw-r--r--  2.0 unx     3413 b- defN 23-Jul-11 01:24 keras_cv/callbacks/waymo_evaluation_callback_test.py
+-rw-r--r--  2.0 unx      936 b- defN 23-Jul-11 01:24 keras_cv/core/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/__init__.py
+-rw-r--r--  2.0 unx     1667 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/constant_factor_sampler.py
+-rw-r--r--  2.0 unx      964 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
+-rw-r--r--  2.0 unx     1343 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/factor_sampler.py
+-rw-r--r--  2.0 unx     2501 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/normal_factor_sampler.py
+-rw-r--r--  2.0 unx     1120 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
+-rw-r--r--  2.0 unx     2183 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/uniform_factor_sampler.py
+-rw-r--r--  2.0 unx     1026 b- defN 23-Jul-11 01:24 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/custom_ops/__init__.py
+-rw-r--r--  2.0 unx      625 b- defN 23-Jul-11 01:24 keras_cv/datasets/__init__.py
+-rw-r--r--  2.0 unx      633 b- defN 23-Jul-11 01:24 keras_cv/datasets/imagenet/__init__.py
+-rw-r--r--  2.0 unx     4439 b- defN 23-Jul-11 01:24 keras_cv/datasets/imagenet/load.py
+-rw-r--r--  2.0 unx      635 b- defN 23-Jul-11 01:24 keras_cv/datasets/pascal_voc/__init__.py
+-rw-r--r--  2.0 unx     3642 b- defN 23-Jul-11 01:24 keras_cv/datasets/pascal_voc/load.py
+-rw-r--r--  2.0 unx    18789 b- defN 23-Jul-11 01:24 keras_cv/datasets/pascal_voc/segmentation.py
+-rw-r--r--  2.0 unx    12179 b- defN 23-Jul-11 01:24 keras_cv/datasets/pascal_voc/segmentation_test.py
+-rw-r--r--  2.0 unx     1103 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/__init__.py
+-rw-r--r--  2.0 unx     2935 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/load.py
+-rw-r--r--  2.0 unx     2114 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/load_test.py
+-rw-r--r--  2.0 unx     1980 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/struct.py
+-rw-r--r--  2.0 unx    27253 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/transformer.py
+-rw-r--r--  2.0 unx     6947 b- defN 23-Jul-11 01:24 keras_cv/datasets/waymo/transformer_test.py
+-rw-r--r--  2.0 unx      783 b- defN 23-Jul-11 01:24 keras_cv/keypoint/__init__.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-Jul-11 01:24 keras_cv/keypoint/converters.py
+-rw-r--r--  2.0 unx     5181 b- defN 23-Jul-11 01:24 keras_cv/keypoint/converters_test.py
+-rw-r--r--  2.0 unx     1725 b- defN 23-Jul-11 01:24 keras_cv/keypoint/formats.py
+-rw-r--r--  2.0 unx     1597 b- defN 23-Jul-11 01:24 keras_cv/keypoint/utils.py
+-rw-r--r--  2.0 unx     2000 b- defN 23-Jul-11 01:24 keras_cv/keypoint/utils_test.py
+-rw-r--r--  2.0 unx     6195 b- defN 23-Jul-11 01:24 keras_cv/layers/__init__.py
+-rw-r--r--  2.0 unx     8828 b- defN 23-Jul-11 01:24 keras_cv/layers/feature_pyramid.py
+-rw-r--r--  2.0 unx     5125 b- defN 23-Jul-11 01:24 keras_cv/layers/feature_pyramid_test.py
+-rw-r--r--  2.0 unx     8171 b- defN 23-Jul-11 01:24 keras_cv/layers/fusedmbconv.py
+-rw-r--r--  2.0 unx     2273 b- defN 23-Jul-11 01:24 keras_cv/layers/fusedmbconv_test.py
+-rw-r--r--  2.0 unx     8450 b- defN 23-Jul-11 01:24 keras_cv/layers/mbconv.py
+-rw-r--r--  2.0 unx     2216 b- defN 23-Jul-11 01:24 keras_cv/layers/mbconv_test.py
+-rw-r--r--  2.0 unx    12061 b- defN 23-Jul-11 01:24 keras_cv/layers/serialization_test.py
+-rw-r--r--  2.0 unx     6281 b- defN 23-Jul-11 01:24 keras_cv/layers/spatial_pyramid.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-Jul-11 01:24 keras_cv/layers/spatial_pyramid_test.py
+-rw-r--r--  2.0 unx     5249 b- defN 23-Jul-11 01:24 keras_cv/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     2135 b- defN 23-Jul-11 01:24 keras_cv/layers/transformer_encoder_test.py
+-rw-r--r--  2.0 unx     7723 b- defN 23-Jul-11 01:24 keras_cv/layers/vit_layers.py
+-rw-r--r--  2.0 unx     2886 b- defN 23-Jul-11 01:24 keras_cv/layers/vit_layers_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/__init__.py
+-rw-r--r--  2.0 unx    11281 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/anchor_generator.py
+-rw-r--r--  2.0 unx     6487 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/anchor_generator_test.py
+-rw-r--r--  2.0 unx    12057 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/box_matcher.py
+-rw-r--r--  2.0 unx     4709 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/box_matcher_test.py
+-rw-r--r--  2.0 unx     5445 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
+-rw-r--r--  2.0 unx     1651 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
+-rw-r--r--  2.0 unx    21573 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/non_max_suppression.py
+-rw-r--r--  2.0 unx     2371 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/non_max_suppression_test.py
+-rw-r--r--  2.0 unx    16285 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_align.py
+-rw-r--r--  2.0 unx     9934 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_generator.py
+-rw-r--r--  2.0 unx     9297 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_generator_test.py
+-rw-r--r--  2.0 unx     6525 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_pool.py
+-rw-r--r--  2.0 unx     9753 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_pool_test.py
+-rw-r--r--  2.0 unx     9164 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_sampler.py
+-rw-r--r--  2.0 unx    11684 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/roi_sampler_test.py
+-rw-r--r--  2.0 unx     9377 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/rpn_label_encoder.py
+-rw-r--r--  2.0 unx     5672 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/rpn_label_encoder_test.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/sampling.py
+-rw-r--r--  2.0 unx     7318 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection/sampling_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    16098 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/centernet_label_encoder.py
+-rw-r--r--  2.0 unx     4743 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
+-rw-r--r--  2.0 unx     8107 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/heatmap_decoder.py
+-rw-r--r--  2.0 unx    10029 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/voxel_utils.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/voxel_utils_test.py
+-rw-r--r--  2.0 unx     9159 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/voxelization.py
+-rw-r--r--  2.0 unx     4114 b- defN 23-Jul-11 01:24 keras_cv/layers/object_detection_3d/voxelization_test.py
+-rw-r--r--  2.0 unx     3914 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/__init__.py
+-rw-r--r--  2.0 unx    12761 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/aug_mix.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/aug_mix_test.py
+-rw-r--r--  2.0 unx     3509 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/auto_contrast.py
+-rw-r--r--  2.0 unx     3331 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/auto_contrast_test.py
+-rw-r--r--  2.0 unx    21145 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    10074 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     4559 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/channel_shuffle.py
+-rw-r--r--  2.0 unx     4065 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/channel_shuffle_test.py
+-rw-r--r--  2.0 unx     6095 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/cut_mix.py
+-rw-r--r--  2.0 unx     5040 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/cut_mix_test.py
+-rw-r--r--  2.0 unx     4936 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/equalization.py
+-rw-r--r--  2.0 unx     2486 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/equalization_test.py
+-rw-r--r--  2.0 unx     8032 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/fourier_mix.py
+-rw-r--r--  2.0 unx     3447 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/fourier_mix_test.py
+-rw-r--r--  2.0 unx     3848 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/grayscale.py
+-rw-r--r--  2.0 unx     2820 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/grayscale_test.py
+-rw-r--r--  2.0 unx     9710 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/grid_mask.py
+-rw-r--r--  2.0 unx     3823 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/grid_mask_test.py
+-rw-r--r--  2.0 unx    11356 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/jittered_resize.py
+-rw-r--r--  2.0 unx     7360 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/jittered_resize_test.py
+-rw-r--r--  2.0 unx     7536 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/mix_up.py
+-rw-r--r--  2.0 unx     6180 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/mix_up_test.py
+-rw-r--r--  2.0 unx    13431 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/mosaic.py
+-rw-r--r--  2.0 unx     3382 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/mosaic_test.py
+-rw-r--r--  2.0 unx     4244 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/posterization.py
+-rw-r--r--  2.0 unx     3792 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/posterization_test.py
+-rw-r--r--  2.0 unx     5101 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/ragged_image_test.py
+-rw-r--r--  2.0 unx    10811 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/rand_augment.py
+-rw-r--r--  2.0 unx     3758 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/rand_augment_test.py
+-rw-r--r--  2.0 unx     5055 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_apply.py
+-rw-r--r--  2.0 unx     4652 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_apply_test.py
+-rw-r--r--  2.0 unx     4690 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_aspect_ratio.py
+-rw-r--r--  2.0 unx     2277 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
+-rw-r--r--  2.0 unx     4761 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
+-rw-r--r--  2.0 unx     3391 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
+-rw-r--r--  2.0 unx     5248 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_brightness.py
+-rw-r--r--  2.0 unx     3337 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_brightness_test.py
+-rw-r--r--  2.0 unx     4403 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_channel_shift.py
+-rw-r--r--  2.0 unx     3964 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_channel_shift_test.py
+-rw-r--r--  2.0 unx     4510 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_choice.py
+-rw-r--r--  2.0 unx     3367 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_choice_test.py
+-rw-r--r--  2.0 unx     3399 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_color_degeneration.py
+-rw-r--r--  2.0 unx     2648 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_color_degeneration_test.py
+-rw-r--r--  2.0 unx     6952 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_color_jitter.py
+-rw-r--r--  2.0 unx     3902 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_color_jitter_test.py
+-rw-r--r--  2.0 unx     4871 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_contrast.py
+-rw-r--r--  2.0 unx     2213 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_contrast_test.py
+-rw-r--r--  2.0 unx    10853 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_crop.py
+-rw-r--r--  2.0 unx    11185 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_crop_and_resize.py
+-rw-r--r--  2.0 unx    10276 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
+-rw-r--r--  2.0 unx     9814 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_crop_test.py
+-rw-r--r--  2.0 unx     7031 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_cutout.py
+-rw-r--r--  2.0 unx     4978 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_cutout_test.py
+-rw-r--r--  2.0 unx     9010 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_flip.py
+-rw-r--r--  2.0 unx    10976 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_flip_test.py
+-rw-r--r--  2.0 unx     4583 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_gaussian_blur.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
+-rw-r--r--  2.0 unx     5440 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_hue.py
+-rw-r--r--  2.0 unx     4230 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_hue_test.py
+-rw-r--r--  2.0 unx     3028 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_jpeg_quality.py
+-rw-r--r--  2.0 unx     1984 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
+-rw-r--r--  2.0 unx    12297 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_rotation.py
+-rw-r--r--  2.0 unx     7251 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_rotation_test.py
+-rw-r--r--  2.0 unx     4958 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_saturation.py
+-rw-r--r--  2.0 unx     8254 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_saturation_test.py
+-rw-r--r--  2.0 unx     5820 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_sharpness.py
+-rw-r--r--  2.0 unx     2724 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_sharpness_test.py
+-rw-r--r--  2.0 unx    13118 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_shear.py
+-rw-r--r--  2.0 unx     9331 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_shear_test.py
+-rw-r--r--  2.0 unx    11155 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_translation.py
+-rw-r--r--  2.0 unx     8917 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_translation_test.py
+-rw-r--r--  2.0 unx    10347 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_zoom.py
+-rw-r--r--  2.0 unx     5859 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/random_zoom_test.py
+-rw-r--r--  2.0 unx     4684 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/repeated_augmentation.py
+-rw-r--r--  2.0 unx     1753 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/repeated_augmentation_test.py
+-rw-r--r--  2.0 unx     2773 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/rescaling.py
+-rw-r--r--  2.0 unx     2123 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/rescaling_test.py
+-rw-r--r--  2.0 unx    15324 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/resizing.py
+-rw-r--r--  2.0 unx    12420 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/resizing_test.py
+-rw-r--r--  2.0 unx     6322 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/solarization.py
+-rw-r--r--  2.0 unx     3152 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/solarization_test.py
+-rw-r--r--  2.0 unx    21021 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    20739 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     4773 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/with_labels_test.py
+-rw-r--r--  2.0 unx     5237 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/with_mixed_precision_test.py
+-rw-r--r--  2.0 unx     4852 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
+-rw-r--r--  2.0 unx     1904 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/__init__.py
+-rw-r--r--  2.0 unx    10608 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
+-rw-r--r--  2.0 unx     4789 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
+-rw-r--r--  2.0 unx     3623 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/input_format_test.py
+-rw-r--r--  2.0 unx      172 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/__init__.py
+-rw-r--r--  2.0 unx     5860 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py
+-rw-r--r--  2.0 unx     5063 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     6556 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py
+-rw-r--r--  2.0 unx     8474 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py
+-rw-r--r--  2.0 unx     3403 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py
+-rw-r--r--  2.0 unx     4709 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     4046 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py
+-rw-r--r--  2.0 unx     2879 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py
+-rw-r--r--  2.0 unx     5604 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py
+-rw-r--r--  2.0 unx     2753 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py
+-rw-r--r--  2.0 unx     6678 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py
+-rw-r--r--  2.0 unx     4205 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py
+-rw-r--r--  2.0 unx     4321 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py
+-rw-r--r--  2.0 unx     2530 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/global_random_translation_test.py
+-rw-r--r--  2.0 unx    11112 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py
+-rw-r--r--  2.0 unx     7069 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes_test.py
+-rw-r--r--  2.0 unx    12254 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py
+-rw-r--r--  2.0 unx     7952 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py
+-rw-r--r--  2.0 unx     5006 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py
+-rw-r--r--  2.0 unx    11932 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py
+-rw-r--r--  2.0 unx     6970 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/swap_background.py
+-rw-r--r--  2.0 unx    10691 b- defN 23-Jul-11 01:24 keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py
+-rw-r--r--  2.0 unx      868 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/__init__.py
+-rw-r--r--  2.0 unx     2545 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/drop_path.py
+-rw-r--r--  2.0 unx     2460 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/drop_path_test.py
+-rw-r--r--  2.0 unx     8831 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/dropblock_2d.py
+-rw-r--r--  2.0 unx     3653 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/dropblock_2d_test.py
+-rw-r--r--  2.0 unx     4960 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/squeeze_excite.py
+-rw-r--r--  2.0 unx     1882 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/squeeze_excite_test.py
+-rw-r--r--  2.0 unx     2738 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/stochastic_depth.py
+-rw-r--r--  2.0 unx     1771 b- defN 23-Jul-11 01:24 keras_cv/layers/regularization/stochastic_depth_test.py
+-rw-r--r--  2.0 unx     1036 b- defN 23-Jul-11 01:24 keras_cv/losses/__init__.py
+-rw-r--r--  2.0 unx     4953 b- defN 23-Jul-11 01:24 keras_cv/losses/centernet_box_loss.py
+-rw-r--r--  2.0 unx     1488 b- defN 23-Jul-11 01:24 keras_cv/losses/centernet_box_loss_test.py
+-rw-r--r--  2.0 unx     3498 b- defN 23-Jul-11 01:24 keras_cv/losses/ciou_loss.py
+-rw-r--r--  2.0 unx     2719 b- defN 23-Jul-11 01:24 keras_cv/losses/ciou_loss_test.py
+-rw-r--r--  2.0 unx     4250 b- defN 23-Jul-11 01:24 keras_cv/losses/focal.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-Jul-11 01:24 keras_cv/losses/focal_test.py
+-rw-r--r--  2.0 unx     7286 b- defN 23-Jul-11 01:24 keras_cv/losses/giou_loss.py
+-rw-r--r--  2.0 unx     2541 b- defN 23-Jul-11 01:24 keras_cv/losses/giou_loss_test.py
+-rw-r--r--  2.0 unx     4802 b- defN 23-Jul-11 01:24 keras_cv/losses/iou_loss.py
+-rw-r--r--  2.0 unx     2364 b- defN 23-Jul-11 01:24 keras_cv/losses/iou_loss_test.py
+-rw-r--r--  2.0 unx     4263 b- defN 23-Jul-11 01:24 keras_cv/losses/penalty_reduced_focal_loss.py
+-rw-r--r--  2.0 unx     3460 b- defN 23-Jul-11 01:24 keras_cv/losses/penalty_reduced_focal_loss_test.py
+-rw-r--r--  2.0 unx     2211 b- defN 23-Jul-11 01:24 keras_cv/losses/serialization_test.py
+-rw-r--r--  2.0 unx     3714 b- defN 23-Jul-11 01:24 keras_cv/losses/simclr_loss.py
+-rw-r--r--  2.0 unx     2076 b- defN 23-Jul-11 01:24 keras_cv/losses/simclr_loss_test.py
+-rw-r--r--  2.0 unx     1892 b- defN 23-Jul-11 01:24 keras_cv/losses/smooth_l1.py
+-rw-r--r--  2.0 unx     1254 b- defN 23-Jul-11 01:24 keras_cv/losses/smooth_l1_test.py
+-rw-r--r--  2.0 unx      663 b- defN 23-Jul-11 01:24 keras_cv/metrics/__init__.py
+-rw-r--r--  2.0 unx      719 b- defN 23-Jul-11 01:24 keras_cv/metrics/coco/__init__.py
+-rw-r--r--  2.0 unx     8149 b- defN 23-Jul-11 01:24 keras_cv/metrics/coco/pycoco_wrapper.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/metrics/object_detection/__init__.py
+-rw-r--r--  2.0 unx    10344 b- defN 23-Jul-11 01:24 keras_cv/metrics/object_detection/box_coco_metrics.py
+-rw-r--r--  2.0 unx     9331 b- defN 23-Jul-11 01:24 keras_cv/metrics/object_detection/box_coco_metrics_test.py
+-rw-r--r--  2.0 unx     4472 b- defN 23-Jul-11 01:24 keras_cv/models/__init__.py
+-rw-r--r--  2.0 unx     7536 b- defN 23-Jul-11 01:24 keras_cv/models/task.py
+-rw-r--r--  2.0 unx     2020 b- defN 23-Jul-11 01:24 keras_cv/models/utils.py
+-rw-r--r--  2.0 unx     1133 b- defN 23-Jul-11 01:24 keras_cv/models/utils_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/__internal__/__init__.py
+-rw-r--r--  2.0 unx     5966 b- defN 23-Jul-11 01:24 keras_cv/models/__internal__/unet.py
+-rw-r--r--  2.0 unx     1693 b- defN 23-Jul-11 01:24 keras_cv/models/__internal__/unet_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/__init__.py
+-rw-r--r--  2.0 unx     6540 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/backbone.py
+-rw-r--r--  2.0 unx     2236 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/backbone_presets.py
+-rw-r--r--  2.0 unx      821 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/test_backbone_presets.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/__init__.py
+-rw-r--r--  2.0 unx    12526 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
+-rw-r--r--  2.0 unx     6518 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
+-rw-r--r--  2.0 unx     4120 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5576 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
+-rw-r--r--  2.0 unx    12164 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/__init__.py
+-rw-r--r--  2.0 unx     4818 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/densenet_aliases.py
+-rw-r--r--  2.0 unx     8042 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/densenet_backbone.py
+-rw-r--r--  2.0 unx     3952 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/densenet_backbone_presets.py
+-rw-r--r--  2.0 unx     3792 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py
+-rw-r--r--  2.0 unx     4764 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/densenet/densenet_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/__init__.py
+-rw-r--r--  2.0 unx     9316 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
+-rw-r--r--  2.0 unx    13053 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
+-rw-r--r--  2.0 unx    19514 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     2350 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     7619 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/__init__.py
+-rw-r--r--  2.0 unx     3931 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py
+-rw-r--r--  2.0 unx    12411 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
+-rw-r--r--  2.0 unx     6080 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
+-rw-r--r--  2.0 unx     2676 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
+-rw-r--r--  2.0 unx     3825 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/__init__.py
+-rw-r--r--  2.0 unx     6506 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py
+-rw-r--r--  2.0 unx    11772 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
+-rw-r--r--  2.0 unx     5551 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
+-rw-r--r--  2.0 unx     3661 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5750 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/__init__.py
+-rw-r--r--  2.0 unx     6666 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py
+-rw-r--r--  2.0 unx    13161 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
+-rw-r--r--  2.0 unx     5617 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     3827 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5155 b- defN 23-Jul-11 01:24 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/classification/__init__.py
+-rw-r--r--  2.0 unx     4644 b- defN 23-Jul-11 01:24 keras_cv/models/classification/image_classifier.py
+-rw-r--r--  2.0 unx     8454 b- defN 23-Jul-11 01:24 keras_cv/models/classification/image_classifier_presets.py
+-rw-r--r--  2.0 unx     7401 b- defN 23-Jul-11 01:24 keras_cv/models/classification/image_classifier_test.py
+-rw-r--r--  2.0 unx     4346 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/__init__.py
+-rw-r--r--  2.0 unx    14440 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/convmixer.py
+-rw-r--r--  2.0 unx     1835 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/convmixer_test.py
+-rw-r--r--  2.0 unx    20264 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/convnext.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/convnext_test.py
+-rw-r--r--  2.0 unx    11050 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/darknet.py
+-rw-r--r--  2.0 unx     1823 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/darknet_test.py
+-rw-r--r--  2.0 unx    22320 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/efficientnet_lite.py
+-rw-r--r--  2.0 unx     2173 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/efficientnet_lite_test.py
+-rw-r--r--  2.0 unx    29352 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/efficientnet_v1.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/efficientnet_v1_test.py
+-rw-r--r--  2.0 unx    14405 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/mlp_mixer.py
+-rw-r--r--  2.0 unx     2050 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/mlp_mixer_test.py
+-rw-r--r--  2.0 unx     6602 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/models_test.py
+-rw-r--r--  2.0 unx    45767 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/regnet.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/regnetx_test.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/regnety_test.py
+-rw-r--r--  2.0 unx     3662 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/utils.py
+-rw-r--r--  2.0 unx     2320 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/utils_test.py
+-rw-r--r--  2.0 unx     8144 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vgg16.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vgg16_test.py
+-rw-r--r--  2.0 unx     7103 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vgg19.py
+-rw-r--r--  2.0 unx     1892 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vgg19_test.py
+-rw-r--r--  2.0 unx    26159 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vit.py
+-rw-r--r--  2.0 unx     2410 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/vit_test.py
+-rw-r--r--  2.0 unx     8708 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/weights.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/object_detection/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py
+-rw-r--r--  2.0 unx    23834 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py
+-rw-r--r--  2.0 unx     3914 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/segmentation/__init__.py
+-rw-r--r--  2.0 unx    12412 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/segmentation/deeplab.py
+-rw-r--r--  2.0 unx     5686 b- defN 23-Jul-11 01:24 keras_cv/models/legacy/segmentation/deeplab_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/__init__.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/__internal__.py
+-rw-r--r--  2.0 unx     1925 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/__test_utils__.py
+-rw-r--r--  2.0 unx     3694 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/predict_utils.py
+-rw-r--r--  2.0 unx      898 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/__init__.py
+-rw-r--r--  2.0 unx     3600 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/feature_pyramid.py
+-rw-r--r--  2.0 unx     3231 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/prediction_head.py
+-rw-r--r--  2.0 unx    23601 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/retinanet.py
+-rw-r--r--  2.0 unx    10055 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
+-rw-r--r--  2.0 unx     4683 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
+-rw-r--r--  2.0 unx     1672 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/retinanet_presets.py
+-rw-r--r--  2.0 unx    10810 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/retinanet/retinanet_test.py
+-rw-r--r--  2.0 unx      687 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/__init__.py
+-rw-r--r--  2.0 unx     7111 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
+-rw-r--r--  2.0 unx     6577 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
+-rw-r--r--  2.0 unx    23491 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
+-rw-r--r--  2.0 unx     1598 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
+-rw-r--r--  2.0 unx     9404 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
+-rw-r--r--  2.0 unx    16408 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
+-rw-r--r--  2.0 unx     2934 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/__init__.py
+-rw-r--r--  2.0 unx     3419 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/binary_crossentropy.py
+-rw-r--r--  2.0 unx      954 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/__init__.py
+-rw-r--r--  2.0 unx     6313 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
+-rw-r--r--  2.0 unx     5423 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_head.py
+-rw-r--r--  2.0 unx     1859 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
+-rw-r--r--  2.0 unx     1923 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
+-rw-r--r--  2.0 unx     2986 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
+-rw-r--r--  2.0 unx     5198 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
+-rw-r--r--  2.0 unx     1806 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    10038 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection_3d/center_pillar.py
+-rw-r--r--  2.0 unx     5577 b- defN 23-Jul-11 01:24 keras_cv/models/object_detection_3d/center_pillar_test.py
+-rw-r--r--  2.0 unx     1324 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__init__.py
+-rw-r--r--  2.0 unx     7025 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/clip_tokenizer.py
+-rw-r--r--  2.0 unx    17410 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/constants.py
+-rw-r--r--  2.0 unx     2690 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/decoder.py
+-rw-r--r--  2.0 unx    13271 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/diffusion_model.py
+-rw-r--r--  2.0 unx     2757 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/image_encoder.py
+-rw-r--r--  2.0 unx     7771 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/noise_scheduler.py
+-rw-r--r--  2.0 unx    19440 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/stable_diffusion.py
+-rw-r--r--  2.0 unx     2414 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/stable_diffusion_test.py
+-rw-r--r--  2.0 unx     6680 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/text_encoder.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__internal__/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
+-rw-r--r--  2.0 unx     1948 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
+-rw-r--r--  2.0 unx     1005 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-Jul-11 01:24 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
+-rw-r--r--  2.0 unx      624 b- defN 23-Jul-11 01:24 keras_cv/ops/__init__.py
+-rw-r--r--  2.0 unx     1589 b- defN 23-Jul-11 01:24 keras_cv/ops/iou_3d.py
+-rw-r--r--  2.0 unx     2243 b- defN 23-Jul-11 01:24 keras_cv/ops/iou_3d_test.py
+-rw-r--r--  2.0 unx     1522 b- defN 23-Jul-11 01:24 keras_cv/point_cloud/__init__.py
+-rw-r--r--  2.0 unx    18327 b- defN 23-Jul-11 01:24 keras_cv/point_cloud/point_cloud.py
+-rw-r--r--  2.0 unx    14038 b- defN 23-Jul-11 01:24 keras_cv/point_cloud/point_cloud_test.py
+-rw-r--r--  2.0 unx     7774 b- defN 23-Jul-11 01:24 keras_cv/point_cloud/within_box_3d_test.py
+-rw-r--r--  2.0 unx      810 b- defN 23-Jul-11 01:24 keras_cv/training/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-11 01:24 keras_cv/training/contrastive/__init__.py
+-rw-r--r--  2.0 unx     9871 b- defN 23-Jul-11 01:24 keras_cv/training/contrastive/contrastive_trainer.py
+-rw-r--r--  2.0 unx     6332 b- defN 23-Jul-11 01:24 keras_cv/training/contrastive/contrastive_trainer_test.py
+-rw-r--r--  2.0 unx     3199 b- defN 23-Jul-11 01:24 keras_cv/training/contrastive/simclr_trainer.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-Jul-11 01:24 keras_cv/training/contrastive/simclr_trainer_test.py
+-rw-r--r--  2.0 unx     1408 b- defN 23-Jul-11 01:24 keras_cv/utils/__init__.py
+-rw-r--r--  2.0 unx     2080 b- defN 23-Jul-11 01:24 keras_cv/utils/conditional_imports.py
+-rw-r--r--  2.0 unx     2474 b- defN 23-Jul-11 01:24 keras_cv/utils/conv_utils.py
+-rw-r--r--  2.0 unx     3105 b- defN 23-Jul-11 01:24 keras_cv/utils/fill_utils.py
+-rw-r--r--  2.0 unx    11265 b- defN 23-Jul-11 01:24 keras_cv/utils/fill_utils_test.py
+-rw-r--r--  2.0 unx    14456 b- defN 23-Jul-11 01:24 keras_cv/utils/preprocessing.py
+-rw-r--r--  2.0 unx     2303 b- defN 23-Jul-11 01:24 keras_cv/utils/preprocessing_test.py
+-rw-r--r--  2.0 unx     1803 b- defN 23-Jul-11 01:24 keras_cv/utils/python_utils.py
+-rw-r--r--  2.0 unx     2843 b- defN 23-Jul-11 01:24 keras_cv/utils/resource_loader.py
+-rw-r--r--  2.0 unx     4379 b- defN 23-Jul-11 01:24 keras_cv/utils/target_gather.py
+-rw-r--r--  2.0 unx     5491 b- defN 23-Jul-11 01:24 keras_cv/utils/target_gather_test.py
+-rw-r--r--  2.0 unx     3628 b- defN 23-Jul-11 01:24 keras_cv/utils/test_utils.py
+-rw-r--r--  2.0 unx      926 b- defN 23-Jul-11 01:24 keras_cv/utils/to_numpy.py
+-rw-r--r--  2.0 unx     2820 b- defN 23-Jul-11 01:24 keras_cv/utils/train.py
+-rw-r--r--  2.0 unx      860 b- defN 23-Jul-11 01:24 keras_cv/visualization/__init__.py
+-rw-r--r--  2.0 unx     5497 b- defN 23-Jul-11 01:24 keras_cv/visualization/draw_bounding_boxes.py
+-rw-r--r--  2.0 unx     6120 b- defN 23-Jul-11 01:24 keras_cv/visualization/plot_bounding_box_gallery.py
+-rw-r--r--  2.0 unx     5899 b- defN 23-Jul-11 01:24 keras_cv/visualization/plot_image_gallery.py
+-rw-r--r--  2.0 unx     4718 b- defN 23-Jul-11 01:24 keras_cv/visualization/plot_segmentation_mask_gallery.py
+-rw-r--r--  2.0 unx    11853 b- defN 23-Jul-11 01:25 keras_cv-0.6.0.dev0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    10796 b- defN 23-Jul-11 01:25 keras_cv-0.6.0.dev0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-11 01:25 keras_cv-0.6.0.dev0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-11 01:25 keras_cv-0.6.0.dev0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    44026 b- defN 23-Jul-11 01:25 keras_cv-0.6.0.dev0.dist-info/RECORD
+421 files, 2437862 bytes uncompressed, 684260 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -6,14 +6,29 @@
 
 Filename: keras_cv/version_check.py
 Comment: 
 
 Filename: keras_cv/version_check_test.py
 Comment: 
 
+Filename: keras_cv/backend/__init__.py
+Comment: 
+
+Filename: keras_cv/backend/config.py
+Comment: 
+
+Filename: keras_cv/backend/ops.py
+Comment: 
+
+Filename: keras_cv/backend/scope.py
+Comment: 
+
+Filename: keras_cv/backend/tf_ops.py
+Comment: 
+
 Filename: keras_cv/bounding_box/__init__.py
 Comment: 
 
 Filename: keras_cv/bounding_box/converters.py
 Comment: 
 
 Filename: keras_cv/bounding_box/converters_test.py
@@ -231,14 +246,20 @@
 
 Filename: keras_cv/layers/object_detection/multi_class_non_max_suppression.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
 Comment: 
 
+Filename: keras_cv/layers/object_detection/non_max_suppression.py
+Comment: 
+
+Filename: keras_cv/layers/object_detection/non_max_suppression_test.py
+Comment: 
+
 Filename: keras_cv/layers/object_detection/roi_align.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/roi_generator.py
 Comment: 
 
 Filename: keras_cv/layers/object_detection/roi_generator_test.py
@@ -1221,23 +1242,23 @@
 
 Filename: keras_cv/visualization/plot_image_gallery.py
 Comment: 
 
 Filename: keras_cv/visualization/plot_segmentation_mask_gallery.py
 Comment: 
 
-Filename: keras_cv-0.5.1.dist-info/LICENSE
+Filename: keras_cv-0.6.0.dev0.dist-info/LICENSE
 Comment: 
 
-Filename: keras_cv-0.5.1.dist-info/METADATA
+Filename: keras_cv-0.6.0.dev0.dist-info/METADATA
 Comment: 
 
-Filename: keras_cv-0.5.1.dist-info/WHEEL
+Filename: keras_cv-0.6.0.dev0.dist-info/WHEEL
 Comment: 
 
-Filename: keras_cv-0.5.1.dist-info/top_level.txt
+Filename: keras_cv-0.6.0.dev0.dist-info/top_level.txt
 Comment: 
 
-Filename: keras_cv-0.5.1.dist-info/RECORD
+Filename: keras_cv-0.6.0.dev0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## keras_cv/__init__.py

```diff
@@ -8,14 +8,23 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+try:
+    # When using torch and tensorflow, torch needs to be imported first,
+    # otherwise it will segfault upon import.
+    import torch
+
+    del torch
+except ImportError:
+    pass
+
 # isort:off
 from keras_cv import version_check
 
 version_check.check_tf_version()
 # isort:on
 
 from keras_cv import bounding_box
@@ -29,8 +38,8 @@
 from keras_cv import utils
 from keras_cv import visualization
 from keras_cv.core import ConstantFactorSampler
 from keras_cv.core import FactorSampler
 from keras_cv.core import NormalFactorSampler
 from keras_cv.core import UniformFactorSampler
 
-__version__ = "0.5.1"
+__version__ = "0.6.0.dev0"
```

## keras_cv/conftest.py

```diff
@@ -12,14 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import pytest
 import tensorflow as tf
 from packaging import version
 
+from keras_cv.backend.config import multi_backend
+
 
 def pytest_addoption(parser):
     parser.addoption(
         "--run_large",
         action="store_true",
         default=False,
         help="run large tests",
@@ -36,14 +38,18 @@
     config.addinivalue_line(
         "markers", "large: mark test as being slow or requiring a network"
     )
     config.addinivalue_line(
         "markers",
         "extra_large: mark test as being too large to run continuously",
     )
+    config.addinivalue_line(
+        "markers",
+        "tf_keras_only: mark test as a tf only test",
+    )
 
 
 def pytest_collection_modifyitems(config, items):
     run_extra_large_tests = config.getoption("--run_extra_large")
     # Run large tests for --run_extra_large or --run_large.
     run_large_tests = config.getoption("--run_large") or run_extra_large_tests
 
@@ -54,16 +60,22 @@
     )
     skip_large = pytest.mark.skipif(
         not run_large_tests, reason="need --run_large option to run"
     )
     skip_extra_large = pytest.mark.skipif(
         not run_extra_large_tests, reason="need --run_extra_large option to run"
     )
+    skip_tf_keras_only = pytest.mark.skipif(
+        multi_backend(),
+        reason="This test is only supported on tf.keras",
+    )
     for item in items:
         if "keras_format" in item.name:
             item.add_marker(skip_keras_saving_test)
         if "tf_format" in item.name:
             item.add_marker(skip_extra_large)
         if "large" in item.keywords:
             item.add_marker(skip_large)
         if "extra_large" in item.keywords:
             item.add_marker(skip_extra_large)
+        if "tf_keras_only" in item.keywords:
+            item.add_marker(skip_tf_keras_only)
```

## keras_cv/version_check.py

```diff
@@ -14,15 +14,15 @@
 
 """KerasCV Version check."""
 
 
 import tensorflow as tf
 from packaging.version import parse
 
-MIN_VERSION = "2.11.0"
+MIN_VERSION = "2.13.0"
 
 
 def check_tf_version():
     if parse(tf.__version__) < parse(MIN_VERSION):
         raise RuntimeError(
             "The Tensorflow package version needs to be at least "
             f"{MIN_VERSION} for KerasCV to run. Currently, your TensorFlow "
```

## keras_cv/version_check_test.py

```diff
@@ -25,26 +25,26 @@
     yield
 
     # Cleanup
     tf.__version__ = actual_tf_version
 
 
 def test_check_tf_version_error():
-    tf.__version__ = "2.8.0"
+    tf.__version__ = "2.12.0"
 
     with pytest.raises(
         RuntimeError,
-        match="Tensorflow package version needs to be at least 2.11.0",
+        match="Tensorflow package version needs to be at least 2.13.0",
     ):
         version_check.check_tf_version()
 
 
 def test_check_tf_version_passes_rc2():
     # should pass
-    tf.__version__ = "2.11.1rc2"
+    tf.__version__ = "2.13.1rc2"
     version_check.check_tf_version()
 
 
 def test_check_tf_version_passes_nightly():
     # should pass
-    tf.__version__ = "2.12.0-dev20230119"
+    tf.__version__ = "2.14.0-dev20230119"
     version_check.check_tf_version()
```

## keras_cv/bounding_box/converters.py

```diff
@@ -9,106 +9,103 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Converter functions for working with bounding box formats."""
 
-from typing import List
-from typing import Optional
-from typing import Union
 
 import tensorflow as tf
-from tensorflow import keras
+
+from keras_cv.backend import keras
+from keras_cv.backend import ops
+from keras_cv.backend.scope import tf_data
 
 
 # Internal exception to propagate the fact images was not passed to a converter
 # that needs it.
 class RequiresImagesException(Exception):
     pass
 
 
-ALL_AXES = [1, 1, 1, 1]
+ALL_AXES = 4
 
 
 def _encode_box_to_deltas(
-    anchors: tf.Tensor,
-    boxes: tf.Tensor,
+    anchors,
+    boxes,
     anchor_format: str,
     box_format: str,
-    variance: Optional[Union[List[float], tf.Tensor]] = None,
+    variance=None,
     image_shape=None,
 ):
     """Converts bounding_boxes from `center_yxhw` to delta format."""
     if variance is not None:
-        if tf.is_tensor(variance):
-            var_len = variance.get_shape().as_list()[-1]
-        else:
-            var_len = len(variance)
+        variance = ops.convert_to_tensor(variance, "float32")
+        var_len = variance.shape[-1]
+
         if var_len != 4:
             raise ValueError(f"`variance` must be length 4, got {variance}")
     encoded_anchors = convert_format(
         anchors,
         source=anchor_format,
         target="center_yxhw",
         image_shape=image_shape,
     )
     boxes = convert_format(
         boxes, source=box_format, target="center_yxhw", image_shape=image_shape
     )
-    anchor_dimensions = tf.maximum(
+    anchor_dimensions = ops.maximum(
         encoded_anchors[..., 2:], keras.backend.epsilon()
     )
-    box_dimensions = tf.maximum(boxes[..., 2:], keras.backend.epsilon())
+    box_dimensions = ops.maximum(boxes[..., 2:], keras.backend.epsilon())
     # anchors be unbatched, boxes can either be batched or unbatched.
-    boxes_delta = tf.concat(
+    boxes_delta = ops.concatenate(
         [
             (boxes[..., :2] - encoded_anchors[..., :2]) / anchor_dimensions,
-            tf.math.log(box_dimensions / anchor_dimensions),
+            ops.log(box_dimensions / anchor_dimensions),
         ],
         axis=-1,
     )
     if variance is not None:
         boxes_delta /= variance
     return boxes_delta
 
 
 def _decode_deltas_to_boxes(
-    anchors: tf.Tensor,
-    boxes_delta: tf.Tensor,
+    anchors,
+    boxes_delta,
     anchor_format: str,
     box_format: str,
-    variance: Optional[Union[List[float], tf.Tensor]] = None,
+    variance=None,
     image_shape=None,
 ):
     """Converts bounding_boxes from delta format to `center_yxhw`."""
     if variance is not None:
-        if tf.is_tensor(variance):
-            var_len = variance.get_shape().as_list()[-1]
-        else:
-            var_len = len(variance)
+        variance = ops.convert_to_tensor(variance, "float32")
+        var_len = variance.shape[-1]
+
         if var_len != 4:
             raise ValueError(f"`variance` must be length 4, got {variance}")
-    tf.nest.assert_same_structure(anchors, boxes_delta)
 
     def decode_single_level(anchor, box_delta):
         encoded_anchor = convert_format(
             anchor,
             source=anchor_format,
             target="center_yxhw",
             image_shape=image_shape,
         )
         if variance is not None:
             box_delta = box_delta * variance
         # anchors be unbatched, boxes can either be batched or unbatched.
-        box = tf.concat(
+        box = ops.concatenate(
             [
                 box_delta[..., :2] * encoded_anchor[..., 2:]
                 + encoded_anchor[..., :2],
-                tf.math.exp(box_delta[..., 2:]) * encoded_anchor[..., 2:],
+                ops.exp(box_delta[..., 2:]) * encoded_anchor[..., 2:],
             ],
             axis=-1,
         )
         box = convert_format(
             box,
             source="center_yxhw",
             target=box_format,
@@ -122,51 +119,51 @@
             boxes[lvl] = decode_single_level(anchor, boxes_delta[lvl])
         return boxes
     else:
         return decode_single_level(anchors, boxes_delta)
 
 
 def _center_yxhw_to_xyxy(boxes, images=None, image_shape=None):
-    y, x, height, width = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    y, x, height, width = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [x - width / 2.0, y - height / 2.0, x + width / 2.0, y + height / 2.0],
         axis=-1,
     )
 
 
 def _center_xywh_to_xyxy(boxes, images=None, image_shape=None):
-    x, y, width, height = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [x - width / 2.0, y - height / 2.0, x + width / 2.0, y + height / 2.0],
         axis=-1,
     )
 
 
 def _xywh_to_xyxy(boxes, images=None, image_shape=None):
-    x, y, width, height = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat([x, y, x + width, y + height], axis=-1)
+    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate([x, y, x + width, y + height], axis=-1)
 
 
 def _xyxy_to_center_yxhw(boxes, images=None, image_shape=None):
-    left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [
             (top + bottom) / 2.0,
             (left + right) / 2.0,
             bottom - top,
             right - left,
         ],
         axis=-1,
     )
 
 
 def _rel_xywh_to_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    x, y, width, height = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [
             image_width * x,
             image_height * y,
             image_width * (x + width),
             image_height * (y + height),
         ],
         axis=-1,
@@ -174,109 +171,109 @@
 
 
 def _xyxy_no_op(boxes, images=None, image_shape=None):
     return boxes
 
 
 def _xyxy_to_xywh(boxes, images=None, image_shape=None):
-    left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [left, top, right - left, bottom - top],
         axis=-1,
     )
 
 
 def _xyxy_to_rel_xywh(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
+    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)
     left, right = (
         left / image_width,
         right / image_width,
     )
     top, bottom = top / image_height, bottom / image_height
-    return tf.concat(
+    return ops.concatenate(
         [left, top, right - left, bottom - top],
         axis=-1,
     )
 
 
 def _xyxy_to_center_xywh(boxes, images=None, image_shape=None):
-    left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat(
+    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate(
         [
             (left + right) / 2.0,
             (top + bottom) / 2.0,
             right - left,
             bottom - top,
         ],
         axis=-1,
     )
 
 
 def _rel_xyxy_to_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    left, top, right, bottom = tf.split(
+    left, top, right, bottom = ops.split(
         boxes,
         ALL_AXES,
         axis=-1,
     )
     left, right = left * image_width, right * image_width
     top, bottom = top * image_height, bottom * image_height
-    return tf.concat(
+    return ops.concatenate(
         [left, top, right, bottom],
         axis=-1,
     )
 
 
 def _xyxy_to_rel_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    left, top, right, bottom = tf.split(
+    left, top, right, bottom = ops.split(
         boxes,
         ALL_AXES,
         axis=-1,
     )
     left, right = left / image_width, right / image_width
     top, bottom = top / image_height, bottom / image_height
-    return tf.concat(
+    return ops.concatenate(
         [left, top, right, bottom],
         axis=-1,
     )
 
 
 def _yxyx_to_xyxy(boxes, images=None, image_shape=None):
-    y1, x1, y2, x2 = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat([x1, y1, x2, y2], axis=-1)
+    y1, x1, y2, x2 = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate([x1, y1, x2, y2], axis=-1)
 
 
 def _rel_yxyx_to_xyxy(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    top, left, bottom, right = tf.split(
+    top, left, bottom, right = ops.split(
         boxes,
         ALL_AXES,
         axis=-1,
     )
     left, right = left * image_width, right * image_width
     top, bottom = top * image_height, bottom * image_height
-    return tf.concat(
+    return ops.concatenate(
         [left, top, right, bottom],
         axis=-1,
     )
 
 
 def _xyxy_to_yxyx(boxes, images=None, image_shape=None):
-    x1, y1, x2, y2 = tf.split(boxes, ALL_AXES, axis=-1)
-    return tf.concat([y1, x1, y2, x2], axis=-1)
+    x1, y1, x2, y2 = ops.split(boxes, ALL_AXES, axis=-1)
+    return ops.concatenate([y1, x1, y2, x2], axis=-1)
 
 
 def _xyxy_to_rel_yxyx(boxes, images=None, image_shape=None):
     image_height, image_width = _image_shape(images, image_shape, boxes)
-    left, top, right, bottom = tf.split(boxes, ALL_AXES, axis=-1)
+    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)
     left, right = left / image_width, right / image_width
     top, bottom = top / image_height, bottom / image_height
-    return tf.concat(
+    return ops.concatenate(
         [top, left, bottom, right],
         axis=-1,
     )
 
 
 TO_XYXY_CONVERTERS = {
     "xywh": _xywh_to_xyxy,
@@ -297,14 +294,15 @@
     "xyxy": _xyxy_no_op,
     "rel_xyxy": _xyxy_to_rel_xyxy,
     "yxyx": _xyxy_to_yxyx,
     "rel_yxyx": _xyxy_to_rel_yxyx,
 }
 
 
+@tf_data
 def convert_format(
     boxes, source, target, images=None, image_shape=None, dtype="float32"
 ):
     f"""Converts bounding_boxes from one format to another.
 
     Supported formats are:
     - `"xyxy"`, also known as `corners` format. In this format the first four
@@ -346,45 +344,45 @@
         boxes,
         source='xyxy',
         target='xyWH'
     )
     ```
 
     Args:
-        boxes: tf.Tensor representing bounding boxes in the format specified in
+        boxes: tensor representing bounding boxes in the format specified in
             the `source` parameter. `boxes` can optionally have extra
             dimensions stacked on the final axis to store metadata. boxes
-            should be a 3D Tensor, with the shape `[batch_size, num_boxes, 4]`.
+            should be a 3D tensor, with the shape `[batch_size, num_boxes, 4]`.
             Alternatively, boxes can be a dictionary with key 'boxes' containing
-            a Tensor matching the aforementioned spec.
+            a tensor matching the aforementioned spec.
         source:One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.
             Used to specify the original format of the `boxes` parameter.
         target:One of {" ".join([f'"{f}"' for f in TO_XYXY_CONVERTERS.keys()])}.
             Used to specify the destination format of the `boxes` parameter.
         images: (Optional) a batch of images aligned with `boxes` on the first
             axis. Should be at least 3 dimensions, with the first 3 dimensions
             representing: `[batch_size, height, width]`. Used in some
             converters to compute relative pixel values of the bounding box
             dimensions. Required when transforming from a rel format to a
             non-rel format.
         dtype: the data type to use when transforming the boxes, defaults to
-            `tf.float32`.
+            `"float32"`.
     """
     if isinstance(boxes, dict):
         boxes["boxes"] = convert_format(
             boxes["boxes"],
             source=source,
             target=target,
             images=images,
             image_shape=image_shape,
             dtype=dtype,
         )
         return boxes
 
-    if boxes.shape[-1] != 4:
+    if boxes.shape[-1] is not None and boxes.shape[-1] != 4:
         raise ValueError(
             "Expected `boxes` to be a Tensor with a final dimension of "
             f"`4`. Instead, got `boxes.shape={boxes.shape}`."
         )
     if images is not None and image_shape is not None:
         raise ValueError(
             "convert_format() expects either `images` or `image_shape`, but "
@@ -404,15 +402,15 @@
     if target not in FROM_XYXY_CONVERTERS:
         raise ValueError(
             "`convert_format()` received an unsupported format for the "
             "argument `target`. `target` should be one of "
             f"{FROM_XYXY_CONVERTERS.keys()}. Got target={target}"
         )
 
-    boxes = tf.cast(boxes, dtype)
+    boxes = ops.cast(boxes, dtype)
     if source == target:
         return boxes
 
     # rel->rel conversions should not require images
     if source.startswith("rel") and target.startswith("rel"):
         source = source.replace("rel_", "", 1)
         target = target.replace("rel_", "", 1)
@@ -458,18 +456,18 @@
                 "or both boxes and images to be unbatched. Received "
                 f"len(boxes.shape)={boxes_rank}, "
                 f"len(images.shape)={images_rank}. Expected either "
                 "len(boxes.shape)=2 AND len(images.shape)=3, or "
                 "len(boxes.shape)=3 AND len(images.shape)=4."
             )
         if not images_include_batch:
-            images = tf.expand_dims(images, axis=0)
+            images = ops.expand_dims(images, axis=0)
 
     if not boxes_includes_batch:
-        return tf.expand_dims(boxes, axis=0), images, True
+        return ops.expand_dims(boxes, axis=0), images, True
     return boxes, images, False
 
 
 def _validate_image_shape(image_shape):
     # Escape early if image_shape is None and skip validation.
     if image_shape is None:
         return
@@ -479,15 +477,15 @@
             raise ValueError(
                 "image_shape should be of length 3, but got "
                 f"image_shape={image_shape}"
             )
         return
 
     # tensor
-    if isinstance(image_shape, tf.Tensor):
+    if ops.is_tensor(image_shape):
         if len(image_shape.shape) > 1:
             raise ValueError(
                 "image_shape.shape should be (3), but got "
                 f"image_shape.shape={image_shape.shape}"
             )
         if image_shape.shape[0] != 3:
             raise ValueError(
@@ -501,29 +499,27 @@
         "Expected image_shape to be either a tuple, list, Tensor. "
         f"Received image_shape={image_shape}"
     )
 
 
 def _format_outputs(boxes, squeeze):
     if squeeze:
-        return tf.squeeze(boxes, axis=0)
+        return ops.squeeze(boxes, axis=0)
     return boxes
 
 
 def _image_shape(images, image_shape, boxes):
     if images is None and image_shape is None:
         raise RequiresImagesException()
 
     if image_shape is None:
         if not isinstance(images, tf.RaggedTensor):
-            image_shape = tf.shape(images)
+            image_shape = ops.shape(images)
             height, width = image_shape[1], image_shape[2]
         else:
-            height = tf.reshape(images.row_lengths(), (-1, 1))
-            width = tf.reshape(
-                tf.reduce_max(images.row_lengths(axis=2), 1), (-1, 1)
-            )
-            height = tf.expand_dims(height, axis=-1)
-            width = tf.expand_dims(width, axis=-1)
+            height = ops.reshape(images.row_lengths(), (-1, 1))
+            width = ops.reshape(ops.max(images.row_lengths(axis=2), 1), (-1, 1))
+            height = ops.expand_dims(height, axis=-1)
+            width = ops.expand_dims(width, axis=-1)
     else:
         height, width = image_shape[0], image_shape[1]
-    return tf.cast(height, boxes.dtype), tf.cast(width, boxes.dtype)
+    return ops.cast(height, boxes.dtype), ops.cast(width, boxes.dtype)
```

## keras_cv/bounding_box/converters_test.py

```diff
@@ -11,58 +11,53 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import itertools
 
 import numpy as np
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import bounding_box
 
-xyxy_box = tf.constant(
-    [[[10, 20, 110, 120], [20, 30, 120, 130]]], dtype=tf.float32
+xyxy_box = np.array([[[10, 20, 110, 120], [20, 30, 120, 130]]], dtype="float32")
+yxyx_box = np.array([[[20, 10, 120, 110], [30, 20, 130, 120]]], dtype="float32")
+rel_xyxy_box = np.array(
+    [[[0.01, 0.02, 0.11, 0.12], [0.02, 0.03, 0.12, 0.13]]], dtype="float32"
 )
-yxyx_box = tf.constant(
-    [[[20, 10, 120, 110], [30, 20, 130, 120]]], dtype=tf.float32
+rel_xyxy_box_ragged_images = np.array(
+    [[[0.10, 0.20, 1.1, 1.20], [0.40, 0.6, 2.40, 2.6]]], dtype="float32"
 )
-rel_xyxy_box = tf.constant(
-    [[[0.01, 0.02, 0.11, 0.12], [0.02, 0.03, 0.12, 0.13]]], dtype=tf.float32
+rel_yxyx_box = np.array(
+    [[[0.02, 0.01, 0.12, 0.11], [0.03, 0.02, 0.13, 0.12]]], dtype="float32"
 )
-rel_xyxy_box_ragged_images = tf.constant(
-    [[[0.10, 0.20, 1.1, 1.20], [0.40, 0.6, 2.40, 2.6]]], dtype=tf.float32
+rel_yxyx_box_ragged_images = np.array(
+    [[[0.2, 0.1, 1.2, 1.1], [0.6, 0.4, 2.6, 2.4]]], dtype="float32"
 )
-rel_yxyx_box = tf.constant(
-    [[[0.02, 0.01, 0.12, 0.11], [0.03, 0.02, 0.13, 0.12]]], dtype=tf.float32
+center_xywh_box = np.array(
+    [[[60, 70, 100, 100], [70, 80, 100, 100]]], dtype="float32"
 )
-rel_yxyx_box_ragged_images = tf.constant(
-    [[[0.2, 0.1, 1.2, 1.1], [0.6, 0.4, 2.6, 2.4]]], dtype=tf.float32
+xywh_box = np.array([[[10, 20, 100, 100], [20, 30, 100, 100]]], dtype="float32")
+rel_xywh_box = np.array(
+    [[[0.01, 0.02, 0.1, 0.1], [0.02, 0.03, 0.1, 0.1]]], dtype="float32"
 )
-center_xywh_box = tf.constant(
-    [[[60, 70, 100, 100], [70, 80, 100, 100]]], dtype=tf.float32
-)
-xywh_box = tf.constant(
-    [[[10, 20, 100, 100], [20, 30, 100, 100]]], dtype=tf.float32
-)
-rel_xywh_box = tf.constant(
-    [[[0.01, 0.02, 0.1, 0.1], [0.02, 0.03, 0.1, 0.1]]], dtype=tf.float32
-)
-rel_xywh_box_ragged_images = tf.constant(
-    [[[0.1, 0.2, 1, 1], [0.4, 0.6, 2, 2]]], dtype=tf.float32
+rel_xywh_box_ragged_images = np.array(
+    [[[0.1, 0.2, 1, 1], [0.4, 0.6, 2, 2]]], dtype="float32"
 )
 
 ragged_images = tf.ragged.constant(
     [np.ones(shape=[100, 100, 3]), np.ones(shape=[50, 50, 3])],  # 2 images
     ragged_rank=2,
 )
 
-images = tf.ones([2, 1000, 1000, 3])
+images = np.ones([2, 1000, 1000, 3])
 
-ragged_classes = tf.ragged.constant([[0], [0]], dtype=tf.float32)
+ragged_classes = tf.ragged.constant([[0], [0]], dtype="float32")
 
 boxes = {
     "xyxy": xyxy_box,
     "center_xywh": center_xywh_box,
     "rel_xywh": rel_xywh_box,
     "xywh": xywh_box,
     "rel_xyxy": rel_xyxy_box,
@@ -103,14 +98,15 @@
             bounding_box.convert_format(
                 source_box, source=source, target=target, images=images
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_image_ragged)
+    @pytest.mark.tf_keras_only
     def test_converters_ragged_images(self, source, target):
         source_box = _raggify(boxes_ragged_images[source])
         target_box = _raggify(boxes_ragged_images[target])
         self.assertAllClose(
             bounding_box.convert_format(
                 source_box, source=source, target=target, images=ragged_images
             ),
@@ -153,50 +149,54 @@
             bounding_box.convert_format(
                 source_box, source="rel_xyxy", target="rel_yxyx"
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_cases)
+    @pytest.mark.tf_keras_only
     def test_ragged_bounding_box(self, source, target):
         source_box = _raggify(boxes[source])
         target_box = _raggify(boxes[target])
         self.assertAllClose(
             bounding_box.convert_format(
                 source_box, source=source, target=target, images=images
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_image_ragged)
+    @pytest.mark.tf_keras_only
     def test_ragged_bounding_box_ragged_images(self, source, target):
         source_box = _raggify(boxes_ragged_images[source])
         target_box = _raggify(boxes_ragged_images[target])
         self.assertAllClose(
             bounding_box.convert_format(
                 source_box, source=source, target=target, images=ragged_images
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_cases)
+    @pytest.mark.tf_keras_only
     def test_ragged_bounding_box_with_image_shape(self, source, target):
         source_box = _raggify(boxes[source])
         target_box = _raggify(boxes[target])
         self.assertAllClose(
             bounding_box.convert_format(
                 source_box,
                 source=source,
                 target=target,
                 image_shape=(1000, 1000, 3),
             ),
             target_box,
         )
 
     @parameterized.named_parameters(*test_image_ragged)
+    @pytest.mark.tf_keras_only
     def test_dense_bounding_box_with_ragged_images(self, source, target):
         source_box = _raggify(boxes_ragged_images[source])
         target_box = _raggify(boxes_ragged_images[target])
         source_bounding_boxes = {"boxes": source_box, "classes": ragged_classes}
         source_bounding_boxes = bounding_box.to_dense(source_bounding_boxes)
 
         result_bounding_boxes = bounding_box.convert_format(
```

## keras_cv/bounding_box/ensure_tensor_test.py

```diff
@@ -11,29 +11,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 
 
 class BoundingBoxEnsureTensorTest(tf.test.TestCase):
     def test_convert_list(self):
         boxes = {"boxes": [[0, 1, 2, 3]], "classes": [0]}
         output = bounding_box.ensure_tensor(boxes)
-        self.assertFalse(
-            any([isinstance(boxes[k], tf.Tensor) for k in boxes.keys()])
-        )
-        self.assertTrue(
-            all([isinstance(output[k], tf.Tensor) for k in output.keys()])
-        )
+        self.assertFalse(any([ops.is_tensor(boxes[k]) for k in boxes.keys()]))
+        self.assertTrue(all([ops.is_tensor(output[k]) for k in output.keys()]))
 
     def test_confidence(self):
         boxes = {"boxes": [[0, 1, 2, 3]], "classes": [0], "confidence": [0.245]}
         output = bounding_box.ensure_tensor(boxes)
-        self.assertFalse(
-            any([isinstance(boxes[k], tf.Tensor) for k in boxes.keys()])
-        )
-        self.assertTrue(
-            all([isinstance(output[k], tf.Tensor) for k in output.keys()])
-        )
+        self.assertFalse(any([ops.is_tensor(boxes[k]) for k in boxes.keys()]))
+        self.assertTrue(all([ops.is_tensor(output[k]) for k in output.keys()]))
```

## keras_cv/bounding_box/iou.py

```diff
@@ -10,62 +10,56 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Contains functions to compute ious of bounding boxes."""
 import math
 
-import tensorflow as tf
-
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
 def _compute_area(box):
     """Computes area for bounding boxes
 
     Args:
       box: [N, 4] or [batch_size, N, 4] float Tensor, either batched
         or unbatched boxes.
     Returns:
       a float Tensor of [N] or [batch_size, N]
     """
-    y_min, x_min, y_max, x_max = tf.split(
-        box[..., :4], num_or_size_splits=4, axis=-1
-    )
-    return tf.squeeze((y_max - y_min) * (x_max - x_min), axis=-1)
+    y_min, x_min, y_max, x_max = ops.split(box[..., :4], 4, axis=-1)
+    return ops.squeeze((y_max - y_min) * (x_max - x_min), axis=-1)
 
 
 def _compute_intersection(boxes1, boxes2):
     """Computes intersection area between two sets of boxes.
 
     Args:
       boxes1: [N, 4] or [batch_size, N, 4] float Tensor boxes.
       boxes2: [M, 4] or [batch_size, M, 4] float Tensor boxes.
     Returns:
       a [N, M] or [batch_size, N, M] float Tensor.
     """
-    y_min1, x_min1, y_max1, x_max1 = tf.split(
-        boxes1[..., :4], num_or_size_splits=4, axis=-1
-    )
-    y_min2, x_min2, y_max2, x_max2 = tf.split(
-        boxes2[..., :4], num_or_size_splits=4, axis=-1
-    )
+    y_min1, x_min1, y_max1, x_max1 = ops.split(boxes1[..., :4], 4, axis=-1)
+    y_min2, x_min2, y_max2, x_max2 = ops.split(boxes2[..., :4], 4, axis=-1)
     boxes2_rank = len(boxes2.shape)
     perm = [1, 0] if boxes2_rank == 2 else [0, 2, 1]
     # [N, M] or [batch_size, N, M]
-    intersect_ymax = tf.minimum(y_max1, tf.transpose(y_max2, perm))
-    intersect_ymin = tf.maximum(y_min1, tf.transpose(y_min2, perm))
-    intersect_xmax = tf.minimum(x_max1, tf.transpose(x_max2, perm))
-    intersect_xmin = tf.maximum(x_min1, tf.transpose(x_min2, perm))
+    intersect_ymax = ops.minimum(y_max1, ops.transpose(y_max2, perm))
+    intersect_ymin = ops.maximum(y_min1, ops.transpose(y_min2, perm))
+    intersect_xmax = ops.minimum(x_max1, ops.transpose(x_max2, perm))
+    intersect_xmin = ops.maximum(x_min1, ops.transpose(x_min2, perm))
 
     intersect_height = intersect_ymax - intersect_ymin
     intersect_width = intersect_xmax - intersect_xmin
-    zeros_t = tf.cast(0, intersect_height.dtype)
-    intersect_height = tf.maximum(zeros_t, intersect_height)
-    intersect_width = tf.maximum(zeros_t, intersect_width)
+    zeros_t = ops.cast(0, intersect_height.dtype)
+    intersect_height = ops.maximum(zeros_t, intersect_height)
+    intersect_width = ops.maximum(zeros_t, intersect_width)
 
     return intersect_height * intersect_width
 
 
 def compute_iou(
     boxes1,
     boxes2,
@@ -149,34 +143,34 @@
     )
 
     intersect_area = _compute_intersection(boxes1, boxes2)
     boxes1_area = _compute_area(boxes1)
     boxes2_area = _compute_area(boxes2)
     boxes2_area_rank = len(boxes2_area.shape)
     boxes2_axis = 1 if (boxes2_area_rank == 2) else 0
-    boxes1_area = tf.expand_dims(boxes1_area, axis=-1)
-    boxes2_area = tf.expand_dims(boxes2_area, axis=boxes2_axis)
+    boxes1_area = ops.expand_dims(boxes1_area, axis=-1)
+    boxes2_area = ops.expand_dims(boxes2_area, axis=boxes2_axis)
     union_area = boxes1_area + boxes2_area - intersect_area
-    res = tf.math.divide_no_nan(intersect_area, union_area)
+    res = ops.divide(intersect_area, union_area + keras.backend.epsilon())
 
     if boxes1_rank == 2:
         perm = [1, 0]
     else:
         perm = [0, 2, 1]
 
     if not use_masking:
         return res
 
-    mask_val_t = tf.cast(mask_val, res.dtype) * tf.ones_like(res)
-    boxes1_mask = tf.less(tf.reduce_max(boxes1, axis=-1, keepdims=True), 0.0)
-    boxes2_mask = tf.less(tf.reduce_max(boxes2, axis=-1, keepdims=True), 0.0)
-    background_mask = tf.logical_or(
-        boxes1_mask, tf.transpose(boxes2_mask, perm)
+    mask_val_t = ops.cast(mask_val, res.dtype) * ops.ones_like(res)
+    boxes1_mask = ops.less(ops.max(boxes1, axis=-1, keepdims=True), 0.0)
+    boxes2_mask = ops.less(ops.max(boxes2, axis=-1, keepdims=True), 0.0)
+    background_mask = ops.logical_or(
+        boxes1_mask, ops.transpose(boxes2_mask, perm)
     )
-    iou_lookup_table = tf.where(background_mask, mask_val_t, res)
+    iou_lookup_table = ops.where(background_mask, mask_val_t, res)
     return iou_lookup_table
 
 
 def compute_ciou(box1, box2, bounding_box_format, eps=1e-7):
     """
     Computes the Complete IoU (CIoU) between two bounding boxes or between
     two batches of bounding boxes.
@@ -184,65 +178,63 @@
     CIoU loss is an extension of GIoU loss, which further improves the IoU
     optimization for object detection. CIoU loss not only penalizes the
     bounding box coordinates but also considers the aspect ratio and center
     distance of the boxes. The length of the last dimension should be 4 to
     represent the bounding boxes.
 
     Args:
-        box1 (tf.Tensor): Tensor representing the first bounding box with
+        box1 (tensor): tensor representing the first bounding box with
             shape (..., 4).
-        box2 (tf.Tensor): Tensor representing the second bounding box with
+        box2 (tensor): tensor representing the second bounding box with
             shape (..., 4).
         bounding_box_format: a case-insensitive string (for example, "xyxy").
             Each bounding box is defined by these 4 values. For detailed
             information on the supported formats, see the [KerasCV bounding box
             documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
         eps (float, optional): A small value to avoid division by zero. Default
             is 1e-7.
 
     Returns:
-        tf.Tensor: The CIoU distance between the two bounding boxes.
+        tensor: The CIoU distance between the two bounding boxes.
     """
     target_format = "xyxy"
     if bounding_box.is_relative(bounding_box_format):
         target_format = bounding_box.as_relative(target_format)
 
     box1 = bounding_box.convert_format(
         box1, source=bounding_box_format, target=target_format
     )
 
     box2 = bounding_box.convert_format(
         box2, source=bounding_box_format, target=target_format
     )
-    b1_x1, b1_y1, b1_x2, b1_y2 = tf.split(box1, 4, axis=-1)
-    b2_x1, b2_y1, b2_x2, b2_y2 = tf.split(box2, 4, axis=-1)
+    b1_x1, b1_y1, b1_x2, b1_y2 = ops.split(box1, 4, axis=-1)
+    b2_x1, b2_y1, b2_x2, b2_y2 = ops.split(box2, 4, axis=-1)
     w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps
     w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps
 
     # Intersection area
-    inter = tf.math.maximum(
-        tf.math.minimum(b1_x2, b2_x2) - tf.math.maximum(b1_x1, b2_x1), 0
-    ) * tf.math.maximum(
-        tf.math.minimum(b1_y2, b2_y2) - tf.math.maximum(b1_y1, b2_y1), 0
-    )
+    inter = ops.maximum(
+        ops.minimum(b1_x2, b2_x2) - ops.maximum(b1_x1, b2_x1), 0
+    ) * ops.maximum(ops.minimum(b1_y2, b2_y2) - ops.maximum(b1_y1, b2_y1), 0)
 
     # Union Area
     union = w1 * h1 + w2 * h2 - inter + eps
 
     # IoU
     iou = inter / union
 
-    cw = tf.math.maximum(b1_x2, b2_x2) - tf.math.minimum(
+    cw = ops.maximum(b1_x2, b2_x2) - ops.minimum(
         b1_x1, b2_x1
     )  # convex (smallest enclosing box) width
-    ch = tf.math.maximum(b1_y2, b2_y2) - tf.math.minimum(
-        b1_y1, b2_y1
-    )  # convex height
+    ch = ops.maximum(b1_y2, b2_y2) - ops.minimum(b1_y1, b2_y1)  # convex height
     c2 = cw**2 + ch**2 + eps  # convex diagonal squared
     rho2 = (
         (b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2
         + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2
     ) / 4  # center dist ** 2
-    v = tf.pow((4 / math.pi**2) * (tf.atan(w2 / h2) - tf.atan(w1 / h1)), 2)
+    v = ops.power(
+        (4 / math.pi**2) * (ops.arctan(w2 / h2) - ops.arctan(w1 / h1)), 2
+    )
     alpha = v / (v - iou + (1 + eps))
 
     return iou - (rho2 / c2 + v * alpha)
```

## keras_cv/bounding_box/iou_test.py

```diff
@@ -17,16 +17,16 @@
 import tensorflow as tf
 
 from keras_cv.bounding_box import iou as iou_lib
 
 
 class IoUTest(tf.test.TestCase):
     def test_compute_single_iou(self):
-        bb1 = tf.constant([[100, 101, 200, 201]], dtype=tf.float32)
-        bb1_off_by_1 = tf.constant([[101, 102, 201, 202]], dtype=tf.float32)
+        bb1 = np.array([[100, 101, 200, 201]])
+        bb1_off_by_1 = np.array([[101, 102, 201, 202]])
         # area of bb1 and bb1_off_by_1 are each 10000.
         # intersection area is 99*99=9801
         # iou=9801/(2*10000 - 9801)=0.96097656633
         self.assertAlmostEqual(
             iou_lib.compute_iou(bb1, bb1_off_by_1, "yxyx")[0], 0.96097656633
         )
 
@@ -40,24 +40,21 @@
 
         # Rows represent predictions, columns ground truths
         expected_result = np.array(
             [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
             dtype=np.float32,
         )
 
-        sample_y_true = tf.constant(
-            [bb1, top_left_bounding_box, far_away_box], dtype=tf.float32
-        )
-        sample_y_pred = tf.constant(
+        sample_y_true = np.array([bb1, top_left_bounding_box, far_away_box])
+        sample_y_pred = np.array(
             [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
-            dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
-        self.assertAllClose(expected_result, result.numpy())
+        self.assertAllClose(expected_result, result)
 
     def test_batched_compute_iou(self):
         bb1 = [100, 101, 200, 201]
         bb1_off_by_1_pred = [101, 102, 201, 202]
         iou_bb1_bb1_off = 0.96097656633
         top_left_bounding_box = [0, 2, 1, 3]
         far_away_box = [1300, 1400, 1500, 1401]
@@ -65,42 +62,39 @@
 
         # Rows represent predictions, columns ground truths
         expected_result = np.array(
             [
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
             ],
-            dtype=np.float32,
         )
 
-        sample_y_true = tf.constant(
+        sample_y_true = np.array(
             [
                 [bb1, top_left_bounding_box, far_away_box],
                 [bb1, top_left_bounding_box, far_away_box],
             ],
-            dtype=tf.float32,
         )
-        sample_y_pred = tf.constant(
+        sample_y_pred = np.array(
             [
                 [
                     bb1_off_by_1_pred,
                     top_left_bounding_box,
                     another_far_away_pred,
                 ],
                 [
                     bb1_off_by_1_pred,
                     top_left_bounding_box,
                     another_far_away_pred,
                 ],
             ],
-            dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
-        self.assertAllClose(expected_result, result.numpy())
+        self.assertAllClose(expected_result, result)
 
     def test_batched_boxes1_unbatched_boxes2(self):
         bb1 = [100, 101, 200, 201]
         bb1_off_by_1_pred = [101, 102, 201, 202]
         iou_bb1_bb1_off = 0.96097656633
         top_left_bounding_box = [0, 2, 1, 3]
         far_away_box = [1300, 1400, 1500, 1401]
@@ -108,31 +102,28 @@
 
         # Rows represent predictions, columns ground truths
         expected_result = np.array(
             [
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
             ],
-            dtype=np.float32,
         )
 
-        sample_y_true = tf.constant(
+        sample_y_true = np.array(
             [
                 [bb1, top_left_bounding_box, far_away_box],
                 [bb1, top_left_bounding_box, far_away_box],
             ],
-            dtype=tf.float32,
         )
-        sample_y_pred = tf.constant(
+        sample_y_pred = np.array(
             [bb1_off_by_1_pred, top_left_bounding_box, another_far_away_pred],
-            dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
-        self.assertAllClose(expected_result, result.numpy())
+        self.assertAllClose(expected_result, result)
 
     def test_unbatched_boxes1_batched_boxes2(self):
         bb1 = [100, 101, 200, 201]
         bb1_off_by_1_pred = [101, 102, 201, 202]
         iou_bb1_bb1_off = 0.96097656633
         top_left_bounding_box = [0, 2, 1, 3]
         far_away_box = [1300, 1400, 1500, 1401]
@@ -140,34 +131,31 @@
 
         # Rows represent predictions, columns ground truths
         expected_result = np.array(
             [
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
                 [[iou_bb1_bb1_off, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
             ],
-            dtype=np.float32,
         )
 
-        sample_y_true = tf.constant(
+        sample_y_true = np.array(
             [
                 [bb1, top_left_bounding_box, far_away_box],
             ],
-            dtype=tf.float32,
         )
-        sample_y_pred = tf.constant(
+        sample_y_pred = np.array(
             [
                 [
                     bb1_off_by_1_pred,
                     top_left_bounding_box,
                     another_far_away_pred,
                 ],
                 [
                     bb1_off_by_1_pred,
                     top_left_bounding_box,
                     another_far_away_pred,
                 ],
             ],
-            dtype=tf.float32,
         )
 
         result = iou_lib.compute_iou(sample_y_true, sample_y_pred, "yxyx")
-        self.assertAllClose(expected_result, result.numpy())
+        self.assertAllClose(expected_result, result)
```

## keras_cv/bounding_box/mask_invalid_detections.py

```diff
@@ -7,46 +7,45 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
 
+from keras_cv import backend
+from keras_cv.backend import ops
 from keras_cv.bounding_box.to_ragged import to_ragged
 from keras_cv.bounding_box.validate_format import validate_format
 
 
 def mask_invalid_detections(bounding_boxes, output_ragged=False):
     """masks out invalid detections with -1s.
 
-    This utility is mainly used on the output of
-    `tf.image.combined_non_max_suppression` operations. The output of
-    `tf.image.combined_non_max_suppression` contains all the detections, even
-    invalid ones. Users are expected to use `num_detections` to determine how
-    many boxes are in each image.
+    This utility is mainly used on the output of non-max supression operations.
+    The output of non-max-supression contains all the detections, even invalid
+    ones. Users are expected to use `num_detections` to determine how many boxes
+    are in each image.
 
     In contrast, KerasCV expects all bounding boxes to be padded with -1s.
     This function uses the value of `num_detections` to mask out
     invalid boxes with -1s.
 
     Args:
         bounding_boxes: a dictionary complying with KerasCV bounding box format.
             In addition to the normal required keys, these boxes are also
             expected to have a `num_detections` key.
         output_ragged: whether to output RaggedTensor based bounding
             boxes.
     Returns:
         bounding boxes with proper masking of the boxes according to
-        `num_detections`. This allows proper interop with
-        `tf.image.combined_non_max_suppression`. Returned boxes match the
-        specification fed to the function, so if the bounding box tensor uses
-        `tf.RaggedTensor` to represent boxes the returned value will also return
-        `tf.RaggedTensor` representations.
+        `num_detections`. This allows proper interop with non-max supression.
+        Returned boxes match the specification fed to the function, so if the
+        bounding box tensor uses `tf.RaggedTensor` to represent boxes the
+        returned value will also return `tf.RaggedTensor` representations.
     """
     # ensure we are complying with KerasCV bounding box format.
     info = validate_format(bounding_boxes)
     if info["ragged"]:
         raise ValueError(
             "`bounding_box.mask_invalid_detections()` requires inputs to be "
             "Dense tensors. Please call "
@@ -61,35 +60,34 @@
 
     boxes = bounding_boxes.get("boxes")
     classes = bounding_boxes.get("classes")
     confidence = bounding_boxes.get("confidence", None)
     num_detections = bounding_boxes.get("num_detections")
 
     # Create a mask to select only the first N boxes from each batch
-    mask = tf.repeat(
-        tf.expand_dims(tf.range(tf.shape(boxes)[1]), axis=0),
-        repeats=tf.shape(boxes)[0],
-        axis=0,
+    mask = ops.cast(
+        ops.expand_dims(ops.arange(boxes.shape[1]), axis=0),
+        num_detections.dtype,
     )
     mask = mask < num_detections[:, None]
 
-    classes = tf.where(mask, classes, -tf.ones_like(classes))
+    classes = ops.where(mask, classes, -ops.ones_like(classes))
 
     if confidence is not None:
-        confidence = tf.where(mask, confidence, -tf.ones_like(confidence))
+        confidence = ops.where(mask, confidence, -ops.ones_like(confidence))
 
     # reuse mask for boxes
-    mask = tf.expand_dims(mask, axis=-1)
-    mask = tf.repeat(mask, repeats=boxes.shape[-1], axis=-1)
-    boxes = tf.where(mask, boxes, -tf.ones_like(boxes))
+    mask = ops.expand_dims(mask, axis=-1)
+    mask = ops.repeat(mask, repeats=boxes.shape[-1], axis=-1)
+    boxes = ops.where(mask, boxes, -ops.ones_like(boxes))
 
     result = bounding_boxes.copy()
 
     result["boxes"] = boxes
     result["classes"] = classes
     if confidence is not None:
         result["confidence"] = confidence
 
-    if output_ragged:
+    if output_ragged and backend.supports_ragged():
         return to_ragged(result)
 
     return result
```

## keras_cv/bounding_box/mask_invalid_detections_test.py

```diff
@@ -8,95 +8,80 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
+import pytest
 import tensorflow as tf
 
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 
 
 class MaskInvalidDetectionsTest(tf.test.TestCase):
     def test_correctly_masks_based_on_max_dets(self):
         bounding_boxes = {
-            "boxes": tf.random.uniform((4, 100, 4)),
-            "num_detections": tf.constant([2, 3, 4, 2]),
-            "classes": tf.random.uniform((4, 100)),
+            "boxes": ops.random.uniform((4, 100, 4)),
+            "num_detections": ops.array([2, 3, 4, 2]),
+            "classes": ops.random.uniform((4, 100)),
         }
 
         result = bounding_box.mask_invalid_detections(bounding_boxes)
 
         negative_one_boxes = result["boxes"][:, 5:, :]
         self.assertAllClose(
-            negative_one_boxes, -tf.ones_like(negative_one_boxes)
-        )
-
-        preserved_boxes = result["boxes"][:, :2, :]
-        self.assertAllClose(preserved_boxes, bounding_boxes["boxes"][:, :2, :])
-
-        boxes_from_image_3 = result["boxes"][2, :4, :]
-        self.assertAllClose(
-            boxes_from_image_3, bounding_boxes["boxes"][2, :4, :]
-        )
-
-    def test_correctly_masks_based_on_max_dets_in_graph(self):
-        bounding_boxes = {
-            "boxes": tf.random.uniform((4, 100, 4)),
-            "num_detections": tf.constant([2, 3, 4, 2]),
-            "classes": tf.random.uniform((4, 100)),
-        }
-
-        @tf.function()
-        def apply_mask_detections(bounding_boxes):
-            return bounding_box.mask_invalid_detections(bounding_boxes)
-
-        result = apply_mask_detections(bounding_boxes)
-
-        negative_one_boxes = result["boxes"][:, 5:, :]
-        self.assertAllClose(
-            negative_one_boxes, -tf.ones_like(negative_one_boxes)
+            negative_one_boxes, -np.ones_like(negative_one_boxes)
         )
 
         preserved_boxes = result["boxes"][:, :2, :]
         self.assertAllClose(preserved_boxes, bounding_boxes["boxes"][:, :2, :])
 
         boxes_from_image_3 = result["boxes"][2, :4, :]
         self.assertAllClose(
             boxes_from_image_3, bounding_boxes["boxes"][2, :4, :]
         )
 
+    @pytest.mark.tf_keras_only
     def test_ragged_outputs(self):
         bounding_boxes = {
-            "boxes": tf.stack(
-                [tf.random.uniform((10, 4)), tf.random.uniform((10, 4))]
+            "boxes": np.stack(
+                [
+                    np.random.uniform(size=(10, 4)),
+                    np.random.uniform(size=(10, 4)),
+                ]
             ),
-            "num_detections": tf.constant([2, 3]),
-            "classes": tf.stack(
-                [tf.random.uniform((10,)), tf.random.uniform((10,))]
+            "num_detections": np.array([2, 3]),
+            "classes": np.stack(
+                [np.random.uniform(size=(10,)), np.random.uniform(size=(10,))]
             ),
         }
 
         result = bounding_box.mask_invalid_detections(
             bounding_boxes, output_ragged=True
         )
         self.assertTrue(isinstance(result["boxes"], tf.RaggedTensor))
         self.assertEqual(result["boxes"][0].shape[0], 2)
         self.assertEqual(result["boxes"][1].shape[0], 3)
 
+    @pytest.mark.tf_keras_only
     def test_correctly_masks_confidence(self):
         bounding_boxes = {
-            "boxes": tf.stack(
-                [tf.random.uniform((10, 4)), tf.random.uniform((10, 4))]
+            "boxes": np.stack(
+                [
+                    np.random.uniform(size=(10, 4)),
+                    np.random.uniform(size=(10, 4)),
+                ]
             ),
-            "confidence": tf.random.uniform((2, 10)),
-            "num_detections": tf.constant([2, 3]),
-            "classes": tf.stack(
-                [tf.random.uniform((10,)), tf.random.uniform((10,))]
+            "confidence": np.random.uniform(size=(2, 10)),
+            "num_detections": np.array([2, 3]),
+            "classes": np.stack(
+                [np.random.uniform(size=(10,)), np.random.uniform(size=(10,))]
             ),
         }
 
         result = bounding_box.mask_invalid_detections(
             bounding_boxes, output_ragged=True
         )
         self.assertTrue(isinstance(result["boxes"], tf.RaggedTensor))
```

## keras_cv/bounding_box/to_dense.py

```diff
@@ -10,14 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 import keras_cv.bounding_box.validate_format as validate_format
+from keras_cv.backend.scope import tf_data
 
 
 def _box_shape(batched, boxes_shape, max_boxes):
     # ensure we dont drop the final axis in RaggedTensor mode
     if max_boxes is None:
         shape = list(boxes_shape)
         shape[-1] = 4
@@ -31,14 +32,15 @@
     if max_boxes is None:
         return None
     if batched:
         return [None, max_boxes] + classes_shape[2:]
     return [max_boxes] + classes_shape[2:]
 
 
+@tf_data
 def to_dense(bounding_boxes, max_boxes=None, default_value=-1):
     """to_dense converts bounding boxes to Dense tensors
 
     Args:
         bounding_boxes: bounding boxes in KerasCV dictionary format.
         max_boxes: the maximum number of boxes, used to pad tensors to a given
             shape. This can be used to make object detection pipelines TPU
```

## keras_cv/bounding_box/to_dense_test.py

```diff
@@ -7,20 +7,22 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import pytest
 import tensorflow as tf
 
 from keras_cv import bounding_box
 
 
 class ToDenseTest(tf.test.TestCase):
+    @pytest.mark.tf_keras_only
     def test_converts_to_dense(self):
         bounding_boxes = {
             "boxes": tf.ragged.constant(
                 [[[0, 0, 1, 1]], [[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1]]]
             ),
             "classes": tf.ragged.constant([[0], [1, 2, 3]]),
         }
```

## keras_cv/bounding_box/to_ragged.py

```diff
@@ -10,14 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
 
 import keras_cv.bounding_box.validate_format as validate_format
+from keras_cv import backend
+from keras_cv.backend import keras
 
 
 def to_ragged(bounding_boxes, sentinel=-1, dtype=tf.float32):
     """converts a Dense padded bounding box `tf.Tensor` to a `tf.RaggedTensor`.
 
     Bounding boxes are ragged tensors in most use cases. Converting them to a
     dense tensor makes it easier to work with Tensorflow ecosystem.
@@ -45,14 +47,21 @@
         sentinel: The value indicating that a bounding box does not exist at the
             current index, and the corresponding box is padding, defaults to -1.
         dtype: the data type to use for the underlying Tensors.
     Returns:
         dictionary of `tf.RaggedTensor` or 'tf.Tensor' containing the filtered
         bounding boxes.
     """
+    if backend.supports_ragged() is False:
+        raise NotImplementedError(
+            "`bounding_box.to_ragged` was called using a backend which does "
+            "not support ragged tensors. "
+            f"Current backend: {keras.backend.backend()}."
+        )
+
     info = validate_format.validate_format(bounding_boxes)
 
     if info["ragged"]:
         return bounding_boxes
 
     boxes = bounding_boxes.get("boxes")
     classes = bounding_boxes.get("classes")
```

## keras_cv/bounding_box/to_ragged_test.py

```diff
@@ -7,27 +7,31 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
+import pytest
 import tensorflow as tf
 
+from keras_cv import backend
 from keras_cv import bounding_box
 
 
 class ToRaggedTest(tf.test.TestCase):
+    @pytest.mark.tf_keras_only
     def test_converts_to_ragged(self):
         bounding_boxes = {
-            "boxes": tf.constant(
+            "boxes": np.array(
                 [[[0, 0, 0, 0], [0, 0, 0, 0]], [[2, 3, 4, 5], [0, 1, 2, 3]]]
             ),
-            "classes": tf.constant([[-1, -1], [-1, 1]]),
-            "confidence": tf.constant([[0.5, 0.7], [0.23, 0.12]]),
+            "classes": np.array([[-1, -1], [-1, 1]]),
+            "confidence": np.array([[0.5, 0.7], [0.23, 0.12]]),
         }
         bounding_boxes = bounding_box.to_ragged(bounding_boxes)
 
         self.assertEqual(bounding_boxes["boxes"][1].shape, [1, 4])
         self.assertEqual(bounding_boxes["classes"][1].shape, [1])
         self.assertEqual(
             bounding_boxes["confidence"][1].shape,
@@ -41,24 +45,25 @@
         self.assertEqual(
             bounding_boxes["confidence"][0].shape,
             [
                 0,
             ],
         )
 
+    @pytest.mark.tf_keras_only
     def test_round_trip(self):
         original = {
-            "boxes": tf.constant(
+            "boxes": np.array(
                 [
                     [[0, 0, 0, 0], [-1, -1, -1, -1]],
                     [[-1, -1, -1, -1], [-1, -1, -1, -1]],
                 ]
             ),
-            "classes": tf.constant([[1, -1], [-1, -1]]),
-            "confidence": tf.constant([[0.5, -1], [-1, -1]]),
+            "classes": np.array([[1, -1], [-1, -1]]),
+            "confidence": np.array([[0.5, -1], [-1, -1]]),
         }
         bounding_boxes = bounding_box.to_ragged(original)
         bounding_boxes = bounding_box.to_dense(bounding_boxes, max_boxes=2)
 
         self.assertEqual(bounding_boxes["boxes"][1].shape, [2, 4])
         self.assertEqual(bounding_boxes["classes"][1].shape, [2])
         self.assertEqual(bounding_boxes["classes"][0].shape, [2])
@@ -66,7 +71,23 @@
         self.assertEqual(bounding_boxes["confidence"][0].shape, [2])
 
         self.assertAllEqual(bounding_boxes["boxes"], original["boxes"])
         self.assertAllEqual(bounding_boxes["classes"], original["classes"])
         self.assertAllEqual(
             bounding_boxes["confidence"], original["confidence"]
         )
+
+    @pytest.mark.skipif(
+        backend.supports_ragged() is True,
+        reason="Only applies to backends which don't support raggeds",
+    )
+    def test_backend_without_raggeds_throws(self):
+        bounding_boxes = {
+            "boxes": np.array(
+                [[[0, 0, 0, 0], [0, 0, 0, 0]], [[2, 3, 4, 5], [0, 1, 2, 3]]]
+            ),
+            "classes": np.array([[-1, -1], [-1, 1]]),
+            "confidence": np.array([[0.5, 0.7], [0.23, 0.12]]),
+        }
+
+        with self.assertRaisesRegex(NotImplementedError, "support ragged"):
+            bounding_box.to_ragged(bounding_boxes)
```

## keras_cv/bounding_box/utils.py

```diff
@@ -9,17 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Utility functions for working with bounding boxes."""
 
-import tensorflow as tf
-
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 from keras_cv.bounding_box.formats import XYWH
 
 
 def is_relative(bounding_box_format):
     """A util to check if a bounding box format uses relative coordinates"""
     if (
         bounding_box_format.lower()
@@ -53,16 +52,16 @@
         boxes,
         source=bounding_box_format,
         target="rel_xywh",
     )
     widths = boxes[..., XYWH.WIDTH]
     heights = boxes[..., XYWH.HEIGHT]
     # handle corner case where shear performs a full inversion.
-    return tf.where(
-        tf.math.logical_and(widths > 0, heights > 0), widths * heights, 0.0
+    return ops.where(
+        ops.logical_and(widths > 0, heights > 0), widths * heights, 0.0
     )
 
 
 # bounding_boxes is a dictionary with shape:
 # {"boxes": [None, None, 4], "mask": [None, None]}
 def clip_to_image(
     bounding_boxes, bounding_box_format, images=None, image_shape=None
@@ -88,42 +87,40 @@
         boxes,
         source=bounding_box_format,
         target="rel_xyxy",
         images=images,
         image_shape=image_shape,
     )
     boxes, classes, images, squeeze = _format_inputs(boxes, classes, images)
-    x1, y1, x2, y2 = tf.split(boxes, [1, 1, 1, 1], axis=-1)
-    clipped_bounding_boxes = tf.concat(
+    x1, y1, x2, y2 = ops.split(boxes, 4, axis=-1)
+    clipped_bounding_boxes = ops.concatenate(
         [
-            tf.clip_by_value(x1, clip_value_min=0, clip_value_max=1),
-            tf.clip_by_value(y1, clip_value_min=0, clip_value_max=1),
-            tf.clip_by_value(x2, clip_value_min=0, clip_value_max=1),
-            tf.clip_by_value(y2, clip_value_min=0, clip_value_max=1),
+            ops.clip(x1, 0, 1),
+            ops.clip(y1, 0, 1),
+            ops.clip(x2, 0, 1),
+            ops.clip(y2, 0, 1),
         ],
         axis=-1,
     )
     areas = _relative_area(
         clipped_bounding_boxes, bounding_box_format="rel_xyxy"
     )
     clipped_bounding_boxes = bounding_box.convert_format(
         clipped_bounding_boxes,
         source="rel_xyxy",
         target=bounding_box_format,
         images=images,
         image_shape=image_shape,
     )
-    clipped_bounding_boxes = tf.where(
-        tf.expand_dims(areas > 0.0, axis=-1), clipped_bounding_boxes, -1.0
-    )
-    classes = tf.where(areas > 0.0, classes, tf.constant(-1, classes.dtype))
-    nan_indices = tf.math.reduce_any(
-        tf.math.is_nan(clipped_bounding_boxes), axis=-1
+    clipped_bounding_boxes = ops.where(
+        ops.expand_dims(areas > 0.0, axis=-1), clipped_bounding_boxes, -1.0
     )
-    classes = tf.where(nan_indices, tf.constant(-1, classes.dtype), classes)
+    classes = ops.where(areas > 0.0, classes, -1)
+    nan_indices = ops.any(ops.isnan(clipped_bounding_boxes), axis=-1)
+    classes = ops.where(nan_indices, -1, classes)
 
     # TODO update dict and return
     clipped_bounding_boxes, classes = _format_outputs(
         clipped_bounding_boxes, classes, squeeze
     )
 
     result = bounding_boxes.copy()
@@ -140,19 +137,20 @@
             "boxes.shape[-1] is {:d}, but must be 4.".format(boxes.shape[-1])
         )
 
     if isinstance(image_shape, list) or isinstance(image_shape, tuple):
         height, width, _ = image_shape
         max_length = [height, width, height, width]
     else:
-        image_shape = tf.cast(image_shape, dtype=boxes.dtype)
-        height, width, _ = tf.unstack(image_shape, axis=-1)
-        max_length = tf.stack([height, width, height, width], axis=-1)
+        image_shape = ops.cast(image_shape, dtype=boxes.dtype)
+        height = image_shape[0]
+        width = image_shape[1]
+        max_length = ops.stack([height, width, height, width], axis=-1)
 
-    clipped_boxes = tf.math.maximum(tf.math.minimum(boxes, max_length), 0.0)
+    clipped_boxes = ops.maximum(ops.minimum(boxes, max_length), 0.0)
     return clipped_boxes
 
 
 def _format_inputs(boxes, classes, images):
     boxes_rank = len(boxes.shape)
     if boxes_rank > 3:
         raise ValueError(
@@ -175,23 +173,23 @@
                 "or both boxes and images to be unbatched. Received "
                 f"len(boxes.shape)={boxes_rank}, "
                 f"len(images.shape)={images_rank}. Expected either "
                 "len(boxes.shape)=2 AND len(images.shape)=3, or "
                 "len(boxes.shape)=3 AND len(images.shape)=4."
             )
         if not images_include_batch:
-            images = tf.expand_dims(images, axis=0)
+            images = ops.expand_dims(images, axis=0)
 
     if not boxes_includes_batch:
         return (
-            tf.expand_dims(boxes, axis=0),
-            tf.expand_dims(classes, axis=0),
+            ops.expand_dims(boxes, axis=0),
+            ops.expand_dims(classes, axis=0),
             images,
             True,
         )
     return boxes, classes, images, False
 
 
 def _format_outputs(boxes, classes, squeeze):
     if squeeze:
-        return tf.squeeze(boxes, axis=0), tf.squeeze(classes, axis=0)
+        return ops.squeeze(boxes, axis=0), ops.squeeze(classes, axis=0)
     return boxes, classes
```

## keras_cv/bounding_box/utils_test.py

```diff
@@ -7,99 +7,93 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
 import tensorflow as tf
 
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 
 
 class BoundingBoxUtilTest(tf.test.TestCase):
     def test_clip_to_image_standard(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
-                [[200, 200, 400, 400], [100, 100, 300, 300]], dtype=tf.float32
-            ),
-            "classes": tf.convert_to_tensor([0, 0], dtype=tf.float32),
+            "boxes": np.array([[200, 200, 400, 400], [100, 100, 300, 300]]),
+            "classes": np.array([0, 0]),
         }
-        image = tf.ones(shape=(height, width, 3))
+        image = ops.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
         boxes = bounding_boxes["boxes"]
         self.assertAllGreaterEqual(boxes, 0)
         (
             x1,
             y1,
             x2,
             y2,
-        ) = tf.split(boxes, [1, 1, 1, 1], axis=1)
-        self.assertAllLessEqual([x1, x2], width)
-        self.assertAllLessEqual([y1, y2], height)
+        ) = ops.split(boxes, 4, axis=1)
+        self.assertAllLessEqual(ops.concatenate([x1, x2], axis=1), width)
+        self.assertAllLessEqual(ops.concatenate([y1, y2], axis=1), height)
         # Test relative format batched
-        image = tf.ones(shape=(1, height, width, 3))
+        image = ops.ones(shape=(1, height, width, 3))
 
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
-                [[[0.2, -1, 1.2, 0.3], [0.4, 1.5, 0.2, 0.3]]], dtype=tf.float32
-            ),
-            "classes": tf.convert_to_tensor([[0, 0]], dtype=tf.float32),
+            "boxes": np.array([[[0.2, -1, 1.2, 0.3], [0.4, 1.5, 0.2, 0.3]]]),
+            "classes": np.array([[0, 0]]),
         }
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="rel_xyxy", images=image
         )
         self.assertAllLessEqual(bounding_boxes["boxes"], 1)
 
     def test_clip_to_image_filters_fully_out_bounding_boxes(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
-                [[257, 257, 400, 400], [100, 100, 300, 300]]
-            ),
-            "classes": tf.convert_to_tensor([0, 0]),
+            "boxes": np.array([[257, 257, 400, 400], [100, 100, 300, 300]]),
+            "classes": np.array([0, 0]),
         }
-        image = tf.ones(shape=(height, width, 3))
+        image = ops.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
 
         self.assertAllEqual(
             bounding_boxes["boxes"],
-            tf.convert_to_tensor([[-1, -1, -1, -1], [100, 100, 256, 256]]),
+            np.array([[-1, -1, -1, -1], [100, 100, 256, 256]]),
         ),
         self.assertAllEqual(
             bounding_boxes["classes"],
-            tf.convert_to_tensor([-1, 0]),
+            np.array([-1, 0]),
         )
 
     def test_clip_to_image_filters_fully_out_bounding_boxes_negative_area(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
-                [[110, 120, 100, 100], [100, 100, 300, 300]]
-            ),
-            "classes": [0, 0],
+            "boxes": np.array([[110, 120, 100, 100], [100, 100, 300, 300]]),
+            "classes": np.array([0, 0]),
         }
-        image = tf.ones(shape=(height, width, 3))
+        image = ops.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
         self.assertAllEqual(
             bounding_boxes["boxes"],
-            tf.convert_to_tensor(
+            np.array(
                 [
                     [
                         -1,
                         -1,
                         -1,
                         -1,
                     ],
@@ -110,34 +104,34 @@
                         256,
                     ],
                 ]
             ),
         )
         self.assertAllEqual(
             bounding_boxes["classes"],
-            tf.convert_to_tensor([-1, 0]),
+            np.array([-1, 0]),
         )
 
     def test_clip_to_image_filters_nans(self):
         # Test xyxy format unbatched
         height = 256
         width = 256
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
+            "boxes": np.array(
                 [[0, float("NaN"), 100, 100], [100, 100, 300, 300]]
             ),
-            "classes": [0, 0],
+            "classes": np.array([0, 0]),
         }
-        image = tf.ones(shape=(height, width, 3))
+        image = ops.ones(shape=(height, width, 3))
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes, bounding_box_format="xyxy", images=image
         )
         self.assertAllEqual(
             bounding_boxes["boxes"],
-            tf.convert_to_tensor(
+            np.array(
                 [
                     [
                         -1,
                         -1,
                         -1,
                         -1,
                     ],
@@ -148,15 +142,15 @@
                         256,
                     ],
                 ]
             ),
         )
         self.assertAllEqual(
             bounding_boxes["classes"],
-            tf.convert_to_tensor([-1, 0]),
+            np.array([-1, 0]),
         )
 
     def test_is_relative_util(self):
         self.assertTrue(bounding_box.is_relative("rel_xyxy"))
         self.assertFalse(bounding_box.is_relative("xyxy"))
 
         with self.assertRaises(ValueError):
```

## keras_cv/callbacks/pycoco_callback.py

```diff
@@ -7,18 +7,19 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
+import numpy as np
 from keras.callbacks import Callback
 
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 from keras_cv.metrics.coco import compute_pycoco_metrics
 from keras_cv.models.object_detection.__internal__ import unpack_input
 from keras_cv.utils.conditional_imports import assert_pycocotools_installed
 
 
 class PyCOCOCallback(Callback):
     def __init__(
@@ -50,68 +51,83 @@
             self.val_data = self.val_data.cache()
         self.bounding_box_format = bounding_box_format
         super().__init__(**kwargs)
 
     def on_epoch_end(self, epoch, logs=None):
         logs = logs or {}
 
-        def images_only(data):
-            images, boxes = unpack_input(data)
+        def images_only(data, maybe_boxes=None):
+            if maybe_boxes is None:
+                images, boxes = unpack_input(data)
+            else:
+                images = data
             return images
 
-        def boxes_only(data):
-            images, boxes = unpack_input(data)
-            return bounding_box.to_ragged(boxes)
+        def boxes_only(data, maybe_boxes=None):
+            if maybe_boxes is None:
+                images, boxes = unpack_input(data)
+            else:
+                boxes = maybe_boxes
+            return boxes
 
         images_only_ds = self.val_data.map(images_only)
         y_pred = self.model.predict(images_only_ds)
         box_pred = y_pred["boxes"]
-        cls_pred = y_pred["classes"]
-        confidence_pred = y_pred["confidence"]
-        valid_det = y_pred["num_detections"]
+        cls_pred = ops.convert_to_numpy(y_pred["classes"])
+        confidence_pred = ops.convert_to_numpy(y_pred["confidence"])
+        valid_det = ops.convert_to_numpy(y_pred["num_detections"])
 
         gt = [boxes for boxes in self.val_data.map(boxes_only)]
-        gt_boxes = tf.concat(
-            [boxes["boxes"] for boxes in gt],
+        gt_boxes = ops.concatenate(
+            [ops.convert_to_numpy(boxes["boxes"]) for boxes in gt],
             axis=0,
         )
-        gt_classes = tf.concat(
-            [boxes["classes"] for boxes in gt],
+        gt_classes = ops.concatenate(
+            [ops.convert_to_numpy(boxes["classes"]) for boxes in gt],
             axis=0,
         )
 
         first_image_batch = next(iter(images_only_ds))
         height = first_image_batch.shape[1]
         width = first_image_batch.shape[2]
         total_images = gt_boxes.shape[0]
 
         gt_boxes = bounding_box.convert_format(
             gt_boxes, source=self.bounding_box_format, target="yxyx"
         )
 
-        source_ids = tf.strings.as_string(
-            tf.linspace(1, total_images, total_images), precision=0
+        source_ids = np.char.mod(
+            "%d", np.linspace(1, total_images, total_images)
         )
+        num_detections = ops.sum(ops.cast(gt_classes > 0, "int32"), axis=-1)
 
         ground_truth = {
             "source_id": [source_ids],
-            "height": [tf.tile(tf.constant([height]), [total_images])],
-            "width": [tf.tile(tf.constant([width]), [total_images])],
-            "num_detections": [gt_boxes.row_lengths(axis=1)],
-            "boxes": [gt_boxes.to_tensor(-1)],
-            "classes": [gt_classes.to_tensor(-1)],
+            "height": [
+                ops.convert_to_numpy(
+                    ops.tile(ops.array([height]), [total_images])
+                )
+            ],
+            "width": [
+                ops.convert_to_numpy(
+                    ops.tile(ops.array([width]), [total_images])
+                )
+            ],
+            "num_detections": [ops.convert_to_numpy(num_detections)],
+            "boxes": [ops.convert_to_numpy(gt_boxes)],
+            "classes": [ops.convert_to_numpy(gt_classes)],
         }
 
         box_pred = bounding_box.convert_format(
             box_pred, source=self.bounding_box_format, target="yxyx"
         )
 
         predictions = {
             "source_id": [source_ids],
-            "detection_boxes": [box_pred],
+            "detection_boxes": [ops.convert_to_numpy(box_pred)],
             "detection_classes": [cls_pred],
             "detection_scores": [confidence_pred],
             "num_detections": [valid_det],
         }
 
         metrics = compute_pycoco_metrics(ground_truth, predictions)
         # Mark these as validation metrics by prepending a val_ prefix
```

## keras_cv/callbacks/pycoco_callback_test.py

```diff
@@ -10,37 +10,30 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import pytest
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv
 from keras_cv.callbacks import PyCOCOCallback
 from keras_cv.metrics.coco.pycoco_wrapper import METRIC_NAMES
 from keras_cv.models.object_detection.__test_utils__ import (
     _create_bounding_box_dataset,
 )
 
 
 class PyCOCOCallbackTest(tf.test.TestCase):
-    @pytest.fixture(autouse=True)
-    def cleanup_global_session(self):
-        # Code before yield runs before the test
-        yield
-        keras.backend.clear_session()
-
     @pytest.mark.large  # Fit is slow, so mark these large.
     def test_model_fit_retinanet(self):
         model = keras_cv.models.RetinaNet(
             num_classes=10,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.CSPDarkNetTinyBackbone(),
         )
         # all metric formats must match
         model.compile(
             optimizer="adam",
             box_loss="smoothl1",
             classification_loss="focal",
         )
@@ -48,16 +41,23 @@
         train_ds = _create_bounding_box_dataset(
             bounding_box_format="xyxy", use_dictionary_box_format=True
         )
         val_ds = _create_bounding_box_dataset(
             bounding_box_format="xyxy", use_dictionary_box_format=True
         )
 
+        def dict_to_tuple(inputs):
+            return inputs["images"], inputs["bounding_boxes"]
+
+        train_ds = train_ds.map(dict_to_tuple)
+        val_ds = val_ds.map(dict_to_tuple)
+
         callback = PyCOCOCallback(
-            validation_data=val_ds, bounding_box_format="xyxy"
+            validation_data=val_ds,
+            bounding_box_format="xyxy",
         )
         history = model.fit(train_ds, callbacks=[callback])
 
         self.assertAllInSet(
             [f"val_{metric}" for metric in METRIC_NAMES], history.history.keys()
         )
```

## keras_cv/layers/__init__.py

```diff
@@ -20,14 +20,17 @@
 from keras_cv.layers.fusedmbconv import FusedMBConvBlock
 from keras_cv.layers.mbconv import MBConvBlock
 from keras_cv.layers.object_detection.anchor_generator import AnchorGenerator
 from keras_cv.layers.object_detection.box_matcher import BoxMatcher
 from keras_cv.layers.object_detection.multi_class_non_max_suppression import (
     MultiClassNonMaxSuppression,
 )
+from keras_cv.layers.object_detection.non_max_suppression import (
+    NonMaxSuppression,
+)
 from keras_cv.layers.object_detection_3d.centernet_label_encoder import (
     CenterNetLabelEncoder,
 )
 from keras_cv.layers.object_detection_3d.voxelization import DynamicVoxelization
 from keras_cv.layers.preprocessing.aug_mix import AugMix
 from keras_cv.layers.preprocessing.auto_contrast import AutoContrast
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
```

## keras_cv/layers/fusedmbconv.py

```diff
@@ -9,32 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-from keras import backend
-from tensorflow import keras
-from tensorflow.keras import layers
+from keras_cv.backend import keras
 
 BN_AXIS = 3
 
 CONV_KERNEL_INITIALIZER = {
     "class_name": "VarianceScaling",
     "config": {
         "scale": 2.0,
         "mode": "fan_out",
         "distribution": "truncated_normal",
     },
 }
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class FusedMBConvBlock(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class FusedMBConvBlock(keras.layers.Layer):
     """
     Implementation of the FusedMBConv block (Fused Mobile Inverted Residual
     Bottleneck) from:
         [EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML](https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html)
         [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298v3).
 
     FusedMBConv blocks are based on MBConv blocks, and replace the depthwise and
@@ -108,115 +106,119 @@
         self.se_ratio = se_ratio
         self.bn_momentum = bn_momentum
         self.activation = activation
         self.survival_probability = survival_probability
         self.filters = self.input_filters * self.expand_ratio
         self.filters_se = max(1, int(input_filters * se_ratio))
 
-        self.conv1 = layers.Conv2D(
+        self.conv1 = keras.layers.Conv2D(
             filters=self.filters,
             kernel_size=kernel_size,
             strides=strides,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "expand_conv",
         )
-        self.bn1 = layers.BatchNormalization(
+        self.bn1 = keras.layers.BatchNormalization(
             axis=BN_AXIS,
             momentum=self.bn_momentum,
             name=self.name + "expand_bn",
         )
-        self.act = layers.Activation(
+        self.act = keras.layers.Activation(
             self.activation, name=self.name + "expand_activation"
         )
 
-        self.bn2 = layers.BatchNormalization(
+        self.bn2 = keras.layers.BatchNormalization(
             axis=BN_AXIS, momentum=self.bn_momentum, name=self.name + "bn"
         )
 
-        self.se_conv1 = layers.Conv2D(
+        self.se_conv1 = keras.layers.Conv2D(
             self.filters_se,
             1,
             padding="same",
             activation=self.activation,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             name=self.name + "se_reduce",
         )
 
-        self.se_conv2 = layers.Conv2D(
+        self.se_conv2 = keras.layers.Conv2D(
             self.filters,
             1,
             padding="same",
             activation="sigmoid",
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             name=self.name + "se_expand",
         )
 
-        self.output_conv = layers.Conv2D(
+        self.output_conv = keras.layers.Conv2D(
             filters=self.output_filters,
             kernel_size=1 if expand_ratio != 1 else kernel_size,
             strides=1,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "project_conv",
         )
 
-        self.bn3 = layers.BatchNormalization(
+        self.bn3 = keras.layers.BatchNormalization(
             axis=BN_AXIS,
             momentum=self.bn_momentum,
             name=self.name + "project_bn",
         )
 
     def build(self, input_shape):
         if self.name is None:
-            self.name = backend.get_uid("block0")
+            self.name = keras.backend.get_uid("block0")
 
     def call(self, inputs):
         # Expansion phase
         if self.expand_ratio != 1:
             x = self.conv1(inputs)
             x = self.bn1(x)
             x = self.act(x)
         else:
             x = inputs
 
         # Squeeze and excite
         if 0 < self.se_ratio <= 1:
-            se = layers.GlobalAveragePooling2D(name=self.name + "se_squeeze")(x)
+            se = keras.layers.GlobalAveragePooling2D(
+                name=self.name + "se_squeeze"
+            )(x)
             if BN_AXIS == 1:
                 se_shape = (self.filters, 1, 1)
             else:
                 se_shape = (1, 1, self.filters)
 
-            se = layers.Reshape(se_shape, name=self.name + "se_reshape")(se)
+            se = keras.layers.Reshape(se_shape, name=self.name + "se_reshape")(
+                se
+            )
 
             se = self.se_conv1(se)
             se = self.se_conv2(se)
 
-            x = layers.multiply([x, se], name=self.name + "se_excite")
+            x = keras.layers.multiply([x, se], name=self.name + "se_excite")
 
         # Output phase:
         x = self.output_conv(x)
         x = self.bn3(x)
         if self.expand_ratio == 1:
             x = self.act(x)
 
         # Residual:
         if self.strides == 1 and self.input_filters == self.output_filters:
             if self.survival_probability:
-                x = layers.Dropout(
+                x = keras.layers.Dropout(
                     self.survival_probability,
                     noise_shape=(None, 1, 1, 1),
                     name=self.name + "drop",
                 )(x)
-            x = layers.add([x, inputs], name=self.name + "add")
+            x = keras.layers.add([x, inputs], name=self.name + "add")
         return x
 
     def get_config(self):
         config = {
             "input_filters": self.input_filters,
             "output_filters": self.output_filters,
             "expand_ratio": self.expand_ratio,
```

## keras_cv/layers/mbconv.py

```diff
@@ -9,32 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-from keras import backend
-from tensorflow import keras
-from tensorflow.keras import layers
+from keras_cv.backend import keras
 
 BN_AXIS = 3
 
 CONV_KERNEL_INITIALIZER = {
     "class_name": "VarianceScaling",
     "config": {
         "scale": 2.0,
         "mode": "fan_out",
         "distribution": "truncated_normal",
     },
 }
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class MBConvBlock(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class MBConvBlock(keras.layers.Layer):
     def __init__(
         self,
         input_filters: int,
         output_filters: int,
         expand_ratio=1,
         kernel_size=3,
         strides=1,
@@ -106,84 +104,84 @@
         self.se_ratio = se_ratio
         self.bn_momentum = bn_momentum
         self.activation = activation
         self.survival_probability = survival_probability
         self.filters = self.input_filters * self.expand_ratio
         self.filters_se = max(1, int(input_filters * se_ratio))
 
-        self.conv1 = layers.Conv2D(
+        self.conv1 = keras.layers.Conv2D(
             filters=self.filters,
             kernel_size=1,
             strides=1,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "expand_conv",
         )
-        self.bn1 = layers.BatchNormalization(
+        self.bn1 = keras.layers.BatchNormalization(
             axis=BN_AXIS,
             momentum=self.bn_momentum,
             name=self.name + "expand_bn",
         )
-        self.act = layers.Activation(
+        self.act = keras.layers.Activation(
             self.activation, name=self.name + "activation"
         )
-        self.depthwise = layers.DepthwiseConv2D(
+        self.depthwise = keras.layers.DepthwiseConv2D(
             kernel_size=self.kernel_size,
             strides=self.strides,
             depthwise_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "dwconv2",
         )
 
-        self.bn2 = layers.BatchNormalization(
+        self.bn2 = keras.layers.BatchNormalization(
             axis=BN_AXIS, momentum=self.bn_momentum, name=self.name + "bn"
         )
 
-        self.se_conv1 = layers.Conv2D(
+        self.se_conv1 = keras.layers.Conv2D(
             self.filters_se,
             1,
             padding="same",
             activation=self.activation,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             name=self.name + "se_reduce",
         )
 
-        self.se_conv2 = layers.Conv2D(
+        self.se_conv2 = keras.layers.Conv2D(
             self.filters,
             1,
             padding="same",
             activation="sigmoid",
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             name=self.name + "se_expand",
         )
 
-        self.output_conv = layers.Conv2D(
+        self.output_conv = keras.layers.Conv2D(
             filters=self.output_filters,
             kernel_size=1 if expand_ratio != 1 else kernel_size,
             strides=1,
             kernel_initializer=CONV_KERNEL_INITIALIZER,
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name=self.name + "project_conv",
         )
 
-        self.bn3 = layers.BatchNormalization(
+        self.bn3 = keras.layers.BatchNormalization(
             axis=BN_AXIS,
             momentum=self.bn_momentum,
             name=self.name + "project_bn",
         )
 
     def build(self, input_shape):
         if self.name is None:
-            self.name = backend.get_uid("block0")
+            self.name = keras.backend.get_uid("block0")
 
     def call(self, inputs):
         # Expansion phase
         if self.expand_ratio != 1:
             x = self.conv1(inputs)
             x = self.bn1(x)
             x = self.act(x)
@@ -193,38 +191,42 @@
         # Depthwise conv
         x = self.depthwise(x)
         x = self.bn2(x)
         x = self.act(x)
 
         # Squeeze and excite
         if 0 < self.se_ratio <= 1:
-            se = layers.GlobalAveragePooling2D(name=self.name + "se_squeeze")(x)
+            se = keras.layers.GlobalAveragePooling2D(
+                name=self.name + "se_squeeze"
+            )(x)
             if BN_AXIS == 1:
                 se_shape = (self.filters, 1, 1)
             else:
                 se_shape = (1, 1, self.filters)
-            se = layers.Reshape(se_shape, name=self.name + "se_reshape")(se)
+            se = keras.layers.Reshape(se_shape, name=self.name + "se_reshape")(
+                se
+            )
 
             se = self.se_conv1(se)
             se = self.se_conv2(se)
 
-            x = layers.multiply([x, se], name=self.name + "se_excite")
+            x = keras.layers.multiply([x, se], name=self.name + "se_excite")
 
         # Output phase
         x = self.output_conv(x)
         x = self.bn3(x)
 
         if self.strides == 1 and self.input_filters == self.output_filters:
             if self.survival_probability:
-                x = layers.Dropout(
+                x = keras.layers.Dropout(
                     self.survival_probability,
                     noise_shape=(None, 1, 1, 1),
                     name=self.name + "drop",
                 )(x)
-            x = layers.add([x, inputs], name=self.name + "add")
+            x = keras.layers.add([x, inputs], name=self.name + "add")
         return x
 
     def get_config(self):
         config = {
             "input_filters": self.input_filters,
             "output_filters": self.output_filters,
             "expand_ratio": self.expand_ratio,
```

## keras_cv/layers/object_detection/anchor_generator.py

```diff
@@ -8,21 +8,22 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
+import math
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class AnchorGenerator(keras.layers.Layer):
     """AnchorGenerator generates anchors for multiple feature maps.
 
     AnchorGenerator takes multiple scales and generates anchor boxes based on
     the anchor sizes, scales, aspect ratios, and strides provided. To invoke
     AnchorGenerator, call it on the image that needs anchor boxes.
 
@@ -52,15 +53,15 @@
     Usage:
     ```python
     strides = [8, 16, 32]
     scales = [1, 1.2599210498948732, 1.5874010519681994]
     sizes = [32.0, 64.0, 128.0]
     aspect_ratios = [0.5, 1.0, 2.0]
 
-    image = tf.random.uniform((512, 512, 3))
+    image = np.random.uniform(size=(512, 512, 3))
     anchor_generator = cv_layers.AnchorGenerator(
         bounding_box_format="rel_yxyx",
         sizes=sizes,
         aspect_ratios=aspect_ratios,
         scales=scales,
         strides=strides,
         clip_boxes=True,
@@ -146,44 +147,40 @@
         for i in range(len(param)):
             result[i] = param[i]
         return result
 
     @staticmethod
     def _match_param_structure_to_sizes(params, sizes):
         """broadcast the params to match sizes."""
-        # if isinstance(sizes, (tuple, list)):
-        #     return [params] * len(sizes)
         if not isinstance(sizes, dict):
             raise ValueError(
                 "the structure of `sizes` must be a dict, "
                 f"received sizes={sizes}"
             )
 
-        return tf.nest.map_structure(lambda _: params, sizes)
+        return {key: params for key in sizes.keys()}
 
     def __call__(self, image=None, image_shape=None):
         if image is None and image_shape is None:
             raise ValueError(
                 "AnchorGenerator() requires `images` or `image_shape`."
             )
 
         if image is not None:
-            if image.shape.rank != 3:
+            if len(image.shape) != 3:
                 raise ValueError(
                     "Expected `image` to be a Tensor of rank 3. Got "
-                    f"image.shape.rank={image.shape.rank}"
+                    f"image.shape.rank={len(image.shape)}"
                 )
-            image_shape = tf.shape(image)
+            image_shape = image.shape
 
-        anchor_generators = tf.nest.flatten(self.anchor_generators)
-        results = [anchor_gen(image_shape) for anchor_gen in anchor_generators]
-        results = tf.nest.pack_sequence_as(self.anchor_generators, results)
-        for key in results:
+        results = {}
+        for key, generator in self.anchor_generators.items():
             results[key] = bounding_box.convert_format(
-                results[key],
+                generator(image_shape),
                 source="yxyx",
                 target=self.bounding_box_format,
                 image_shape=image_shape,
             )
         return results
 
 
@@ -233,68 +230,74 @@
         self.scales = scales
         self.aspect_ratios = aspect_ratios
         self.stride = stride
         self.clip_boxes = clip_boxes
         self.dtype = dtype
 
     def __call__(self, image_size):
-        image_height = tf.cast(image_size[0], tf.float32)
-        image_width = tf.cast(image_size[1], tf.float32)
+        image_height = image_size[0]
+        image_width = image_size[1]
 
-        aspect_ratios = tf.cast(self.aspect_ratios, tf.float32)
-        aspect_ratios_sqrt = tf.cast(tf.sqrt(aspect_ratios), dtype=tf.float32)
-        anchor_size = tf.cast(self.sizes, tf.float32)
+        aspect_ratios = ops.cast(self.aspect_ratios, "float32")
+        aspect_ratios_sqrt = ops.cast(ops.sqrt(aspect_ratios), dtype="float32")
+        anchor_size = ops.cast(self.sizes, "float32")
 
         # [K]
         anchor_heights = []
         anchor_widths = []
         for scale in self.scales:
             anchor_size_t = anchor_size * scale
             anchor_height = anchor_size_t / aspect_ratios_sqrt
             anchor_width = anchor_size_t * aspect_ratios_sqrt
             anchor_heights.append(anchor_height)
             anchor_widths.append(anchor_width)
-        anchor_heights = tf.concat(anchor_heights, axis=0)
-        anchor_widths = tf.concat(anchor_widths, axis=0)
-        half_anchor_heights = tf.reshape(0.5 * anchor_heights, [1, 1, -1])
-        half_anchor_widths = tf.reshape(0.5 * anchor_widths, [1, 1, -1])
+        anchor_heights = ops.concatenate(anchor_heights, axis=0)
+        anchor_widths = ops.concatenate(anchor_widths, axis=0)
+        half_anchor_heights = ops.reshape(0.5 * anchor_heights, [1, 1, -1])
+        half_anchor_widths = ops.reshape(0.5 * anchor_widths, [1, 1, -1])
 
-        stride = tf.cast(self.stride, tf.float32)
+        stride = self.stride
         # make sure range of `cx` is within limit of `image_width` with
         # `stride`, also for sizes where `image_width % stride != 0`.
         # [W]
-        cx = tf.range(
-            0.5 * stride, tf.math.ceil(image_width / stride) * stride, stride
+        cx = ops.cast(
+            ops.arange(
+                0.5 * stride, math.ceil(image_width / stride) * stride, stride
+            ),
+            "float32",
         )
         # make sure range of `cy` is within limit of `image_height` with
         # `stride`, also for sizes where `image_height % stride != 0`.
         # [H]
-        cy = tf.range(
-            0.5 * stride, tf.math.ceil(image_height / stride) * stride, stride
+        cy = ops.cast(
+            ops.arange(
+                0.5 * stride, math.ceil(image_height / stride) * stride, stride
+            ),
+            "float32",
         )
         # [H, W]
-        cx_grid, cy_grid = tf.meshgrid(cx, cy)
+        cx_grid, cy_grid = ops.meshgrid(cx, cy)
         # [H, W, 1]
-        cx_grid = tf.expand_dims(cx_grid, axis=-1)
-        cy_grid = tf.expand_dims(cy_grid, axis=-1)
+        cx_grid = ops.expand_dims(cx_grid, axis=-1)
+        cy_grid = ops.expand_dims(cy_grid, axis=-1)
 
-        y_min = tf.reshape(cy_grid - half_anchor_heights, (-1,))
-        y_max = tf.reshape(cy_grid + half_anchor_heights, (-1,))
-        x_min = tf.reshape(cx_grid - half_anchor_widths, (-1,))
-        x_max = tf.reshape(cx_grid + half_anchor_widths, (-1,))
+        y_min = ops.reshape(cy_grid - half_anchor_heights, (-1,))
+        y_max = ops.reshape(cy_grid + half_anchor_heights, (-1,))
+        x_min = ops.reshape(cx_grid - half_anchor_widths, (-1,))
+        x_max = ops.reshape(cx_grid + half_anchor_widths, (-1,))
 
         # [H * W * K, 1]
-        y_min = tf.expand_dims(y_min, axis=-1)
-        y_max = tf.expand_dims(y_max, axis=-1)
-        x_min = tf.expand_dims(x_min, axis=-1)
-        x_max = tf.expand_dims(x_max, axis=-1)
+        y_min = ops.expand_dims(y_min, axis=-1)
+        y_max = ops.expand_dims(y_max, axis=-1)
+        x_min = ops.expand_dims(x_min, axis=-1)
+        x_max = ops.expand_dims(x_max, axis=-1)
 
         if self.clip_boxes:
-            y_min = tf.maximum(tf.minimum(y_min, image_height), 0.0)
-            y_max = tf.maximum(tf.minimum(y_max, image_height), 0.0)
-            x_min = tf.maximum(tf.minimum(x_min, image_width), 0.0)
-            x_max = tf.maximum(tf.minimum(x_max, image_width), 0.0)
+            y_min = ops.maximum(ops.minimum(y_min, image_height), 0.0)
+            y_max = ops.maximum(ops.minimum(y_max, image_height), 0.0)
+            x_min = ops.maximum(ops.minimum(x_min, image_width), 0.0)
+            x_max = ops.maximum(ops.minimum(x_max, image_width), 0.0)
 
         # [H * W * K, 4]
-        return tf.cast(
-            tf.concat([y_min, x_min, y_max, x_max], axis=-1), self.dtype
+        return ops.cast(
+            ops.concatenate([y_min, x_min, y_max, x_max], axis=-1), self.dtype
         )
```

## keras_cv/layers/object_detection/anchor_generator_test.py

```diff
@@ -7,18 +7,20 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import layers as cv_layers
+from keras_cv.backend import ops
 
 
 class AnchorGeneratorTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
         ("unequal_lists", [0, 1, 2], [1]),
         ("unequal_levels_dicts", {"level_1": [0, 1, 2]}, {"1": [0, 1, 2]}),
     )
@@ -41,49 +43,49 @@
             bounding_box_format="xyxy",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
 
-        image = tf.random.uniform((4, 8, 8, 3))
+        image = np.random.uniform(size=(4, 8, 8, 3))
         with self.assertRaisesRegex(ValueError, "rank"):
             _ = anchor_generator(image=image)
 
     @parameterized.parameters(
         ((640, 480, 3),),
         ((512, 512, 3),),
         ((224, 224, 3),),
     )
     def test_output_shapes_image(self, image_shape):
         strides = [2**i for i in range(3, 8)]
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]
         aspect_ratios = [0.5, 1.0, 2.0]
 
-        image = tf.random.uniform(image_shape)
+        image = np.random.uniform(size=image_shape)
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="yxyx",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
         boxes = anchor_generator(image=image)
-        boxes = tf.concat(list(boxes.values()), axis=0)
+        boxes = ops.concatenate(list(boxes.values()), axis=0)
 
-        expected_box_shapes = tf.cast(
-            tf.math.ceil(image_shape[0] / tf.constant(strides))
-            * tf.math.ceil(image_shape[1] / tf.constant(strides))
+        expected_box_shapes = ops.cast(
+            ops.ceil(image_shape[0] / ops.array(strides))
+            * ops.ceil(image_shape[1] / ops.array(strides))
             * len(scales)
             * len(aspect_ratios),
-            tf.int32,
+            "int32",
         )
 
-        sum_expected_shape = (expected_box_shapes.numpy().sum(), 4)
+        sum_expected_shape = (ops.sum(expected_box_shapes), 4)
         self.assertEqual(boxes.shape, sum_expected_shape)
 
     @parameterized.parameters(
         ((640, 480, 3),),
         ((512, 512, 3),),
         ((224, 224, 3),),
     )
@@ -97,25 +99,25 @@
             bounding_box_format="yxyx",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
         boxes = anchor_generator(image_shape=image_shape)
-        boxes = tf.concat(list(boxes.values()), axis=0)
+        boxes = ops.concatenate(list(boxes.values()), axis=0)
 
-        expected_box_shapes = tf.cast(
-            tf.math.ceil(image_shape[0] / tf.constant(strides))
-            * tf.math.ceil(image_shape[1] / tf.constant(strides))
+        expected_box_shapes = ops.cast(
+            ops.ceil(image_shape[0] / ops.array(strides))
+            * ops.ceil(image_shape[1] / ops.array(strides))
             * len(scales)
             * len(aspect_ratios),
-            tf.int32,
+            "int32",
         )
 
-        sum_expected_shape = (expected_box_shapes.numpy().sum(), 4)
+        sum_expected_shape = (ops.sum(expected_box_shapes), 4)
         self.assertEqual(boxes.shape, sum_expected_shape)
 
     def test_hand_crafted_aspect_ratios(self):
         strides = [4]
         scales = [1.0]
         sizes = [4]
         aspect_ratios = [3 / 4, 1.0, 4 / 3]
@@ -123,22 +125,22 @@
             bounding_box_format="xyxy",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
 
-        image = tf.random.uniform((8, 8, 3))
+        image = np.random.uniform(size=(8, 8, 3))
         boxes = anchor_generator(image=image)
         level_0 = boxes[0]
 
         # width/4 * height/4 * aspect_ratios =
         self.assertAllEqual(level_0.shape, [12, 4])
 
-        image = tf.random.uniform((4, 4, 3))
+        image = np.random.uniform(size=(4, 4, 3))
         boxes = anchor_generator(image=image)
         level_0 = boxes[0]
 
         expected_boxes = [
             [0.267949224, -0.309401035, 3.7320509, 4.30940104],
             [0, 0, 4, 4],
             [-0.309401035, 0.267949104, 4.30940104, 3.7320509],
@@ -154,15 +156,15 @@
             bounding_box_format="xyxy",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
 
-        image = tf.random.uniform((8, 8, 3))
+        image = np.random.uniform(size=(8, 8, 3))
         boxes = anchor_generator(image=image)
         level_0 = boxes[0]
         expected_boxes = [
             [0, 0, 4, 4],
             [4, 0, 8, 4],
             [0, 4, 4, 8],
             [4, 4, 8, 8],
@@ -173,20 +175,20 @@
         strides = [8, 16, 32]
 
         # 0, 1 / 3, 2 / 3
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [32.0, 64.0, 128.0]
         aspect_ratios = [0.5, 1.0, 2.0]
 
-        image = tf.random.uniform((512, 512, 3))
+        image = np.random.uniform(size=(512, 512, 3))
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="rel_yxyx",
             sizes=sizes,
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
             clip_boxes=False,
         )
         boxes = anchor_generator(image=image)
-        boxes = tf.concat(list(boxes.values()), axis=0)
+        boxes = np.concatenate(list(boxes.values()), axis=0)
         self.assertAllLessEqual(boxes, 1.5)
         self.assertAllGreaterEqual(boxes, -0.50)
```

## keras_cv/layers/object_detection/box_matcher.py

```diff
@@ -9,21 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List
-from typing import Tuple
 
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class BoxMatcher(keras.layers.Layer):
     """Box matching logic based on argmax of highest value (e.g., IOU).
 
     This class computes matches from a similarity matrix. Each row will be
     matched to at least one column, the matched result can either be positive
      / negative, or simply ignored depending on the setting.
 
@@ -69,15 +68,15 @@
 
     Usage:
 
     ```python
     box_matcher = keras_cv.layers.BoxMatcher([0.3, 0.7], [-1, 0, 1])
     iou_metric = keras_cv.bounding_box.compute_iou(anchors, boxes)
     matched_columns, matched_match_values = box_matcher(iou_metric)
-    cls_mask = tf.less_equal(matched_match_values, 0)
+    cls_mask = ops.less_equal(matched_match_values, 0)
     ```
 
     TODO(tanzhenyu): document when to use which mode.
 
     """
 
     def __init__(
@@ -98,15 +97,15 @@
             )
         thresholds.insert(0, -float("inf"))
         thresholds.append(float("inf"))
         self.thresholds = thresholds
         self.force_match_for_each_col = force_match_for_each_col
         self.built = True
 
-    def call(self, similarity_matrix: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
+    def call(self, similarity_matrix):
         """Matches each row to a column based on argmax
 
         TODO(tanzhenyu): consider swapping rows and cols.
         Args:
           similarity_matrix: A float Tensor of shape [num_rows, num_cols] or
             [batch_size, num_rows, num_cols] representing any similarity metric.
 
@@ -116,143 +115,154 @@
           matched_values: An integer tensor of shape [num_rows] or [batch_size,
             num_rows] storing the match result (positive match, negative match,
             ignored match).
         """
         squeeze_result = False
         if len(similarity_matrix.shape) == 2:
             squeeze_result = True
-            similarity_matrix = tf.expand_dims(similarity_matrix, axis=0)
-        static_shape = similarity_matrix.shape.as_list()
-        num_rows = static_shape[1] or tf.shape(similarity_matrix)[1]
-        batch_size = static_shape[0] or tf.shape(similarity_matrix)[0]
+            similarity_matrix = ops.expand_dims(similarity_matrix, axis=0)
+        static_shape = list(similarity_matrix.shape)
+        num_rows = static_shape[1] or ops.shape(similarity_matrix)[1]
+        batch_size = static_shape[0] or ops.shape(similarity_matrix)[0]
 
         def _match_when_cols_are_empty():
             """Performs matching when the rows of similarity matrix are empty.
             When the rows are empty, all detections are false positives. So we
             return a tensor of -1's to indicate that the rows do not match to
             any columns.
             Returns:
                 matched_columns: An integer tensor of shape [batch_size,
                     num_rows] storing the index of the matched column for each
                     row.
                 matched_values: An integer tensor of shape [batch_size,
                     num_rows] storing the match type indicator (e.g. positive or
                     negative or ignored match).
             """
-            with tf.name_scope("empty_boxes"):
-                matched_columns = tf.zeros(
-                    [batch_size, num_rows], dtype=tf.int32
+            with ops.name_scope("empty_boxes"):
+                matched_columns = ops.zeros(
+                    [batch_size, num_rows], dtype="int32"
                 )
-                matched_values = -tf.ones(
-                    [batch_size, num_rows], dtype=tf.int32
+                matched_values = -ops.ones(
+                    [batch_size, num_rows], dtype="int32"
                 )
                 return matched_columns, matched_values
 
         def _match_when_cols_are_non_empty():
             """Performs matching when the rows of similarity matrix are
             non-empty.
             Returns:
                 matched_columns: An integer tensor of shape [batch_size,
                     num_rows] storing the index of the matched column for each
                     row.
                 matched_values: An integer tensor of shape [batch_size,
                     num_rows] storing the match type indicator (e.g. positive or
                     negative or ignored match).
             """
-            with tf.name_scope("non_empty_boxes"):
-                matched_columns = tf.argmax(
-                    similarity_matrix, axis=-1, output_type=tf.int32
+            with ops.name_scope("non_empty_boxes"):
+                # Jax traces this function even when running eagerly and the
+                # columns are non-empty. Therefore, we need to handle the case
+                # where the similarity matrix is empty. We do this by padding
+                # some -1s to the end. -1s are guaranteed to not affect argmax
+                # matching because all values in a similarity matrix are [0,1]
+                # and the indexing won't change because these are added at the
+                # end.
+                padded_similarity_matrix = ops.concatenate(
+                    [similarity_matrix, -ops.ones((batch_size, num_rows, 1))],
+                    axis=-1,
                 )
 
-                # Get logical indices of ignored and unmatched columns as
-                # tf.int64
-                matched_vals = tf.reduce_max(similarity_matrix, axis=-1)
-                matched_values = tf.zeros([batch_size, num_rows], tf.int32)
+                matched_columns = ops.argmax(
+                    padded_similarity_matrix,
+                    axis=-1,
+                )
+
+                # Get logical indices of ignored and unmatched columns as int32
+                matched_vals = ops.max(padded_similarity_matrix, axis=-1)
+                matched_values = ops.zeros([batch_size, num_rows], "int32")
 
                 match_dtype = matched_vals.dtype
                 for ind, low, high in zip(
                     self.match_values, self.thresholds[:-1], self.thresholds[1:]
                 ):
-                    low_threshold = tf.cast(low, match_dtype)
-                    high_threshold = tf.cast(high, match_dtype)
-                    mask = tf.logical_and(
-                        tf.greater_equal(matched_vals, low_threshold),
-                        tf.less(matched_vals, high_threshold),
+                    low_threshold = ops.cast(low, match_dtype)
+                    high_threshold = ops.cast(high, match_dtype)
+                    mask = ops.logical_and(
+                        ops.greater_equal(matched_vals, low_threshold),
+                        ops.less(matched_vals, high_threshold),
                     )
                     matched_values = self._set_values_using_indicator(
                         matched_values, mask, ind
                     )
 
                 if self.force_match_for_each_col:
                     # [batch_size, num_cols], for each column (groundtruth_box),
                     # find the best matching row (anchor).
-                    matching_rows = tf.argmax(
-                        input=similarity_matrix, axis=1, output_type=tf.int32
+                    matching_rows = ops.argmax(
+                        padded_similarity_matrix,
+                        axis=1,
                     )
                     # [batch_size, num_cols, num_rows], a transposed 0-1 mapping
                     # matrix M, where M[j, i] = 1 means column j is matched to
                     # row i.
-                    column_to_row_match_mapping = tf.one_hot(
-                        matching_rows, depth=num_rows
+                    column_to_row_match_mapping = ops.one_hot(
+                        matching_rows, num_rows
                     )
                     # [batch_size, num_rows], for each row (anchor), find the
                     # matched column (groundtruth_box).
-                    force_matched_columns = tf.argmax(
-                        input=column_to_row_match_mapping,
+                    force_matched_columns = ops.argmax(
+                        column_to_row_match_mapping,
                         axis=1,
-                        output_type=tf.int32,
                     )
                     # [batch_size, num_rows]
-                    force_matched_column_mask = tf.cast(
-                        tf.reduce_max(column_to_row_match_mapping, axis=1),
-                        tf.bool,
+                    force_matched_column_mask = ops.cast(
+                        ops.max(column_to_row_match_mapping, axis=1),
+                        "bool",
                     )
                     # [batch_size, num_rows]
-                    matched_columns = tf.where(
+                    matched_columns = ops.where(
                         force_matched_column_mask,
                         force_matched_columns,
                         matched_columns,
                     )
-                    matched_values = tf.where(
+                    matched_values = ops.where(
                         force_matched_column_mask,
                         self.match_values[-1]
-                        * tf.ones([batch_size, num_rows], dtype=tf.int32),
+                        * ops.ones([batch_size, num_rows], dtype="int32"),
                         matched_values,
                     )
 
-                return matched_columns, matched_values
+                return ops.cast(matched_columns, "int32"), matched_values
 
         num_boxes = (
-            similarity_matrix.shape.as_list()[-1]
-            or tf.shape(similarity_matrix)[-1]
+            similarity_matrix.shape[-1] or ops.shape(similarity_matrix)[-1]
         )
-        matched_columns, matched_values = tf.cond(
-            pred=tf.greater(num_boxes, 0),
+        matched_columns, matched_values = ops.cond(
+            pred=ops.greater(num_boxes, 0),
             true_fn=_match_when_cols_are_non_empty,
             false_fn=_match_when_cols_are_empty,
         )
 
         if squeeze_result:
-            matched_columns = tf.squeeze(matched_columns, axis=0)
-            matched_values = tf.squeeze(matched_values, axis=0)
+            matched_columns = ops.squeeze(matched_columns, axis=0)
+            matched_values = ops.squeeze(matched_values, axis=0)
 
         return matched_columns, matched_values
 
     def _set_values_using_indicator(self, x, indicator, val):
         """Set the indicated fields of x to val.
 
         Args:
           x: tensor.
           indicator: boolean with same shape as x.
           val: scalar with value to set.
         Returns:
           modified tensor.
         """
-        indicator = tf.cast(indicator, x.dtype)
-        return tf.add(tf.multiply(x, 1 - indicator), val * indicator)
+        indicator = ops.cast(indicator, x.dtype)
+        return ops.add(ops.multiply(x, 1 - indicator), val * indicator)
 
     def get_config(self):
         config = {
             "thresholds": self.thresholds[1:-1],
             "match_values": self.match_values,
             "force_match_for_each_col": self.force_match_for_each_col,
         }
```

## keras_cv/layers/object_detection/box_matcher_test.py

```diff
@@ -8,16 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
+from keras_cv.backend import ops
 from keras_cv.layers.object_detection.box_matcher import BoxMatcher
 
 
 class BoxMatcherTest(tf.test.TestCase):
     def test_box_matcher_invalid_length(self):
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
@@ -37,97 +39,90 @@
         with self.assertRaisesRegex(ValueError, "must be sorted"):
             _ = BoxMatcher(
                 thresholds=[bg_thresh_hi, bg_thresh_lo, fg_threshold],
                 match_values=[-3, -2, -1, 1],
             )
 
     def test_box_matcher_unbatched(self):
-        sim_matrix = tf.constant(
-            [[0.04, 0, 0, 0], [0, 0, 1.0, 0]], dtype=tf.float32
-        )
+        sim_matrix = np.array([[0.04, 0, 0, 0], [0, 0, 1.0, 0]])
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
             match_values=[-3, -2, -1, 1],
         )
         match_indices, matched_values = matcher(sim_matrix)
-        positive_matches = tf.greater_equal(matched_values, 0)
-        negative_matches = tf.equal(matched_values, -2)
+        positive_matches = ops.greater_equal(matched_values, 0)
+        negative_matches = ops.equal(matched_values, -2)
 
-        self.assertAllEqual(positive_matches.numpy(), [False, True])
-        self.assertAllEqual(negative_matches.numpy(), [True, False])
-        self.assertAllEqual(match_indices.numpy(), [0, 2])
-        self.assertAllEqual(matched_values.numpy(), [-2, 1])
+        self.assertAllEqual(positive_matches, [False, True])
+        self.assertAllEqual(negative_matches, [True, False])
+        self.assertAllEqual(match_indices, [0, 2])
+        self.assertAllEqual(matched_values, [-2, 1])
 
     def test_box_matcher_batched(self):
-        sim_matrix = tf.constant(
-            [[[0.04, 0, 0, 0], [0, 0, 1.0, 0]]], dtype=tf.float32
-        )
+        sim_matrix = np.array([[[0.04, 0, 0, 0], [0, 0, 1.0, 0]]])
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
             match_values=[-3, -2, -1, 1],
         )
         match_indices, matched_values = matcher(sim_matrix)
-        positive_matches = tf.greater_equal(matched_values, 0)
-        negative_matches = tf.equal(matched_values, -2)
+        positive_matches = ops.greater_equal(matched_values, 0)
+        negative_matches = ops.equal(matched_values, -2)
 
-        self.assertAllEqual(positive_matches.numpy(), [[False, True]])
-        self.assertAllEqual(negative_matches.numpy(), [[True, False]])
-        self.assertAllEqual(match_indices.numpy(), [[0, 2]])
-        self.assertAllEqual(matched_values.numpy(), [[-2, 1]])
+        self.assertAllEqual(positive_matches, [[False, True]])
+        self.assertAllEqual(negative_matches, [[True, False]])
+        self.assertAllEqual(match_indices, [[0, 2]])
+        self.assertAllEqual(matched_values, [[-2, 1]])
 
     def test_box_matcher_force_match(self):
-        sim_matrix = tf.constant(
+        sim_matrix = np.array(
             [[0, 0.04, 0, 0.1], [0, 0, 1.0, 0], [0.1, 0, 0, 0], [0, 0, 0, 0.6]],
-            dtype=tf.float32,
         )
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
             match_values=[-3, -2, -1, 1],
             force_match_for_each_col=True,
         )
         match_indices, matched_values = matcher(sim_matrix)
-        positive_matches = tf.greater_equal(matched_values, 0)
-        negative_matches = tf.equal(matched_values, -2)
+        positive_matches = ops.greater_equal(matched_values, 0)
+        negative_matches = ops.equal(matched_values, -2)
 
-        self.assertAllEqual(positive_matches.numpy(), [True, True, True, True])
-        self.assertAllEqual(
-            negative_matches.numpy(), [False, False, False, False]
-        )
+        self.assertAllEqual(positive_matches, [True, True, True, True])
+        self.assertAllEqual(negative_matches, [False, False, False, False])
         # the first anchor cannot be matched to 4th gt box given that is matched
         # to the last anchor.
-        self.assertAllEqual(match_indices.numpy(), [1, 2, 0, 3])
-        self.assertAllEqual(matched_values.numpy(), [1, 1, 1, 1])
+        self.assertAllEqual(match_indices, [1, 2, 0, 3])
+        self.assertAllEqual(matched_values, [1, 1, 1, 1])
 
     def test_box_matcher_empty_gt_boxes(self):
-        sim_matrix = tf.constant([[], []], dtype=tf.float32)
+        sim_matrix = np.array([[], []])
 
         fg_threshold = 0.5
         bg_thresh_hi = 0.2
         bg_thresh_lo = 0.0
 
         matcher = BoxMatcher(
             thresholds=[bg_thresh_lo, bg_thresh_hi, fg_threshold],
             match_values=[-3, -2, -1, 1],
         )
         match_indices, matched_values = matcher(sim_matrix)
-        positive_matches = tf.greater_equal(matched_values, 0)
-        ignore_matches = tf.equal(matched_values, -1)
+        positive_matches = ops.greater_equal(matched_values, 0)
+        ignore_matches = ops.equal(matched_values, -1)
 
-        self.assertAllEqual(positive_matches.numpy(), [False, False])
-        self.assertAllEqual(ignore_matches.numpy(), [True, True])
-        self.assertAllEqual(match_indices.numpy(), [0, 0])
-        self.assertAllEqual(matched_values.numpy(), [-1, -1])
+        self.assertAllEqual(positive_matches, [False, False])
+        self.assertAllEqual(ignore_matches, [True, True])
+        self.assertAllEqual(match_indices, [0, 0])
+        self.assertAllEqual(matched_values, [-1, -1])
```

## keras_cv/layers/object_detection/multi_class_non_max_suppression.py

```diff
@@ -9,21 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
+from keras_cv.backend.config import multi_backend
 
 
-# TODO(tanzhenyu): provide a TPU compatible NMS decoder.
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class MultiClassNonMaxSuppression(keras.layers.Layer):
     """A Keras layer that decodes predictions of an object detection model.
 
     Arguments:
       bounding_box_format: The format of bounding boxes of input dataset. Refer
         [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
         for more details on supported bounding box
@@ -67,29 +68,35 @@
         predictions.
 
         Args:
             box_prediction: Dense Tensor of shape [batch, boxes, 4] in the
                 `bounding_box_format` specified in the constructor.
             class_prediction: Dense Tensor of shape [batch, boxes, num_classes].
         """
+        if multi_backend() and keras.backend.backend() != "tensorflow":
+            raise NotImplementedError(
+                "MultiClassNonMaxSuppression does not support non-TensorFlow "
+                "backends. Consider using NonMaxSuppression instead."
+            )
+
         target_format = "yxyx"
         if bounding_box.is_relative(self.bounding_box_format):
             target_format = bounding_box.as_relative(target_format)
 
         box_prediction = bounding_box.convert_format(
             box_prediction,
             source=self.bounding_box_format,
             target=target_format,
             images=images,
             image_shape=image_shape,
         )
         if self.from_logits:
-            class_prediction = tf.math.sigmoid(class_prediction)
+            class_prediction = ops.sigmoid(class_prediction)
 
-        box_prediction = tf.expand_dims(box_prediction, axis=-2)
+        box_prediction = ops.expand_dims(box_prediction, axis=-2)
         (
             box_prediction,
             confidence_prediction,
             class_prediction,
             valid_det,
         ) = tf.image.combined_non_max_suppression(
             boxes=box_prediction,
```

## keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv import layers as cv_layers
 
 
 def decode_predictions_output_shapes():
     num_classes = 10
@@ -37,13 +38,14 @@
         max_detections=100,
     )
 
     result = layer(box_prediction=box_pred, class_prediction=class_prediction)
     return result
 
 
+@pytest.mark.tf_keras_only
 class NmsPredictionDecoderTest(tf.test.TestCase):
     def test_decode_predictions_output_shapes(self):
         result = decode_predictions_output_shapes()
         self.assertEqual(result["boxes"].shape, [8, None, 4])
         self.assertEqual(result["classes"].shape, [8, None])
         self.assertEqual(result["confidence"].shape, [8, None])
```

## keras_cv/layers/object_detection/roi_align.py

```diff
@@ -17,14 +17,15 @@
 from typing import Optional
 from typing import Tuple
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import assert_tf_keras
 
 
 def _feature_bilinear_interpolation(
     features: tf.Tensor, kernel_y: tf.Tensor, kernel_x: tf.Tensor
 ) -> tf.Tensor:
     """
     Feature bilinear interpolation.
@@ -390,14 +391,15 @@
 
         Args:
           bounding_box_format: the input format for boxes.
           crop_size: An `int` of the output size of the cropped features.
           sample_offset: A `float` in [0, 1] of the subpixel sample offset.
           **kwargs: Additional keyword arguments passed to Layer.
         """
+        assert_tf_keras("keras_cv.layers._ROIAligner")
         self._config_dict = {
             "bounding_box_format": bounding_box_format,
             "crop_size": target_size,
             "sample_offset": sample_offset,
         }
         super().__init__(**kwargs)
```

## keras_cv/layers/object_detection/roi_generator.py

```diff
@@ -17,14 +17,15 @@
 from typing import Tuple
 from typing import Union
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import assert_tf_keras
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
 class ROIGenerator(keras.layers.Layer):
     """
     Generates region of interests (ROI, or proposal) from scores.
 
@@ -91,14 +92,15 @@
         post_nms_topk_train: int = 1000,
         pre_nms_topk_test: int = 1000,
         nms_score_threshold_test: float = 0.0,
         nms_iou_threshold_test: float = 0.7,
         post_nms_topk_test: int = 1000,
         **kwargs,
     ):
+        assert_tf_keras("keras_cv.layers.ROIGenerator")
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.pre_nms_topk_train = pre_nms_topk_train
         self.nms_score_threshold_train = nms_score_threshold_train
         self.nms_iou_threshold_train = nms_iou_threshold_train
         self.post_nms_topk_train = post_nms_topk_train
         self.pre_nms_topk_test = pre_nms_topk_test
```

## keras_cv/layers/object_detection/roi_generator_test.py

```diff
@@ -8,19 +8,21 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv.layers.object_detection.roi_generator import ROIGenerator
 
 
+@pytest.mark.tf_keras_only
 class ROIGeneratorTest(tf.test.TestCase):
     def test_single_tensor(self):
         roi_generator = ROIGenerator("xyxy", nms_iou_threshold_train=0.96)
         rpn_boxes = tf.constant(
             [
                 [
                     [0, 0, 10, 10],
```

## keras_cv/layers/object_detection/roi_pool.py

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import assert_tf_keras
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
 class ROIPooler(keras.layers.Layer):
     """
     Pooling feature map of dynamic shape into region of interest (ROI) of fixed
     shape.
@@ -53,14 +54,15 @@
         self,
         bounding_box_format,
         # TODO(consolidate size vs shape for KPL and here)
         target_size,
         image_shape,
         **kwargs,
     ):
+        assert_tf_keras("keras_cv.layers.ROIPooler")
         if not isinstance(target_size, (tuple, list)):
             raise ValueError(
                 "Expected `target_size` to be tuple or list, got "
                 f"{type(target_size)}"
             )
         if len(target_size) != 2:
             raise ValueError(
```

## keras_cv/layers/object_detection/roi_pool_test.py

```diff
@@ -8,19 +8,21 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv.layers.object_detection.roi_pool import ROIPooler
 
 
+@pytest.mark.tf_keras_only
 class ROIPoolTest(tf.test.TestCase):
     def test_no_quantize(self):
         roi_pooler = ROIPooler(
             "rel_yxyx", target_size=[2, 2], image_shape=[224, 224, 3]
         )
         feature_map = tf.expand_dims(
             tf.reshape(tf.range(64), [8, 8, 1]), axis=0
```

## keras_cv/layers/object_detection/roi_sampler.py

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import assert_tf_keras
 from keras_cv.bounding_box import iou
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.layers.object_detection import sampling
 from keras_cv.utils import target_gather
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
@@ -64,14 +65,15 @@
         roi_matcher: box_matcher.BoxMatcher,
         positive_fraction: float = 0.25,
         background_class: int = 0,
         num_sampled_rois: int = 256,
         append_gt_boxes: bool = True,
         **kwargs,
     ):
+        assert_tf_keras("keras_cv.layers._ROISampler")
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.roi_matcher = roi_matcher
         self.positive_fraction = positive_fraction
         self.background_class = background_class
         self.num_sampled_rois = num_sampled_rois
         self.append_gt_boxes = append_gt_boxes
```

## keras_cv/layers/object_detection/roi_sampler_test.py

```diff
@@ -8,20 +8,22 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv.layers.object_detection.box_matcher import BoxMatcher
 from keras_cv.layers.object_detection.roi_sampler import _ROISampler
 
 
+@pytest.mark.tf_keras_only
 class ROISamplerTest(tf.test.TestCase):
     def test_roi_sampler(self):
         box_matcher = BoxMatcher(thresholds=[0.3], match_values=[-1, 1])
         roi_sampler = _ROISampler(
             bounding_box_format="xyxy",
             roi_matcher=box_matcher,
             positive_fraction=0.5,
```

## keras_cv/layers/object_detection/rpn_label_encoder.py

```diff
@@ -14,14 +14,15 @@
 
 from typing import Mapping
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import assert_tf_keras
 from keras_cv.bounding_box import iou
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.layers.object_detection import sampling
 from keras_cv.utils import target_gather
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
@@ -68,14 +69,15 @@
         positive_threshold,
         negative_threshold,
         samples_per_image,
         positive_fraction,
         box_variance=[0.1, 0.1, 0.2, 0.2],
         **kwargs,
     ):
+        assert_tf_keras("keras_cv.layers._RpnLabelEncoder")
         super().__init__(**kwargs)
         self.anchor_format = anchor_format
         self.ground_truth_box_format = ground_truth_box_format
         self.positive_threshold = positive_threshold
         self.negative_threshold = negative_threshold
         self.samples_per_image = samples_per_image
         self.positive_fraction = positive_fraction
```

## keras_cv/layers/object_detection/rpn_label_encoder_test.py

```diff
@@ -8,19 +8,21 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv.layers.object_detection.rpn_label_encoder import _RpnLabelEncoder
 
 
+@pytest.mark.tf_keras_only
 class RpnLabelEncoderTest(tf.test.TestCase):
     def test_rpn_label_encoder(self):
         rpn_encoder = _RpnLabelEncoder(
             anchor_format="xyxy",
             ground_truth_box_format="xyxy",
             positive_threshold=0.7,
             negative_threshold=0.3,
```

## keras_cv/layers/object_detection/sampling_test.py

```diff
@@ -8,19 +8,21 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 
 from keras_cv.layers.object_detection.sampling import balanced_sample
 
 
+@pytest.mark.tf_keras_only
 class BalancedSamplingTest(tf.test.TestCase):
     def test_balanced_sampling(self):
         positive_matches = tf.constant(
             [
                 True,
                 False,
                 False,
```

## keras_cv/layers/preprocessing/aug_mix.py

```diff
@@ -9,24 +9,24 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import layers
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class AugMix(BaseImageAugmentationLayer):
     """Performs the AugMix data augmentation technique.
 
     AugMix aims to produce images with variety while preserving the image
     semantics and local statistics. During the augmentation process, each image
     is augmented `num_chains` different ways, each way consisting of
     `chain_depth` augmentations. Augmentations are sampled from the list:
```

## keras_cv/layers/preprocessing/base_image_augmentation_layer.py

```diff
@@ -9,17 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
+from keras.src import backend as keras_backend
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import scope
+from keras_cv.backend.config import multi_backend
 from keras_cv.utils import preprocessing
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
@@ -29,16 +32,23 @@
 BOUNDING_BOXES = "bounding_boxes"
 KEYPOINTS = "keypoints"
 SEGMENTATION_MASKS = "segmentation_masks"
 IS_DICT = "is_dict"
 USE_TARGETS = "use_targets"
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class BaseImageAugmentationLayer(keras.__internal__.layers.BaseRandomLayer):
+base_class = (
+    keras.src.layers.preprocessing.tf_data_layer.TFDataLayer
+    if multi_backend()
+    else keras.layers.Layer
+)
+
+
+@keras.saving.register_keras_serializable(package="keras_cv")
+class BaseImageAugmentationLayer(base_class):
     """Abstract base layer for image augmentation.
 
     This layer contains base functionalities for preprocessing layers which
     augment image related data, e.g. image and in the future, label and bounding
     boxes. The subclasses could avoid making certain mistakes and reduce code
     duplications.
 
@@ -110,15 +120,20 @@
     Note that since the randomness is also a common functionality, this layer
     also includes a keras.backend.RandomGenerator, which can be used to
     produce the random numbers. The random number generator is stored in the
     `self._random_generator` attribute.
     """
 
     def __init__(self, seed=None, **kwargs):
-        super().__init__(seed=seed, **kwargs)
+        force_generator = kwargs.pop("force_generator", False)
+        self._random_generator = keras_backend.RandomGenerator(
+            seed=seed, force_generator=force_generator
+        )
+        super().__init__(**kwargs)
+        self._convert_input_args = False
 
     @property
     def force_output_ragged_images(self):
         """Control whether to force outputting of ragged images."""
         return getattr(self, "_force_output_ragged_images", False)
 
     @force_output_ragged_images.setter
@@ -387,27 +402,31 @@
           Any type of object, which will be forwarded to `augment_image`,
           `augment_label` and `augment_bounding_box` as the `transformation`
           parameter.
         """
         return None
 
     def call(self, inputs):
-        inputs = self._ensure_inputs_are_compute_dtype(inputs)
-        inputs, metadata = self._format_inputs(inputs)
-        images = inputs[IMAGES]
-        if images.shape.rank == 3:
-            return self._format_output(self._augment(inputs), metadata)
-        elif images.shape.rank == 4:
-            return self._format_output(self._batch_augment(inputs), metadata)
-        else:
-            raise ValueError(
-                "Image augmentation layers are expecting inputs to be "
-                "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
-                f"{images.shape}"
-            )
+        with scope.TFDataScope():
+            inputs = self._ensure_inputs_are_compute_dtype(inputs)
+            inputs, metadata = self._format_inputs(inputs)
+            images = inputs[IMAGES]
+            if images.shape.rank == 3:
+                outputs = self._format_output(self._augment(inputs), metadata)
+            elif images.shape.rank == 4:
+                outputs = self._format_output(
+                    self._batch_augment(inputs), metadata
+                )
+            else:
+                raise ValueError(
+                    "Image augmentation layers are expecting inputs to be "
+                    "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
+                    f"{images.shape}"
+                )
+            return outputs
 
     def _augment(self, inputs):
         raw_image = inputs.get(IMAGES, None)
         image = raw_image
         label = inputs.get(LABELS, None)
         bounding_boxes = inputs.get(BOUNDING_BOXES, None)
         keypoints = inputs.get(KEYPOINTS, None)
@@ -494,15 +513,15 @@
             metadata[IS_DICT] = False
             inputs = {IMAGES: inputs}
             return inputs, metadata
 
         if not isinstance(inputs, dict):
             raise ValueError(
                 "Expect the inputs to be image tensor or dict. Got "
-                f"inputs={inputs}"
+                f"inputs={inputs} of type {type(inputs)}"
             )
 
         if BOUNDING_BOXES in inputs:
             inputs[BOUNDING_BOXES] = self._format_bounding_boxes(
                 inputs[BOUNDING_BOXES]
             )
```

## keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py

```diff
@@ -122,16 +122,16 @@
         # Make sure the first image and second image get different augmentation
         self.assertNotAllClose(image_diff[0], image_diff[1])
         self.assertNotAllClose(label_diff[0], label_diff[1])
 
     def test_augment_leaves_extra_dict_entries_unmodified(self):
         add_layer = RandomAddLayer(fixed_value=0.5)
         images = np.random.random(size=(8, 8, 3)).astype("float32")
-        filenames = tf.constant("/path/to/first.jpg")
-        inputs = {"images": images, "filenames": filenames}
+        image_timestamp = np.array(123123123)
+        inputs = {"images": images, "image_timestamp": image_timestamp}
         _ = add_layer(inputs)
 
     def test_augment_ragged_images(self):
         images = tf.ragged.stack(
             [
                 np.random.random(size=(8, 8, 3)).astype("float32"),
                 np.random.random(size=(16, 8, 3)).astype("float32"),
@@ -206,40 +206,14 @@
             {
                 "images": images,
                 "bounding_boxes": bounding_boxes,
                 "keypoints": keypoints,
                 "segmentation_masks": segmentation_masks,
             }
         )
-
-        bounding_boxes_diff = (
-            output["bounding_boxes"]["boxes"] - bounding_boxes["boxes"]
-        )
-        keypoints_diff = output["keypoints"] - keypoints
-        segmentation_mask_diff = (
-            output["segmentation_masks"] - segmentation_masks
-        )
-        self.assertNotAllClose(bounding_boxes_diff[0], bounding_boxes_diff[1])
-        self.assertNotAllClose(keypoints_diff[0], keypoints_diff[1])
-        self.assertNotAllClose(
-            segmentation_mask_diff[0], segmentation_mask_diff[1]
-        )
-
-        @tf.function
-        def in_tf_function(inputs):
-            return add_layer(inputs)
-
-        output = in_tf_function(
-            {
-                "images": images,
-                "bounding_boxes": bounding_boxes,
-                "keypoints": keypoints,
-                "segmentation_masks": segmentation_masks,
-            }
-        )
 
         bounding_boxes_diff = (
             output["bounding_boxes"]["boxes"] - bounding_boxes["boxes"]
         )
         keypoints_diff = output["keypoints"] - keypoints
         segmentation_mask_diff = (
             output["segmentation_masks"] - segmentation_masks
```

## keras_cv/layers/preprocessing/cut_mix.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class CutMix(BaseImageAugmentationLayer):
     """CutMix implements the CutMix data augmentation technique.
 
     Args:
         alpha: Float between 0 and 1. Inverse scale parameter for the gamma
             distribution. This controls the shape of the distribution from which
             the smoothing values are sampled. Defaults to 1.0, which is a
@@ -123,15 +123,20 @@
             random_center_width,
             random_center_height,
             cut_width,
             cut_height,
             tf.gather(images, permutation_order),
         )
 
-        return images, labels, lambda_sample, permutation_order
+        return (
+            images,
+            tf.cast(labels, dtype=self.compute_dtype),
+            lambda_sample,
+            permutation_order,
+        )
 
     def _update_labels(self, images, labels, lambda_sample, permutation_order):
         cutout_labels = tf.gather(labels, permutation_order)
 
         lambda_sample = tf.reshape(lambda_sample, [-1, 1])
         labels = lambda_sample * labels + (1.0 - lambda_sample) * cutout_labels
         return images, labels
```

## keras_cv/layers/preprocessing/equalization.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Equalization(BaseImageAugmentationLayer):
     """Equalization performs histogram equalization on a channel-wise basis.
 
     Args:
         value_range: a tuple or a list of two elements. The first value
             represents the lower bound for values in passed images, the second
             represents the upper bound. Images passed to the layer should have
```

## keras_cv/layers/preprocessing/equalization_test.py

```diff
@@ -8,37 +8,39 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.equalization import Equalization
 
 
 class EqualizationTest(tf.test.TestCase, parameterized.TestCase):
     def test_return_shapes(self):
         xs = 255 * tf.ones((2, 512, 512, 3), dtype=tf.int32)
         layer = Equalization(value_range=(0, 255))
         xs = layer(xs)
 
         self.assertEqual(xs.shape, [2, 512, 512, 3])
         self.assertAllEqual(xs, 255 * tf.ones((2, 512, 512, 3)))
 
+    @pytest.mark.tf_keras_only
     def test_return_shapes_inside_model(self):
         layer = Equalization(value_range=(0, 255))
         inp = keras.layers.Input(shape=[512, 512, 5])
         out = layer(inp)
         model = keras.models.Model(inp, out)
 
-        self.assertEqual(model.layers[-1].output_shape, (None, 512, 512, 5))
+        self.assertEqual(model.output_shape, (None, 512, 512, 5))
 
     def test_equalizes_to_all_bins(self):
         xs = tf.random.uniform((2, 512, 512, 3), 0, 255, dtype=tf.float32)
         layer = Equalization(value_range=(0, 255))
         xs = layer(xs)
 
         for i in range(0, 256):
```

## keras_cv/layers/preprocessing/fourier_mix.py

```diff
@@ -9,22 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class FourierMix(BaseImageAugmentationLayer):
     """FourierMix implements the FMix data augmentation technique.
 
     Args:
         alpha: Float value for beta distribution. Inverse scale parameter for
             the gamma distribution. This controls the shape of the distribution
             from which the smoothing values are sampled. Defaults to 0.5, which
```

## keras_cv/layers/preprocessing/grayscale.py

```diff
@@ -9,22 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Grayscale(VectorizedBaseImageAugmentationLayer):
     """Grayscale is a preprocessing layer that transforms RGB images to
     Grayscale images.
     Input images should have values in the range of [0, 255].
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
```

## keras_cv/layers/preprocessing/grid_mask.py

```diff
@@ -9,18 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import layers
 
 from keras_cv import core
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
 from keras_cv.utils import preprocessing
 
 
@@ -30,15 +29,15 @@
     w_diff = masks_shape[1] - width
 
     h_start = tf.cast(h_diff / 2, tf.int32)
     w_start = tf.cast(w_diff / 2, tf.int32)
     return tf.image.crop_to_bounding_box(mask, h_start, w_start, height, width)
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class GridMask(BaseImageAugmentationLayer):
     """GridMask class for grid-mask augmentation.
 
 
     Input shape:
         Int or float tensor with values in the range [0, 255].
         3D (unbatched) or 4D (batched) tensor with shape:
@@ -114,15 +113,15 @@
                 "release. For now, please pass a float for the "
                 "`rotation_factor` argument."
             )
 
         self.fill_mode = fill_mode
         self.fill_value = fill_value
         self.rotation_factor = rotation_factor
-        self.random_rotate = layers.RandomRotation(
+        self.random_rotate = keras.layers.RandomRotation(
             factor=rotation_factor,
             fill_mode="constant",
             fill_value=0.0,
             seed=seed,
         )
         self.auto_vectorize = False
         self._check_parameter_values()
```

## keras_cv/layers/preprocessing/jittered_resize.py

```diff
@@ -13,27 +13,27 @@
 # limitations under the License.
 #
 # Some code in this file was inspired & adapted from `tensorflow_models`.
 # Reference:
 # https://github.com/tensorflow/models/blob/master/official/vision/ops/preprocess_ops.py
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class JitteredResize(VectorizedBaseImageAugmentationLayer):
     """JitteredResize implements resize with scale distortion.
 
     JitteredResize takes a three-step approach to size-distortion based image
     augmentation. This technique is specifically tuned for object detection
     pipelines. The layer takes an input of images and bounding boxes, both of
     which may be ragged. It outputs a dense image tensor, ready to feed to a
```

## keras_cv/layers/preprocessing/mix_up.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class MixUp(BaseImageAugmentationLayer):
     """MixUp implements the MixUp data augmentation technique.
 
     Args:
         alpha: Float between 0 and 1. Inverse scale parameter for the gamma
             distribution. This controls the shape of the distribution from which
             the smoothing values are sampled. Defaults to 0.2, which is a
@@ -69,15 +69,17 @@
         images = inputs.get("images", None)
         labels = inputs.get("labels", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
         segmentation_masks = inputs.get("segmentation_masks", None)
         images, lambda_sample, permutation_order = self._mixup(images)
         if labels is not None:
             labels = self._update_labels(
-                labels, lambda_sample, permutation_order
+                tf.cast(labels, dtype=self.compute_dtype),
+                lambda_sample,
+                permutation_order,
             )
             inputs["labels"] = labels
         if bounding_boxes is not None:
             bounding_boxes = self._update_bounding_boxes(
                 bounding_boxes, permutation_order
             )
             inputs["bounding_boxes"] = bounding_boxes
```

## keras_cv/layers/preprocessing/mosaic.py

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     BATCHED,
 )
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     BOUNDING_BOXES,
 )
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
@@ -30,15 +30,15 @@
 )
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Mosaic(VectorizedBaseImageAugmentationLayer):
     """Mosaic implements the mosaic data augmentation technique.
 
     Mosaic data augmentation first takes 4 images from the batch and makes a
     grid. After that based on the offset, a crop is taken to form the mosaic
     image. Labels are in the same ratio as the area of their images in the
     output image. Bounding boxes are translated according to the position of the
```

## keras_cv/layers/preprocessing/mosaic_test.py

```diff
@@ -92,24 +92,14 @@
         inputs = {"images": xs, "labels": ys}
         layer = Mosaic()
         with self.assertRaisesRegexp(
             ValueError, "Mosaic received a single image to `call`"
         ):
             _ = layer(inputs)
 
-    def test_int_labels(self):
-        xs = tf.ones((2, 512, 512, 3))
-        ys = tf.one_hot(tf.constant([1, 0]), 2, dtype=tf.int32)
-        inputs = {"images": xs, "labels": ys}
-        layer = Mosaic()
-        with self.assertRaisesRegexp(
-            ValueError, "Mosaic received labels with type"
-        ):
-            _ = layer(inputs)
-
     def test_image_input(self):
         xs = tf.ones((2, 512, 512, 3))
         layer = Mosaic()
         with self.assertRaisesRegexp(
             ValueError, "Mosaic expects inputs in a dictionary with format"
         ):
             _ = layer(xs)
```

## keras_cv/layers/preprocessing/posterization.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils.preprocessing import transform_value_range
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Posterization(BaseImageAugmentationLayer):
     """Reduces the number of bits for each color channel.
 
     References:
     - [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501)
     - [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719)
```

## keras_cv/layers/preprocessing/rand_augment.py

```diff
@@ -8,25 +8,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
 from keras_cv import core
+from keras_cv.backend import keras
 from keras_cv.layers import preprocessing as cv_preprocessing
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandAugment(RandomAugmentationPipeline):
     """RandAugment performs the Rand Augment operation on input images.
 
     This layer can be thought of as an all-in-one image augmentation layer. The
     policy implemented by this layer has been benchmarked extensively and is
     effective on a wide variety of datasets.
```

## keras_cv/layers/preprocessing/random_apply.py

```diff
@@ -8,22 +8,21 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomApply(BaseImageAugmentationLayer):
     """Apply provided layer to random elements in a batch.
 
     Args:
         layer: a keras `Layer` or `BaseImageAugmentationLayer`. This layer will
             be applied to randomly chosen samples in a batch. Layer should not
             modify the size of provided inputs.
@@ -106,14 +105,15 @@
             )
 
         self._layer = layer
         self._rate = rate
         self.auto_vectorize = auto_vectorize
         self.batchwise = batchwise
         self.seed = seed
+        self.built = True
 
     def _should_augment(self):
         return (
             self._random_generator.random_uniform(shape=()) > 1.0 - self._rate
         )
 
     def _batch_augment(self, inputs):
```

## keras_cv/layers/preprocessing/random_apply_test.py

```diff
@@ -10,17 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
 from keras_cv import layers
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.layers.preprocessing.random_apply import RandomApply
 
 
 class ZeroOut(BaseImageAugmentationLayer):
```

## keras_cv/layers/preprocessing/random_aspect_ratio.py

```diff
@@ -9,24 +9,24 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomAspectRatio(BaseImageAugmentationLayer):
     """RandomAspectRatio randomly distorts the aspect ratio of the provided
     image.
 
     This is done on an element-wise basis, and as a consequence this layer
     always returns a tf.RaggedTensor.
```

## keras_cv/layers/preprocessing/random_augmentation_pipeline.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers import preprocessing
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomAugmentationPipeline(BaseImageAugmentationLayer):
     """RandomAugmentationPipeline constructs a pipeline based on provided
     arguments.
 
     The implemented policy does the following: for each input provided in
     `call`(), the policy first inputs a random number, if the number is < rate,
     the policy then selects a random layer from the provided list of `layers`.
```

## keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py

```diff
@@ -8,19 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
 from keras_cv import layers
+from keras_cv.backend import keras
 
 
 class AddOneToInputs(keras.layers.Layer):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
     def call(self, inputs):
@@ -50,14 +51,15 @@
             layers=[], augmentations_per_image=1, rate=1.0
         )
         xs = tf.random.uniform((2, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs, os)
 
+    @pytest.mark.tf_keras_only
     def test_calls_layers_augmentations_in_graph(self):
         layer = AddOneToInputs()
         pipeline = layers.RandomAugmentationPipeline(
             layers=[layer], augmentations_per_image=3, rate=1.0
         )
 
         @tf.function()
```

## keras_cv/layers/preprocessing/random_brightness.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomBrightness(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly adjusts brightness.
 
     This layer will randomly increase/reduce the brightness for the input RGB
     images.
 
     Note that different brightness adjustment factors
```

## keras_cv/layers/preprocessing/random_channel_shift.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomChannelShift(BaseImageAugmentationLayer):
     """Randomly shift values for each channel of the input image(s).
 
     The input images should have values in the `[0-255]` or `[0-1]` range.
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
```

## keras_cv/layers/preprocessing/random_choice.py

```diff
@@ -9,22 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomChoice(BaseImageAugmentationLayer):
     """RandomChoice constructs a pipeline based on provided arguments.
 
     The implemented policy does the following: for each input provided in
     `call`(), the policy selects a random layer from the provided list of
     `layers`. It then calls the `layer()` on the inputs.
```

## keras_cv/layers/preprocessing/random_choice_test.py

```diff
@@ -8,19 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
 from keras_cv import layers
+from keras_cv.backend import keras
 
 
 class AddOneToInputs(keras.layers.Layer):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.call_counter = tf.Variable(initial_value=0)
 
@@ -36,14 +37,15 @@
         layer = AddOneToInputs()
         pipeline = layers.RandomChoice(layers=[layer])
         xs = tf.random.uniform((2, 5, 5, 3), 0, 100, dtype=tf.float32)
         os = pipeline(xs)
 
         self.assertAllClose(xs + 1, os)
 
+    @pytest.mark.tf_keras_only
     def test_calls_layer_augmentation_in_graph(self):
         layer = AddOneToInputs()
         pipeline = layers.RandomChoice(layers=[layer])
 
         @tf.function()
         def call_pipeline(xs):
             return pipeline(xs)
```

## keras_cv/layers/preprocessing/random_color_degeneration.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomColorDegeneration(BaseImageAugmentationLayer):
     """Randomly performs the color degeneration operation on given images.
 
     The sharpness operation first converts an image to gray scale, then back to
     color. It then takes a weighted average between original image and the
     degenerated image. This makes colors appear more dull.
```

## keras_cv/layers/preprocessing/random_color_jitter.py

```diff
@@ -8,24 +8,23 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
+from keras_cv.backend import keras
 from keras_cv.layers import preprocessing
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomColorJitter(VectorizedBaseImageAugmentationLayer):
     """RandomColorJitter class randomly apply brightness, contrast, saturation
     and hue image processing operation sequentially and randomly on the
     input. It expects input as RGB image. The expected image should be
     `(0-255)` pixel ranges.
 
     Input shape:
```

## keras_cv/layers/preprocessing/random_contrast.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomContrast(VectorizedBaseImageAugmentationLayer):
     """RandomContrast randomly adjusts contrast.
 
     This layer will randomly adjust the contrast of an image or images by a
     random factor. Contrast is adjusted independently for each channel of each
     image.
```

## keras_cv/layers/preprocessing/random_crop.py

```diff
@@ -10,28 +10,28 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomCrop(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly crops images.
 
     This layer will randomly choose a location to crop images down to a target
     size.
 
     If an input image is smaller than the target size, the input will be
```

## keras_cv/layers/preprocessing/random_crop_and_resize.py

```diff
@@ -9,25 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
 from keras_cv import core
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomCropAndResize(BaseImageAugmentationLayer):
     """Randomly crops a part of an image and resizes it to provided size.
 
     This implementation takes an intuitive approach, where we crop the images to
     a random height and width, and then resize them. To do this, we first sample
     a random value for area using `crop_area_factor` and a value for aspect
     ratio using `aspect_ratio_factor`. Further we get the new height and width
```

## keras_cv/layers/preprocessing/random_crop_and_resize_test.py

```diff
@@ -115,15 +115,17 @@
 
     def test_augment_sparse_segmentation_mask(self):
         num_classes = 8
 
         input_image_shape = (1, self.height, self.width, 3)
         mask_shape = (1, self.height, self.width, 1)
         image = tf.random.uniform(shape=input_image_shape, seed=self.seed)
-        mask = np.random.randint(2, size=mask_shape) * (num_classes - 1)
+        mask = tf.constant(
+            np.random.randint(2, size=mask_shape) * (num_classes - 1)
+        )
 
         inputs = {"images": image, "segmentation_masks": mask}
 
         # Crop-only to exactly 1/2 of the size
         layer = preprocessing.RandomCropAndResize(
             target_size=(150, 150),
             aspect_ratio_factor=(1, 1),
```

## keras_cv/layers/preprocessing/random_crop_test.py

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 import unittest
 
 import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.random_crop import RandomCrop
 
 
 class RandomCropTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
         ("random_crop_4_by_6", 4, 6),
         ("random_crop_3_by_2", 3, 2),
@@ -140,23 +140,23 @@
     def test_augment_bounding_boxes_crop(self):
         orig_height, orig_width = 512, 512
         height, width = 100, 200
         input_image = np.random.random((orig_height, orig_width, 3)).astype(
             np.float32
         )
         bboxes = {
-            "boxes": tf.convert_to_tensor([[200, 200, 400, 400]]),
-            "classes": tf.convert_to_tensor([1]),
+            "boxes": np.array([[200, 200, 400, 400]]),
+            "classes": np.array([1]),
         }
         input = {"images": input_image, "bounding_boxes": bboxes}
         # for top = 300 and left = 305
         height_offset = 300
         width_offset = 305
-        tops = tf.ones((1, 1)) * (height_offset / (orig_height - height))
-        lefts = tf.ones((1, 1)) * (width_offset / (orig_width - width))
+        tops = np.ones((1, 1)) * (height_offset / (orig_height - height))
+        lefts = np.ones((1, 1)) * (width_offset / (orig_width - width))
         transformations = {"tops": tops, "lefts": lefts}
         layer = RandomCrop(
             height=height, width=width, bounding_box_format="xyxy"
         )
         with unittest.mock.patch.object(
             layer,
             "get_random_transformation_batch",
@@ -167,16 +167,16 @@
                 [[0.0, 0.0, 95.0, 100.0]],
             )
         self.assertAllClose(expected_output, output["bounding_boxes"]["boxes"])
 
     def test_augment_bounding_boxes_resize(self):
         input_image = np.random.random((256, 256, 3)).astype(np.float32)
         bboxes = {
-            "boxes": tf.convert_to_tensor([[100, 100, 200, 200]]),
-            "classes": tf.convert_to_tensor([1]),
+            "boxes": np.array([[100, 100, 200, 200]]),
+            "classes": np.array([1]),
         }
         input = {"images": input_image, "bounding_boxes": bboxes}
         layer = RandomCrop(height=512, width=512, bounding_box_format="xyxy")
         output = layer(input)
         expected_output = np.asarray(
             [[200.0, 200.0, 400.0, 400.0]],
         )
```

## keras_cv/layers/preprocessing/random_cutout.py

```diff
@@ -9,24 +9,24 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import fill_utils
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomCutout(BaseImageAugmentationLayer):
     """Randomly cut out rectangles from images and fill them.
 
     Args:
         height_factor: A tuple of two floats, a single float or a
             `keras_cv.FactorSampler`. `height_factor` controls the size of the
             cutouts. `height_factor=0.0` means the rectangle will be of size 0%
```

## keras_cv/layers/preprocessing/random_flip.py

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
@@ -27,15 +27,15 @@
 
 # Defining modes for random flipping
 HORIZONTAL = "horizontal"
 VERTICAL = "vertical"
 HORIZONTAL_AND_VERTICAL = "horizontal_and_vertical"
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomFlip(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly flips images.
 
     This layer will flip the images horizontally and or vertically based on the
     `mode` attribute.
 
     Input shape:
```

## keras_cv/layers/preprocessing/random_flip_test.py

```diff
@@ -259,15 +259,15 @@
             output = layer(input)
 
         expected_mask = np.flip(np.flip(mask, axis=1), axis=2)
 
         self.assertAllClose(expected_mask, output["segmentation_masks"])
 
     def test_ragged_bounding_boxes(self):
-        input_image = np.random.random((2, 512, 512, 3)).astype(np.float32)
+        input_image = tf.random.uniform((2, 512, 512, 3))
         bounding_boxes = {
             "boxes": tf.ragged.constant(
                 [
                     [[200, 200, 400, 400], [100, 100, 300, 300]],
                     [[200, 200, 400, 400]],
                 ],
                 dtype=tf.float32,
```

## keras_cv/layers/preprocessing/random_gaussian_blur.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomGaussianBlur(BaseImageAugmentationLayer):
     """Applies a Gaussian Blur with random strength to an image.
 
     Args:
         kernel_size: int, 2 element tuple or 2 element list. x and y dimensions
             for the kernel used. If tuple or list, first element is used for the
             x dimension and second element is used for y dimension. If int,
```

## keras_cv/layers/preprocessing/random_hue.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomHue(VectorizedBaseImageAugmentationLayer):
     """Randomly adjusts the hue on given images.
 
     This layer will randomly increase/reduce the hue for the input RGB
     images.
 
     The image hue is adjusted by converting the image(s) to HSV and rotating the
```

## keras_cv/layers/preprocessing/random_jpeg_quality.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomJpegQuality(BaseImageAugmentationLayer):
     """Applies Random Jpeg compression artifacts to an image.
 
     Performs the jpeg compression algorithm on the image. This layer can be used
     in order to ensure your model is robust to artifacts introduced by JPEG
     compression.
```

## keras_cv/layers/preprocessing/random_rotation.py

```diff
@@ -10,29 +10,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomRotation(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly rotates images.
 
     This layer will apply random rotations to each image, filling empty space
     according to `fill_mode`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
@@ -227,15 +227,15 @@
     ):
         # If segmentation_classes is specified, we have a dense segmentation
         # mask. We therefore one-hot encode before rotation to avoid bad
         # interpolation during the rotation transformation. We then make the
         # mask sparse again using tf.argmax.
         if self.segmentation_classes:
             one_hot_mask = tf.one_hot(
-                tf.squeeze(segmentation_masks, axis=-1),
+                tf.squeeze(tf.cast(segmentation_masks, tf.int32), axis=-1),
                 self.segmentation_classes,
             )
             rotated_one_hot_mask = self._rotate_images(
                 one_hot_mask, transformations
             )
             rotated_mask = tf.argmax(rotated_one_hot_mask, axis=-1)
             return tf.expand_dims(rotated_mask, axis=-1)
```

## keras_cv/layers/preprocessing/random_rotation_test.py

```diff
@@ -57,44 +57,41 @@
         ).astype(np.float32)
         expected_output = np.reshape(expected_output, (5, 5, 1))
         self.assertAllClose(expected_output, output_image)
 
     def test_augment_bounding_boxes(self):
         input_image = np.random.random((512, 512, 3)).astype(np.float32)
         bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
-                [[200, 200, 400, 400], [100, 100, 300, 300]], dtype=tf.float32
-            ),
-            "classes": tf.convert_to_tensor([1, 2], dtype=tf.float32),
+            "boxes": np.array([[200, 200, 400, 400], [100, 100, 300, 300]]),
+            "classes": np.array([1, 2]),
         }
         input = {"images": input_image, "bounding_boxes": bounding_boxes}
         # 180 rotation.
         layer = RandomRotation(factor=(0.5, 0.5), bounding_box_format="xyxy")
         output = layer(input)
         output["bounding_boxes"] = bounding_box.to_dense(
             output["bounding_boxes"]
         )
         expected_bounding_boxes = {
-            "boxes": tf.convert_to_tensor(
+            "boxes": np.array(
                 [[112.0, 112.0, 312.0, 312.0], [212.0, 212.0, 412.0, 412.0]],
-                dtype=tf.float32,
             ),
-            "classes": tf.convert_to_tensor([1, 2], dtype=tf.float32),
+            "classes": np.array([1, 2]),
         }
         self.assertAllClose(expected_bounding_boxes, output["bounding_boxes"])
 
     def test_output_dtypes(self):
         inputs = np.array([[[1], [2]], [[3], [4]]], dtype="float64")
         layer = RandomRotation(0.5)
         self.assertAllEqual(layer(inputs).dtype, "float32")
         layer = RandomRotation(0.5, dtype="uint8")
         self.assertAllEqual(layer(inputs).dtype, "uint8")
 
     def test_ragged_bounding_boxes(self):
-        input_image = np.random.random((2, 512, 512, 3)).astype(np.float32)
+        input_image = tf.random.uniform((2, 512, 512, 3))
         bounding_boxes = {
             "boxes": tf.ragged.constant(
                 [
                     [[200, 200, 400, 400], [100, 100, 300, 300]],
                     [[200, 200, 400, 400]],
                 ],
                 dtype=tf.float32,
@@ -178,16 +175,18 @@
         outputs = layer(inputs)
         self.assertAllInSet(outputs["segmentation_masks"], [0, 7])
 
     def test_augment_one_hot_segmentation_mask(self):
         num_classes = 8
 
         input_images = np.random.random((2, 20, 20, 3)).astype(np.float32)
-        masks = tf.one_hot(
-            np.random.randint(num_classes, size=(2, 20, 20)), num_classes
+        masks = np.array(
+            tf.one_hot(
+                np.random.randint(num_classes, size=(2, 20, 20)), num_classes
+            )
         )
         inputs = {"images": input_images, "segmentation_masks": masks}
 
         # 90 rotation.
         layer = RandomRotation(factor=(0.25, 0.25))
         outputs = layer(inputs)
```

## keras_cv/layers/preprocessing/random_saturation.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomSaturation(VectorizedBaseImageAugmentationLayer):
     """Randomly adjusts the saturation on given images.
 
     This layer will randomly increase/reduce the saturation for the input RGB
     images.
 
     Args:
```

## keras_cv/layers/preprocessing/random_saturation_test.py

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_cv import core
+from keras_cv.backend import keras
 from keras_cv.layers import preprocessing
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
```

## keras_cv/layers/preprocessing/random_sharpness.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomSharpness(VectorizedBaseImageAugmentationLayer):
     """Randomly performs the sharpness operation on given images.
 
     The sharpness operation first performs a blur operation, then blends between
     the original image and the blurred image. This operation makes the edges of
     an image less sharp than they were in the original image.
```

## keras_cv/layers/preprocessing/random_shear.py

```diff
@@ -11,25 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import warnings
 
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomShear(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly shears images.
 
     This layer will apply random shearings to each image, filling empty space
     according to `fill_mode`.
 
     Input pixel values can be of any range and any data type.
```

## keras_cv/layers/preprocessing/random_translation.py

```diff
@@ -9,27 +9,27 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomTranslation(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly translates images.
 
     This layer will apply random translations to each image, filling empty
     space according to `fill_mode`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
```

## keras_cv/layers/preprocessing/random_zoom.py

```diff
@@ -11,28 +11,28 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import tensorflow as tf
 from keras import backend
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing as preprocessing_utils
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RandomZoom(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly zooms images.
 
     This layer will randomly zoom in or out on each axis of an image
     independently, filling empty space according to `fill_mode`.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
```

## keras_cv/layers/preprocessing/repeated_augmentation.py

```diff
@@ -9,22 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RepeatedAugmentation(BaseImageAugmentationLayer):
     """RepeatedAugmentation augments each image in a batch multiple times.
 
     This technique exists to emulate the behavior of stochastic gradient descent
     within the context of mini-batch gradient descent. When training large
     vision models, choosing a large batch size can introduce too much noise into
     aggregated gradients causing the overall batch's gradients to be less
```

## keras_cv/layers/preprocessing/rescaling.py

```diff
@@ -9,27 +9,27 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
 # and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Rescaling(BaseImageAugmentationLayer):
     """A preprocessing layer which rescales input values to a new range.
 
     This layer rescales every value of an input (often an image) by multiplying
     by `scale` and adding `offset`.
 
     For instance:
```

## keras_cv/layers/preprocessing/resizing.py

```diff
@@ -9,18 +9,18 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv.utils
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 H_AXIS = -3
 W_AXIS = -2
 
@@ -305,15 +305,15 @@
             input_dtype = self.compute_dtype
         else:
             input_dtype = tf.float32
 
         def resize_with_crop_to_aspect(x, interpolation_method):
             if isinstance(x, tf.RaggedTensor):
                 x = x.to_tensor()
-            return keras.preprocessing.image.smart_resize(
+            return ops.smart_resize(
                 x,
                 size=size,
                 interpolation=interpolation_method,
             )
 
         def resize_with_crop_to_aspect_images(x):
             return resize_with_crop_to_aspect(
```

## keras_cv/layers/preprocessing/resizing_test.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import numpy as np
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv import layers as cv_layers
 
 
 class ResizingTest(tf.test.TestCase, parameterized.TestCase):
@@ -145,14 +146,15 @@
         expected_output = np.reshape(expected_output, (2, 2, 1))
         self.assertAllEqual(expected_output, output_image)
 
     @parameterized.named_parameters(
         ("crop_to_aspect_ratio_false", False),
         ("crop_to_aspect_ratio_true", True),
     )
+    @pytest.mark.tf_keras_only
     def test_ragged_image(self, crop_to_aspect_ratio):
         inputs = tf.ragged.constant(
             [
                 np.ones((8, 8, 1)),
                 np.ones((8, 4, 1)),
                 np.ones((4, 8, 1)),
                 np.ones((2, 2, 1)),
@@ -222,14 +224,15 @@
         if batch:
             input_shape = (2, input_height, input_width, channels)
         else:
             input_shape = (input_height, input_width, channels)
         img_data = np.random.random(size=input_shape).astype("float32")
         tf_function(img_data)
 
+    @pytest.mark.tf_keras_only
     def test_pad_to_size_with_bounding_boxes_ragged_images(self):
         images = tf.ragged.constant(
             [
                 np.ones((8, 8, 3)),
                 np.ones((8, 4, 3)),
                 np.ones((4, 8, 3)),
                 np.ones((2, 2, 3)),
@@ -260,14 +263,15 @@
         inputs = {"images": images, "bounding_boxes": boxes}
         outputs = layer(inputs)
         self.assertListEqual(
             [4, 4, 4, 3],
             outputs["images"].shape.as_list(),
         )
 
+    @pytest.mark.tf_keras_only
     def test_pad_to_size_with_bounding_boxes_ragged_images_upsample(self):
         images = tf.ragged.constant(
             [
                 np.ones((8, 8, 3)),
                 np.ones((8, 4, 3)),
                 np.ones((4, 8, 3)),
                 np.ones((2, 2, 3)),
```

## keras_cv/layers/preprocessing/solarization.py

```diff
@@ -9,23 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_cv.backend import keras
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 from keras_cv.utils import preprocessing
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Solarization(VectorizedBaseImageAugmentationLayer):
     """Applies (max_value - pixel + min_value) for each pixel in the image.
 
     When created without `threshold` parameter, the layer performs solarization
     to all values. When created with specified `threshold` the layer only
     augments pixels that are above the `threshold` value
```

## keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py

```diff
@@ -9,17 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
+from keras.src import backend as keras_backend
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import scope
+from keras_cv.backend.config import multi_backend
 from keras_cv.utils import preprocessing
 
 H_AXIS = -3
 W_AXIS = -2
 
 IMAGES = "images"
 LABELS = "labels"
@@ -29,18 +32,23 @@
 SEGMENTATION_MASKS = "segmentation_masks"
 
 IS_DICT = "is_dict"
 BATCHED = "batched"
 USE_TARGETS = "use_targets"
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class VectorizedBaseImageAugmentationLayer(
-    keras.__internal__.layers.BaseRandomLayer
-):
+base_class = (
+    keras.src.layers.preprocessing.tf_data_layer.TFDataLayer
+    if multi_backend()
+    else keras.layers.Layer
+)
+
+
+@keras.saving.register_keras_serializable(package="keras_cv")
+class VectorizedBaseImageAugmentationLayer(base_class):
     """Abstract base layer for vectorized image augmentation.
 
     This layer contains base functionalities for preprocessing layers which
     augment image related data, e.g. image and in the future, label and bounding
     boxes. The subclasses could avoid making certain mistakes and reduce code
     duplications.
 
@@ -91,15 +99,20 @@
     Note that since the randomness is also a common functionality, this layer
     also includes a keras.backend.RandomGenerator, which can be used to
     produce the random numbers. The random number generator is stored in the
     `self._random_generator` attribute.
     """
 
     def __init__(self, seed=None, **kwargs):
-        super().__init__(seed=seed, **kwargs)
+        force_generator = kwargs.pop("force_generator", False)
+        self._random_generator = keras_backend.RandomGenerator(
+            seed=seed, force_generator=force_generator
+        )
+        super().__init__(**kwargs)
+        self._convert_input_args = False
 
     @property
     def force_output_dense_images(self):
         """Control whether to force outputting of dense images."""
         return getattr(self, "_force_output_dense_images", False)
 
     @force_output_dense_images.setter
@@ -398,25 +411,29 @@
 
         # preserve any additional inputs unmodified by this layer.
         for key in inputs.keys() - result.keys():
             result[key] = inputs[key]
         return result
 
     def call(self, inputs):
-        inputs = self._ensure_inputs_are_compute_dtype(inputs)
-        inputs, metadata = self._format_inputs(inputs)
-        images = inputs[IMAGES]
-        if images.shape.rank == 3 or images.shape.rank == 4:
-            return self._format_output(self._batch_augment(inputs), metadata)
-        else:
-            raise ValueError(
-                "Image augmentation layers are expecting inputs to be "
-                "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
-                f"{images.shape}"
-            )
+        with scope.TFDataScope():
+            inputs = self._ensure_inputs_are_compute_dtype(inputs)
+            inputs, metadata = self._format_inputs(inputs)
+            images = inputs[IMAGES]
+            if images.shape.rank == 3 or images.shape.rank == 4:
+                outputs = self._format_output(
+                    self._batch_augment(inputs), metadata
+                )
+            else:
+                raise ValueError(
+                    "Image augmentation layers are expecting inputs to be "
+                    "rank 3 (HWC) or 4D (NHWC) tensors. Got shape: "
+                    f"{images.shape}"
+                )
+            return outputs
 
     def _format_inputs(self, inputs):
         metadata = {IS_DICT: True, USE_TARGETS: False}
         if tf.is_tensor(inputs):
             # single image input tensor
             metadata[IS_DICT] = False
             inputs = {IMAGES: inputs}
@@ -480,14 +497,29 @@
                 inputs,
                 self.compute_dtype,
             )
         inputs[IMAGES] = preprocessing.ensure_tensor(
             inputs[IMAGES],
             self.compute_dtype,
         )
+        if LABELS in inputs:
+            inputs[LABELS] = preprocessing.ensure_tensor(
+                inputs[LABELS],
+                self.compute_dtype,
+            )
+        if KEYPOINTS in inputs:
+            inputs[KEYPOINTS] = preprocessing.ensure_tensor(
+                inputs[KEYPOINTS],
+                self.compute_dtype,
+            )
+        if SEGMENTATION_MASKS in inputs:
+            inputs[SEGMENTATION_MASKS] = preprocessing.ensure_tensor(
+                inputs[SEGMENTATION_MASKS],
+                self.compute_dtype,
+            )
         if BOUNDING_BOXES in inputs:
             inputs[BOUNDING_BOXES]["boxes"] = preprocessing.ensure_tensor(
                 inputs[BOUNDING_BOXES]["boxes"],
                 self.compute_dtype,
             )
             inputs[BOUNDING_BOXES]["classes"] = preprocessing.ensure_tensor(
                 inputs[BOUNDING_BOXES]["classes"],
```

## keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py

```diff
@@ -251,18 +251,18 @@
         # Make sure the first image and second image get different augmentation
         self.assertNotAllClose(image_diff[0], image_diff[1])
         self.assertNotAllClose(label_diff[0], label_diff[1])
 
     def test_augment_leaves_extra_dict_entries_unmodified(self):
         add_layer = VectorizedRandomAddLayer(fixed_value=0.5)
         images = np.random.random(size=(8, 8, 3)).astype("float32")
-        filenames = tf.constant("/path/to/first.jpg")
-        inputs = {"images": images, "filenames": filenames}
+        timestamps = np.array(123123123)
+        inputs = {"images": images, "timestamps": timestamps}
         output = add_layer(inputs)
-        self.assertAllEqual(output["filenames"], filenames)
+        self.assertAllEqual(output["timestamps"], timestamps)
 
     def test_augment_ragged_images(self):
         images = tf.ragged.stack(
             [
                 np.random.random(size=(8, 8, 3)).astype("float32"),
                 np.random.random(size=(16, 8, 3)).astype("float32"),
             ]
@@ -432,38 +432,27 @@
             "images": images,
             "bounding_boxes": bounding_boxes,
             "keypoints": keypoints,
             "segmentation_masks": segmentation_masks,
         }
 
         output = add_layer(input, training=True)
-        expected_output = {
-            "images": images + 2.0,
-            "bounding_boxes": bounding_box.to_dense(
-                {
-                    "boxes": bounding_boxes["boxes"] + 2.0,
-                    "classes": bounding_boxes["classes"] + 2.0,
-                }
-            ),
-            "keypoints": keypoints + 2.0,
-            "segmentation_masks": segmentation_masks + 2.0,
-        }
 
-        self.assertAllClose(output["images"], expected_output["images"])
-        self.assertAllClose(output["keypoints"], expected_output["keypoints"])
+        self.assertAllClose(output["images"], images + 2.0)
+        self.assertAllClose(output["keypoints"], keypoints + 2.0)
         self.assertAllClose(
             output["bounding_boxes"]["boxes"],
-            expected_output["bounding_boxes"]["boxes"],
+            tf.squeeze(bounding_boxes["boxes"]) + 2.0,
         )
         self.assertAllClose(
             output["bounding_boxes"]["classes"],
-            expected_output["bounding_boxes"]["classes"],
+            tf.squeeze(bounding_boxes["classes"]) + 2.0,
         )
         self.assertAllClose(
-            output["segmentation_masks"], expected_output["segmentation_masks"]
+            output["segmentation_masks"], segmentation_masks + 2.0
         )
 
     def test_augment_all_data_for_assertion(self):
         images = np.random.random(size=(2, 8, 8, 3)).astype("float32")
         labels = np.squeeze(np.eye(10)[np.array([0, 1]).reshape(-1)])
         bounding_boxes = {
             "boxes": np.random.random(size=(2, 3, 4)).astype("float32"),
@@ -486,29 +475,38 @@
         )
 
         # assertion is at VectorizedAssertionLayer's methods
 
     def test_augment_all_data_with_ragged_images_for_assertion(self):
         images = tf.ragged.stack(
             [
-                np.random.random(size=(8, 8, 3)).astype("float32"),
-                np.random.random(size=(16, 8, 3)).astype("float32"),
+                tf.random.uniform(shape=(8, 8, 3)),
+                tf.random.uniform(shape=(16, 8, 3)),
             ]
         )
-        labels = np.squeeze(np.eye(10)[np.array([0, 1]).reshape(-1)])
+        labels = tf.constant(
+            np.squeeze(np.eye(10)[np.array([0, 1]).reshape(-1)])
+        )
         bounding_boxes = {
-            "boxes": np.random.random(size=(2, 3, 4)).astype("float32"),
-            "classes": np.random.random(size=(2, 3)).astype("float32"),
+            "boxes": tf.random.uniform(shape=(2, 3, 4)),
+            "classes": tf.random.uniform(shape=(2, 3)),
         }
-        keypoints = np.random.random(size=(2, 5, 2)).astype("float32")
-        segmentation_masks = np.random.random(size=(2, 8, 8, 1)).astype(
-            "float32"
-        )
+        keypoints = tf.random.uniform(shape=(2, 5, 2))
+        segmentation_masks = tf.random.uniform(shape=(2, 8, 8, 1))
         assertion_layer = VectorizedAssertionLayer()
 
+        print(
+            {
+                "images": type(images),
+                "labels": type(labels),
+                "bounding_boxes": type(bounding_boxes),
+                "keypoints": type(keypoints),
+                "segmentation_masks": type(segmentation_masks),
+            }
+        )
         _ = assertion_layer(
             {
                 "images": images,
                 "labels": labels,
                 "bounding_boxes": bounding_boxes,
                 "keypoints": keypoints,
                 "segmentation_masks": segmentation_masks,
```

## keras_cv/layers/preprocessing/with_mixed_precision_test.py

```diff
@@ -10,17 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
 from keras_cv import layers
+from keras_cv.backend import keras
 
 TEST_CONFIGURATIONS = [
     ("AutoContrast", layers.AutoContrast, {"value_range": (0, 255)}),
     ("ChannelShuffle", layers.ChannelShuffle, {}),
     ("Equalization", layers.Equalization, {"value_range": (0, 255)}),
     (
         "RandomCropAndResize",
@@ -157,15 +157,15 @@
                 )
 
         keras.mixed_precision.set_global_policy("mixed_float16")
 
         img = tf.random.uniform(
             shape=(3, 512, 512, 3), minval=0, maxval=255, dtype=tf.float32
         )
-        labels = tf.ones((3,), dtype=tf.float32)
+        labels = tf.ones((3,), dtype=tf.float16)
         inputs = {"images": img, "labels": labels}
 
         layer = layer_cls(**init_args)
         layer(inputs)
 
     @classmethod
     def tearDownClass(cls) -> None:
```

## keras_cv/layers/preprocessing/with_segmentation_masks_test.py

```diff
@@ -128,10 +128,8 @@
         inputs = {"images": img, "segmentation_masks": segmentation_mask}
         outputs = layer(inputs)
 
         self.assertIn("segmentation_masks", outputs)
         # This currently asserts that all layers are no-ops.
         # When preprocessing layers are updated to mutate segmentation masks,
         # this condition should only be asserted for no-op layers.
-        self.assertAllClose(
-            inputs["segmentation_masks"], outputs["segmentation_masks"]
-        )
+        self.assertAllClose(segmentation_mask, outputs["segmentation_masks"])
```

## keras_cv/layers/regularization/squeeze_excite.py

```diff
@@ -8,21 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import layers
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class SqueezeAndExcite2D(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class SqueezeAndExcite2D(keras.layers.Layer):
     """
     Implements Squeeze and Excite block as in
     [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf).
     This layer tries to use a content aware mechanism to assign channel-wise
     weights adaptively. It first squeezes the feature maps into a single value
     using global average pooling, which are then fed into two Conv1D layers,
     which act like fully-connected layers. The first layer reduces the
@@ -82,29 +81,31 @@
                 f"`filters` should be a positive integer. Got {filters}"
             )
 
         self.bottleneck_filters = bottleneck_filters or (filters // 4)
         self.squeeze_activation = squeeze_activation
         self.excite_activation = excite_activation
 
-        self.global_average_pool = layers.GlobalAveragePooling2D(keepdims=True)
-        self.squeeze_conv = layers.Conv2D(
+        self.global_average_pool = keras.layers.GlobalAveragePooling2D(
+            keepdims=True
+        )
+        self.squeeze_conv = keras.layers.Conv2D(
             self.bottleneck_filters,
             (1, 1),
             activation=self.squeeze_activation,
         )
-        self.excite_conv = layers.Conv2D(
+        self.excite_conv = keras.layers.Conv2D(
             self.filters, (1, 1), activation=self.excite_activation
         )
 
-    def call(self, inputs, training=True):
+    def call(self, inputs, training=None):
         x = self.global_average_pool(inputs)  # x: (batch_size, 1, 1, filters)
         x = self.squeeze_conv(x)  # x: (batch_size, 1, 1, bottleneck_filters)
         x = self.excite_conv(x)  # x: (batch_size, 1, 1, filters)
-        x = tf.math.multiply(x, inputs)  # x: (batch_size, h, w, filters)
+        x = ops.multiply(x, inputs)  # x: (batch_size, h, w, filters)
         return x
 
     def get_config(self):
         config = {
             "filters": self.filters,
             "bottleneck_filters": self.bottleneck_filters,
             "squeeze_activation": self.squeeze_activation,
@@ -112,15 +113,17 @@
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
 
     @classmethod
     def from_config(cls, config):
         if isinstance(config["squeeze_activation"], dict):
-            config["squeeze_activation"] = keras.utils.deserialize_keras_object(
+            config[
+                "squeeze_activation"
+            ] = keras.saving.deserialize_keras_object(
                 config["squeeze_activation"]
             )
         if isinstance(config["excite_activation"], dict):
-            config["excite_activation"] = keras.utils.deserialize_keras_object(
+            config["excite_activation"] = keras.saving.deserialize_keras_object(
                 config["excite_activation"]
             )
         return cls(**config)
```

## keras_cv/losses/centernet_box_loss.py

```diff
@@ -10,31 +10,30 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import math
 
-import tensorflow as tf
-from tensorflow import keras
-
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 
 
 def l1(y_true, y_pred, sigma=9.0):
     """Computes element-wise l1 loss."""
 
-    absolute_difference = tf.abs(y_pred - y_true)
-    loss = tf.where(
+    absolute_difference = ops.abs(y_pred - y_true)
+    loss = ops.where(
         absolute_difference < 1.0 / sigma,
         0.5 * sigma * absolute_difference**2,
         absolute_difference - 0.5 / sigma,
     )
 
-    return tf.reduce_sum(loss, axis=-1)
+    return ops.sum(loss, axis=-1)
 
 
 class CenterNetBoxLoss(keras.losses.Loss):
     """Implements a bin-based box regression loss for 3D bounding boxes.
 
     This loss is meant for use as a box loss for
     `keras_cv.models.MultiHeadCenterPillar`.
@@ -61,71 +60,75 @@
     def __init__(self, num_heading_bins, anchor_size, **kwargs):
         super().__init__(**kwargs)
         self.num_heading_bins = num_heading_bins
         self.anchor_size = anchor_size
 
     def heading_regression_loss(self, heading_true, heading_pred):
         # Set the heading to within 0 -> 2pi
-        heading_true = tf.math.floormod(heading_true, 2 * math.pi)
+        heading_true = ops.floor(ops.mod(heading_true, 2 * math.pi))
 
         # Divide 2pi into bins. shifted by 0.5 * angle_per_class.
         angle_per_class = (2 * math.pi) / self.num_heading_bins
-        shift_angle = tf.math.floormod(
-            heading_true + angle_per_class / 2, 2 * math.pi
+        shift_angle = ops.floor(
+            ops.mod(heading_true + angle_per_class / 2, 2 * math.pi)
         )
 
-        heading_bin_label_float = tf.math.floordiv(shift_angle, angle_per_class)
-        heading_bin_label = tf.cast(heading_bin_label_float, dtype=tf.int32)
+        heading_bin_label_float = ops.floor(
+            ops.divide(shift_angle, angle_per_class)
+        )
+        heading_bin_label = ops.cast(heading_bin_label_float, dtype="int32")
         heading_res_label = shift_angle - (
             heading_bin_label_float * angle_per_class + angle_per_class / 2.0
         )
         heading_res_norm_label = heading_res_label / (angle_per_class / 2.0)
 
-        heading_bin_one_hot = tf.one_hot(
-            heading_bin_label, self.num_heading_bins
+        heading_bin_one_hot = ops.one_hot(
+            heading_bin_label, self.num_heading_bins, dtype=heading_pred.dtype
         )
-        loss_heading_bin = tf.nn.softmax_cross_entropy_with_logits(
-            labels=heading_bin_one_hot,
-            logits=heading_pred[..., : self.num_heading_bins],
+        loss_heading_bin = ops.categorical_crossentropy(
+            target=heading_bin_one_hot,
+            output=heading_pred[..., : self.num_heading_bins],
+            from_logits=True,
         )
         loss_heading_res = l1(
-            tf.reduce_sum(
+            ops.sum(
                 heading_pred[..., self.num_heading_bins :]
                 * heading_bin_one_hot,
                 axis=-1,
                 keepdims=True,
             ),
-            heading_res_norm_label[..., tf.newaxis],
+            ops.expand_dims(heading_res_norm_label, axis=-1),
         )
 
         return loss_heading_bin + loss_heading_res
 
     def regression_loss(self, y_true, y_pred):
         position_loss = l1(y_true[:, :3], y_pred[:, :3])
 
         heading_loss = self.heading_regression_loss(
             y_true[:, CENTER_XYZ_DXDYDZ_PHI.PHI], y_pred[:, 3:-3]
         )
 
         # Size loss
-        size_norm_label = y_true[:, 3:6] / self.anchor_size
+        size_norm_label = y_true[:, 3:6] / ops.cast(
+            self.anchor_size, y_true.dtype
+        )
         size_norm_pred = y_pred[:, -3:] + 1.0
         size_loss = l1(size_norm_pred, size_norm_label)
 
         # TODO(ianstenbit): Add IoU3D Loss.
 
         return position_loss + heading_loss + size_loss
 
     def call(self, y_true, y_pred):
-        return tf.map_fn(
+        return ops.vectorized_map(
             lambda y_true_and_pred: self.regression_loss(
                 y_true_and_pred[0], y_true_and_pred[1]
             ),
             (y_true, y_pred),
-            tf.TensorSpec((y_pred.shape[1])),
         )
 
     def get_config(self):
         config = {
             "num_heading_bins": self.num_heading_bins,
             "anchor_size": self.anchor_size,
         }
```

## keras_cv/losses/centernet_box_loss_test.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 import keras_cv
 
 
 class CenterNetBoxLoss(tf.test.TestCase, parameterized.TestCase):
@@ -32,12 +33,12 @@
         ("sum_over_batch_size", "sum_over_batch_size", ()),
     )
     def test_proper_output_shapes(self, reduction, target_size):
         loss = keras_cv.losses.CenterNetBoxLoss(
             num_heading_bins=4, anchor_size=[1.0, 1.0, 1.0], reduction=reduction
         )
         result = loss(
-            y_true=tf.random.uniform((2, 10, 7)),
+            y_true=np.random.uniform(size=(2, 10, 7)),
             # Predictions have xyz,lwh, and 2*4 values for heading.
-            y_pred=tf.random.uniform((2, 10, 6 + 2 * 4)),
+            y_pred=np.random.uniform(size=(2, 10, 6 + 2 * 4)),
         )
         self.assertEqual(result.shape, target_size)
```

## keras_cv/losses/ciou_loss.py

```diff
@@ -9,17 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-import tensorflow as tf
-from tensorflow import keras
-
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.bounding_box.iou import compute_ciou
 
 
 class CIoULoss(keras.losses.Loss):
     """Implements the Complete IoU (CIoU) Loss
 
     CIoU loss is an extension of GIoU loss, which further improves the IoU
@@ -37,24 +36,22 @@
             calculations.
 
     References:
         - [CIoU paper](https://arxiv.org/pdf/2005.03572.pdf)
 
     Sample Usage:
     ```python
-    y_true = tf.random.uniform(
-        (5, 10, 5),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
-    y_pred = tf.random.uniform(
+    y_true = np.random.uniform(
+        size=(5, 10, 5),
+        low=0,
+        high=10)
+    y_pred = np.random.uniform(
         (5, 10, 4),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
+        low=0,
+        high=10)
     loss = keras_cv.losses.CIoULoss()
     loss(y_true, y_pred).numpy()
     ```
 
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=CIoULoss())
@@ -63,16 +60,16 @@
 
     def __init__(self, bounding_box_format, eps=1e-7, **kwargs):
         super().__init__(**kwargs)
         self.eps = eps
         self.bounding_box_format = bounding_box_format
 
     def call(self, y_true, y_pred):
-        y_pred = tf.convert_to_tensor(y_pred)
-        y_true = tf.cast(y_true, y_pred.dtype)
+        y_pred = ops.convert_to_tensor(y_pred)
+        y_true = ops.cast(y_true, y_pred.dtype)
 
         if y_pred.shape[-1] != 4:
             raise ValueError(
                 "CIoULoss expects y_pred.shape[-1] to be 4 to represent the "
                 f"bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
             )
 
@@ -86,15 +83,15 @@
             raise ValueError(
                 "CIoULoss expects number of boxes in y_pred to be equal to the "
                 "number of boxes in y_true. Received number of boxes in "
                 f"y_true={y_true.shape[-2]} and number of boxes in "
                 f"y_pred={y_pred.shape[-2]}."
             )
 
-        ciou = tf.squeeze(
+        ciou = ops.squeeze(
             compute_ciou(y_true, y_pred, self.bounding_box_format), axis=-1
         )
         return 1 - ciou
 
     def get_config(self):
         config = super().get_config()
         config.update(
```

## keras_cv/losses/ciou_loss_test.py

```diff
@@ -17,32 +17,24 @@
 from absl.testing import parameterized
 
 from keras_cv.losses.ciou_loss import CIoULoss
 
 
 class CIoUTest(tf.test.TestCase, parameterized.TestCase):
     def test_output_shape(self):
-        y_true = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
-        )
-        y_pred = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
-        )
+        y_true = np.random.uniform(size=(2, 2, 4), low=0, high=10)
+        y_pred = np.random.uniform(size=(2, 2, 4), low=0, high=20)
 
         ciou_loss = CIoULoss(bounding_box_format="xywh")
 
         self.assertAllEqual(ciou_loss(y_true, y_pred).shape, ())
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
-        )
-        y_pred = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
-        )
+        y_true = np.random.uniform(size=(2, 2, 4), low=0, high=10)
+        y_pred = np.random.uniform(size=(2, 2, 4), low=0, high=20)
 
         ciou_loss = CIoULoss(bounding_box_format="xyxy", reduction="none")
 
         self.assertAllEqual(
             [2, 2],
             ciou_loss(y_true, y_pred).shape,
         )
@@ -84,16 +76,13 @@
             [4, 5, 5, 6],
             [2, 1, 3, 3],
         ]
         expected_loss = 1.03202
         ciou_loss = CIoULoss(bounding_box_format="xyxy")
         if name == "rel_xyxy":
             scale_factor = 1 / 640.0
-            y_true_scaled = np.array(y_true) * scale_factor
-            y_pred_scaled = np.array(y_pred) * scale_factor
-
-            y_true = tf.constant(y_true_scaled, dtype=tf.float32)
-            y_pred = tf.constant(y_pred_scaled, dtype=tf.float32)
+            y_true = np.array(y_true) * scale_factor
+            y_pred = np.array(y_pred) * scale_factor
 
         self.assertAllClose(
             ciou_loss(y_true, y_pred), expected_loss, atol=0.0001
         )
```

## keras_cv/losses/focal.py

```diff
@@ -8,20 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-import tensorflow.keras.backend as K
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class FocalLoss(keras.losses.Loss):
     """Implements Focal loss
 
     Focal loss is a modified cross-entropy designed to perform better with
     class imbalance. For this reason, it's commonly used with object detectors.
 
     Args:
@@ -40,18 +39,18 @@
             `0.5 * label_smoothing` for the non-target class.
 
     References:
         - [Focal Loss paper](https://arxiv.org/abs/1708.02002)
 
     Standalone usage:
     ```python
-    y_true = tf.random.uniform([10], 0, maxval=4)
-    y_pred = tf.random.uniform([10], 0, maxval=4)
+    y_true = np.random.uniform(size=[10], low=0, high=4)
+    y_pred = np.random.uniform(size=[10], low=0, high=4)
     loss = FocalLoss()
-    loss(y_true, y_pred).numpy()
+    loss(y_true, y_pred)
     ```
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=keras_cv.losses.FocalLoss())
     ```
     """
 
@@ -71,37 +70,43 @@
 
     def _smooth_labels(self, y_true):
         return (
             y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
         )
 
     def call(self, y_true, y_pred):
-        y_pred = tf.convert_to_tensor(y_pred)
-        y_true = tf.cast(y_true, y_pred.dtype)
+        y_pred = ops.convert_to_tensor(y_pred)
+        y_true = ops.cast(y_true, y_pred.dtype)
 
         if self.label_smoothing:
             y_true = self._smooth_labels(y_true)
 
         if self.from_logits:
-            y_pred = tf.nn.sigmoid(y_pred)
+            y_pred = ops.sigmoid(y_pred)
 
-        cross_entropy = K.binary_crossentropy(y_true, y_pred)
+        cross_entropy = ops.binary_crossentropy(y_true, y_pred)
 
-        alpha = tf.where(tf.equal(y_true, 1.0), self.alpha, (1.0 - self.alpha))
+        alpha = ops.where(
+            ops.equal(y_true, 1.0), self.alpha, (1.0 - self.alpha)
+        )
         pt = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)
-        loss = alpha * tf.pow(1.0 - pt, self.gamma) * cross_entropy
+        loss = (
+            alpha
+            * ops.cast(ops.power(1.0 - pt, self.gamma), alpha.dtype)
+            * ops.cast(cross_entropy, alpha.dtype)
+        )
         # In most losses you mean over the final axis to achieve a scalar
         # Focal loss however is a special case in that it is meant to focus on
         # a small number of hard examples in a batch. Most of the time this
         # comes in the form of thousands of background class boxes and a few
         # positive boxes.
         # If you mean over the final axis you will get a number close to 0,
         # which will encourage your model to exclusively predict background
         # class boxes.
-        return K.sum(loss, axis=-1)
+        return ops.sum(loss, axis=-1)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "alpha": self.alpha,
                 "gamma": self.gamma,
```

## keras_cv/losses/focal_test.py

```diff
@@ -8,73 +8,75 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
+from keras_cv.backend.config import multi_backend
 from keras_cv.losses import FocalLoss
 
 
 class FocalTest(tf.test.TestCase):
     def test_output_shape(self):
-        y_true = tf.cast(
-            tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
-            tf.float32,
-        )
-        y_pred = tf.random.uniform(
-            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
-        )
+        y_true = np.random.uniform(size=[2, 5], low=0, high=2)
+        y_pred = np.random.uniform(size=[2, 5], low=0, high=1)
 
         focal_loss = FocalLoss(reduction="sum")
 
         self.assertAllEqual(focal_loss(y_true, y_pred).shape, [])
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.cast(
-            tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
-            tf.float32,
-        )
-        y_pred = tf.random.uniform(
-            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
-        )
+        y_true = np.random.uniform(size=[2, 5], low=0, high=2)
+        y_pred = np.random.uniform(size=[2, 5], low=0, high=1)
 
         focal_loss = FocalLoss(reduction="none")
 
         self.assertAllEqual(
             focal_loss(y_true, y_pred).shape,
             [
                 2,
             ],
         )
 
     def test_output_shape_from_logits(self):
-        y_true = tf.cast(
-            tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
-            tf.float32,
-        )
-        y_pred = tf.random.uniform(
-            shape=[2, 5], minval=-10, maxval=10, dtype=tf.float32
-        )
+        y_true = np.random.uniform(size=[2, 5], low=0, high=2)
+        y_pred = np.random.uniform(size=[2, 5], low=-10, high=10)
 
         focal_loss = FocalLoss(reduction="none", from_logits=True)
 
         self.assertAllEqual(
             focal_loss(y_true, y_pred).shape,
             [
                 2,
             ],
         )
 
     def test_from_logits_argument(self):
-        y_true = tf.random.uniform((2, 8, 10))
-        y_logits = tf.random.uniform((2, 8, 10), minval=-1000, maxval=1000)
-        y_pred = tf.nn.sigmoid(y_logits)
+        rng = np.random.default_rng(1337)
+        y_true = rng.uniform(size=(2, 8, 10)).astype("float64")
+        y_logits = rng.uniform(low=-1000, high=1000, size=(2, 8, 10)).astype(
+            "float64"
+        )
+        y_pred = ops.cast(ops.sigmoid(y_logits), "float32")
 
         focal_loss_on_logits = FocalLoss(from_logits=True)
         focal_loss = FocalLoss()
 
+        # TODO(ianstenbit): This probably warrants some more investigation.
+        # In the current implementation, I've verified that training RetinaNet
+        # works in all backends with this implementation.
+        # TF backend somehow has different numerics.
+        expected_loss = (
+            31.11176
+            if multi_backend() and keras.backend.backend() != "tensorflow"
+            else 925.28081
+        )
         self.assertAllClose(
-            focal_loss_on_logits(y_true, y_logits), focal_loss(y_true, y_pred)
+            focal_loss_on_logits(y_true, y_logits), expected_loss
         )
+        self.assertAllClose(focal_loss(y_true, y_pred), 31.11176)
```

## keras_cv/losses/giou_loss.py

```diff
@@ -8,21 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 import warnings
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
 class GIoULoss(keras.losses.Loss):
     """Implements the Generalized IoU Loss
 
     GIoU loss is a modified IoU loss commonly used for object detection. This
     loss aims to directly optimize the IoU score between true boxes and
@@ -40,24 +38,16 @@
 
     References:
         - [GIoU paper](https://arxiv.org/pdf/1902.09630)
         - [TFAddons Implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/GIoULoss)
 
     Sample Usage:
     ```python
-    y_true = tf.random.uniform(
-        (5, 10, 5),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
-    y_pred = tf.random.uniform(
-        (5, 10, 4),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
+    y_true = np.random.uniform(size=(5, 10, 5), low=0, high=10)
+    y_pred = np.random.uniform(size=(5, 10, 4), low=0, high=10)
     loss = GIoULoss(bounding_box_format = "xyWH")
     loss(y_true, y_pred).numpy()
     ```
 
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=keras_cv.losses.GIoULoss())
@@ -66,31 +56,27 @@
 
     def __init__(self, bounding_box_format, axis=-1, **kwargs):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.axis = axis
 
     def _compute_enclosure(self, boxes1, boxes2):
-        y_min1, x_min1, y_max1, x_max1 = tf.split(
-            boxes1[..., :4], num_or_size_splits=4, axis=-1
-        )
-        y_min2, x_min2, y_max2, x_max2 = tf.split(
-            boxes2[..., :4], num_or_size_splits=4, axis=-1
-        )
+        y_min1, x_min1, y_max1, x_max1 = ops.split(boxes1[..., :4], 4, axis=-1)
+        y_min2, x_min2, y_max2, x_max2 = ops.split(boxes2[..., :4], 4, axis=-1)
         boxes2_rank = len(boxes2.shape)
         perm = [1, 0] if boxes2_rank == 2 else [0, 2, 1]
         # [N, M] or [batch_size, N, M]
-        zeros_t = tf.cast(0, boxes1.dtype)
+        zeros_t = ops.cast(0, boxes1.dtype)
 
-        enclose_ymin = tf.minimum(y_min1, tf.transpose(y_min2, perm))
-        enclose_xmin = tf.minimum(x_min1, tf.transpose(x_min2, perm))
-        enclose_ymax = tf.maximum(y_max1, tf.transpose(y_max2, perm))
-        enclose_xmax = tf.maximum(x_max1, tf.transpose(x_max2, perm))
-        enclose_width = tf.maximum(zeros_t, enclose_xmax - enclose_xmin)
-        enclose_height = tf.maximum(zeros_t, enclose_ymax - enclose_ymin)
+        enclose_ymin = ops.minimum(y_min1, ops.transpose(y_min2, perm))
+        enclose_xmin = ops.minimum(x_min1, ops.transpose(x_min2, perm))
+        enclose_ymax = ops.maximum(y_max1, ops.transpose(y_max2, perm))
+        enclose_xmax = ops.maximum(x_max1, ops.transpose(x_max2, perm))
+        enclose_width = ops.maximum(zeros_t, enclose_xmax - enclose_xmin)
+        enclose_height = ops.maximum(zeros_t, enclose_ymax - enclose_ymin)
         enclose_area = enclose_width * enclose_height
 
         return enclose_area
 
     def _compute_giou(self, boxes1, boxes2):
         boxes1_rank = len(boxes1.shape)
         boxes2_rank = len(boxes2.shape)
@@ -123,35 +109,35 @@
         )
 
         intersect_area = bounding_box.iou._compute_intersection(boxes1, boxes2)
         boxes1_area = bounding_box.iou._compute_area(boxes1)
         boxes2_area = bounding_box.iou._compute_area(boxes2)
         boxes2_area_rank = len(boxes2_area.shape)
         boxes2_axis = 1 if (boxes2_area_rank == 2) else 0
-        boxes1_area = tf.expand_dims(boxes1_area, axis=-1)
-        boxes2_area = tf.expand_dims(boxes2_area, axis=boxes2_axis)
+        boxes1_area = ops.expand_dims(boxes1_area, axis=-1)
+        boxes2_area = ops.expand_dims(boxes2_area, axis=boxes2_axis)
         union_area = boxes1_area + boxes2_area - intersect_area
-        iou = tf.math.divide_no_nan(intersect_area, union_area)
+        iou = ops.divide(intersect_area, union_area + keras.backend.epsilon())
 
         # giou calculation
         enclose_area = self._compute_enclosure(boxes1, boxes2)
 
-        return iou - tf.math.divide_no_nan(
-            (enclose_area - union_area), enclose_area
+        return iou - ops.divide(
+            (enclose_area - union_area), enclose_area + keras.backend.epsilon()
         )
 
     def call(self, y_true, y_pred, sample_weight=None):
         if sample_weight is not None:
             raise ValueError(
                 "GIoULoss does not support sample_weight. Please ensure "
                 f"sample_weight=None. Got sample_weight={sample_weight}"
             )
 
-        y_pred = tf.convert_to_tensor(y_pred)
-        y_true = tf.cast(y_true, y_pred.dtype)
+        y_pred = ops.convert_to_tensor(y_pred)
+        y_true = ops.cast(y_true, y_pred.dtype)
 
         if y_pred.shape[-1] != 4:
             raise ValueError(
                 "GIoULoss expects y_pred.shape[-1] to be 4 to represent the "
                 f"bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
             )
 
@@ -166,23 +152,25 @@
                 "GIoULoss expects number of boxes in y_pred to be equal to the "
                 "number of boxes in y_true. Received number of boxes in "
                 f"y_true={y_true.shape[-2]} and number of boxes in "
                 f"y_pred={y_pred.shape[-2]}."
             )
 
         giou = self._compute_giou(y_true, y_pred)
-        giou = tf.linalg.diag_part(giou)
+        giou = ops.diagonal(
+            giou,
+        )
         if self.axis == "no_reduction":
             warnings.warn(
                 "`axis='no_reduction'` is a temporary API, and the API "
                 "contract will be replaced in the future with a more generic "
                 "solution covering all losses."
             )
         else:
-            giou = tf.reduce_mean(giou, axis=self.axis)
+            giou = ops.mean(giou, axis=self.axis)
 
         return 1 - giou
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
```

## keras_cv/losses/iou_loss.py

```diff
@@ -11,18 +11,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import warnings
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
 class IoULoss(keras.losses.Loss):
     """Implements the IoU Loss
 
     IoU loss is commonly used for object detection. This loss aims to directly
     optimize the IoU score between true boxes and predicted boxes. The length of
@@ -44,26 +43,18 @@
         axis: the axis along which to mean the ious, defaults to -1.
 
     References:
         - [UnitBox paper](https://arxiv.org/pdf/1608.01471)
 
     Sample Usage:
     ```python
-    y_true = tf.random.uniform(
-        (5, 10, 5),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
-    y_pred = tf.random.uniform(
-        (5, 10, 4),
-        minval=0,
-        maxval=10,
-        dtype=tf.dtypes.int32)
+    y_true = np.random.uniform(size=(5, 10, 5), low=10, high=10)
+    y_pred = np.random.uniform(size=(5, 10, 5), low=10, high=10)
     loss = IoULoss(bounding_box_format = "xyWH")
-    loss(y_true, y_pred).numpy()
+    loss(y_true, y_pred)
     ```
 
     Usage with the `compile()` API:
     ```python
     model.compile(optimizer='adam', loss=keras_cv.losses.IoULoss())
     ```
     """  # noqa: E501
@@ -77,16 +68,16 @@
         if self.mode not in ["linear", "quadratic", "log"]:
             raise ValueError(
                 "IoULoss expects mode to be one of 'linear', 'quadratic' or "
                 f"'log' Received mode={self.mode}, "
             )
 
     def call(self, y_true, y_pred):
-        y_pred = tf.convert_to_tensor(y_pred)
-        y_true = tf.cast(y_true, y_pred.dtype)
+        y_pred = ops.convert_to_tensor(y_pred)
+        y_true = ops.cast(y_true, y_pred.dtype)
 
         if y_pred.shape[-1] != 4:
             raise ValueError(
                 "IoULoss expects y_pred.shape[-1] to be 4 to represent the "
                 f"bounding boxes. Received y_pred.shape[-1]={y_pred.shape[-1]}."
             )
 
@@ -102,30 +93,30 @@
                 "number of boxes in y_true. Received number of boxes in "
                 f"y_true={y_true.shape[-2]} and number of boxes in "
                 f"y_pred={y_pred.shape[-2]}."
             )
 
         iou = bounding_box.compute_iou(y_true, y_pred, self.bounding_box_format)
         # pick out the diagonal for corresponding ious
-        iou = tf.linalg.diag_part(iou)
+        iou = ops.diagonal(iou)
         if self.axis == "no_reduction":
             warnings.warn(
                 "`axis='no_reduction'` is a temporary API, and the API "
                 "contract will be replaced in the future with a more generic "
                 "solution covering all losses."
             )
         else:
-            iou = tf.reduce_mean(iou, axis=self.axis)
+            iou = ops.mean(iou, axis=self.axis)
 
         if self.mode == "linear":
             loss = 1 - iou
         elif self.mode == "quadratic":
             loss = 1 - iou**2
         elif self.mode == "log":
-            loss = -tf.math.log(iou)
+            loss = -ops.log(iou)
 
         return loss
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
```

## keras_cv/losses/iou_loss_test.py

```diff
@@ -8,39 +8,32 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
 from keras_cv.losses.iou_loss import IoULoss
 
 
 class IoUTest(tf.test.TestCase):
     def test_output_shape(self):
-        y_true = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
-        )
-        y_pred = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
-        )
+        y_true = np.random.uniform(size=(2, 2, 4), low=0, high=10)
+        y_pred = np.random.uniform(size=(2, 2, 4), low=0, high=20)
 
         iou_loss = IoULoss(bounding_box_format="xywh")
 
         self.assertAllEqual(iou_loss(y_true, y_pred).shape, ())
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=10, dtype=tf.int32
-        )
-        y_pred = tf.random.uniform(
-            shape=(2, 2, 4), minval=0, maxval=20, dtype=tf.int32
-        )
+        y_true = np.random.uniform(size=(2, 2, 4), low=0, high=10)
+        y_pred = np.random.uniform(size=(2, 2, 4), low=0, high=20)
 
         iou_loss = IoULoss(bounding_box_format="xywh", reduction="none")
 
         self.assertAllEqual(
             iou_loss(y_true, y_pred).shape,
             [
                 2,
```

## keras_cv/losses/penalty_reduced_focal_loss.py

```diff
@@ -8,16 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
 # TODO(tanzhenyu): consider inherit from LossFunctionWrapper to
 #  get the dimension squeeze.
 @keras.utils.register_keras_serializable(package="keras_cv")
 class BinaryPenaltyReducedFocalCrossEntropy(keras.losses.Loss):
     """Implements CenterNet modified Focal loss.
@@ -58,47 +58,47 @@
         self,
         alpha=2.0,
         beta=4.0,
         from_logits=False,
         positive_threshold=0.99,
         positive_weight=1.0,
         negative_weight=1.0,
-        reduction=keras.losses.Reduction.AUTO,
+        reduction="sum_over_batch_size",
         name="binary_penalty_reduced_focal_cross_entropy",
     ):
         super().__init__(reduction=reduction, name=name)
         self.alpha = alpha
         self.beta = beta
         self.from_logits = from_logits
         self.positive_threshold = positive_threshold
         self.positive_weight = positive_weight
         self.negative_weight = negative_weight
 
     def call(self, y_true, y_pred):
-        y_pred = tf.convert_to_tensor(y_pred)
-        y_true = tf.cast(y_true, y_pred.dtype)
+        y_pred = ops.convert_to_tensor(y_pred)
+        y_true = ops.cast(y_true, y_pred.dtype)
 
         if self.from_logits:
-            y_pred = tf.nn.sigmoid(y_pred)
+            y_pred = ops.sigmoid(y_pred)
 
         # TODO(tanzhenyu): Evaluate whether we need clipping after model is
         #  trained.
-        y_pred = tf.clip_by_value(y_pred, 1e-4, 0.9999)
-        y_true = tf.clip_by_value(y_true, 0.0, 1.0)
+        y_pred = ops.clip(y_pred, 1e-4, 0.9999)
+        y_true = ops.clip(y_true, 0.0, 1.0)
 
-        pos_loss = tf.math.pow(1.0 - y_pred, self.alpha) * tf.math.log(y_pred)
+        pos_loss = ops.power(1.0 - y_pred, self.alpha) * ops.log(y_pred)
         neg_loss = (
-            tf.math.pow(1.0 - y_true, self.beta)
-            * tf.math.pow(y_pred, self.alpha)
-            * tf.math.log(1.0 - y_pred)
+            ops.power(1.0 - y_true, self.beta)
+            * ops.power(y_pred, self.alpha)
+            * ops.log(1.0 - y_pred)
         )
 
         positive_mask = y_true > self.positive_threshold
 
-        loss = tf.where(
+        loss = ops.where(
             positive_mask,
             self.positive_weight * pos_loss,
             self.negative_weight * neg_loss,
         )
 
         return -1.0 * loss
```

## keras_cv/losses/penalty_reduced_focal_loss_test.py

```diff
@@ -16,86 +16,76 @@
 import tensorflow as tf
 
 from keras_cv.losses import BinaryPenaltyReducedFocalCrossEntropy
 
 
 class BinaryPenaltyReducedFocalLossTest(tf.test.TestCase):
     def test_output_shape(self):
-        y_true = tf.cast(
-            tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
-            tf.float32,
-        )
-        y_pred = tf.random.uniform(
-            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
-        )
+        y_true = (np.random.uniform(size=[2, 5], low=0, high=2),)
+        y_pred = np.random.uniform(size=[2, 5], low=0, high=1)
 
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
 
         self.assertAllEqual(focal_loss(y_true, y_pred).shape, [])
 
     def test_output_shape_reduction_none(self):
-        y_true = tf.cast(
-            tf.random.uniform(shape=[2, 5], minval=0, maxval=2, dtype=tf.int32),
-            tf.float32,
-        )
-        y_pred = tf.random.uniform(
-            shape=[2, 5], minval=0, maxval=1, dtype=tf.float32
-        )
+        y_true = np.random.uniform(size=[2, 5], low=0, high=2)
+        y_pred = np.random.uniform(size=[2, 5], low=0, high=2)
 
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="none")
 
         self.assertAllEqual(
             [2, 5],
             focal_loss(y_true, y_pred).shape,
         )
 
     def test_output_with_pos_label_pred(self):
-        y_true = tf.constant([1.0])
-        y_pred = tf.constant([1.0])
+        y_true = np.array([1.0])
+        y_pred = np.array([1.0])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         self.assertAllClose(0.0, focal_loss(y_true, y_pred))
 
     def test_output_with_pos_label_neg_pred(self):
-        y_true = tf.constant([1.0])
-        y_pred = tf.constant([np.exp(-1.0)])
+        y_true = np.array([1.0])
+        y_pred = np.array([np.exp(-1.0)])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         # (1-1/e)^2 * log(1/e)
         self.assertAllClose(
             np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
         )
 
     def test_output_with_neg_label_pred(self):
-        y_true = tf.constant([0.0])
-        y_pred = tf.constant([0.0])
+        y_true = np.array([0.0])
+        y_pred = np.array([0.0])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         self.assertAllClose(0.0, focal_loss(y_true, y_pred))
 
     def test_output_with_neg_label_pos_pred(self):
-        y_true = tf.constant([0.0])
-        y_pred = tf.constant([1.0 - np.exp(-1.0)])
+        y_true = np.array([0.0])
+        y_pred = np.array([1.0 - np.exp(-1.0)])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         # (1-0)^4 * (1-1/e)^2 * log(1/e)
         self.assertAllClose(
             np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
         )
 
     def test_output_with_weak_label_pos_pred(self):
-        y_true = tf.constant([0.5])
-        y_pred = tf.constant([1.0 - np.exp(-1.0)])
+        y_true = np.array([0.5])
+        y_pred = np.array([1.0 - np.exp(-1.0)])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(
             beta=2.0, reduction="sum"
         )
         # (1-0.5)^2 * (1-1/e)^2 * log(1/e)
         self.assertAllClose(
             0.25 * np.square(1 - np.exp(-1.0)), focal_loss(y_true, y_pred)
         )
 
     def test_output_with_sample_weight(self):
-        y_true = tf.constant([0.0])
-        y_pred = tf.constant([1.0 - np.exp(-1.0)])
-        sample_weight = tf.constant([0.5])
+        y_true = np.array([0.0])
+        y_pred = np.array([1.0 - np.exp(-1.0)])
+        sample_weight = np.array([0.5])
         focal_loss = BinaryPenaltyReducedFocalCrossEntropy(reduction="sum")
         # (1-0)^4 * (1-1/e)^2 * log(1/e)
         self.assertAllClose(
             0.5 * np.square(1 - np.exp(-1.0)),
             focal_loss(y_true, y_pred, sample_weight=sample_weight),
         )
```

## keras_cv/losses/simclr_loss.py

```diff
@@ -8,20 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 LARGE_NUM = 1e9
 
 
+def l2_normalize(x, axis):
+    epsilon = keras.backend.epsilon()
+    power_sum = ops.sum(ops.square(x), axis=axis, keepdims=True)
+    norm = ops.reciprocal(ops.sqrt(ops.maximum(power_sum, epsilon)))
+    return ops.multiply(x, norm)
+
+
 class SimCLRLoss(keras.losses.Loss):
     """Implements SimCLR Cosine Similarity loss.
 
     SimCLR loss is used for contrastive self-supervised learning.
 
     Args:
         temperature: a float value between 0 and 1, used as a scaling factor for
@@ -49,47 +56,47 @@
             projections_2: a tensor with the output of the second projection
                 model in a contrastive learning trainer
 
         Returns:
             A tensor with the SimCLR loss computed from the input projections
         """
         # Normalize the projections
-        projections_1 = tf.math.l2_normalize(projections_1, axis=1)
-        projections_2 = tf.math.l2_normalize(projections_2, axis=1)
+        projections_1 = l2_normalize(projections_1, axis=1)
+        projections_2 = l2_normalize(projections_2, axis=1)
 
         # Produce artificial labels, 1 for each image in the batch.
-        batch_size = tf.shape(projections_1)[0]
-        labels = tf.one_hot(tf.range(batch_size), batch_size * 2)
-        masks = tf.one_hot(tf.range(batch_size), batch_size)
+        batch_size = ops.shape(projections_1)[0]
+        labels = ops.one_hot(ops.arange(batch_size), batch_size * 2)
+        masks = ops.one_hot(ops.arange(batch_size), batch_size)
 
         # Compute logits
         logits_11 = (
-            tf.matmul(projections_1, projections_1, transpose_b=True)
+            ops.matmul(projections_1, ops.transpose(projections_1))
             / self.temperature
         )
-        logits_11 = logits_11 - tf.cast(masks * LARGE_NUM, logits_11.dtype)
+        logits_11 = logits_11 - ops.cast(masks * LARGE_NUM, logits_11.dtype)
         logits_22 = (
-            tf.matmul(projections_2, projections_2, transpose_b=True)
+            ops.matmul(projections_2, ops.transpose(projections_2))
             / self.temperature
         )
-        logits_22 = logits_22 - tf.cast(masks * LARGE_NUM, logits_22.dtype)
+        logits_22 = logits_22 - ops.cast(masks * LARGE_NUM, logits_22.dtype)
         logits_12 = (
-            tf.matmul(projections_1, projections_2, transpose_b=True)
+            ops.matmul(projections_1, ops.transpose(projections_2))
             / self.temperature
         )
         logits_21 = (
-            tf.matmul(projections_2, projections_1, transpose_b=True)
+            ops.matmul(projections_2, ops.transpose(projections_1))
             / self.temperature
         )
 
         loss_a = keras.losses.categorical_crossentropy(
-            labels, tf.concat([logits_12, logits_11], 1), from_logits=True
+            labels, ops.concatenate([logits_12, logits_11], 1), from_logits=True
         )
         loss_b = keras.losses.categorical_crossentropy(
-            labels, tf.concat([logits_21, logits_22], 1), from_logits=True
+            labels, ops.concatenate([logits_21, logits_22], 1), from_logits=True
         )
 
         return loss_a + loss_b
 
     def get_config(self):
         config = super().get_config()
         config.update({"temperature": self.temperature})
```

## keras_cv/losses/simclr_loss_test.py

```diff
@@ -8,57 +8,54 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
 from keras_cv.losses.simclr_loss import SimCLRLoss
 
 
 class SimCLRLossTest(tf.test.TestCase):
     def test_output_shape(self):
-        projections_1 = tf.random.uniform(
-            shape=(10, 128), minval=0, maxval=10, dtype=tf.float32
-        )
-        projections_2 = tf.random.uniform(
-            shape=(10, 128), minval=0, maxval=10, dtype=tf.float32
-        )
+        projections_1 = np.random.uniform(size=(10, 128), low=0, high=10)
+        projections_2 = np.random.uniform(size=(10, 128), low=0, high=10)
 
         simclr_loss = SimCLRLoss(temperature=1)
 
         self.assertAllEqual(simclr_loss(projections_1, projections_2).shape, ())
 
     def test_output_shape_reduction_none(self):
-        projections_1 = tf.random.uniform(
-            shape=(10, 128), minval=0, maxval=10, dtype=tf.float32
-        )
-        projections_2 = tf.random.uniform(
-            shape=(10, 128), minval=0, maxval=10, dtype=tf.float32
-        )
+        projections_1 = np.random.uniform(size=(10, 128), low=0, high=10)
+        projections_2 = np.random.uniform(size=(10, 128), low=0, high=10)
 
         simclr_loss = SimCLRLoss(temperature=1, reduction="none")
 
         self.assertAllEqual(
             simclr_loss(projections_1, projections_2).shape, (10,)
         )
 
     def test_output_value(self):
-        projections_1 = [
-            [1.0, 2.0, 3.0, 4.0],
-            [2.0, 3.0, 4.0, 5.0],
-            [3.0, 4.0, 5.0, 6.0],
-        ]
-
-        projections_2 = [
-            [6.0, 5.0, 4.0, 3.0],
-            [5.0, 4.0, 3.0, 2.0],
-            [4.0, 3.0, 2.0, 1.0],
-        ]
+        projections_1 = np.array(
+            [
+                [1.0, 2.0, 3.0, 4.0],
+                [2.0, 3.0, 4.0, 5.0],
+                [3.0, 4.0, 5.0, 6.0],
+            ]
+        )
+
+        projections_2 = np.array(
+            [
+                [6.0, 5.0, 4.0, 3.0],
+                [5.0, 4.0, 3.0, 2.0],
+                [4.0, 3.0, 2.0, 1.0],
+            ]
+        )
 
         simclr_loss = SimCLRLoss(temperature=0.5)
         self.assertAllClose(simclr_loss(projections_1, projections_2), 3.566689)
 
         simclr_loss = SimCLRLoss(temperature=0.1)
         self.assertAllClose(simclr_loss(projections_1, projections_2), 5.726100)
```

## keras_cv/losses/smooth_l1.py

```diff
@@ -8,16 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
 # --- Implementing Smooth L1 loss and Focal Loss as keras custom losses ---
 class SmoothL1Loss(keras.losses.Loss):
     """Implements Smooth L1 loss.
 
     SmoothL1Loss implements the SmoothL1 function, where values less than
@@ -32,22 +32,22 @@
 
     def __init__(self, l1_cutoff=1.0, **kwargs):
         super().__init__(**kwargs)
         self.l1_cutoff = l1_cutoff
 
     def call(self, y_true, y_pred):
         difference = y_true - y_pred
-        absolute_difference = tf.abs(difference)
+        absolute_difference = ops.abs(difference)
         squared_difference = difference**2
-        loss = tf.where(
+        loss = ops.where(
             absolute_difference < self.l1_cutoff,
             0.5 * squared_difference,
             absolute_difference - 0.5,
         )
-        return keras.backend.mean(loss, axis=-1)
+        return ops.mean(loss, axis=-1)
 
     def get_config(self):
         config = {
             "l1_cutoff": self.l1_cutoff,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
```

## keras_cv/losses/smooth_l1_test.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 from absl.testing import parameterized
 
 import keras_cv
 
 
 class SmoothL1LossTest(tf.test.TestCase, parameterized.TestCase):
@@ -23,11 +24,11 @@
         ("none", "none", (20,)),
         ("sum", "sum", ()),
         ("sum_over_batch_size", "sum_over_batch_size", ()),
     )
     def test_proper_output_shapes(self, reduction, target_size):
         loss = keras_cv.losses.SmoothL1Loss(l1_cutoff=0.5, reduction=reduction)
         result = loss(
-            y_true=tf.random.uniform((20, 300)),
-            y_pred=tf.random.uniform((20, 300)),
+            y_true=np.random.uniform(size=(20, 300)),
+            y_pred=np.random.uniform(size=(20, 300)),
         )
         self.assertEqual(result.shape, target_size)
```

## keras_cv/metrics/coco/pycoco_wrapper.py

```diff
@@ -10,15 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 
 import numpy as np
-import tensorflow as tf
 
 try:
     from pycocotools.coco import COCO
     from pycocotools.cocoeval import COCOeval
 except ImportError:
     COCO = object
     COCOeval = None
@@ -189,42 +188,35 @@
         "images": gt_images,
         "categories": gt_categories,
         "annotations": copy.deepcopy(gt_annotations),
     }
     return gt_dataset
 
 
-def _convert_to_numpy(groundtruths, predictions):
+def _concat_numpy(groundtruths, predictions):
     """Converts tensors to numpy arrays."""
-    labels = tf.nest.map_structure(lambda x: x.numpy(), groundtruths)
     numpy_groundtruths = {}
-    for key, val in labels.items():
+    for key, val in groundtruths.items():
         if isinstance(val, tuple):
             val = np.concatenate(val)
         numpy_groundtruths[key] = val
 
-    def _to_numpy(x):
-        if isinstance(x, np.ndarray):
-            return x
-        return x.numpy()
-
-    outputs = tf.nest.map_structure(lambda x: _to_numpy(x), predictions)
     numpy_predictions = {}
-    for key, val in outputs.items():
+    for key, val in predictions.items():
         if isinstance(val, tuple):
             val = np.concatenate(val)
         numpy_predictions[key] = val
 
     return numpy_groundtruths, numpy_predictions
 
 
 def compute_pycoco_metrics(groundtruths, predictions):
     assert_pycocotools_installed("compute_pycoco_metrics")
 
-    groundtruths, predictions = _convert_to_numpy(groundtruths, predictions)
+    groundtruths, predictions = _concat_numpy(groundtruths, predictions)
 
     gt_dataset = _convert_groundtruths_to_coco_dataset(groundtruths)
     coco_gt = PyCOCOWrapper(gt_dataset=gt_dataset)
     coco_predictions = _convert_predictions_to_coco_annotations(predictions)
     coco_dt = coco_gt.loadRes(predictions=coco_predictions)
     image_ids = [ann["image_id"] for ann in coco_predictions]
```

## keras_cv/metrics/object_detection/box_coco_metrics.py

```diff
@@ -11,19 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import os
 import sys
 import types
 
+import numpy as np
 import tensorflow as tf
 import tensorflow.keras as keras
 
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.backend import ops
 
 
 class HidePrints:
     """A basic internal only context manager to hide print statements."""
 
     def __enter__(self):
         self._original_stdout = sys.stdout
@@ -263,14 +265,17 @@
         for key in METRIC_NAMES:
             # Workaround for the state where there are 0 boxes in a category.
             results.append(max(metrics[key], 0.0))
         return results
 
 
 def compute_pycocotools_metric(y_true, y_pred, bounding_box_format):
+    y_true = bounding_box.to_dense(y_true)
+    y_pred = bounding_box.to_dense(y_pred)
+
     box_pred = y_pred["boxes"]
     cls_pred = y_pred["classes"]
     confidence_pred = y_pred["confidence"]
 
     gt_boxes = y_true["boxes"]
     gt_classes = y_true["classes"]
 
@@ -279,32 +284,30 @@
     )
     gt_boxes = bounding_box.convert_format(
         gt_boxes, source=bounding_box_format, target="yxyx"
     )
 
     total_images = gt_boxes.shape[0]
 
-    source_ids = tf.strings.as_string(
-        tf.linspace(1, total_images, total_images), precision=0
-    )
+    source_ids = np.char.mod("%d", np.linspace(1, total_images, total_images))
 
     ground_truth = {}
     ground_truth["source_id"] = [source_ids]
 
     ground_truth["num_detections"] = [
-        tf.math.reduce_sum(tf.cast(y_true["classes"] != -1, tf.int32), axis=-1)
+        ops.sum(ops.cast(y_true["classes"] >= 0, "int32"), axis=-1)
     ]
-    ground_truth["boxes"] = [gt_boxes]
-    ground_truth["classes"] = [gt_classes]
+    ground_truth["boxes"] = [ops.convert_to_numpy(gt_boxes)]
+    ground_truth["classes"] = [ops.convert_to_numpy(gt_classes)]
 
     predictions = {}
     predictions["source_id"] = [source_ids]
-    predictions["detection_boxes"] = [box_pred]
-    predictions["detection_classes"] = [cls_pred]
-    predictions["detection_scores"] = [confidence_pred]
+    predictions["detection_boxes"] = [ops.convert_to_numpy(box_pred)]
+    predictions["detection_classes"] = [ops.convert_to_numpy(cls_pred)]
+    predictions["detection_scores"] = [ops.convert_to_numpy(confidence_pred)]
     predictions["num_detections"] = [
-        tf.math.reduce_sum(tf.cast(y_pred["classes"] != -1, tf.int32), axis=-1)
+        ops.sum(ops.cast(confidence_pred > 0, "int32"), axis=-1)
     ]
 
     return keras_cv.metrics.coco.compute_pycoco_metrics(
         ground_truth, predictions
     )
```

## keras_cv/models/task.py

```diff
@@ -11,21 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Base class for Task models."""
 
 import os
 
-from tensorflow import keras
-
+from keras_cv.backend import keras
 from keras_cv.utils.python_utils import classproperty
 from keras_cv.utils.python_utils import format_docstring
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class Task(keras.Model):
     """Base class for Task models."""
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._backbone = None
 
@@ -126,15 +125,15 @@
                 "the following presets with weights:"
                 f""" "{'", "'.join(cls.presets_with_weights)}"."""
             )
 
         metadata = cls.presets[preset]
         # Check if preset is backbone-only model
         if preset in cls.backbone_presets:
-            backbone_cls = keras.utils.get_registered_object(
+            backbone_cls = keras.saving.get_registered_object(
                 metadata["class_name"]
             )
             backbone = backbone_cls.from_preset(preset, load_weights)
             return cls(backbone, **kwargs)
 
         # Otherwise must be one of class presets
         config = metadata["config"]
@@ -160,21 +159,28 @@
     @property
     def layers(self):
         # Some of our task models don't use the Backbone directly, but create
         # a feature extractor from it. In these cases, we don't want to count
         # the `backbone` as a layer, because it will be included in the model
         # summary and saves weights despite not being part of the model graph.
         layers = super().layers
-        if hasattr(self, "_backbone") and self.backbone in layers:
+        if hasattr(self, "backbone") and self.backbone in layers:
             # We know that the backbone is not part of the graph if it has no
             # inbound nodes.
             if len(self.backbone._inbound_nodes) == 0:
                 layers.remove(self.backbone)
         return layers
 
+    def __setattr__(self, name, value):
+        # Work around torch setattr for properties.
+        if name in ["backbone"]:
+            object.__setattr__(self, name, value)
+        else:
+            super().__setattr__(name, value)
+
     def __init_subclass__(cls, **kwargs):
         # Use __init_subclass__ to set up a correct docstring for from_preset.
         super().__init_subclass__(**kwargs)
 
         # If the subclass does not define from_preset, assign a wrapper so that
         # each class can have a distinct docstring.
         if "from_preset" not in cls.__dict__:
```

## keras_cv/models/utils.py

```diff
@@ -10,25 +10,31 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utility functions for models"""
 
-from keras import backend
-from keras import layers
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend.config import multi_backend
+
+
+def get_tensor_input_name(tensor):
+    if multi_backend():
+        return tensor._keras_history.operation.name
+    else:
+        return tensor.node.layer.name
 
 
 def parse_model_inputs(input_shape, input_tensor):
     if input_tensor is None:
-        return layers.Input(shape=input_shape)
+        return keras.layers.Input(shape=input_shape)
     else:
         if not keras.backend.is_keras_tensor(input_tensor):
-            return layers.Input(tensor=input_tensor, shape=input_shape)
+            return keras.layers.Input(tensor=input_tensor, shape=input_shape)
         else:
             return input_tensor
 
 
 def correct_pad_downsample(inputs, kernel_size):
     """Returns a tuple for zero-padding for 2D convolution with downsampling.
 
@@ -36,15 +42,15 @@
         inputs: Input tensor.
         kernel_size: An integer or tuple/list of 2 integers.
 
     Returns:
         A tuple.
     """
     img_dim = 1
-    input_size = backend.int_shape(inputs)[img_dim : (img_dim + 2)]
+    input_size = inputs.shape[img_dim : (img_dim + 2)]
     if isinstance(kernel_size, int):
         kernel_size = (kernel_size, kernel_size)
     if input_size[0] is None:
         adjust = (1, 1)
     else:
         adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)
     correct = (kernel_size[0] // 2, kernel_size[1] // 2)
```

## keras_cv/models/backbones/backbone.py

```diff
@@ -11,16 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Base class for Backbone models."""
 
 import os
 
-from tensorflow import keras
-
+from keras_cv.backend import keras
 from keras_cv.utils.python_utils import classproperty
 from keras_cv.utils.python_utils import format_docstring
 
 
 class Backbone(keras.Model):
     """Base class for Backbone models.
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py

```diff
@@ -11,17 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """CSPDarkNet models for KerasCV. """
 import copy
 
-from tensorflow import keras
-from tensorflow.keras import layers
-
+from keras_cv.backend import keras
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone_presets import (
     backbone_presets,
 )
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone_presets import (
     backbone_presets_with_weights,
@@ -38,15 +36,15 @@
 from keras_cv.models.backbones.csp_darknet.csp_darknet_utils import Focus
 from keras_cv.models.backbones.csp_darknet.csp_darknet_utils import (
     SpatialPyramidPoolingBottleneck,
 )
 from keras_cv.utils.python_utils import classproperty
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class CSPDarkNetBackbone(Backbone):
     """This class represents the CSPDarkNet architecture.
 
     Reference:
         - [YoloV4 Paper](https://arxiv.org/abs/1804.02767)
         - [CSPNet Paper](https://arxiv.org/abs/1911.11929)
         - [YoloX Paper](https://arxiv.org/abs/2107.08430)
@@ -60,16 +58,16 @@
         stackwise_depth: A list of ints, the depth for each dark level in the
             model.
         include_rescaling: bool, whether to rescale the inputs. If set to
             True, inputs will be passed through a `Rescaling(1/255.0)` layer.
         use_depthwise: bool, whether a `DarknetConvBlockDepthwise` should be
             used over a `DarknetConvBlock`, defaults to False.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
+        input_tensor: optional Keras tensor (i.e. output of
+            `keras.layers.Input()`) to use as image input for the model.
 
     Returns:
         A `keras.Model` instance.
 
     Examples:
     ```python
     input_data = tf.ones(shape=(8, 224, 224, 3))
@@ -107,15 +105,15 @@
 
         base_channels = stackwise_channels[0] // 2
 
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
 
         x = inputs
         if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
+            x = keras.layers.Rescaling(1 / 255.0)(x)
 
         # stem
         x = Focus(name="stem_focus")(x)
         x = DarknetConvBlock(
             base_channels, kernel_size=3, strides=1, name="stem_conv"
         )(x)
 
@@ -140,15 +138,17 @@
             x = CrossStagePartial(
                 channels,
                 num_bottlenecks=depth,
                 use_depthwise=use_depthwise,
                 residual=(index != len(stackwise_depth) - 1),
                 name=f"dark{index + 2}_csp",
             )(x)
-            pyramid_level_inputs[f"P{index + 2}"] = x.node.layer.name
+            pyramid_level_inputs[f"P{index + 2}"] = utils.get_tensor_input_name(
+                x
+            )
 
         super().__init__(inputs=inputs, outputs=x, **kwargs)
         self.pyramid_level_inputs = pyramid_level_inputs
 
         self.stackwise_channels = stackwise_channels
         self.stackwise_depth = stackwise_depth
         self.include_rescaling = include_rescaling
@@ -191,16 +191,16 @@
 
     For transfer learning use cases, make sure to read the
     [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
 
     Args:
         include_rescaling: bool, whether or not to rescale the inputs. If set to
             True, inputs will be passed through a `Rescaling(1/255.0)` layer.
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
+        input_tensor: optional Keras tensor (i.e. output of
+            `keras.layers.Input()`) to use as image input for the model.
         input_shape: optional shape tuple, defaults to (None, None, 3).
 
     Examples:
     ```python
     input_data = tf.ones(shape=(8, 224, 224, 3))
 
     # Randomly initialized backbone
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv.backend import ops
 from keras_cv.models.backbones.csp_darknet import csp_darknet_backbone
 
 
 @pytest.mark.large
 class CSPDarkNetPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
     """
     A smoke test for CSPDarkNet presets we run continuously.
@@ -48,15 +49,18 @@
         # network code in a way that would invalidate our preset weights.
         # We should only update these numbers if we are updating a weights
         # file, or have found a discrepancy with the upstream source.
 
         expected = [-0.16216235, 0.7333651, 0.4312072, 0.738807, -0.2515305]
         # Keep a high tolerance, so we are robust to different hardware.
         self.assertAllClose(
-            outputs[0, 0, 0, :5], expected, atol=0.01, rtol=0.01
+            ops.convert_to_numpy(outputs[0, 0, 0, :5]),
+            expected,
+            atol=0.01,
+            rtol=0.01,
         )
 
     def test_applications_model_output(self):
         model = csp_darknet_backbone.CSPDarkNetMBackbone()
         model(self.input_batch)
 
     def test_applications_model_output_with_preset(self):
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py

```diff
@@ -10,26 +10,28 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.csp_darknet import csp_darknet_backbone
 from keras_cv.utils.train import get_feature_extractor
 
 
 class CSPDarkNetBackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_valid_call(self):
         model = csp_darknet_backbone.CSPDarkNetBackbone(
             stackwise_channels=[48, 96, 192, 384],
             stackwise_depth=[1, 3, 3, 1],
             include_rescaling=False,
         )
@@ -43,100 +45,102 @@
         model = csp_darknet_backbone.CSPDarkNetBackbone(
             stackwise_channels=[48, 96, 192, 384],
             stackwise_depth=[1, 3, 3, 1],
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = csp_darknet_backbone.CSPDarkNetBackbone(
             stackwise_channels=[48, 96, 192, 384],
             stackwise_depth=[1, 3, 3, 1],
             include_rescaling=True,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "csp_darknet_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(
             restored_model, csp_darknet_backbone.CSPDarkNetBackbone
         )
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_alias_model(self, save_format, filename):
+    def test_saved_alias_model(self):
         model = csp_darknet_backbone.CSPDarkNetLBackbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "csp_darknet_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         # Note that these aliases serialized as the base class
         self.assertIsInstance(
             restored_model, csp_darknet_backbone.CSPDarkNetBackbone
         )
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = csp_darknet_backbone.CSPDarkNetLBackbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 128],
+            (None, input_size // 2**2, input_size // 2**2, 128),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 256],
+            (None, input_size // 2**3, input_size // 2**3, 256),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 512],
+            (None, input_size // 2**4, input_size // 2**4, 512),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 1024],
+            (None, input_size // 2**5, input_size // 2**5, 1024),
         )
 
     @parameterized.named_parameters(
         ("Tiny", csp_darknet_backbone.CSPDarkNetTinyBackbone),
         ("S", csp_darknet_backbone.CSPDarkNetSBackbone),
         ("M", csp_darknet_backbone.CSPDarkNetMBackbone),
         ("L", csp_darknet_backbone.CSPDarkNetLBackbone),
         ("XL", csp_darknet_backbone.CSPDarkNetXLBackbone),
     )
     def test_specific_arch_forward_pass(self, arch_class):
         backbone = arch_class()
-        backbone(tf.random.uniform(shape=[2, 256, 256, 3]))
+        backbone(np.random.uniform(size=(2, 256, 256, 3)))
 
     @parameterized.named_parameters(
         ("Tiny", csp_darknet_backbone.CSPDarkNetTinyBackbone),
         ("S", csp_darknet_backbone.CSPDarkNetSBackbone),
         ("M", csp_darknet_backbone.CSPDarkNetMBackbone),
         ("L", csp_darknet_backbone.CSPDarkNetLBackbone),
         ("XL", csp_darknet_backbone.CSPDarkNetXLBackbone),
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py

```diff
@@ -14,18 +14,15 @@
 
 """CSPDarkNet model utils for KerasCV.
 Reference:
   - [YoloV3 Paper](https://arxiv.org/abs/1804.02767)
   - [YoloV3 implementation](https://github.com/ultralytics/yolov3)
 """
 
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend
-from tensorflow.keras import layers
+from keras_cv.backend import keras
 
 
 def DarknetConvBlock(
     filters, kernel_size, strides, use_bias=False, activation="silu", name=None
 ):
     """The basic conv block used in Darknet. Applies Conv2D followed by a
     BatchNorm.
@@ -42,36 +39,38 @@
         use_bias: Boolean, whether the layer uses a bias vector.
         activation: the activation applied after the BatchNorm layer. One of
             "silu", "relu" or "leaky_relu", defaults to "silu".
         name: the prefix for the layer names used in the block.
     """
 
     if name is None:
-        name = f"conv_block{backend.get_uid('conv_block')}"
+        name = f"conv_block{keras.backend.get_uid('conv_block')}"
 
     model_layers = [
-        layers.Conv2D(
+        keras.layers.Conv2D(
             filters,
             kernel_size,
             strides,
             padding="same",
             use_bias=use_bias,
             name=name + "_conv",
         ),
-        layers.BatchNormalization(name=name + "_bn"),
+        keras.layers.BatchNormalization(name=name + "_bn"),
     ]
 
     if activation == "silu":
-        model_layers.append(layers.Lambda(lambda x: keras.activations.swish(x)))
+        model_layers.append(
+            keras.layers.Lambda(lambda x: keras.activations.silu(x))
+        )
     elif activation == "relu":
-        model_layers.append(layers.ReLU())
+        model_layers.append(keras.layers.ReLU())
     elif activation == "leaky_relu":
-        model_layers.append(layers.LeakyReLU(0.1))
+        model_layers.append(keras.layers.LeakyReLU(0.1))
 
-    return keras.Sequential(model_layers, name=None)
+    return keras.Sequential(model_layers, name=name)
 
 
 def ResidualBlocks(filters, num_blocks, name=None):
     """A residual block used in DarkNet models, repeated `num_blocks` times.
 
     Args:
         filters: Integer, the dimensionality of the output spaces (i.e. the
@@ -80,15 +79,15 @@
         name: the prefix for the layer names used in the block.
 
     Returns:
         a function that takes an input Tensor representing a ResidualBlock.
     """
 
     if name is None:
-        name = f"residual_block{backend.get_uid('residual_block')}"
+        name = f"residual_block{keras.backend.get_uid('residual_block')}"
 
     def apply(x):
         x = DarknetConvBlock(
             filters,
             kernel_size=3,
             strides=2,
             activation="leaky_relu",
@@ -110,17 +109,17 @@
                 kernel_size=3,
                 strides=1,
                 activation="leaky_relu",
                 name=f"{name}_conv{2*i + 1}",
             )(x)
 
             if i == num_blocks:
-                x = layers.Add(name=f"{name}_out")([residual, x])
+                x = keras.layers.Add(name=f"{name}_out")([residual, x])
             else:
-                x = layers.Add(name=f"{name}_add_{i}")([residual, x])
+                x = keras.layers.Add(name=f"{name}_add_{i}")([residual, x])
 
         return x
 
     return apply
 
 
 def SpatialPyramidPoolingBottleneck(
@@ -145,15 +144,15 @@
         name: the prefix for the layer names used in the block.
 
     Returns:
         a function that takes an input Tensor representing an
         SpatialPyramidPoolingBottleneck.
     """
     if name is None:
-        name = f"spp{backend.get_uid('spp')}"
+        name = f"spp{keras.backend.get_uid('spp')}"
 
     if hidden_filters is None:
         hidden_filters = filters
 
     def apply(x):
         x = DarknetConvBlock(
             hidden_filters,
@@ -162,23 +161,23 @@
             activation=activation,
             name=f"{name}_conv1",
         )(x)
         x = [x]
 
         for kernel_size in kernel_sizes:
             x.append(
-                layers.MaxPooling2D(
+                keras.layers.MaxPooling2D(
                     kernel_size,
                     strides=1,
                     padding="same",
                     name=f"{name}_maxpool_{kernel_size}",
                 )(x[0])
             )
 
-        x = layers.Concatenate(name=f"{name}_concat")(x)
+        x = keras.layers.Concatenate(name=f"{name}_concat")(x)
         x = DarknetConvBlock(
             filters,
             kernel_size=1,
             strides=1,
             activation=activation,
             name=f"{name}_conv2",
         )(x)
@@ -205,41 +204,43 @@
         activation: the activation applied after the final layer. One of "silu",
             "relu" or "leaky_relu", defaults to "silu".
         name: the prefix for the layer names used in the block.
 
     """
 
     if name is None:
-        name = f"conv_block{backend.get_uid('conv_block')}"
+        name = f"conv_block{keras.backend.get_uid('conv_block')}"
 
     model_layers = [
-        layers.DepthwiseConv2D(
+        keras.layers.DepthwiseConv2D(
             kernel_size, strides, padding="same", use_bias=False
         ),
-        layers.BatchNormalization(),
+        keras.layers.BatchNormalization(),
     ]
 
     if activation == "silu":
-        model_layers.append(layers.Lambda(lambda x: keras.activations.swish(x)))
+        model_layers.append(
+            keras.layers.Lambda(lambda x: keras.activations.swish(x))
+        )
     elif activation == "relu":
-        model_layers.append(layers.ReLU())
+        model_layers.append(keras.layers.ReLU())
     elif activation == "leaky_relu":
-        model_layers.append(layers.LeakyReLU(0.1))
+        model_layers.append(keras.layers.LeakyReLU(0.1))
 
     model_layers.append(
         DarknetConvBlock(
             filters, kernel_size=1, strides=1, activation=activation
         )
     )
 
     return keras.Sequential(model_layers, name=name)
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class CrossStagePartial(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class CrossStagePartial(keras.layers.Layer):
     """A block used in Cross Stage Partial Darknet.
 
     Args:
         filters: Integer, the dimensionality of the output space (i.e. the
             number of output filters in the final convolution).
         num_bottlenecks: an integer representing the number of blocks added in
             the layer bottleneck.
@@ -305,16 +306,16 @@
                     hidden_channels,
                     kernel_size=3,
                     strides=1,
                     activation=activation,
                 )
             )
 
-        self.add = layers.Add()
-        self.concatenate = layers.Concatenate()
+        self.add = keras.layers.Add()
+        self.concatenate = keras.layers.Concatenate()
 
         self.darknet_conv3 = DarknetConvBlock(
             filters, kernel_size=1, strides=1, activation=activation
         )
 
     def call(self, x):
         x1 = self.darknet_conv1(x)
@@ -356,21 +357,17 @@
         name: the name for the lambda layer used in the block.
 
     Returns:
         a function that takes an input Tensor representing a Focus layer.
     """  # noqa: E501
 
     def apply(x):
-        return layers.Lambda(
-            lambda x: tf.concat(
-                [
-                    x[..., ::2, ::2, :],
-                    x[..., 1::2, ::2, :],
-                    x[..., ::2, 1::2, :],
-                    x[..., 1::2, 1::2, :],
-                ],
-                axis=-1,
-            ),
-            name=name,
-        )(x)
+        return keras.layers.Concatenate(name=name)(
+            [
+                x[..., ::2, ::2, :],
+                x[..., 1::2, ::2, :],
+                x[..., ::2, 1::2, :],
+                x[..., 1::2, 1::2, :],
+            ],
+        )
 
     return apply
```

## keras_cv/models/backbones/densenet/densenet_backbone.py

```diff
@@ -17,45 +17,42 @@
 Reference:
   - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
   - [Based on the Original keras.applications DenseNet](https://github.com/keras-team/keras/blob/master/keras/applications/densenet.py)
 """  # noqa: E501
 
 import copy
 
-from tensorflow import keras
-from tensorflow.keras import backend
-from tensorflow.keras import layers
-
+from keras_cv.backend import keras
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.densenet.densenet_backbone_presets import (
     backbone_presets,
 )
 from keras_cv.models.backbones.densenet.densenet_backbone_presets import (
     backbone_presets_with_weights,
 )
 from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
 BN_EPSILON = 1.001e-5
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class DenseNetBackbone(Backbone):
     """Instantiates the DenseNet architecture.
 
     Args:
         stackwise_num_repeats: list of ints, number of repeated convolutional
             blocks per dense block.
         include_rescaling: bool, whether to rescale the inputs. If set
             to `True`, inputs will be passed through a `Rescaling(1/255.0)`
             layer.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
+        input_tensor: optional Keras tensor (i.e. output of
+            `keras.layers.Input()`) to use as image input for the model.
         compression_ratio: float, compression rate at transition layers.
         growth_rate: int, number of filters added by each dense block.
 
     Examples:
     ```python
     input_data = tf.ones(shape=(8, 224, 224, 3))
 
@@ -83,53 +80,55 @@
         growth_rate=32,
         **kwargs,
     ):
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
 
         x = inputs
         if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
+            x = keras.layers.Rescaling(1 / 255.0)(x)
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             64, 7, strides=2, use_bias=False, padding="same", name="conv1/conv"
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1/bn"
         )(x)
-        x = layers.Activation("relu", name="conv1/relu")(x)
-        x = layers.MaxPooling2D(3, strides=2, padding="same", name="pool1")(x)
+        x = keras.layers.Activation("relu", name="conv1/relu")(x)
+        x = keras.layers.MaxPooling2D(
+            3, strides=2, padding="same", name="pool1"
+        )(x)
 
         pyramid_level_inputs = {}
         for stack_index in range(len(stackwise_num_repeats) - 1):
             index = stack_index + 2
             x = apply_dense_block(
                 x,
                 stackwise_num_repeats[stack_index],
                 growth_rate,
                 name=f"conv{index}",
             )
-            pyramid_level_inputs[f"P{index}"] = x.node.layer.name
+            pyramid_level_inputs[f"P{index}"] = utils.get_tensor_input_name(x)
             x = apply_transition_block(
                 x, compression_ratio, name=f"pool{index}"
             )
 
         x = apply_dense_block(
             x,
             stackwise_num_repeats[-1],
             growth_rate,
             name=f"conv{len(stackwise_num_repeats) + 1}",
         )
+
         pyramid_level_inputs[
             f"P{len(stackwise_num_repeats) + 1}"
-        ] = x.node.layer.name
-
-        x = layers.BatchNormalization(
+        ] = utils.get_tensor_input_name(x)
+        x = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name="bn"
         )(x)
-        x = layers.Activation("relu", name="relu")(x)
+        x = keras.layers.Activation("relu", name="relu")(x)
 
         # Create model.
         super().__init__(inputs=inputs, outputs=x, **kwargs)
 
         # All references to `self` below this line
         self.pyramid_level_inputs = pyramid_level_inputs
         self.stackwise_num_repeats = stackwise_num_repeats
@@ -170,15 +169,15 @@
     Args:
       x: input tensor.
       num_repeats: int, number of repeated convolutional blocks.
       growth_rate: int, number of filters added by each dense block.
       name: string, block label.
     """
     if name is None:
-        name = f"dense_block_{backend.get_uid('dense_block')}"
+        name = f"dense_block_{keras.backend.get_uid('dense_block')}"
 
     for i in range(num_repeats):
         x = apply_conv_block(x, growth_rate, name=f"{name}_block_{i}")
     return x
 
 
 def apply_transition_block(x, compression_ratio, name=None):
@@ -186,55 +185,57 @@
 
     Args:
       x: input tensor.
       compression_ratio: float, compression rate at transition layers.
       name: string, block label.
     """
     if name is None:
-        name = f"transition_block_{backend.get_uid('transition_block')}"
+        name = f"transition_block_{keras.backend.get_uid('transition_block')}"
 
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_bn"
     )(x)
-    x = layers.Activation("relu", name=f"{name}_relu")(x)
-    x = layers.Conv2D(
-        int(backend.int_shape(x)[BN_AXIS] * compression_ratio),
+    x = keras.layers.Activation("relu", name=f"{name}_relu")(x)
+    x = keras.layers.Conv2D(
+        int(x.shape[BN_AXIS] * compression_ratio),
         1,
         use_bias=False,
         name=f"{name}_conv",
     )(x)
-    x = layers.AveragePooling2D(2, strides=2, name=f"{name}_pool")(x)
+    x = keras.layers.AveragePooling2D(2, strides=2, name=f"{name}_pool")(x)
     return x
 
 
 def apply_conv_block(x, growth_rate, name=None):
     """A building block for a dense block.
 
     Args:
       x: input tensor.
       growth_rate: int, number of filters added by each dense block.
       name: string, block label.
     """
     if name is None:
-        name = f"conv_block_{backend.get_uid('conv_block')}"
+        name = f"conv_block_{keras.backend.get_uid('conv_block')}"
 
     shortcut = x
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_0_bn"
     )(x)
-    x = layers.Activation("relu", name=f"{name}_0_relu")(x)
-    x = layers.Conv2D(
+    x = keras.layers.Activation("relu", name=f"{name}_0_relu")(x)
+    x = keras.layers.Conv2D(
         4 * growth_rate, 1, use_bias=False, name=f"{name}_1_conv"
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=f"{name}_1_bn"
     )(x)
-    x = layers.Activation("relu", name=f"{name}_1_relu")(x)
-    x = layers.Conv2D(
+    x = keras.layers.Activation("relu", name=f"{name}_1_relu")(x)
+    x = keras.layers.Conv2D(
         growth_rate,
         3,
         padding="same",
         use_bias=False,
         name=f"{name}_2_conv",
     )(x)
-    x = layers.Concatenate(axis=BN_AXIS, name=f"{name}_concat")([shortcut, x])
+    x = keras.layers.Concatenate(axis=BN_AXIS, name=f"{name}_concat")(
+        [shortcut, x]
+    )
     return x
```

## keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py

```diff
@@ -9,18 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv.backend import ops
 from keras_cv.models.backbones.densenet.densenet_aliases import (
     DenseNet121Backbone,
 )
 from keras_cv.models.backbones.densenet.densenet_backbone import (
     DenseNetBackbone,
 )
 
@@ -30,34 +32,37 @@
     """
     A smoke test for DenseNet presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py --run_large`  # noqa: E501
     """
 
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_backbone_output(self):
         model = DenseNetBackbone.from_preset("densenet121")
         model(self.input_batch)
 
     def test_backbone_output_with_weights(self):
         model = DenseNetBackbone.from_preset("densenet121_imagenet")
 
         # The forward pass from a preset should be stable!
         # This test should catch cases where we unintentionally change our
         # network code in a way that would invalidate our preset weights.
         # We should only update these numbers if we are updating a weights
         # file, or have found a discrepancy with the upstream source.
 
-        outputs = model(tf.ones(shape=(1, 512, 512, 3)))
+        outputs = model(np.ones(shape=(1, 512, 512, 3)))
         expected = [0.0, 0.0, 0.09920305, 0.0, 0.0]
         # Keep a high tolerance, so we are robust to different hardware.
         self.assertAllClose(
-            outputs[0, 0, 0, :5], expected, atol=0.01, rtol=0.01
+            ops.convert_to_numpy(outputs[0, 0, 0, :5]),
+            expected,
+            atol=0.01,
+            rtol=0.01,
         )
 
     def test_applications_model_output(self):
         model = DenseNet121Backbone()
         model(self.input_batch)
 
     def test_applications_model_output_with_preset(self):
@@ -86,11 +91,11 @@
     Test the full enumeration of our preset.
     This tests every preset for DenseNet and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py --run_extra_large`  # noqa: E501
     """
 
     def test_load_densenet(self):
-        input_data = tf.ones(shape=(2, 224, 224, 3))
+        input_data = np.ones(shape=(2, 224, 224, 3))
         for preset in DenseNetBackbone.presets:
             model = DenseNetBackbone.from_preset(preset)
             model(input_data)
```

## keras_cv/models/backbones/densenet/densenet_backbone_test.py

```diff
@@ -10,31 +10,33 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.densenet.densenet_aliases import (
     DenseNet121Backbone,
 )
 from keras_cv.models.backbones.densenet.densenet_backbone import (
     DenseNetBackbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class DenseNetBackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_valid_call(self):
         model = DenseNetBackbone(
             stackwise_num_repeats=[6, 12, 24, 16],
             include_rescaling=False,
         )
         model(self.input_batch)
@@ -46,83 +48,83 @@
     def test_valid_call_with_rescaling(self):
         model = DenseNetBackbone(
             stackwise_num_repeats=[6, 12, 24, 16],
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = DenseNetBackbone(
             stackwise_num_repeats=[6, 12, 24, 16],
             include_rescaling=False,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(self.get_temp_dir(), "densenet_backbone.keras")
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, DenseNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_alias_model(self, save_format, filename):
+    def test_saved_alias_model(self):
         model = DenseNet121Backbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "densenet_alias_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, DenseNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = DenseNet121Backbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 256],
+            (None, input_size // 2**2, input_size // 2**2, 256),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 512],
+            (None, input_size // 2**3, input_size // 2**3, 512),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 1024],
+            (None, input_size // 2**4, input_size // 2**4, 1024),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 1024],
+            (None, input_size // 2**5, input_size // 2**5, 1024),
         )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py

```diff
@@ -10,31 +10,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 import math
 
-from tensorflow import keras
-from tensorflow.keras import layers
-
+from keras_cv.backend import keras
 from keras_cv.layers import FusedMBConvBlock
 from keras_cv.layers import MBConvBlock
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_backbone_presets import (  # noqa: E501
     backbone_presets,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_backbone_presets import (  # noqa: E501
     backbone_presets_with_weights,
 )
 from keras_cv.utils.python_utils import classproperty
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class EfficientNetV2Backbone(Backbone):
     """Instantiates the EfficientNetV2 architecture.
 
     Reference:
     - [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298)
       (ICML 2021)
 
@@ -63,15 +61,15 @@
             and a 1x1 output convolution blocks fused blocks use a single 3x3
             convolution block.
         skip_connection_dropout: float, dropout rate at skip connections.
         depth_divisor: integer, a unit of network width.
         min_depth: integer, minimum number of filters.
         activation: activation function to use between each convolutional layer.
         input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+        input_tensor: optional Keras tensor (i.e. output of `keras.layers.Input()`)
             to use as image input for the model.
 
     Usage:
     ```python
     # Construct an EfficientNetV2 from a preset:
     efficientnet = keras_cv.models.EfficientNetV2Backbone.from_preset(
         "efficientnetv2_s"
@@ -129,37 +127,37 @@
     ):
         # Determine proper input shape
         img_input = utils.parse_model_inputs(input_shape, input_tensor)
 
         x = img_input
 
         if include_rescaling:
-            x = layers.Rescaling(scale=1 / 255.0)(x)
+            x = keras.layers.Rescaling(scale=1 / 255.0)(x)
 
         # Build stem
         stem_filters = round_filters(
             filters=stackwise_input_filters[0],
             width_coefficient=width_coefficient,
             min_depth=min_depth,
             depth_divisor=depth_divisor,
         )
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             filters=stem_filters,
             kernel_size=3,
             strides=2,
             kernel_initializer=conv_kernel_initializer(),
             padding="same",
             use_bias=False,
             name="stem_conv",
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             momentum=0.9,
             name="stem_bn",
         )(x)
-        x = layers.Activation(activation, name="stem_activation")(x)
+        x = keras.layers.Activation(activation, name="stem_activation")(x)
 
         # Build blocks
         block_id = 0
         blocks = float(
             sum(num_repeats for num_repeats in stackwise_num_repeats)
         )
 
@@ -194,15 +192,15 @@
                 # The first block needs to take care of stride and filter size
                 # increase.
                 if j > 0:
                     strides = 1
                     input_filters = output_filters
 
                 if strides != 1:
-                    pyramid_level_inputs.append(x.node.layer.name)
+                    pyramid_level_inputs.append(utils.get_tensor_input_name(x))
 
                 # 97 is the start of the lowercase alphabet.
                 letter_identifier = chr(j + 97)
                 block = get_conv_constructor(stackwise_conv_types[i])(
                     input_filters=input_filters,
                     output_filters=output_filters,
                     expand_ratio=stackwise_expansion_ratios[i],
@@ -223,31 +221,33 @@
         top_filters = round_filters(
             filters=1280,
             width_coefficient=width_coefficient,
             min_depth=min_depth,
             depth_divisor=depth_divisor,
         )
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             filters=top_filters,
             kernel_size=1,
             strides=1,
             kernel_initializer=conv_kernel_initializer(),
             padding="same",
             data_format="channels_last",
             use_bias=False,
             name="top_conv",
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             momentum=0.9,
             name="top_bn",
         )(x)
-        x = layers.Activation(activation=activation, name="top_activation")(x)
+        x = keras.layers.Activation(
+            activation=activation, name="top_activation"
+        )(x)
 
-        pyramid_level_inputs.append(x.node.layer.name)
+        pyramid_level_inputs.append(utils.get_tensor_input_name(x))
 
         # Create model.
         super().__init__(inputs=img_input, outputs=x, **kwargs)
 
         self.include_rescaling = include_rescaling
         self.width_coefficient = width_coefficient
         self.depth_coefficient = depth_coefficient
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py

```diff
@@ -9,18 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv.backend import keras
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2SBackbone,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_backbone import (
     EfficientNetV2Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
@@ -35,25 +37,25 @@
     `pytest keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py --run_extra_large`
     """  # noqa: E501
 
     @parameterized.named_parameters(
         *[(preset, preset) for preset in EfficientNetV2Backbone.presets]
     )
     def test_load_efficientnet(self, preset):
-        input_data = tf.ones(shape=(2, 224, 224, 3))
+        input_data = np.ones(shape=(2, 224, 224, 3))
         model = EfficientNetV2Backbone.from_preset(preset)
         model(input_data)
 
     def test_efficientnet_feature_extractor(self):
         model = EfficientNetV2SBackbone(
             include_rescaling=False,
             input_shape=[256, 256, 3],
         )
         levels = ["P3", "P4"]
         layer_names = [model.pyramid_level_inputs[level] for level in levels]
         backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = tf.keras.Input(shape=[256, 256, 3])
+        inputs = keras.Input(shape=[256, 256, 3])
         outputs = backbone_model(inputs)
         self.assertLen(outputs, 2)
         self.assertEquals(list(outputs.keys()), levels)
-        self.assertEquals(outputs["P3"].shape[:3], [None, 32, 32])
-        self.assertEquals(outputs["P4"].shape[:3], [None, 16, 16])
+        self.assertEquals(outputs["P3"].shape[:3], (None, 32, 32))
+        self.assertEquals(outputs["P4"].shape[:3], (None, 16, 16))
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py

```diff
@@ -10,31 +10,33 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2SBackbone,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_backbone import (
     EfficientNetV2Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class EfficientNetV2BackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(8, 224, 224, 3))
+        self.input_batch = np.ones(shape=(8, 224, 224, 3))
 
     def test_valid_call(self):
         model = EfficientNetV2Backbone(
             stackwise_kernel_sizes=[3, 3, 3, 3, 3, 3],
             stackwise_num_repeats=[2, 4, 4, 6, 9, 15],
             stackwise_input_filters=[24, 24, 48, 64, 128, 160],
             stackwise_output_filters=[24, 48, 64, 128, 160, 256],
@@ -78,20 +80,16 @@
             ],
             width_coefficient=1.0,
             depth_coefficient=1.0,
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = EfficientNetV2Backbone(
             stackwise_kernel_sizes=[3, 3, 3, 3, 3, 3],
             stackwise_num_repeats=[2, 4, 4, 6, 9, 15],
             stackwise_input_filters=[24, 24, 48, 64, 128, 160],
             stackwise_output_filters=[24, 48, 64, 128, 160, 256],
             stackwise_expansion_ratios=[1, 4, 4, 4, 6, 6],
             stackwise_squeeze_and_excite_ratios=[0.0, 0.0, 0, 0.25, 0.25, 0.25],
@@ -105,76 +103,82 @@
                 "unfused",
             ],
             width_coefficient=1.0,
             depth_coefficient=1.0,
             include_rescaling=True,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "efficientnet_v2_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, EfficientNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_alias_model(self, save_format, filename):
+    def test_saved_alias_model(self):
         model = EfficientNetV2SBackbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "efficientnet_v2_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, EfficientNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = EfficientNetV2SBackbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P1", "P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P1"].shape,
-            [None, input_size // 2**1, input_size // 2**1, 24],
+            (None, input_size // 2**1, input_size // 2**1, 24),
         )
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 48],
+            (None, input_size // 2**2, input_size // 2**2, 48),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 64],
+            (None, input_size // 2**3, input_size // 2**3, 64),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 160],
+            (None, input_size // 2**4, input_size // 2**4, 160),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 1280],
+            (None, input_size // 2**5, input_size // 2**5, 1280),
         )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py

```diff
@@ -18,19 +18,16 @@
     - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
     (ICCV 2019)
     - [Based on the original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
 """  # noqa: E501
 
 import copy
 
-from tensorflow import keras
-from tensorflow.keras import backend
-from tensorflow.keras import layers
-
 from keras_cv import layers as cv_layers
+from keras_cv.backend import keras
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone_presets import (  # noqa: E501
     backbone_presets,
 )
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone_presets import (  # noqa: E501
     backbone_presets_with_weights,
@@ -38,15 +35,15 @@
 from keras_cv.utils.python_utils import classproperty
 
 CHANNEL_AXIS = -1
 BN_EPSILON = 1e-3
 BN_MOMENTUM = 0.999
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class MobileNetV3Backbone(Backbone):
     """Instantiates the MobileNetV3 architecture.
 
     References:
         - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
         (ICCV 2019)
         - [Based on the Original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
@@ -110,60 +107,60 @@
         alpha=1.0,
         **kwargs,
     ):
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
         x = inputs
 
         if include_rescaling:
-            x = layers.Rescaling(scale=1 / 255)(x)
+            x = keras.layers.Rescaling(scale=1 / 255)(x)
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             16,
             kernel_size=3,
             strides=(2, 2),
             padding="same",
             use_bias=False,
             name="Conv",
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=CHANNEL_AXIS,
             epsilon=BN_EPSILON,
             momentum=BN_MOMENTUM,
             name="Conv/BatchNorm",
         )(x)
         x = apply_hard_swish(x)
 
         pyramid_level_inputs = []
         for stack_index in range(len(stackwise_filters)):
             if stackwise_stride[stack_index] != 1:
-                pyramid_level_inputs.append(x.node.layer.name)
+                pyramid_level_inputs.append(utils.get_tensor_input_name(x))
             x = apply_inverted_res_block(
                 x,
                 expansion=stackwise_expansion[stack_index],
                 filters=adjust_channels(
                     (stackwise_filters[stack_index]) * alpha
                 ),
                 kernel_size=stackwise_kernel_size[stack_index],
                 stride=stackwise_stride[stack_index],
                 se_ratio=stackwise_se_ratio[stack_index],
                 activation=stackwise_activation[stack_index],
                 expansion_index=stack_index,
             )
-        pyramid_level_inputs.append(x.node.layer.name)
+        pyramid_level_inputs.append(utils.get_tensor_input_name(x))
 
-        last_conv_ch = adjust_channels(backend.int_shape(x)[CHANNEL_AXIS] * 6)
+        last_conv_ch = adjust_channels(x.shape[CHANNEL_AXIS] * 6)
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             last_conv_ch,
             kernel_size=1,
             padding="same",
             use_bias=False,
             name="Conv_1",
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=CHANNEL_AXIS,
             epsilon=BN_EPSILON,
             momentum=BN_MOMENTUM,
             name="Conv_1/BatchNorm",
         )(x)
         x = apply_hard_swish(x)
 
@@ -208,15 +205,15 @@
     @classproperty
     def presets_with_weights(cls):
         """Dictionary of preset names and configurations that include
         weights."""
         return copy.deepcopy(backbone_presets_with_weights)
 
 
-class HardSigmoidActivation(layers.Layer):
+class HardSigmoidActivation(keras.layers.Layer):
     def __init__(self):
         super().__init__()
 
     def call(self, x):
         return apply_hard_sigmoid(x)
 
     def get_config(self):
@@ -245,20 +242,20 @@
     # make sure that round down does not go down by more than 10%.
     if new_x < 0.9 * x:
         new_x += divisor
     return new_x
 
 
 def apply_hard_sigmoid(x):
-    activation = layers.ReLU(6.0)
+    activation = keras.layers.ReLU(6.0)
     return activation(x + 3.0) * (1.0 / 6.0)
 
 
 def apply_hard_swish(x):
-    return layers.Multiply()([x, apply_hard_sigmoid(x)])
+    return keras.layers.Multiply()([x, apply_hard_sigmoid(x)])
 
 
 def apply_inverted_res_block(
     x,
     expansion,
     filters,
     kernel_size,
@@ -290,48 +287,48 @@
         if activation == "hard_swish":
             activation = apply_hard_swish
         else:
             activation = keras.activations.get(activation)
 
     shortcut = x
     prefix = "expanded_conv/"
-    infilters = backend.int_shape(x)[CHANNEL_AXIS]
+    infilters = x.shape[CHANNEL_AXIS]
 
     if expansion_index > 0:
         prefix = f"expanded_conv_{expansion_index}/"
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             adjust_channels(infilters * expansion),
             kernel_size=1,
             padding="same",
             use_bias=False,
             name=prefix + "expand",
         )(x)
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=CHANNEL_AXIS,
             epsilon=BN_EPSILON,
             momentum=BN_MOMENTUM,
             name=prefix + "expand/BatchNorm",
         )(x)
         x = activation(x)
 
     if stride == 2:
-        x = layers.ZeroPadding2D(
+        x = keras.layers.ZeroPadding2D(
             padding=utils.correct_pad_downsample(x, kernel_size),
             name=prefix + "depthwise/pad",
         )(x)
 
-    x = layers.DepthwiseConv2D(
+    x = keras.layers.DepthwiseConv2D(
         kernel_size,
         strides=stride,
         padding="same" if stride == 1 else "valid",
         use_bias=False,
         name=prefix + "depthwise",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=CHANNEL_AXIS,
         epsilon=BN_EPSILON,
         momentum=BN_MOMENTUM,
         name=prefix + "depthwise/BatchNorm",
     )(x)
     x = activation(x)
 
@@ -340,25 +337,25 @@
         x = cv_layers.SqueezeAndExcite2D(
             filters=se_filters,
             bottleneck_filters=adjust_channels(se_filters * se_ratio),
             squeeze_activation="relu",
             excite_activation=HardSigmoidActivation(),
         )(x)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size=1,
         padding="same",
         use_bias=False,
         name=prefix + "project",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=CHANNEL_AXIS,
         epsilon=BN_EPSILON,
         momentum=BN_MOMENTUM,
         name=prefix + "project/BatchNorm",
     )(x)
 
     if stride == 1 and infilters == filters:
-        x = layers.Add(name=prefix + "Add")([shortcut, x])
+        x = keras.layers.Add(name=prefix + "Add")([shortcut, x])
 
     return x
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py

```diff
@@ -10,56 +10,60 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for loading pretrained model presets."""
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv.backend import ops
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
     MobileNetV3Backbone,
 )
 
 
 @pytest.mark.large
 class MobileNetV3PresetSmokeTest(tf.test.TestCase):
     """
     A smoke test for MobileNetV3 presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py --run_large`
     """  # noqa: E501
 
     def setUp(self):
-        self.input_batch = tf.ones(shape=(8, 224, 224, 3))
+        self.input_batch = np.ones(shape=(8, 224, 224, 3))
 
     def test_backbone_output(self):
         model = MobileNetV3Backbone.from_preset("mobilenet_v3_large_imagenet")
         outputs = model(self.input_batch)
 
         # The forward pass from a preset should be stable!
         # This test should catch cases where we unintentionally change our
         # network code in a way that would invalidate our preset weights.
         # We should only update these numbers if we are updating a weights
         # file, or have found a discrepancy with the upstream source.
         outputs = outputs[0, 0, 0, :5]
         expected = [0.27, 0.01, 0.29, 0.08, -0.12]
         # Keep a high tolerance, so we are robust to different hardware.
-        self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+        self.assertAllClose(
+            ops.convert_to_numpy(outputs), expected, atol=0.01, rtol=0.01
+        )
 
 
 @pytest.mark.extra_large
 class MobileNetV3PresetFullTest(tf.test.TestCase, parameterized.TestCase):
     """
     Test the full enumeration of our preset.
     This tests every preset for MobileNetV3 and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py --run_extra_large`
     """  # noqa: E501
 
     def test_load_mobilenet_v3(self):
-        input_data = tf.ones(shape=(2, 224, 224, 3))
+        input_data = np.ones(shape=(2, 224, 224, 3))
         for preset in MobileNetV3Backbone.presets:
             model = MobileNetV3Backbone.from_preset(preset)
             model(input_data)
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py

```diff
@@ -10,94 +10,97 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_aliases import (
     MobileNetV3SmallBackbone,
 )
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
     MobileNetV3Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class MobileNetV3BackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_valid_call(self):
         model = MobileNetV3SmallBackbone(
             include_rescaling=False,
         )
         model(self.input_batch)
 
     def test_valid_call_with_rescaling(self):
         model = MobileNetV3SmallBackbone(
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = MobileNetV3SmallBackbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "mobilenet_v3_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, MobileNetV3Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = MobileNetV3SmallBackbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P1", "P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P1"].shape,
-            [None, input_size // 2**1, input_size // 2**1, 16],
+            (None, input_size // 2**1, input_size // 2**1, 16),
         )
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 16],
+            (None, input_size // 2**2, input_size // 2**2, 16),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 24],
+            (None, input_size // 2**3, input_size // 2**3, 24),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 48],
+            (None, input_size // 2**4, input_size // 2**4, 48),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 96],
+            (None, input_size // 2**5, input_size // 2**5, 96),
         )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py

```diff
@@ -16,33 +16,30 @@
   - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
     (CVPR 2015)
   - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet.py)  # noqa: E501
 """
 
 import copy
 
-from tensorflow import keras
-from tensorflow.keras import backend
-from tensorflow.keras import layers
-
+from keras_cv.backend import keras
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone_presets import (
     backbone_presets,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone_presets import (
     backbone_presets_with_weights,
 )
 from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
 BN_EPSILON = 1.001e-5
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class ResNetBackbone(Backbone):
     """Instantiates the ResNet architecture.
 
     Reference:
         - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
 
     The difference in ResNetV1 and ResNetV2 rests in the structure of their
@@ -100,26 +97,26 @@
         block_type="block",
         **kwargs,
     ):
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
         x = inputs
 
         if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
+            x = keras.layers.Rescaling(1 / 255.0)(x)
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             64, 7, strides=2, use_bias=False, padding="same", name="conv1_conv"
         )(x)
 
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1_bn"
         )(x)
-        x = layers.Activation("relu", name="conv1_relu")(x)
+        x = keras.layers.Activation("relu", name="conv1_relu")(x)
 
-        x = layers.MaxPooling2D(
+        x = keras.layers.MaxPooling2D(
             3, strides=2, padding="same", name="pool1_pool"
         )(x)
 
         num_stacks = len(stackwise_filters)
 
         pyramid_level_inputs = {}
         for stack_index in range(num_stacks):
@@ -128,15 +125,17 @@
                 filters=stackwise_filters[stack_index],
                 blocks=stackwise_blocks[stack_index],
                 stride=stackwise_strides[stack_index],
                 block_type=block_type,
                 first_shortcut=(block_type == "block" or stack_index > 0),
                 name=f"v2_stack_{stack_index}",
             )
-            pyramid_level_inputs[f"P{stack_index + 2}"] = x.node.layer.name
+            pyramid_level_inputs[
+                f"P{stack_index + 2}"
+            ] = utils.get_tensor_input_name(x)
 
         # Create model.
         super().__init__(inputs=inputs, outputs=x, **kwargs)
 
         # All references to `self` below this line
         self.pyramid_level_inputs = pyramid_level_inputs
         self.stackwise_filters = stackwise_filters
@@ -189,56 +188,56 @@
         name: string, optional prefix for the layer names used in the block.
 
     Returns:
       Output tensor for the residual block.
     """
 
     if name is None:
-        name = f"v1_basic_block_{backend.get_uid('v1_basic_block_')}"
+        name = f"v1_basic_block_{keras.backend.get_uid('v1_basic_block_')}"
 
     if conv_shortcut:
-        shortcut = layers.Conv2D(
+        shortcut = keras.layers.Conv2D(
             filters,
             1,
             strides=stride,
             use_bias=False,
             name=name + "_0_conv",
         )(x)
-        shortcut = layers.BatchNormalization(
+        shortcut = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_0_bn"
         )(shortcut)
     else:
         shortcut = x
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         padding="SAME",
         strides=stride,
         use_bias=False,
         name=name + "_1_conv",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_1_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_1_relu")(x)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         padding="SAME",
         use_bias=False,
         name=name + "_2_conv",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
     )(x)
 
-    x = layers.Add(name=name + "_add")([shortcut, x])
-    x = layers.Activation("relu", name=name + "_out")(x)
+    x = keras.layers.Add(name=name + "_add")([shortcut, x])
+    x = keras.layers.Activation("relu", name=name + "_out")(x)
     return x
 
 
 def apply_block(
     x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None
 ):
     """A residual block (v1).
@@ -253,57 +252,59 @@
         name: string, optional prefix for the layer names used in the block.
 
     Returns:
       Output tensor for the residual block.
     """
 
     if name is None:
-        name = f"v1_block_{backend.get_uid('v1_block')}"
+        name = f"v1_block_{keras.backend.get_uid('v1_block')}"
 
     if conv_shortcut:
-        shortcut = layers.Conv2D(
+        shortcut = keras.layers.Conv2D(
             4 * filters,
             1,
             strides=stride,
             use_bias=False,
             name=name + "_0_conv",
         )(x)
-        shortcut = layers.BatchNormalization(
+        shortcut = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_0_bn"
         )(shortcut)
     else:
         shortcut = x
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters, 1, strides=stride, use_bias=False, name=name + "_1_conv"
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_1_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_1_relu")(x)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         padding="SAME",
         use_bias=False,
         name=name + "_2_conv",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_2_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_2_relu")(x)
 
-    x = layers.Conv2D(4 * filters, 1, use_bias=False, name=name + "_3_conv")(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.Conv2D(
+        4 * filters, 1, use_bias=False, name=name + "_3_conv"
+    )(x)
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_3_bn"
     )(x)
 
-    x = layers.Add(name=name + "_add")([shortcut, x])
-    x = layers.Activation("relu", name=name + "_out")(x)
+    x = keras.layers.Add(name=name + "_add")([shortcut, x])
+    x = keras.layers.Activation("relu", name=name + "_out")(x)
     return x
 
 
 def apply_stack(
     x,
     filters,
     blocks,
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py

```diff
@@ -9,17 +9,19 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
+import numpy as np
 import pytest
 import tensorflow as tf
 
+from keras_cv.backend import ops
 from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet50Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
     ResNetBackbone,
 )
 
@@ -29,34 +31,37 @@
     """
     A smoke test for ResNet presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py --run_large`  # noqa: E501
     """
 
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_backbone_output(self):
         model = ResNetBackbone.from_preset("resnet50")
         model(self.input_batch)
 
     def test_backbone_output_with_weights(self):
         model = ResNetBackbone.from_preset("resnet50_imagenet")
 
         # The forward pass from a preset should be stable!
         # This test should catch cases where we unintentionally change our
         # network code in a way that would invalidate our preset weights.
         # We should only update these numbers if we are updating a weights
         # file, or have found a discrepancy with the upstream source.
 
-        outputs = model(tf.ones(shape=(1, 512, 512, 3)))
+        outputs = model(np.ones(shape=(1, 512, 512, 3)))
         expected = [0.0, 0.0, 0.0, 0.05175382, 0.0]
         # Keep a high tolerance, so we are robust to different hardware.
         self.assertAllClose(
-            outputs[0, 0, 0, :5], expected, atol=0.01, rtol=0.01
+            ops.convert_to_numpy(outputs[0, 0, 0, :5]),
+            expected,
+            atol=0.01,
+            rtol=0.01,
         )
 
     def test_applications_model_output(self):
         model = ResNet50Backbone()
         model(self.input_batch)
 
     def test_applications_model_output_with_preset(self):
@@ -85,11 +90,11 @@
     Test the full enumeration of our preset.
     This tests every preset for ResNet and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py --run_extra_large`  # noqa: E501
     """
 
     def test_load_resnet(self):
-        input_data = tf.ones(shape=(2, 224, 224, 3))
+        input_data = np.ones(shape=(2, 224, 224, 3))
         for preset in ResNetBackbone.presets:
             model = ResNetBackbone.from_preset(preset)
             model(input_data)
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py

```diff
@@ -10,19 +10,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet18Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet50Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
@@ -35,15 +37,15 @@
     ResNetBackbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class ResNetBackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     def test_valid_call(self):
         model = ResNetBackbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
@@ -59,85 +61,87 @@
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = ResNetBackbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
-        restored_model = keras.models.load_model(save_path)
+        save_path = os.path.join(
+            self.get_temp_dir(), "resnet_v1_backbone.keras"
+        )
+        model.save(save_path)
+        restored_model = keras.saving.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, ResNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_alias_model(self, save_format, filename):
+    def test_saved_alias_model(self):
         model = ResNet50Backbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
-        restored_model = keras.models.load_model(save_path)
+        save_path = os.path.join(
+            self.get_temp_dir(), "resnet_v1_alias_backbone.keras"
+        )
+        model.save(save_path)
+        restored_model = keras.saving.load_model(save_path)
 
         # Check we got the real object back.
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, ResNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = ResNet50Backbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 256],
+            (None, input_size // 2**2, input_size // 2**2, 256),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 512],
+            (None, input_size // 2**3, input_size // 2**3, 512),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 1024],
+            (None, input_size // 2**4, input_size // 2**4, 1024),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 2048],
+            (None, input_size // 2**5, input_size // 2**5, 2048),
         )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py

```diff
@@ -15,33 +15,30 @@
 Reference:
   - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
   - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet_v2.py)
 """  # noqa: E501
 
 import copy
 
-from tensorflow import keras
-from tensorflow.keras import backend
-from tensorflow.keras import layers
-
+from keras_cv.backend import keras
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone_presets import (
     backbone_presets,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone_presets import (
     backbone_presets_with_weights,
 )
 from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
 BN_EPSILON = 1.001e-5
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class ResNetV2Backbone(Backbone):
     """Instantiates the ResNetV2 architecture.
 
     Reference:
         - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
 
     The difference in Resnet and ResNetV2 rests in the structure of their
@@ -103,26 +100,26 @@
         block_type="block",
         **kwargs,
     ):
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
         x = inputs
 
         if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
+            x = keras.layers.Rescaling(1 / 255.0)(x)
 
-        x = layers.Conv2D(
+        x = keras.layers.Conv2D(
             64,
             7,
             strides=2,
             use_bias=True,
             padding="same",
             name="conv1_conv",
         )(x)
 
-        x = layers.MaxPooling2D(
+        x = keras.layers.MaxPooling2D(
             3, strides=2, padding="same", name="pool1_pool"
         )(x)
 
         num_stacks = len(stackwise_filters)
         if stackwise_dilations is None:
             stackwise_dilations = [1] * num_stacks
 
@@ -134,20 +131,22 @@
                 blocks=stackwise_blocks[stack_index],
                 stride=stackwise_strides[stack_index],
                 dilations=stackwise_dilations[stack_index],
                 block_type=block_type,
                 first_shortcut=(block_type == "block" or stack_index > 0),
                 name=f"v2_stack_{stack_index}",
             )
-            pyramid_level_inputs[f"P{stack_index + 2}"] = x.node.layer.name
+            pyramid_level_inputs[
+                f"P{stack_index + 2}"
+            ] = utils.get_tensor_input_name(x)
 
-        x = layers.BatchNormalization(
+        x = keras.layers.BatchNormalization(
             axis=BN_AXIS, epsilon=BN_EPSILON, name="post_bn"
         )(x)
-        x = layers.Activation("relu", name="post_relu")(x)
+        x = keras.layers.Activation("relu", name="post_relu")(x)
 
         # Create model.
         super().__init__(inputs=inputs, outputs=x, **kwargs)
 
         # All references to `self` below this line
         self.pyramid_level_inputs = pyramid_level_inputs
         self.stackwise_filters = stackwise_filters
@@ -210,62 +209,62 @@
         name: string, optional prefix for the layer names used in the block.
 
     Returns:
       Output tensor for the residual block.
     """
 
     if name is None:
-        name = f"v2_basic_block_{backend.get_uid('v2_basic_block')}"
+        name = f"v2_basic_block_{keras.backend.get_uid('v2_basic_block')}"
 
-    use_preactivation = layers.BatchNormalization(
+    use_preactivation = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_use_preactivation_bn"
     )(x)
 
-    use_preactivation = layers.Activation(
+    use_preactivation = keras.layers.Activation(
         "relu", name=name + "_use_preactivation_relu"
     )(use_preactivation)
 
     s = stride if dilation == 1 else 1
     if conv_shortcut:
-        shortcut = layers.Conv2D(filters, 1, strides=s, name=name + "_0_conv")(
-            use_preactivation
-        )
+        shortcut = keras.layers.Conv2D(
+            filters, 1, strides=s, name=name + "_0_conv"
+        )(use_preactivation)
     else:
         shortcut = (
-            layers.MaxPooling2D(
+            keras.layers.MaxPooling2D(
                 1, strides=stride, name=name + "_0_max_pooling"
             )(x)
             if s > 1
             else x
         )
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         padding="SAME",
         strides=1,
         use_bias=False,
         name=name + "_1_conv",
     )(use_preactivation)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_1_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_1_relu")(x)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         strides=s,
         padding="same",
         dilation_rate=dilation,
         use_bias=False,
         name=name + "_2_conv",
     )(x)
 
-    x = layers.Add(name=name + "_out")([shortcut, x])
+    x = keras.layers.Add(name=name + "_out")([shortcut, x])
     return x
 
 
 def apply_block(
     x,
     filters,
     kernel_size=3,
@@ -287,65 +286,65 @@
             (default), uses identity or pooling shortcut, based on stride.
         name: string, optional prefix for the layer names used in the block.
 
     Returns:
       Output tensor for the residual block.
     """
     if name is None:
-        name = f"v2_block_{backend.get_uid('v2_block')}"
+        name = f"v2_block_{keras.backend.get_uid('v2_block')}"
 
-    use_preactivation = layers.BatchNormalization(
+    use_preactivation = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_use_preactivation_bn"
     )(x)
 
-    use_preactivation = layers.Activation(
+    use_preactivation = keras.layers.Activation(
         "relu", name=name + "_use_preactivation_relu"
     )(use_preactivation)
 
     s = stride if dilation == 1 else 1
     if conv_shortcut:
-        shortcut = layers.Conv2D(
+        shortcut = keras.layers.Conv2D(
             4 * filters,
             1,
             strides=s,
             name=name + "_0_conv",
         )(use_preactivation)
     else:
         shortcut = (
-            layers.MaxPooling2D(
+            keras.layers.MaxPooling2D(
                 1, strides=stride, name=name + "_0_max_pooling"
             )(x)
             if s > 1
             else x
         )
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters, 1, strides=1, use_bias=False, name=name + "_1_conv"
     )(use_preactivation)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_1_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_1_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_1_relu")(x)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters,
         kernel_size,
         strides=s,
         use_bias=False,
         padding="same",
         dilation_rate=dilation,
         name=name + "_2_conv",
     )(x)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         axis=BN_AXIS, epsilon=BN_EPSILON, name=name + "_2_bn"
     )(x)
-    x = layers.Activation("relu", name=name + "_2_relu")(x)
+    x = keras.layers.Activation("relu", name=name + "_2_relu")(x)
 
-    x = layers.Conv2D(4 * filters, 1, name=name + "_3_conv")(x)
-    x = layers.Add(name=name + "_out")([shortcut, x])
+    x = keras.layers.Conv2D(4 * filters, 1, name=name + "_3_conv")(x)
+    x = keras.layers.Add(name=name + "_out")([shortcut, x])
     return x
 
 
 def apply_stack(
     x,
     filters,
     blocks,
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py

```diff
@@ -9,18 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_cv.backend import ops
 from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet50V2Backbone,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
     ResNetV2Backbone,
 )
 
@@ -30,15 +32,15 @@
     """
     A smoke test for ResNetV2 presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_cv/models/backbones/resnet_v2/resnetv2_presets_test.py --run_large`
     """  # noqa: E501
 
     def setUp(self):
-        self.input_batch = tf.ones(shape=(8, 224, 224, 3))
+        self.input_batch = np.ones(shape=(8, 224, 224, 3))
 
     @parameterized.named_parameters(
         ("preset_with_weights", "resnet50_v2_imagenet"),
         ("preset_no_weights", "resnet50_v2"),
     )
     def test_backbone_output(self, preset):
         model = ResNetV2Backbone.from_preset(preset)
@@ -49,15 +51,17 @@
             # This test should catch cases where we unintentionally change our
             # network code in a way that would invalidate our preset weights.
             # We should only update these numbers if we are updating a weights
             # file, or have found a discrepancy with the upstream source.
             outputs = outputs[0, 0, 0, :5]
             expected = [1.051145, 0, 0, 1.16328, 0]
             # Keep a high tolerance, so we are robust to different hardware.
-            self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+            self.assertAllClose(
+                ops.convert_to_numpy(outputs), expected, atol=0.01, rtol=0.01
+            )
 
     def test_applications_model_output(self):
         model = ResNet50V2Backbone()
         model(self.input_batch)
 
     def test_applications_model_output_with_preset(self):
         model = ResNet50V2Backbone.from_preset("resnet50_v2_imagenet")
@@ -85,11 +89,11 @@
     Test the full enumeration of our preset.
     This every presets for ResNetV2 and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/resnet_v2/resnet_v2_presets_test.py --run_extra_large`
     """  # noqa: E501
 
     def test_load_resnetv2(self):
-        input_data = tf.ones(shape=(8, 224, 224, 3))
+        input_data = np.ones(shape=(8, 224, 224, 3))
         for preset in ResNetV2Backbone.presets:
             model = ResNetV2Backbone.from_preset(preset)
             model(input_data)
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py

```diff
@@ -10,31 +10,33 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet50V2Backbone,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
     ResNetV2Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class ResNetV2BackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(8, 224, 224, 3))
+        self.input_batch = np.ones(shape=(8, 224, 224, 3))
 
     def test_valid_call(self):
         model = ResNetV2Backbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
@@ -50,85 +52,87 @@
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=True,
         )
         model(self.input_batch)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = ResNetV2Backbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "resnet_v2_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, ResNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_alias_model(self, save_format, filename):
+    def test_saved_alias_model(self):
         model = ResNet50V2Backbone()
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(
+            self.get_temp_dir(), "resnet_v2_backbone.keras"
+        )
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, ResNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
     def test_feature_pyramid_inputs(self):
         model = ResNet50V2Backbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
         input_size = 256
-        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
+        inputs = keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
         levels = ["P2", "P3", "P4", "P5"]
         self.assertEquals(list(outputs.keys()), levels)
         self.assertEquals(
             outputs["P2"].shape,
-            [None, input_size // 2**2, input_size // 2**2, 256],
+            (None, input_size // 2**2, input_size // 2**2, 256),
         )
         self.assertEquals(
             outputs["P3"].shape,
-            [None, input_size // 2**3, input_size // 2**3, 512],
+            (None, input_size // 2**3, input_size // 2**3, 512),
         )
         self.assertEquals(
             outputs["P4"].shape,
-            [None, input_size // 2**4, input_size // 2**4, 1024],
+            (None, input_size // 2**4, input_size // 2**4, 1024),
         )
         self.assertEquals(
             outputs["P5"].shape,
-            [None, input_size // 2**5, input_size // 2**5, 2048],
+            (None, input_size // 2**5, input_size // 2**5, 2048),
         )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
```

## keras_cv/models/classification/image_classifier.py

```diff
@@ -11,29 +11,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Image classifier model using pooling and dense layers."""
 
 import copy
 
-from keras import layers
-from tensorflow import keras
-
+from keras_cv.backend import keras
 from keras_cv.models.backbones.backbone_presets import backbone_presets
 from keras_cv.models.backbones.backbone_presets import (
     backbone_presets_with_weights,
 )
 from keras_cv.models.classification.image_classifier_presets import (
     classifier_presets,
 )
 from keras_cv.models.task import Task
 from keras_cv.utils.python_utils import classproperty
 
 
-@keras.utils.register_keras_serializable(package="keras_cv.models")
+@keras.saving.register_keras_serializable(package="keras_cv.models")
 class ImageClassifier(Task):
     """Image classifier with pooling and dense layer prediction head.
 
     Args:
         backbone: `keras.Model` instance, the backbone architecture of the
             classifier called on the inputs. Pooling will be called on the last
             dimension of the backbone output.
@@ -77,25 +75,25 @@
         backbone,
         num_classes,
         pooling="avg",
         activation="softmax",
         **kwargs,
     ):
         if pooling == "avg":
-            pooling_layer = layers.GlobalAveragePooling2D(name="avg_pool")
+            pooling_layer = keras.layers.GlobalAveragePooling2D(name="avg_pool")
         elif pooling == "max":
-            pooling_layer = layers.GlobalMaxPooling2D(name="max_pool")
+            pooling_layer = keras.layers.GlobalMaxPooling2D(name="max_pool")
         else:
             raise ValueError(
                 f'`pooling` must be one of "avg", "max". Received: {pooling}.'
             )
         inputs = backbone.input
         x = backbone(inputs)
         x = pooling_layer(x)
-        outputs = layers.Dense(
+        outputs = keras.layers.Dense(
             num_classes,
             activation=activation,
             name="predictions",
         )(x)
 
         # Instantiate using Functional API Model constructor
         super().__init__(
```

## keras_cv/models/classification/image_classifier_test.py

```diff
@@ -12,28 +12,30 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for ImageClassifier."""
 
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet18V2Backbone,
 )
 from keras_cv.models.classification.image_classifier import ImageClassifier
 
 
 class ImageClassifierTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
         self.dataset = tf.data.Dataset.from_tensor_slices(
             (self.input_batch, tf.one_hot(tf.ones((2,), dtype="int32"), 2))
         ).batch(4)
 
     def test_valid_call(self):
         model = ImageClassifier(
             backbone=ResNet18V2Backbone(),
@@ -41,14 +43,15 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
     @pytest.mark.large  # Fit is slow, so mark these large.
+    @pytest.mark.filterwarnings("ignore::UserWarning")  # Torch + jit_compile
     def test_classifier_fit(self, jit_compile):
         model = ImageClassifier(
             backbone=ResNet18V2Backbone(),
             num_classes=2,
         )
         model.compile(
             loss="categorical_crossentropy",
@@ -73,47 +76,46 @@
         with self.assertRaises(ValueError):
             ImageClassifier(
                 backbone=ResNet18V2Backbone(),
                 num_classes=2,
                 pooling="clowntown",
             )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = ImageClassifier(
             backbone=ResNet18V2Backbone(),
             num_classes=2,
         )
         model_output = model(self.input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(self.get_temp_dir(), "image_classifier.keras")
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, ImageClassifier)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output),
+            ops.convert_to_numpy(restored_output),
+        )
 
 
 @pytest.mark.large
 class ImageClassifierPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
     """
     A smoke test for ImageClassifier presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_cv/models/classification/image_classifier_test.py --run_large`
     """
 
     def setUp(self):
-        self.input_batch = tf.ones(shape=(2, 224, 224, 3))
+        self.input_batch = np.ones(shape=(2, 224, 224, 3))
 
     @parameterized.named_parameters(
         (
             "efficientnetv2_b0_imagenet_classifier",
             "efficientnetv2_b0_imagenet_classifier",
             [-0.278459, -0.278462, -0.159786, -0.277514, 0.537921],
         )
@@ -127,15 +129,17 @@
         # This test should catch cases where we unintentionally change our
         # network code in a way that would invalidate our preset weights.
         # We should only update these numbers if we are updating a weights
         # file, or have found a discrepancy with the upstream source.
         outputs = model.backbone(self.input_batch)
         outputs = outputs[0, 0, 0, :5]
         # Keep a high tolerance, so we are robust to different hardware.
-        self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+        self.assertAllClose(
+            ops.convert_to_numpy(outputs), expected, atol=0.01, rtol=0.01
+        )
 
     @parameterized.named_parameters(
         ("preset_with_weights", "resnet50_v2_imagenet"),
         ("preset_no_weights", "resnet50_v2"),
     )
     def test_backbone_preset_call(self, preset):
         model = ImageClassifier.from_preset(
@@ -161,15 +165,17 @@
             "resnet50_v2_imagenet",
             num_classes=2,
         )
         outputs = model.backbone(self.input_batch)
         outputs = outputs[0, 0, 0, :5]
         expected = [1.051145, 0, 0, 1.16328, 0]
         # Keep a high tolerance, so we are robust to different hardware.
-        self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+        self.assertAllClose(
+            ops.convert_to_numpy(outputs), expected, atol=0.01, rtol=0.01
+        )
 
     def test_classifier_preset_call(self):
         model = ImageClassifier.from_preset("resnet50_v2_imagenet_classifier")
         outputs = model(self.input_batch)
         # The forward pass from a preset should be stable!
         # This test should catch cases where we unintentionally change our
         # network code in a way that would invalidate our preset weights.
@@ -180,12 +186,14 @@
             7.866630e-05,
             4.669575e-05,
             8.475207e-05,
             1.728923e-04,
             3.414580e-04,
         ]
         # Keep a high tolerance, so we are robust to different hardware.
-        self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+        self.assertAllClose(
+            ops.convert_to_numpy(outputs), expected, atol=0.01, rtol=0.01
+        )
 
 
 if __name__ == "__main__":
     tf.test.main()
```

## keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py

```diff
@@ -456,22 +456,21 @@
             "classification": self.cls_loss,
             "rpn_box": self.rpn_box_loss,
             "rpn_classification": self.rpn_cls_loss,
         }
         super().compile(loss=losses, **kwargs)
 
     def compute_loss(self, images, boxes, classes, training):
-        image_shape = tf.shape(images[0])
         local_batch = images.get_shape().as_list()[0]
         if tf.distribute.has_strategy():
             num_sync = tf.distribute.get_strategy().num_replicas_in_sync
         else:
             num_sync = 1
         global_batch = local_batch * num_sync
-        anchors = self.anchor_generator(image_shape=image_shape)
+        anchors = self.anchor_generator(image_shape=tuple(images[0].shape))
         (
             rpn_box_targets,
             rpn_box_weights,
             rpn_cls_targets,
             rpn_cls_weights,
         ) = self.rpn_labeler(
             tf.concat(tf.nest.flatten(anchors), axis=0), boxes, classes
```

## keras_cv/models/object_detection/__test_utils__.py

```diff
@@ -7,47 +7,49 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import numpy as np
 import tensorflow as tf
 
 import keras_cv
+from keras_cv.backend import ops
 
 
 def _create_bounding_box_dataset(
     bounding_box_format, use_dictionary_box_format=False
 ):
     # Just about the easiest dataset you can have, all classes are 0, all boxes
     # are exactly the same. [1, 1, 2, 2] are the coordinates in xyxy.
-    xs = tf.random.normal(shape=(1, 512, 512, 3), dtype=tf.float32)
-    xs = tf.tile(xs, [5, 1, 1, 1])
+    xs = np.random.normal(size=(1, 512, 512, 3))
+    xs = ops.tile(xs, [5, 1, 1, 1])
 
-    y_classes = tf.zeros((5, 3), dtype=tf.float32)
+    y_classes = ops.zeros((5, 3), "float32")
 
-    ys = tf.constant(
+    ys = ops.array(
         [
             [0.1, 0.1, 0.23, 0.23],
             [0.67, 0.75, 0.23, 0.23],
             [0.25, 0.25, 0.23, 0.23],
         ],
-        dtype=tf.float32,
+        "float32",
     )
-    ys = tf.expand_dims(ys, axis=0)
-    ys = tf.tile(ys, [5, 1, 1])
+    ys = ops.expand_dims(ys, axis=0)
+    ys = ops.tile(ys, [5, 1, 1])
     ys = keras_cv.bounding_box.convert_format(
         ys,
         source="rel_xywh",
         target=bounding_box_format,
         images=xs,
-        dtype=tf.float32,
+        dtype="float32",
     )
-    num_dets = tf.ones([5])
+    num_dets = ops.ones([5])
 
     if use_dictionary_box_format:
         return tf.data.Dataset.from_tensor_slices(
             {
                 "images": xs,
                 "bounding_boxes": {
                     "boxes": ys,
```

## keras_cv/models/object_detection/retinanet/feature_pyramid.py

```diff
@@ -8,19 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class FeaturePyramid(keras.layers.Layer):
     """Builds the Feature Pyramid with the feature maps from the backbone."""
 
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
         self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, "same")
@@ -44,9 +44,35 @@
         p5_output = self.conv_c5_1x1(c5_output, training=training)
         p4_output = p4_output + self.upsample_2x(p5_output, training=training)
         p3_output = p3_output + self.upsample_2x(p4_output, training=training)
         p3_output = self.conv_c3_3x3(p3_output, training=training)
         p4_output = self.conv_c4_3x3(p4_output, training=training)
         p5_output = self.conv_c5_3x3(p5_output, training=training)
         p6_output = self.conv_c6_3x3(c5_output, training=training)
-        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output), training=training)
+        p7_output = self.conv_c7_3x3(ops.relu(p6_output), training=training)
         return p3_output, p4_output, p5_output, p6_output, p7_output
+
+    def build(self, input_shape):
+        p3_channels = input_shape["P3"][-1]
+        p4_channels = input_shape["P4"][-1]
+        p5_channels = input_shape["P5"][-1]
+        self.conv_c3_1x1.build((None, None, None, p3_channels))
+        self.conv_c4_1x1.build((None, None, None, p4_channels))
+        self.conv_c5_1x1.build((None, None, None, p5_channels))
+        self.conv_c3_3x3.build((None, None, None, 256))
+        self.conv_c4_3x3.build((None, None, None, 256))
+        self.conv_c5_3x3.build((None, None, None, 256))
+        self.conv_c6_3x3.build((None, None, None, p5_channels))
+        self.conv_c7_3x3.build((None, None, None, 256))
+        self.built = True
+
+    def compute_output_shape(self, input_shape):
+        p3_shape = input_shape["P3"][:-1]
+        p4_shape = input_shape["P4"][:-1]
+        p5_shape = input_shape["P5"][:-1]
+        return (
+            (tuple(p3_shape) + (256,)),
+            (tuple(p4_shape) + (256,)),
+            (tuple(p5_shape) + (256,)),
+            (tuple(p5_shape) + (256,)),
+            (tuple(p5_shape) + (256,)),
+        )
```

## keras_cv/models/object_detection/retinanet/prediction_head.py

```diff
@@ -8,20 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-from tensorflow.keras import layers
+from keras_cv.backend import keras
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class PredictionHead(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class PredictionHead(keras.layers.Layer):
     """The class/box predictions head.
 
     Arguments:
       output_filters: Number of convolution filters in the final layer.
       bias_initializer: Bias Initializer for the final convolution layer.
 
     Returns:
@@ -34,38 +33,41 @@
     ):
         super().__init__(**kwargs)
         self.output_filters = output_filters
         self.bias_initializer = bias_initializer
         self.num_conv_layers = num_conv_layers
 
         self.conv_layers = [
-            layers.Conv2D(
+            keras.layers.Conv2D(
                 256,
                 kernel_size=3,
                 padding="same",
-                kernel_initializer=keras.initializers.Orthogonal(),
+                kernel_initializer="orthogonal",
                 activation="relu",
             )
             for _ in range(num_conv_layers)
         ]
-        self.prediction_layer = layers.Conv2D(
+        self.prediction_layer = keras.layers.Conv2D(
             self.output_filters,
             kernel_size=3,
             strides=1,
             padding="same",
-            kernel_initializer=keras.initializers.Orthogonal(),
+            kernel_initializer="orthogonal",
             bias_initializer=self.bias_initializer,
         )
 
     def call(self, x, training=False):
         for layer in self.conv_layers:
             x = layer(x, training=training)
         x = self.prediction_layer(x, training=training)
         return x
 
+    def compute_output_shape(self, input_shape):
+        return tuple(input_shape[:-1]) + (self.output_filters,)
+
     def get_config(self):
         config = {
             "bias_initializer": keras.initializers.serialize(
                 self.bias_initializer
             ),
             "output_filters": self.output_filters,
             "num_conv_layers": self.num_conv_layers,
@@ -79,7 +81,17 @@
             {
                 "bias_initializer": keras.initializers.deserialize(
                     config["bias_initializer"]
                 )
             }
         )
         return super().from_config(config)
+
+    def build(self, input_shape):
+        self.conv_layers[0].build(input_shape)
+
+        intermediate_shape = tuple(input_shape[:-1]) + (256,)
+        for conv_layer in self.conv_layers[1:]:
+            conv_layer.build(intermediate_shape)
+
+        self.prediction_layer.build(intermediate_shape)
+        self.built = True
```

## keras_cv/models/object_detection/retinanet/retinanet.py

```diff
@@ -11,26 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 
 import numpy as np
-import tensorflow as tf
-from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
 from keras_cv import layers as cv_layers
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.bounding_box.converters import _decode_deltas_to_boxes
 from keras_cv.models.backbones.backbone_presets import backbone_presets
 from keras_cv.models.backbones.backbone_presets import (
     backbone_presets_with_weights,
 )
-from keras_cv.models.object_detection import predict_utils
 from keras_cv.models.object_detection.__internal__ import unpack_input
 from keras_cv.models.object_detection.retinanet import FeaturePyramid
 from keras_cv.models.object_detection.retinanet import PredictionHead
 from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
 from keras_cv.models.object_detection.retinanet.retinanet_presets import (
     retinanet_presets,
 )
@@ -39,25 +38,25 @@
 from keras_cv.utils.train import get_feature_extractor
 
 BOX_VARIANCE = [0.1, 0.1, 0.2, 0.2]
 
 
 # TODO(jbischof): Generalize `FeaturePyramid` class to allow for any P-levels
 #  and add `feature_pyramid_levels` param.
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class RetinaNet(Task):
     """A Keras model implementing the RetinaNet meta-architecture.
 
     Implements the RetinaNet architecture for object detection. The constructor
     requires `num_classes`, `bounding_box_format`, and a backbone. Optionally,
     a custom label encoder, and prediction decoder may be provided.
 
     Examples:
     ```python
-    images = tf.ones(shape=(1, 512, 512, 3))
+    images = np.ones((1, 512, 512, 3))
     labels = {
         "boxes": [
             [
                 [0, 0, 100, 100],
                 [100, 100, 200, 200],
                 [300, 300, 100, 100],
             ]
@@ -75,15 +74,15 @@
     # Evaluate model
     model(images)
 
     # Train model
     model.compile(
         classification_loss='focal',
         box_loss='smoothl1',
-        optimizer=tf.optimizers.SGD(global_clipnorm=10.0),
+        optimizer=keras.optimizers.SGD(global_clipnorm=10.0),
         jit_compile=False,
     )
     model.fit(images, labels)
     ```
 
     Args:
         num_classes: the number of classes in your dataset excluding the
@@ -185,39 +184,39 @@
             bias_initializer=prior_probability,
         )
         box_head = box_head or PredictionHead(
             output_filters=9 * 4, bias_initializer=keras.initializers.Zeros()
         )
 
         # Begin construction of forward pass
-        images = keras.layers.Input(feature_extractor.input_shape[1:])
+        images = keras.layers.Input(
+            feature_extractor.input_shape[1:], name="images"
+        )
 
         backbone_outputs = feature_extractor(images)
         features = feature_pyramid(backbone_outputs)
 
-        batch_size = tf.shape(images)[0]
         cls_pred = []
         box_pred = []
         for feature in features:
-            box_pred.append(tf.reshape(box_head(feature), [batch_size, -1, 4]))
+            box_pred.append(keras.layers.Reshape((-1, 4))(box_head(feature)))
             cls_pred.append(
-                tf.reshape(
-                    classification_head(feature),
-                    [batch_size, -1, num_classes],
+                keras.layers.Reshape((-1, num_classes))(
+                    classification_head(feature)
                 )
             )
 
         cls_pred = keras.layers.Concatenate(axis=1, name="classification")(
             cls_pred
         )
         box_pred = keras.layers.Concatenate(axis=1, name="box")(box_pred)
         # box_pred is always in "center_yxhw" delta-encoded no matter what
         # format you pass in.
 
-        inputs = images
+        inputs = {"images": images}
         outputs = {"box": box_pred, "classification": cls_pred}
 
         super().__init__(
             inputs=inputs,
             outputs=outputs,
             **kwargs,
         )
@@ -226,27 +225,31 @@
         self.bounding_box_format = bounding_box_format
         self.num_classes = num_classes
         self.backbone = backbone
 
         self.feature_extractor = feature_extractor
         self._prediction_decoder = (
             prediction_decoder
-            or cv_layers.MultiClassNonMaxSuppression(
+            or cv_layers.NonMaxSuppression(
                 bounding_box_format=bounding_box_format,
                 from_logits=True,
             )
         )
 
         self.feature_pyramid = feature_pyramid
         self.classification_head = classification_head
         self.box_head = box_head
         self.build(backbone.input_shape)
 
-    def make_predict_function(self, force=False):
-        return predict_utils.make_predict_function(self, force=force)
+    def predict_step(self, *args):
+        outputs = super().predict_step(*args)
+        if type(outputs) is tuple:
+            return self.decode_predictions(outputs[0], args[-1]), outputs[1]
+        else:
+            return self.decode_predictions(outputs, args[-1])
 
     @property
     def prediction_decoder(self):
         return self._prediction_decoder
 
     @prediction_decoder.setter
     def prediction_decoder(self, prediction_decoder):
@@ -278,17 +281,17 @@
             strides=strides,
             clip_boxes=True,
         )
 
     def decode_predictions(self, predictions, images):
         box_pred, cls_pred = predictions["box"], predictions["classification"]
         # box_pred is on "center_yxhw" format, convert to target format.
-        image_shape = tf.shape(images[0])
+        image_shape = tuple(images[0].shape)
         anchors = self.anchor_generator(image_shape=image_shape)
-        anchors = tf.concat(tf.nest.flatten(anchors), axis=0)
+        anchors = ops.concatenate([a for a in anchors.values()], axis=0)
 
         box_pred = _decode_deltas_to_boxes(
             anchors=anchors,
             boxes_delta=box_pred,
             anchor_format=self.anchor_generator.bounding_box_format,
             box_format=self.bounding_box_format,
             variance=BOX_VARIANCE,
@@ -312,15 +315,14 @@
         )
         return y_pred
 
     def compile(
         self,
         box_loss=None,
         classification_loss=None,
-        weight_decay=0.0001,
         loss=None,
         metrics=None,
         **kwargs,
     ):
         """compiles the RetinaNet.
 
         compile() mirrors the standard Keras compile() method, but has a few key
@@ -379,24 +381,27 @@
                     f"{box_loss.bounding_box_format}`, want "
                     "`box_loss.bounding_box_format="
                     f"{self.bounding_box_format}`"
                 )
 
         self.box_loss = box_loss
         self.classification_loss = classification_loss
-        self.weight_decay = weight_decay
         losses = {
             "box": self.box_loss,
             "classification": self.classification_loss,
         }
         self._has_user_metrics = metrics is not None and len(metrics) != 0
         self._user_metrics = metrics
         super().compile(loss=losses, **kwargs)
 
-    def compute_loss(self, x, box_pred, cls_pred, boxes, classes):
+    def compute_loss(self, x, y, y_pred, sample_weight, **kwargs):
+        box_pred = y_pred["box"]
+        cls_pred = y_pred["classification"]
+        boxes = y["box"]
+        classes = y["classification"]
         if boxes.shape[-1] != 4:
             raise ValueError(
                 "boxes should have shape (None, None, 4). Got "
                 f"boxes.shape={tuple(boxes.shape)}"
             )
 
         if box_pred.shape[-1] != 4:
@@ -410,91 +415,67 @@
             raise ValueError(
                 "cls_pred should have shape (None, None, 4). Got "
                 f"cls_pred.shape={tuple(cls_pred.shape)}. Does your model's "
                 "`num_classes` parameter match your losses `num_classes` "
                 "parameter?"
             )
 
-        cls_labels = tf.one_hot(
-            tf.cast(classes, dtype=tf.int32),
-            depth=self.num_classes,
-            dtype=tf.float32,
-        )
-
-        positive_mask = tf.cast(tf.greater(classes, -1.0), dtype=tf.float32)
-        normalizer = tf.reduce_sum(positive_mask)
-        cls_weights = tf.cast(
-            tf.math.not_equal(classes, -2.0), dtype=tf.float32
+        cls_labels = ops.one_hot(
+            ops.cast(classes, "int32"), self.num_classes, dtype="float32"
         )
+        positive_mask = ops.cast(ops.greater(classes, -1.0), dtype="float32")
+        normalizer = ops.sum(positive_mask)
+        cls_weights = ops.cast(ops.not_equal(classes, -2.0), dtype="float32")
         cls_weights /= normalizer
         box_weights = positive_mask / normalizer
         y_true = {
             "box": boxes,
             "classification": cls_labels,
         }
-        y_pred = {
-            "box": box_pred,
-            "classification": cls_pred,
-        }
         sample_weights = {
             "box": box_weights,
             "classification": cls_weights,
         }
         zero_weight = {
-            "box": tf.zeros_like(box_weights),
-            "classification": tf.zeros_like(cls_weights),
+            "box": ops.zeros_like(box_weights),
+            "classification": ops.zeros_like(cls_weights),
         }
 
-        sample_weights = tf.cond(
+        sample_weights = ops.cond(
             normalizer == 0,
             lambda: zero_weight,
             lambda: sample_weights,
         )
         return super().compute_loss(
             x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights
         )
 
-    def train_step(self, data):
+    def train_step(self, *args):
+        data = args[-1]
         x, y = unpack_input(data)
+
         y_for_label_encoder = bounding_box.convert_format(
             y,
             source=self.bounding_box_format,
             target=self.label_encoder.bounding_box_format,
             images=x,
         )
-        boxes, classes = self.label_encoder(x, y_for_label_encoder)
-        # boxes are now in `center_yxhw`. This is always the case in training
-        with tf.GradientTape() as tape:
-            outputs = self(x, training=True)
-            box_pred, cls_pred = outputs["box"], outputs["classification"]
-            total_loss = self.compute_loss(
-                x, box_pred, cls_pred, boxes, classes
-            )
-
-            reg_losses = []
-            if self.weight_decay:
-                for var in self.trainable_variables:
-                    if "bn" not in var.name:
-                        reg_losses.append(
-                            self.weight_decay * tf.nn.l2_loss(var)
-                        )
-                l2_loss = tf.math.add_n(reg_losses)
-            total_loss += l2_loss
-        # Training specific code
-        trainable_vars = self.trainable_variables
-        gradients = tape.gradient(total_loss, trainable_vars)
-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
 
-        if not self._has_user_metrics:
-            return super().compute_metrics(x, {}, {}, sample_weight={})
+        boxes, classes = self.label_encoder(x, y_for_label_encoder)
+        super_args = args[:-1] + (
+            (
+                x,
+                {"box": boxes, "classification": classes, "unencoded": y},
+            ),
+        )
 
-        y_pred = self.decode_predictions(outputs, x)
-        return self.compute_metrics(x, y, y_pred, sample_weight=None)
+        return super().train_step(*super_args)
 
-    def test_step(self, data):
+    def test_step(self, *args):
+        data = args[-1]
         x, y = unpack_input(data)
         y_for_label_encoder = bounding_box.convert_format(
             y,
             source=self.bounding_box_format,
             target=self.label_encoder.bounding_box_format,
             images=x,
         )
@@ -502,27 +483,36 @@
         boxes = bounding_box.convert_format(
             boxes,
             source=self.label_encoder.bounding_box_format,
             target=self.bounding_box_format,
             images=x,
         )
 
-        outputs = self(x, training=False)
-        box_pred, cls_pred = outputs["box"], outputs["classification"]
-        _ = self.compute_loss(x, box_pred, cls_pred, boxes, classes)
+        super_args = args[:-1] + (
+            (
+                x,
+                {"box": boxes, "classification": classes, "unencoded": y},
+            ),
+        )
 
-        if not self._has_user_metrics:
-            return super().compute_metrics(x, {}, {}, sample_weight={})
-        y_pred = self.decode_predictions(outputs, x)
-        return self.compute_metrics(x, y, y_pred, sample_weight=None)
+        return super().test_step(*super_args)
 
     def compute_metrics(self, x, y, y_pred, sample_weight):
         metrics = {}
         metrics.update(super().compute_metrics(x, {}, {}, sample_weight={}))
 
+        if not self._has_user_metrics:
+            return metrics
+
+        # For computing non-loss metrics, we don't care about the encoded
+        # boxes and classes, just the raw input boxes.
+        y = y["unencoded"]
+
+        y_pred = self.decode_predictions(y_pred, x)
+
         for metric in self._user_metrics:
             metric.update_state(y, y_pred, sample_weight=sample_weight)
 
         for metric in self._user_metrics:
             result = metric.result()
             if isinstance(result, dict):
                 metrics.update(result)
@@ -530,33 +520,41 @@
                 metrics[metric.name] = result
         return metrics
 
     def get_config(self):
         return {
             "num_classes": self.num_classes,
             "bounding_box_format": self.bounding_box_format,
-            "backbone": keras.utils.serialize_keras_object(self.backbone),
-            "label_encoder": self.label_encoder,
+            "backbone": keras.saving.serialize_keras_object(self.backbone),
+            "label_encoder": keras.saving.serialize_keras_object(
+                self.label_encoder
+            ),
             "prediction_decoder": self._prediction_decoder,
-            "classification_head": keras.utils.serialize_keras_object(
+            "classification_head": keras.saving.serialize_keras_object(
                 self.classification_head
             ),
-            "box_head": keras.utils.serialize_keras_object(self.box_head),
+            "box_head": keras.saving.serialize_keras_object(self.box_head),
         }
 
     @classmethod
     def from_config(cls, config):
         if "box_head" in config and isinstance(config["box_head"], dict):
             config["box_head"] = keras.layers.deserialize(config["box_head"])
         if "classification_head" in config and isinstance(
             config["classification_head"], dict
         ):
             config["classification_head"] = keras.layers.deserialize(
                 config["classification_head"]
             )
+        if "label_encoder" in config and isinstance(
+            config["label_encoder"], dict
+        ):
+            config["label_encoder"] = keras.layers.deserialize(
+                config["label_encoder"]
+            )
         return super().from_config(config)
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return copy.deepcopy({**backbone_presets, **retinanet_presets})
 
@@ -578,34 +576,30 @@
 def _parse_box_loss(loss):
     if not isinstance(loss, str):
         # support arbitrary callables
         return loss
 
     # case insensitive comparison
     if loss.lower() == "smoothl1":
-        return keras_cv.losses.SmoothL1Loss(
-            l1_cutoff=1.0, reduction=keras.losses.Reduction.SUM
-        )
+        return keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction="sum")
     if loss.lower() == "huber":
-        return keras.losses.Huber(reduction=keras.losses.Reduction.SUM)
+        return keras.losses.Huber(reduction="sum")
 
     raise ValueError(
         "Expected `box_loss` to be either a Keras Loss, "
         f"callable, or the string 'SmoothL1'. Got loss={loss}."
     )
 
 
 def _parse_classification_loss(loss):
     if not isinstance(loss, str):
         # support arbitrary callables
         return loss
 
     # case insensitive comparison
     if loss.lower() == "focal":
-        return keras_cv.losses.FocalLoss(
-            from_logits=True, reduction=keras.losses.Reduction.SUM
-        )
+        return keras_cv.losses.FocalLoss(from_logits=True, reduction="sum")
 
     raise ValueError(
         "Expected `classification_loss` to be either a Keras Loss, "
         f"callable, or the string 'Focal'. Got loss={loss}."
     )
```

## keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py

```diff
@@ -9,24 +9,24 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import layers
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.layers.object_detection import box_matcher
 from keras_cv.utils import target_gather
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
-class RetinaNetLabelEncoder(layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_cv")
+class RetinaNetLabelEncoder(keras.layers.Layer):
     """Transforms the raw labels into targets for training.
 
     This class has operations to generate targets for a batch of samples which
     is made up of the input images, bounding boxes for the objects present and
     their class ids. Targets are always represented in `center_yxwh` format.
     This done for numerical reasons, to ensure numerical consistency when
     training in any format.
@@ -60,15 +60,15 @@
         background_class=-1.0,
         ignore_class=-2.0,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.bounding_box_format = bounding_box_format
         self.anchor_generator = anchor_generator
-        self.box_variance = tf.convert_to_tensor(box_variance, dtype=self.dtype)
+        self.box_variance = ops.array(box_variance, "float32")
         self.background_class = background_class
         self.ignore_class = ignore_class
         self.matched_boxes_metric = keras.metrics.BinaryAccuracy(
             name="percent_boxes_matched_with_anchor"
         )
         self.positive_threshold = positive_threshold
         self.negative_threshold = negative_threshold
@@ -112,72 +112,76 @@
         iou_matrix = bounding_box.compute_iou(
             anchor_boxes,
             gt_boxes,
             bounding_box_format=self.bounding_box_format,
             image_shape=image_shape,
         )
         matched_gt_idx, matched_vals = self.box_matcher(iou_matrix)
-        matched_vals = matched_vals[..., tf.newaxis]
-        positive_mask = tf.cast(tf.math.equal(matched_vals, 1), self.dtype)
-        ignore_mask = tf.cast(tf.math.equal(matched_vals, -2), self.dtype)
+        matched_vals = ops.expand_dims(matched_vals, axis=-1)
+        positive_mask = ops.cast(ops.equal(matched_vals, 1), self.dtype)
+        ignore_mask = ops.cast(ops.equal(matched_vals, -2), self.dtype)
         matched_gt_boxes = target_gather._target_gather(
             gt_boxes, matched_gt_idx
         )
+        matched_gt_boxes = ops.reshape(
+            matched_gt_boxes, (-1, matched_gt_boxes.shape[1], 4)
+        )
+
         box_target = bounding_box._encode_box_to_deltas(
             anchors=anchor_boxes,
             boxes=matched_gt_boxes,
             anchor_format=self.bounding_box_format,
             box_format=self.bounding_box_format,
             variance=self.box_variance,
             image_shape=image_shape,
         )
         matched_gt_cls_ids = target_gather._target_gather(
             gt_classes, matched_gt_idx
         )
-        cls_target = tf.where(
-            tf.not_equal(positive_mask, 1.0),
+        cls_target = ops.where(
+            ops.not_equal(positive_mask, 1.0),
             self.background_class,
             matched_gt_cls_ids,
         )
-        cls_target = tf.where(
-            tf.equal(ignore_mask, 1.0), self.ignore_class, cls_target
+        cls_target = ops.where(
+            ops.equal(ignore_mask, 1.0), self.ignore_class, cls_target
+        )
+        label = ops.concatenate(
+            [box_target, ops.cast(cls_target, box_target.dtype)], axis=-1
         )
-        label = tf.concat([box_target, cls_target], axis=-1)
 
         # In the case that a box in the corner of an image matches with an all
         # -1 box that is outside the image, we should assign the box to the
         # ignore class. There are rare cases where a -1 box can be matched,
         # resulting in a NaN during training. The unit test passing all -1s to
         # the label encoder ensures that we properly handle this edge-case.
-        label = tf.where(
-            tf.expand_dims(
-                tf.math.reduce_any(tf.math.is_nan(label), axis=-1), axis=-1
-            ),
+        label = ops.where(
+            ops.expand_dims(ops.any(ops.isnan(label), axis=-1), axis=-1),
             self.ignore_class,
             label,
         )
 
         result = {"boxes": label[:, :, :4], "classes": label[:, :, 4]}
 
-        box_shape = tf.shape(gt_boxes)
+        box_shape = ops.shape(gt_boxes)
         batch_size = box_shape[0]
         n_boxes = box_shape[1]
-        box_ids = tf.range(n_boxes, dtype=matched_gt_idx.dtype)
-        matched_ids = tf.expand_dims(matched_gt_idx, axis=-1)
+        box_ids = ops.arange(gt_boxes.shape[1], dtype=matched_gt_idx.dtype)
+        matched_ids = ops.expand_dims(matched_gt_idx, axis=-1)
         matches = box_ids == matched_ids
-        matches = tf.math.reduce_any(matches, axis=1)
+        matches = ops.any(matches, axis=1)
         self.matched_boxes_metric.update_state(
-            tf.zeros(
+            ops.zeros(
                 (
                     batch_size,
                     n_boxes,
                 ),
-                dtype=tf.int32,
+                dtype="int32",
             ),
-            tf.cast(matches, tf.int32),
+            ops.cast(matches, "int32"),
         )
         return result
 
     def call(self, images, box_labels):
         """Creates box and classification targets for a batch
 
         Args:
@@ -188,29 +192,34 @@
         if isinstance(images, tf.RaggedTensor):
             raise ValueError(
                 "`RetinaNetLabelEncoder`'s `call()` method does not "
                 "support RaggedTensor inputs for the `images` argument. "
                 f"Received `type(images)={type(images)}`."
             )
 
-        image_shape = tf.shape(images[0])
+        image_shape = tuple(images[0].shape)
         box_labels = bounding_box.to_dense(box_labels)
-        if box_labels["classes"].get_shape().rank == 2:
-            box_labels["classes"] = box_labels["classes"][..., tf.newaxis]
+        if len(box_labels["classes"].shape) == 2:
+            box_labels["classes"] = ops.expand_dims(
+                box_labels["classes"], axis=-1
+            )
         anchor_boxes = self.anchor_generator(image_shape=image_shape)
-        anchor_boxes = tf.concat(list(anchor_boxes.values()), axis=0)
+        anchor_boxes = ops.concatenate(list(anchor_boxes.values()), axis=0)
         anchor_boxes = bounding_box.convert_format(
             anchor_boxes,
             source=self.anchor_generator.bounding_box_format,
             target=self.bounding_box_format,
             image_shape=image_shape,
         )
 
         result = self._encode_sample(box_labels, anchor_boxes, image_shape)
         encoded_box_targets = result["boxes"]
+        encoded_box_targets = ops.reshape(
+            encoded_box_targets, (-1, encoded_box_targets.shape[1], 4)
+        )
         class_targets = result["classes"]
         return encoded_box_targets, class_targets
 
     def get_config(self):
         config = {
             "bounding_box_format": self.bounding_box_format,
             "anchor_generator": self.anchor_generator,
```

## keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py

```diff
@@ -8,33 +8,33 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
+import pytest
 import tensorflow as tf
 
+from keras_cv import backend
 from keras_cv import layers as cv_layers
+from keras_cv.backend import ops
 from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
 
 
 class RetinaNetLabelEncoderTest(tf.test.TestCase):
     def test_label_encoder_output_shapes(self):
         images_shape = (8, 512, 512, 3)
         boxes_shape = (8, 10, 4)
         classes_shape = (8, 10)
 
-        images = tf.random.uniform(shape=images_shape)
-        boxes = tf.random.uniform(
-            shape=boxes_shape, minval=0.0, maxval=1.0, dtype=tf.float32
-        )
-        classes = tf.random.uniform(
-            shape=classes_shape, minval=0, maxval=5, dtype=tf.float32
-        )
+        images = np.random.uniform(size=images_shape)
+        boxes = np.random.uniform(size=boxes_shape, low=0.0, high=1.0)
+        classes = np.random.uniform(size=classes_shape, low=0, high=5)
         strides = [2**i for i in range(3, 8)]
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]
         aspect_ratios = [0.5, 1.0, 2.0]
 
         anchor_generator = cv_layers.AnchorGenerator(
             bounding_box_format="yxyx",
@@ -46,25 +46,25 @@
         encoder = RetinaNetLabelEncoder(
             anchor_generator=anchor_generator,
             bounding_box_format="xyxy",
         )
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
 
-        self.assertEqual(box_targets.shape, [8, 49104, 4])
-        self.assertEqual(class_targets.shape, [8, 49104])
+        self.assertEqual(box_targets.shape, (8, 49104, 4))
+        self.assertEqual(class_targets.shape, (8, 49104))
 
     def test_all_negative_1(self):
         images_shape = (8, 512, 512, 3)
         boxes_shape = (8, 10, 4)
         classes_shape = (8, 10)
 
-        images = tf.random.uniform(shape=images_shape)
-        boxes = -tf.ones(shape=boxes_shape, dtype=tf.float32)
-        classes = -tf.ones(shape=classes_shape, dtype=tf.float32)
+        images = np.random.uniform(size=images_shape)
+        boxes = -np.ones(shape=boxes_shape, dtype="float32")
+        classes = -np.ones(shape=classes_shape, dtype="float32")
 
         strides = [2**i for i in range(3, 8)]
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]
         aspect_ratios = [0.5, 1.0, 2.0]
 
         anchor_generator = cv_layers.AnchorGenerator(
@@ -78,31 +78,35 @@
             anchor_generator=anchor_generator,
             bounding_box_format="xyxy",
         )
 
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
 
-        self.assertFalse(tf.math.reduce_any(tf.math.is_nan(box_targets)))
-        self.assertFalse(tf.math.reduce_any(tf.math.is_nan(class_targets)))
+        self.assertFalse(ops.any(ops.isnan(box_targets)))
+        self.assertFalse(ops.any(ops.isnan(class_targets)))
 
+    @pytest.mark.skipif(
+        backend.supports_ragged() is False,
+        reason="Only TensorFlow supports raggeds",
+    )
     def test_ragged_encoding(self):
         images_shape = (2, 512, 512, 3)
 
         images = tf.random.uniform(shape=images_shape)
         boxes = tf.ragged.stack(
             [
-                tf.constant([[0, 0, 10, 10], [5, 5, 10, 10]], tf.float32),
-                tf.constant([[0, 0, 10, 10]], tf.float32),
+                tf.constant([[0, 0, 10, 10], [5, 5, 10, 10]], "float32"),
+                tf.constant([[0, 0, 10, 10]], "float32"),
             ]
         )
         classes = tf.ragged.stack(
             [
-                tf.constant([[1], [1]], tf.float32),
-                tf.constant([[1]], tf.float32),
+                tf.constant([[1], [1]], "float32"),
+                tf.constant([[1]], "float32"),
             ]
         )
         strides = [2**i for i in range(3, 8)]
         scales = [2**x for x in [0, 1 / 3, 2 / 3]]
         sizes = [x**2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]
         aspect_ratios = [0.5, 1.0, 2.0]
 
@@ -118,9 +122,9 @@
             bounding_box_format="xywh",
         )
 
         bounding_boxes = {"boxes": boxes, "classes": classes}
         box_targets, class_targets = encoder(images, bounding_boxes)
 
         # 49104 is the anchor generator shape
-        self.assertEqual(box_targets.shape, [2, 49104, 4])
-        self.assertEqual(class_targets.shape, [2, 49104])
+        self.assertEqual(box_targets.shape, (2, 49104, 4))
+        self.assertEqual(class_targets.shape, (2, 49104))
```

## keras_cv/models/object_detection/retinanet/retinanet_test.py

```diff
@@ -10,41 +10,32 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
-from tensorflow.keras import optimizers
 
 import keras_cv
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.test_backbone_presets import (
     test_backbone_presets,
 )
 from keras_cv.models.object_detection.__test_utils__ import (
     _create_bounding_box_dataset,
 )
 from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
 
 
 class RetinaNetTest(tf.test.TestCase, parameterized.TestCase):
-    @pytest.fixture(autouse=True)
-    def cleanup_global_session(self):
-        # Code before yield runs before the test
-        tf.config.set_soft_device_placement(False)
-        yield
-        # Reset soft device placement to not interfere with other unit test
-        # files
-        tf.config.set_soft_device_placement(True)
-        keras.backend.clear_session()
-
     def test_retinanet_construction(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
             backbone=keras_cv.models.ResNet18V2Backbone(),
         )
         retinanet.compile(
@@ -86,15 +77,15 @@
     @pytest.mark.large  # Fit is slow, so mark these large.
     def test_retinanet_call(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
             backbone=keras_cv.models.ResNet18V2Backbone(),
         )
-        images = tf.random.uniform((2, 512, 512, 3))
+        images = np.random.uniform(size=(2, 512, 512, 3))
         _ = retinanet(images)
         _ = retinanet.predict(images)
 
     def test_wrong_logits(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format="xywh",
@@ -102,15 +93,15 @@
         )
 
         with self.assertRaisesRegex(
             ValueError,
             "from_logits",
         ):
             retinanet.compile(
-                optimizer=optimizers.SGD(learning_rate=0.25),
+                optimizer=keras.optimizers.SGD(learning_rate=0.25),
                 classification_loss=keras_cv.losses.FocalLoss(
                     from_logits=False, reduction="none"
                 ),
                 box_loss=keras_cv.losses.SmoothL1Loss(
                     l1_cutoff=1.0, reduction="none"
                 ),
             )
@@ -120,90 +111,88 @@
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format=bounding_box_format,
             backbone=keras_cv.models.ResNet18V2Backbone(),
         )
         retinanet.backbone.trainable = False
         retinanet.compile(
-            optimizer=optimizers.Adam(),
+            optimizer=keras.optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
             box_loss=keras_cv.losses.SmoothL1Loss(
                 l1_cutoff=1.0, reduction="none"
             ),
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
 
         # call once
         _ = retinanet(xs)
-        variable_names = [x.name for x in retinanet.trainable_variables]
-        # classification_head
-        self.assertIn("prediction_head/conv2d_8/kernel:0", variable_names)
-        # box_head
-        self.assertIn("prediction_head_1/conv2d_12/kernel:0", variable_names)
+        self.assertEqual(len(retinanet.trainable_variables), 32)
 
     @pytest.mark.large  # Fit is slow, so mark these large.
     def test_no_nans(self):
         retina_net = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet18V2Backbone(),
+            backbone=keras_cv.models.CSPDarkNetTinyBackbone(),
         )
 
         retina_net.compile(
-            optimizer=optimizers.Adam(),
+            optimizer=keras.optimizers.Adam(),
             classification_loss="focal",
             box_loss="smoothl1",
         )
 
         # only a -1 box
-        xs = tf.ones((1, 512, 512, 3), tf.float32)
+        xs = ops.ones((1, 512, 512, 3), "float32")
         ys = {
-            "classes": tf.constant([[-1]], tf.float32),
-            "boxes": tf.constant([[[0, 0, 0, 0]]], tf.float32),
+            "classes": ops.array([[-1]], "float32"),
+            "boxes": ops.array([[[0, 0, 0, 0]]], "float32"),
         }
         ds = tf.data.Dataset.from_tensor_slices((xs, ys))
         ds = ds.repeat(2)
-        ds = ds.batch(2)
+        ds = ds.batch(2, drop_remainder=True)
         retina_net.fit(ds, epochs=1)
 
         weights = retina_net.get_weights()
         for weight in weights:
-            self.assertFalse(tf.math.reduce_any(tf.math.is_nan(weight)))
+            self.assertFalse(ops.any(ops.isnan(weight)))
 
     @pytest.mark.large  # Fit is slow, so mark these large.
     def test_weights_change(self):
         bounding_box_format = "xywh"
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format=bounding_box_format,
-            backbone=keras_cv.models.ResNet18V2Backbone(),
+            backbone=keras_cv.models.CSPDarkNetTinyBackbone(),
         )
 
         retinanet.compile(
-            optimizer=optimizers.Adam(),
+            optimizer=keras.optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
-                from_logits=True, reduction="none"
+                from_logits=True, reduction="sum"
             ),
             box_loss=keras_cv.losses.SmoothL1Loss(
-                l1_cutoff=1.0, reduction="none"
+                l1_cutoff=1.0, reduction="sum"
             ),
         )
-        xs, ys = _create_bounding_box_dataset(bounding_box_format)
+        ds = _create_bounding_box_dataset(
+            bounding_box_format, use_dictionary_box_format=True
+        )
 
         # call once
-        _ = retinanet(xs)
+        _ = retinanet(ops.ones((1, 512, 512, 3)))
         original_fpn_weights = retinanet.feature_pyramid.get_weights()
         original_box_head_weights = retinanet.box_head.get_weights()
         original_classification_head_weights = (
             retinanet.classification_head.get_weights()
         )
 
-        retinanet.fit(x=xs, y=ys, epochs=1)
+        retinanet.fit(ds, epochs=1)
         fpn_after_fit = retinanet.feature_pyramid.get_weights()
         box_head_after_fit_weights = retinanet.box_head.get_weights()
         classification_head_after_fit_weights = (
             retinanet.classification_head.get_weights()
         )
 
         for w1, w2 in zip(
@@ -216,53 +205,52 @@
             original_box_head_weights, box_head_after_fit_weights
         ):
             self.assertNotAllClose(w1, w2)
 
         for w1, w2 in zip(original_fpn_weights, fpn_after_fit):
             self.assertNotAllClose(w1, w2)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet18V2Backbone(),
+            backbone=keras_cv.models.CSPDarkNetTinyBackbone(),
         )
-        input_batch = tf.ones(shape=(2, 224, 224, 3))
+        input_batch = ops.ones(shape=(2, 224, 224, 3))
         model_output = model(input_batch)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
+        save_path = os.path.join(self.get_temp_dir(), "retinanet.keras")
+        model.save(save_path)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, keras_cv.models.RetinaNet)
 
         # Check that output matches.
         restored_output = restored_model(input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            tf.nest.map_structure(ops.convert_to_numpy, model_output),
+            tf.nest.map_structure(ops.convert_to_numpy, restored_output),
+        )
 
     def test_call_with_custom_label_encoder(self):
-        anchor_generator = (
-            keras_cv.models.RetinaNet.default_anchor_generator("xywh"),
+        anchor_generator = keras_cv.models.RetinaNet.default_anchor_generator(
+            "xywh"
         )
         model = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
             backbone=keras_cv.models.ResNet18V2Backbone(),
             label_encoder=RetinaNetLabelEncoder(
                 bounding_box_format="xywh",
                 anchor_generator=anchor_generator,
                 box_variance=[0.1, 0.1, 0.2, 0.2],
             ),
         )
-        model(tf.ones(shape=(2, 224, 224, 3)))
+        model(ops.ones(shape=(2, 224, 224, 3)))
 
 
 @pytest.mark.large
 class RetinaNetSmokeTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
         *[(preset, preset) for preset in test_backbone_presets]
     )
@@ -280,23 +268,27 @@
         self.assertEqual(output["box"].shape, (xs.shape[0], 49104, 4))
 
     def test_full_preset_weight_loading(self):
         model = keras_cv.models.RetinaNet.from_preset(
             "retinanet_resnet50_pascalvoc",
             bounding_box_format="xywh",
         )
-        xs = tf.ones((1, 512, 512, 3), tf.float32)
+        xs = ops.ones((1, 512, 512, 3))
         output = model(xs)
 
-        expected_box = tf.constant(
+        expected_box = ops.array(
             [-1.2427993, 0.05179548, -1.9953268, 0.32456252]
         )
-        self.assertAllClose(output["box"][0, 123, :], expected_box, atol=1e-5)
+        self.assertAllClose(
+            ops.convert_to_numpy(output["box"][0, 123, :]),
+            expected_box,
+            atol=1e-5,
+        )
 
-        expected_class = tf.constant(
+        expected_class = ops.array(
             [
                 -8.387445,
                 -7.891776,
                 -8.14204,
                 -8.117359,
                 -7.2517176,
                 -7.906804,
@@ -312,11 +304,13 @@
                 -7.071624,
                 -6.9687414,
                 -6.6398506,
                 -8.598567,
                 -6.484198,
             ]
         )
-        expected_class = tf.reshape(expected_class, (20,))
+        expected_class = ops.reshape(expected_class, (20,))
         self.assertAllClose(
-            output["classification"][0, 123], expected_class, atol=1e-5
+            ops.convert_to_numpy(output["classification"][0, 123]),
+            expected_class,
+            atol=1e-5,
         )
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py

```diff
@@ -9,18 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 
-import tensorflow as tf
-from keras import layers
-from tensorflow import keras
-
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_backbone_presets import (
     backbone_presets,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_backbone_presets import (
     backbone_presets_with_weights,
@@ -44,25 +42,25 @@
     x = apply_conv_bn(
         inputs,
         hidden_channels,
         kernel_size=1,
         activation=activation,
         name=f"{name}_pre",
     )
-    pool_1 = layers.MaxPool2D(
+    pool_1 = keras.layers.MaxPooling2D(
         pool_size=pool_size, strides=1, padding="same", name=f"{name}_pool1"
     )(x)
-    pool_2 = layers.MaxPool2D(
+    pool_2 = keras.layers.MaxPooling2D(
         pool_size=pool_size, strides=1, padding="same", name=f"{name}_pool2"
     )(pool_1)
-    pool_3 = layers.MaxPool2D(
+    pool_3 = keras.layers.MaxPooling2D(
         pool_size=pool_size, strides=1, padding="same", name=f"{name}_pool3"
     )(pool_2)
 
-    out = tf.concat([x, pool_1, pool_2, pool_3], axis=channel_axis)
+    out = ops.concatenate([x, pool_1, pool_2, pool_3], axis=channel_axis)
     out = apply_conv_bn(
         out,
         input_channels,
         kernel_size=1,
         activation=activation,
         name=f"{name}_output",
     )
@@ -125,15 +123,15 @@
         input_tensor=None,
         **kwargs,
     ):
         inputs = utils.parse_model_inputs(input_shape, input_tensor)
 
         x = inputs
         if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
+            x = keras.layers.Rescaling(1 / 255.0)(x)
 
         """ Stem """
         stem_width = stackwise_channels[0]
         x = apply_conv_bn(
             x,
             stem_width // 2,
             kernel_size=3,
@@ -147,15 +145,15 @@
             kernel_size=3,
             strides=2,
             activation=activation,
             name="stem_2",
         )
 
         """ blocks """
-        pyramid_level_inputs = {"P1": x.node.layer.name}
+        pyramid_level_inputs = {"P1": utils.get_tensor_input_name(x)}
         for stack_id, (channel, depth) in enumerate(
             zip(stackwise_channels, stackwise_depth)
         ):
             stack_name = f"stack{stack_id + 1}"
             if stack_id >= 1:
                 x = apply_conv_bn(
                     x,
@@ -176,15 +174,17 @@
             if stack_id == len(stackwise_depth) - 1:
                 x = apply_spatial_pyramid_pooling_fast(
                     x,
                     pool_size=5,
                     activation=activation,
                     name=f"{stack_name}_spp_fast",
                 )
-            pyramid_level_inputs[f"P{stack_id + 2}"] = x.node.layer.name
+            pyramid_level_inputs[
+                f"P{stack_id + 2}"
+            ] = utils.get_tensor_input_name(x)
 
         super().__init__(inputs=inputs, outputs=x, **kwargs)
         self.pyramid_level_inputs = pyramid_level_inputs
         self.stackwise_channels = stackwise_channels
         self.stackwise_depth = stackwise_depth
         self.include_rescaling = include_rescaling
         self.activation = activation
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py

```diff
@@ -10,26 +10,23 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 import warnings
 
-import tensorflow as tf
-from keras import layers
-from tensorflow import keras
-
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.losses.ciou_loss import CIoULoss
 from keras_cv.models.backbones.backbone_presets import backbone_presets
 from keras_cv.models.backbones.backbone_presets import (
     backbone_presets_with_weights,
 )
-from keras_cv.models.object_detection import predict_utils
 from keras_cv.models.object_detection.__internal__ import unpack_input
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_detector_presets import (
     yolo_v8_detector_presets,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_label_encoder import (
     YOLOV8LabelEncoder,
 )
@@ -68,39 +65,44 @@
             from the edge of the image.
 
     Returns:
         A tuple of anchor centerpoints and anchor strides. Multiplying the
         two together will yield the centerpoints in absolute x,y format.
 
     """
-    base_anchors = tf.constant(base_anchors, dtype=tf.float32)
+    base_anchors = ops.array(base_anchors, dtype="float32")
 
     all_anchors = []
     all_strides = []
     for stride in strides:
-        hh_centers = tf.range(start=0, limit=image_shape[0], delta=stride)
-        ww_centers = tf.range(start=0, limit=image_shape[1], delta=stride)
-        ww_grid, hh_grid = tf.meshgrid(ww_centers, hh_centers)
-        grid = tf.cast(
-            tf.reshape(tf.stack([hh_grid, ww_grid], 2), [-1, 1, 2]),
-            tf.float32,
+        hh_centers = ops.arange(0, image_shape[0], stride)
+        ww_centers = ops.arange(0, image_shape[1], stride)
+        ww_grid, hh_grid = ops.meshgrid(ww_centers, hh_centers)
+        grid = ops.cast(
+            ops.reshape(ops.stack([hh_grid, ww_grid], 2), [-1, 1, 2]),
+            "float32",
+        )
+        anchors = (
+            ops.expand_dims(
+                base_anchors * ops.array([stride, stride], "float32"), 0
+            )
+            + grid
         )
-        anchors = tf.expand_dims(base_anchors * [stride, stride], 0) + grid
-        anchors = tf.reshape(anchors, [-1, 2])
+        anchors = ops.reshape(anchors, [-1, 2])
         all_anchors.append(anchors)
-        all_strides.append(tf.repeat(stride, anchors.shape[0]))
+        all_strides.append(ops.repeat(stride, anchors.shape[0]))
 
-    all_anchors = tf.cast(tf.concat(all_anchors, axis=0), tf.float32)
-    all_strides = tf.cast(tf.concat(all_strides, axis=0), tf.float32)
+    all_anchors = ops.cast(ops.concatenate(all_anchors, axis=0), "float32")
+    all_strides = ops.cast(ops.concatenate(all_strides, axis=0), "float32")
 
     all_anchors = all_anchors / all_strides[:, None]
 
     # Swap the x and y coordinates of the anchors.
-    all_anchors = tf.concat(
-        [all_anchors[:, 1, tf.newaxis], all_anchors[:, 0, tf.newaxis]], axis=-1
+    all_anchors = ops.concatenate(
+        [all_anchors[:, 1, None], all_anchors[:, 0, None]], axis=-1
     )
     return all_anchors, all_strides
 
 
 def apply_path_aggregation_fpn(features, depth=3, name="fpn"):
     """Applies the Feature Pyramid Network (FPN) to the outputs of a backbone.
 
@@ -116,28 +118,28 @@
         resolution of the P3 inputs with the strong feature representations of
         the P5 inputs.
 
     """
     p3, p4, p5 = features
 
     # Upsample P5 and concatenate with P4, then apply a CSPBlock.
-    p5_upsampled = tf.image.resize(p5, tf.shape(p4)[1:-1], method="nearest")
-    p4p5 = tf.concat([p5_upsampled, p4], axis=-1)
+    p5_upsampled = ops.repeat(ops.repeat(p5, 2, axis=1), 2, axis=2)
+    p4p5 = ops.concatenate([p5_upsampled, p4], axis=-1)
     p4p5 = apply_csp_block(
         p4p5,
         channels=p4.shape[-1],
         depth=depth,
         shortcut=False,
         activation="swish",
         name=f"{name}_p4p5",
     )
 
     # Upsample P4P5 and concatenate with P3, then apply a CSPBlock.
-    p4p5_upsampled = tf.image.resize(p4p5, tf.shape(p3)[1:-1], method="nearest")
-    p3p4p5 = tf.concat([p4p5_upsampled, p3], axis=-1)
+    p4p5_upsampled = ops.repeat(ops.repeat(p4p5, 2, axis=1), 2, axis=2)
+    p3p4p5 = ops.concatenate([p4p5_upsampled, p3], axis=-1)
     p3p4p5 = apply_csp_block(
         p3p4p5,
         channels=p3.shape[-1],
         depth=depth,
         shortcut=False,
         activation="swish",
         name=f"{name}_p3p4p5",
@@ -148,15 +150,15 @@
         p3p4p5,
         p3p4p5.shape[-1],
         kernel_size=3,
         strides=2,
         activation="swish",
         name=f"{name}_p3p4p5_downsample1",
     )
-    p3p4p5_d1 = tf.concat([p3p4p5_d1, p4p5], axis=-1)
+    p3p4p5_d1 = ops.concatenate([p3p4p5_d1, p4p5], axis=-1)
     p3p4p5_d1 = apply_csp_block(
         p3p4p5_d1,
         channels=p4p5.shape[-1],
         shortcut=False,
         activation="swish",
         name=f"{name}_p3p4p5_downsample1_block",
     )
@@ -167,15 +169,15 @@
         p3p4p5_d1,
         p3p4p5_d1.shape[-1],
         kernel_size=3,
         strides=2,
         activation="swish",
         name=f"{name}_p3p4p5_downsample2",
     )
-    p3p4p5_d2 = tf.concat([p3p4p5_d2, p5], axis=-1)
+    p3p4p5_d2 = ops.concatenate([p3p4p5_d2, p5], axis=-1)
     p3p4p5_d2 = apply_csp_block(
         p3p4p5_d2,
         channels=p5.shape[-1],
         shortcut=False,
         activation="swish",
         name=f"{name}_p3p4p5_downsample2_block",
     )
@@ -232,15 +234,15 @@
         box_predictions = apply_conv_bn(
             box_predictions,
             box_channels,
             kernel_size=3,
             activation="swish",
             name=f"{cur_name}_box_2",
         )
-        box_predictions = layers.Conv2D(
+        box_predictions = keras.layers.Conv2D(
             filters=BOX_REGRESSION_CHANNELS,
             kernel_size=1,
             name=f"{cur_name}_box_3_conv",
         )(box_predictions)
 
         class_predictions = apply_conv_bn(
             feature,
@@ -252,33 +254,33 @@
         class_predictions = apply_conv_bn(
             class_predictions,
             class_channels,
             kernel_size=3,
             activation="swish",
             name=f"{cur_name}_class_2",
         )
-        class_predictions = layers.Conv2D(
+        class_predictions = keras.layers.Conv2D(
             filters=num_classes,
             kernel_size=1,
             name=f"{cur_name}_class_3_conv",
         )(class_predictions)
-        class_predictions = layers.Activation(
+        class_predictions = keras.layers.Activation(
             "sigmoid", name=f"{cur_name}_classifier"
         )(class_predictions)
 
-        out = tf.concat([box_predictions, class_predictions], axis=-1)
-        out = layers.Reshape(
+        out = ops.concatenate([box_predictions, class_predictions], axis=-1)
+        out = keras.layers.Reshape(
             [-1, out.shape[-1]], name=f"{cur_name}_output_reshape"
         )(out)
         outputs.append(out)
 
-    outputs = tf.concat(outputs, axis=1)
-    outputs = layers.Activation("linear", dtype="float32", name="box_outputs")(
-        outputs
-    )
+    outputs = ops.concatenate(outputs, axis=1)
+    outputs = keras.layers.Activation(
+        "linear", dtype="float32", name="box_outputs"
+    )(outputs)
 
     return {
         "boxes": outputs[:, :, :BOX_REGRESSION_CHANNELS],
         "classes": outputs[:, :, BOX_REGRESSION_CHANNELS:],
     }
 
 
@@ -289,39 +291,39 @@
     points.
 
     Each coordinate is encoded with 16 predicted values. Those predictions are
     softmaxed and multiplied by [0..15] to make predictions. The resulting
     predictions are relative to the stride of an anchor box (and correspondingly
     relative to the scale of the feature map from which the predictions came).
     """
-    preds_bbox = tf.reshape(
-        preds, (-1, preds.shape[1], 4, BOX_REGRESSION_CHANNELS // 4)
+    preds_bbox = keras.layers.Reshape((-1, 4, BOX_REGRESSION_CHANNELS // 4))(
+        preds
     )
-    preds_bbox = tf.nn.softmax(preds_bbox, axis=-1) * tf.range(
+    preds_bbox = ops.nn.softmax(preds_bbox, axis=-1) * ops.arange(
         BOX_REGRESSION_CHANNELS // 4, dtype="float32"
     )
-    return tf.reduce_sum(preds_bbox, axis=-1)
+    return ops.sum(preds_bbox, axis=-1)
 
 
 def dist2bbox(distance, anchor_points):
     """Decodes distance predictions into xyxy boxes.
 
     Input left / top / right / bottom predictions are transformed into xyxy box
     predictions based on anchor points.
 
     The resulting xyxy predictions must be scaled by the stride of their
     corresponding anchor points to yield an absolute xyxy box.
     """
-    left_top, right_bottom = tf.split(distance, 2, axis=-1)
+    left_top, right_bottom = ops.split(distance, 2, axis=-1)
     x1y1 = anchor_points - left_top
     x2y2 = anchor_points + right_bottom
-    return tf.concat((x1y1, x2y2), axis=-1)  # xyxy bbox
+    return ops.concatenate((x1y1, x2y2), axis=-1)  # xyxy bbox
 
 
-@keras.utils.register_keras_serializable(package="keras_cv")
+@keras.saving.register_keras_serializable(package="keras_cv")
 class YOLOV8Detector(Task):
     """Implements the YOLOV8 architecture for object detection.
 
     Args:
         backbone: `keras.Model`, must implement the `pyramid_level_inputs`
             property with keys "P2", "P3", and "P4" and layer names as values.
             A sensible backbone to use is the `keras_cv.models.YOLOV8Backbone`.
@@ -399,15 +401,15 @@
         extractor_layer_names = [
             backbone.pyramid_level_inputs[i] for i in extractor_levels
         ]
         feature_extractor = get_feature_extractor(
             backbone, extractor_layer_names, extractor_levels
         )
 
-        images = layers.Input(feature_extractor.input_shape[1:])
+        images = keras.layers.Input(feature_extractor.input_shape[1:])
         features = list(feature_extractor(images).values())
 
         fpn_features = apply_path_aggregation_fpn(
             features, depth=fpn_depth, name="pa_fpn"
         )
 
         outputs = apply_yolo_v8_head(
@@ -423,15 +425,15 @@
 
         outputs = {"boxes": boxes, "classes": scores}
         super().__init__(inputs=images, outputs=outputs, **kwargs)
 
         self.bounding_box_format = bounding_box_format
         self._prediction_decoder = (
             prediction_decoder
-            or keras_cv.layers.MultiClassNonMaxSuppression(
+            or keras_cv.layers.NonMaxSuppression(
                 bounding_box_format=bounding_box_format,
                 from_logits=False,
                 confidence_threshold=0.2,
                 iou_threshold=0.7,
             )
         )
         self.backbone = backbone
@@ -506,115 +508,109 @@
         losses = {
             "box": self.box_loss,
             "class": self.classification_loss,
         }
 
         super().compile(loss=losses, **kwargs)
 
-    def train_step(self, data):
+    def train_step(self, *args):
+        data = args[-1]
+        args = args[:-1]
         x, y = unpack_input(data)
+        return super().train_step(*args, (x, y))
 
-        with tf.GradientTape() as tape:
-            outputs = self(x, training=True)
-            box_pred, cls_pred = outputs["boxes"], outputs["classes"]
-            total_loss = self.compute_loss(x, y, box_pred, cls_pred)
-
-        trainable_vars = self.trainable_variables
-
-        gradients = tape.gradient(total_loss, trainable_vars)
-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
-
-        return super().compute_metrics(x, {}, {}, sample_weight={})
-
-    def test_step(self, data):
+    def test_step(self, *args):
+        data = args[-1]
+        args = args[:-1]
         x, y = unpack_input(data)
+        return super().test_step(*args, (x, y))
 
-        outputs = self(x, training=False)
-        box_pred, cls_pred = outputs["boxes"], outputs["classes"]
-        _ = self.compute_loss(x, y, box_pred, cls_pred)
-
-        return super().compute_metrics(x, {}, {}, sample_weight={})
+    def compute_loss(self, x, y, y_pred, sample_weight=None, **kwargs):
+        box_pred, cls_pred = y_pred["boxes"], y_pred["classes"]
 
-    def compute_loss(self, x, y, box_pred, cls_pred):
         pred_boxes = decode_regression_to_boxes(box_pred)
         pred_scores = cls_pred
 
         anchor_points, stride_tensor = get_anchors(image_shape=x.shape[1:])
-        stride_tensor = tf.expand_dims(stride_tensor, axis=-1)
+        stride_tensor = ops.expand_dims(stride_tensor, axis=-1)
 
         gt_labels = y["classes"]
 
-        mask_gt = tf.reduce_all(y["boxes"] > -1.0, axis=-1, keepdims=True)
+        mask_gt = ops.all(y["boxes"] > -1.0, axis=-1, keepdims=True)
         gt_bboxes = bounding_box.convert_format(
             y["boxes"],
             source=self.bounding_box_format,
             target="xyxy",
             images=x,
         )
 
         pred_bboxes = dist2bbox(pred_boxes, anchor_points)
 
         target_bboxes, target_scores, fg_mask = self.label_encoder(
             pred_scores,
-            tf.cast(pred_bboxes * stride_tensor, gt_bboxes.dtype),
+            ops.cast(pred_bboxes * stride_tensor, gt_bboxes.dtype),
             anchor_points * stride_tensor,
             gt_labels,
             gt_bboxes,
             mask_gt,
         )
 
         target_bboxes /= stride_tensor
-        target_scores_sum = tf.math.maximum(tf.reduce_sum(target_scores), 1)
-        box_weight = tf.expand_dims(
-            tf.boolean_mask(tf.reduce_sum(target_scores, axis=-1), fg_mask),
+        target_scores_sum = ops.maximum(ops.sum(target_scores), 1)
+        box_weight = ops.expand_dims(
+            ops.sum(target_scores, axis=-1) * fg_mask,
             axis=-1,
         )
 
         y_true = {
-            "box": target_bboxes[fg_mask],
+            "box": target_bboxes * fg_mask[..., None],
             "class": target_scores,
         }
         y_pred = {
-            "box": pred_bboxes[fg_mask],
+            "box": pred_bboxes * fg_mask[..., None],
             "class": pred_scores,
         }
         sample_weights = {
             "box": self.box_loss_weight * box_weight / target_scores_sum,
             "class": self.classification_loss_weight / target_scores_sum,
         }
 
         return super().compute_loss(
-            x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights
+            x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights, **kwargs
         )
 
     def decode_predictions(
         self,
         pred,
         images,
     ):
         boxes = pred["boxes"]
         scores = pred["classes"]
 
         boxes = decode_regression_to_boxes(boxes)
 
         anchor_points, stride_tensor = get_anchors(image_shape=images.shape[1:])
-        stride_tensor = tf.expand_dims(stride_tensor, axis=-1)
+        stride_tensor = ops.expand_dims(stride_tensor, axis=-1)
 
         box_preds = dist2bbox(boxes, anchor_points) * stride_tensor
         box_preds = bounding_box.convert_format(
             box_preds,
             source="xyxy",
             target=self.bounding_box_format,
             images=images,
         )
 
         return self.prediction_decoder(box_preds, scores)
 
-    def make_predict_function(self, force=False):
-        return predict_utils.make_predict_function(self, force=force)
+    def predict_step(self, *args):
+        outputs = super().predict_step(*args)
+        if isinstance(outputs, tuple):
+            return self.decode_predictions(outputs[0], args[-1]), outputs[1]
+        else:
+            return self.decode_predictions(outputs, args[-1])
 
     @property
     def prediction_decoder(self):
         return self._prediction_decoder
 
     @prediction_decoder.setter
     def prediction_decoder(self, prediction_decoder):
@@ -633,15 +629,15 @@
         self.make_test_function(force=True)
 
     def get_config(self):
         return {
             "num_classes": self.num_classes,
             "bounding_box_format": self.bounding_box_format,
             "fpn_depth": self.fpn_depth,
-            "backbone": keras.utils.serialize_keras_object(self.backbone),
+            "backbone": keras.saving.serialize_keras_object(self.backbone),
             "label_encoder": self.label_encoder,
             "prediction_decoder": self._prediction_decoder,
         }
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py

```diff
@@ -10,21 +10,23 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import numpy as np
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.models.backbones.test_backbone_presets import (
     test_backbone_presets,
 )
 from keras_cv.models.object_detection.__test_utils__ import (
     _create_bounding_box_dataset,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_detector_presets import (
@@ -50,14 +52,15 @@
             classification_loss="binary_crossentropy",
             box_loss="ciou",
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
 
         yolo.fit(x=xs, y=ys, epochs=1)
 
+    @pytest.mark.tf_keras_only
     @pytest.mark.large  # Fit is slow, so mark these large.
     def test_fit_with_ragged_tensors(self):
         bounding_box_format = "xywh"
         yolo = keras_cv.models.YOLOV8Detector(
             num_classes=2,
             fpn_depth=1,
             bounding_box_format=bounding_box_format,
@@ -91,16 +94,15 @@
         yolo.compile(
             optimizer="adam",
             classification_loss="binary_crossentropy",
             box_loss="ciou",
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
         # Make all bounding_boxes invalid and filter out them
-        ys["classes"] = -tf.ones_like(ys["classes"])
-        ys = bounding_box.to_ragged(ys)
+        ys["classes"] = -ops.ones_like(ys["classes"])
 
         yolo.fit(x=xs, y=ys, epochs=1)
 
     def test_trainable_weight_count(self):
         yolo = keras_cv.models.YOLOV8Detector(
             num_classes=2,
             fpn_depth=1,
@@ -132,80 +134,98 @@
 
         with self.assertRaisesRegex(
             ValueError,
             "Invalid classification loss",
         ):
             yolo.compile(box_loss="ciou", classification_loss="bad_loss")
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = keras_cv.models.YOLOV8Detector(
             num_classes=20,
             bounding_box_format="xywh",
             fpn_depth=1,
             backbone=keras_cv.models.YOLOV8Backbone.from_preset(
                 "yolo_v8_xs_backbone"
             ),
         )
         xs, _ = _create_bounding_box_dataset("xywh")
         model_output = model(xs)
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        model.save(save_path, save_format=save_format)
-        restored_model = keras.models.load_model(save_path)
+        save_path = os.path.join(
+            self.get_temp_dir(), "yolo_v8_xs_detector.keras"
+        )
+        model.save(save_path)
+        # TODO: Remove the need to pass the `custom_objects` parameter.
+        restored_model = keras.saving.load_model(
+            save_path,
+            custom_objects={"YOLOV8Detector": keras_cv.models.YOLOV8Detector},
+        )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, keras_cv.models.YOLOV8Detector)
 
         # Check that output matches.
         restored_output = restored_model(xs)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output["boxes"]),
+            ops.convert_to_numpy(restored_output["boxes"]),
+        )
+        self.assertAllClose(
+            ops.convert_to_numpy(model_output["classes"]),
+            ops.convert_to_numpy(restored_output["classes"]),
+        )
 
+    # TODO(tirthasheshpatel): Support updating prediction decoder in Keras Core.
+    @pytest.mark.tf_keras_only
     def test_update_prediction_decoder(self):
         yolo = keras_cv.models.YOLOV8Detector(
             num_classes=2,
             fpn_depth=1,
             bounding_box_format="xywh",
             backbone=keras_cv.models.YOLOV8Backbone.from_preset(
                 "yolo_v8_s_backbone"
             ),
-            prediction_decoder=keras_cv.layers.MultiClassNonMaxSuppression(
+            prediction_decoder=keras_cv.layers.NonMaxSuppression(
                 bounding_box_format="xywh",
                 from_logits=False,
                 confidence_threshold=0.0,
                 iou_threshold=1.0,
             ),
         )
 
-        image = tf.ones((1, 512, 512, 3))
+        image = np.ones((1, 512, 512, 3))
 
         outputs = yolo.predict(image)
         # We predicted at least 1 box with confidence_threshold 0
-        self.assertGreater(outputs["boxes"]._values.shape[0], 0)
+        self.assertGreater(outputs["boxes"].shape[0], 0)
 
-        yolo.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(
+        yolo.prediction_decoder = keras_cv.layers.NonMaxSuppression(
             bounding_box_format="xywh",
             from_logits=False,
             confidence_threshold=1.0,
             iou_threshold=1.0,
         )
 
         outputs = yolo.predict(image)
         # We predicted no boxes with confidence threshold 1
-        self.assertEqual(outputs["boxes"]._values.shape[0], 0)
+        self.assertAllEqual(outputs["boxes"], -np.ones_like(outputs["boxes"]))
+        self.assertAllEqual(
+            outputs["confidence"], -np.ones_like(outputs["confidence"])
+        )
+        self.assertAllEqual(
+            outputs["classes"], -np.ones_like(outputs["classes"])
+        )
 
 
 @pytest.mark.large
 class YOLOV8DetectorSmokeTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(
         *[(preset, preset) for preset in test_backbone_presets]
     )
+    @pytest.mark.extra_large
     def test_backbone_preset(self, preset):
         model = keras_cv.models.YOLOV8Detector.from_preset(
             preset,
             num_classes=20,
             bounding_box_format="xywh",
         )
         xs, _ = _create_bounding_box_dataset(bounding_box_format="xywh")
@@ -217,23 +237,23 @@
 
     def test_preset_with_forward_pass(self):
         model = keras_cv.models.YOLOV8Detector.from_preset(
             "yolo_v8_m_pascalvoc",
             bounding_box_format="xywh",
         )
 
-        image = tf.ones((1, 512, 512, 3))
+        image = np.ones((1, 512, 512, 3))
         encoded_predictions = model(image)
 
         self.assertAllClose(
-            encoded_predictions["boxes"][0, 0:5, 0],
+            ops.convert_to_numpy(encoded_predictions["boxes"][0, 0:5, 0]),
             [-0.8303556, 0.75213313, 1.809204, 1.6576759, 1.4134747],
         )
         self.assertAllClose(
-            encoded_predictions["classes"][0, 0:5, 0],
+            ops.convert_to_numpy(encoded_predictions["classes"][0, 0:5, 0]),
             [
                 7.6146556e-08,
                 8.0103280e-07,
                 9.7873999e-07,
                 2.2314548e-06,
                 2.5051115e-06,
             ],
@@ -246,13 +266,13 @@
     Test the full enumeration of our presets.
     This every presets for YOLOV8Detector and is only run manually.
     Run with:
     `pytest keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py --run_extra_large`
     """  # noqa: E501
 
     def test_load_yolo_v8_detector(self):
-        input_data = tf.ones(shape=(2, 224, 224, 3))
+        input_data = np.ones(shape=(2, 224, 224, 3))
         for preset in yolo_v8_detector_presets:
             model = keras_cv.models.YOLOV8Detector.from_preset(
                 preset, bounding_box_format="xywh"
             )
             model(input_data)
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py

```diff
@@ -12,91 +12,91 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Label encoder for YOLOV8. This uses the TOOD Task Aligned Assigner approach,
 and is adapted from https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/tal.py
 """  # noqa: E501
 
 import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import layers
 
 from keras_cv import bounding_box
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 from keras_cv.bounding_box.iou import compute_ciou
 
 
 def select_highest_overlaps(mask_pos, overlaps, max_num_boxes):
     """Break ties when two GT boxes match to the same anchor.
 
     Picks the GT box with the highest IoU.
     """
     # (b, max_num_boxes, num_anchors) -> (b, num_anchors)
-    fg_mask = tf.reduce_sum(mask_pos, axis=-2)
+    fg_mask = ops.sum(mask_pos, axis=-2)
 
     def handle_anchor_with_two_gt_boxes(
         fg_mask, mask_pos, overlaps, max_num_boxes
     ):
-        mask_multi_gts = tf.repeat(
-            tf.expand_dims(fg_mask, axis=1) > 1, max_num_boxes, axis=1
+        mask_multi_gts = ops.repeat(
+            ops.expand_dims(fg_mask, axis=1) > 1, max_num_boxes, axis=1
         )  # (b, max_num_boxes, num_anchors)
-        max_overlaps_idx = tf.argmax(overlaps, axis=1)  # (b, num_anchors)
-        is_max_overlaps = tf.one_hot(
+        max_overlaps_idx = ops.argmax(overlaps, axis=1)  # (b, num_anchors)
+        is_max_overlaps = ops.one_hot(
             max_overlaps_idx,
-            tf.cast(max_num_boxes, dtype=tf.int32),  # tf.one_hot must use int32
+            max_num_boxes,  # tf.one_hot must use int32
         )  # (b, num_anchors, max_num_boxes)
-        is_max_overlaps = tf.cast(
-            tf.transpose(is_max_overlaps, perm=(0, 2, 1)), overlaps.dtype
+        is_max_overlaps = ops.cast(
+            ops.transpose(is_max_overlaps, axes=(0, 2, 1)), overlaps.dtype
         )  # (b, max_num_boxes, num_anchors)
-        mask_pos = tf.where(
+        mask_pos = ops.where(
             mask_multi_gts, is_max_overlaps, mask_pos
         )  # (b, max_num_boxes, num_anchors)
-        fg_mask = tf.reduce_sum(mask_pos, axis=-2)
+        fg_mask = ops.sum(mask_pos, axis=-2)
         return fg_mask, mask_pos
 
-    fg_mask, mask_pos = tf.cond(
-        tf.reduce_max(fg_mask) > 1,
+    fg_mask, mask_pos = ops.cond(
+        ops.max(fg_mask) > 1,
         lambda: handle_anchor_with_two_gt_boxes(
             fg_mask, mask_pos, overlaps, max_num_boxes
         ),
         lambda: (fg_mask, mask_pos),
     )
 
-    target_gt_idx = tf.argmax(mask_pos, axis=-2)  # (b, num_anchors)
+    target_gt_idx = ops.argmax(mask_pos, axis=-2)  # (b, num_anchors)
     return target_gt_idx, fg_mask, mask_pos
 
 
 def select_candidates_in_gts(xy_centers, gt_bboxes, epsilon=1e-9):
     """Selects candidate anchors for GT boxes.
 
     Returns:
         a boolean mask Tensor of shape (batch_size, num_gt_boxes, num_anchors)
         where the value is `True` if the anchor point falls inside the gt box,
         and `False` otherwise.
     """
     n_anchors = xy_centers.shape[0]
-    n_boxes = tf.shape(gt_bboxes)[1]
+    n_boxes = ops.shape(gt_bboxes)[1]
 
-    left_top, right_bottom = tf.split(
-        tf.reshape(gt_bboxes, (-1, 1, 4)), 2, axis=-1
+    left_top, right_bottom = ops.split(
+        ops.reshape(gt_bboxes, (-1, 1, 4)), 2, axis=-1
     )
-    bbox_deltas = tf.reshape(
-        tf.concat(
+    bbox_deltas = ops.reshape(
+        ops.concatenate(
             [
-                xy_centers[tf.newaxis] - left_top,
-                right_bottom - xy_centers[tf.newaxis],
+                xy_centers[None] - left_top,
+                right_bottom - xy_centers[None],
             ],
             axis=2,
         ),
         (-1, n_boxes, n_anchors, 4),
     )
 
-    return tf.reduce_min(bbox_deltas, axis=-1) > epsilon
+    return ops.min(bbox_deltas, axis=-1, initial=float("inf")) > epsilon
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
-class YOLOV8LabelEncoder(layers.Layer):
+class YOLOV8LabelEncoder(keras.layers.Layer):
     """
     Encodes ground truth boxes to target boxes and class labels for training a
     YOLOV8 model. This is an implementation of the Task-aligned sample
     assignment scheme proposed in https://arxiv.org/abs/2108.07755.
 
     Args:
         num_classes: integer, the number of classes in the training dataset
@@ -170,15 +170,15 @@
             )
             gt_bboxes = dense_bounding_boxes["boxes"]
             gt_labels = dense_bounding_boxes["classes"]
 
         if isinstance(mask_gt, tf.RaggedTensor):
             mask_gt = mask_gt.to_tensor()
 
-        max_num_boxes = tf.cast(tf.shape(gt_bboxes)[1], dtype=tf.int64)
+        max_num_boxes = ops.shape(gt_bboxes)[1]
 
         def encode_to_targets(
             pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt
         ):
             mask_pos, align_metric, overlaps = self.get_pos_mask(
                 pd_scores,
                 pd_bboxes,
@@ -194,48 +194,48 @@
             )
 
             target_bboxes, target_scores = self.get_targets(
                 gt_labels, gt_bboxes, target_gt_idx, fg_mask, max_num_boxes
             )
 
             align_metric *= mask_pos
-            pos_align_metrics = tf.reduce_max(
+            pos_align_metrics = ops.max(
                 align_metric, axis=-1, keepdims=True
             )  # b, max_num_boxes
-            pos_overlaps = tf.reduce_max(
+            pos_overlaps = ops.max(
                 overlaps * mask_pos, axis=-1, keepdims=True
             )  # b, max_num_boxes
-            norm_align_metric = tf.expand_dims(
-                tf.reduce_max(
+            norm_align_metric = ops.expand_dims(
+                ops.max(
                     align_metric
                     * pos_overlaps
                     / (pos_align_metrics + self.epsilon),
                     axis=-2,
                 ),
                 axis=-1,
             )
             target_scores = target_scores * norm_align_metric
 
             # No need to compute gradients for these, as they're all targets
             return (
-                tf.stop_gradient(target_bboxes),
-                tf.stop_gradient(target_scores),
-                tf.stop_gradient(tf.cast(fg_mask, tf.bool)),
+                ops.stop_gradient(target_bboxes),
+                ops.stop_gradient(target_scores),
+                ops.stop_gradient(fg_mask),
             )
 
         # return zeros if no gt boxes are present
-        return tf.cond(
+        return ops.cond(
             max_num_boxes > 0,
             lambda: encode_to_targets(
                 pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt
             ),
             lambda: (
-                tf.zeros_like(pd_bboxes),
-                tf.zeros_like(pd_scores),
-                tf.zeros_like(pd_scores[..., 0], dtype=tf.bool),
+                ops.zeros_like(pd_bboxes),
+                ops.zeros_like(pd_scores),
+                ops.zeros_like(pd_scores[..., 0]),
             ),
         )
 
     def get_pos_mask(
         self,
         pd_scores,
         pd_bboxes,
@@ -263,29 +263,29 @@
         mask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes)
 
         align_metric, overlaps = self.get_box_metrics(
             pd_scores,
             pd_bboxes,
             gt_labels,
             gt_bboxes,
-            tf.cast(mask_in_gts, tf.int32) * tf.cast(mask_gt, tf.int32),
+            ops.cast(mask_in_gts, "int32") * ops.cast(mask_gt, "int32"),
             max_num_boxes,
         )
         # get topk_metric mask, (b, max_num_boxes, num_anchors)
         mask_topk = self.select_topk_candidates(
             align_metric,
-            topk_mask=tf.cast(
-                tf.repeat(mask_gt, self.max_anchor_matches, axis=2), tf.bool
+            topk_mask=ops.cast(
+                ops.repeat(mask_gt, self.max_anchor_matches, axis=2), "bool"
             ),
         )
         # merge all masks to a final mask, (b, max_num_boxes, num_anchors)
         mask_pos = (
             mask_topk
-            * tf.cast(mask_in_gts, tf.float32)
-            * tf.cast(mask_gt, tf.float32)
+            * ops.cast(mask_in_gts, "float32")
+            * ops.cast(mask_gt, "float32")
         )
 
         return mask_pos, align_metric, overlaps
 
     def get_box_metrics(
         self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt, max_num_boxes
     ):
@@ -297,41 +297,46 @@
                     num_anchors) representing the alignment metrics for each
                     ground truth box, anchor pair.
                 - A Float Tensor of shape (batch_size, num_gt_boxes,
                     num_anchors) representing the IoUs between each ground truth
                     box and the predicted box at each anchor.
         """
         na = pd_bboxes.shape[-2]
-        mask_gt = tf.cast(mask_gt, tf.bool)  # b, max_num_boxes, num_anchors
+        mask_gt = ops.cast(mask_gt, "bool")  # b, max_num_boxes, num_anchors
 
-        ind_1 = tf.cast(gt_labels, tf.int64)
-        pd_scores = tf.gather(
-            pd_scores, tf.math.maximum(ind_1, 0), axis=-1, batch_dims=1
+        ind_1 = ops.cast(gt_labels, "int64")
+        pd_scores = ops.squeeze(
+            ops.take_along_axis(
+                pd_scores[:, None, :, :],
+                ops.cast(ops.maximum(ind_1[:, None, None, :], 0), "int32"),
+                axis=-1,
+            ),
+            axis=1,
         )
-        pd_scores = tf.where(ind_1[:, tf.newaxis, :] >= 0, pd_scores, 0.0)
-        pd_scores = tf.transpose(pd_scores, perm=(0, 2, 1))
+        pd_scores = ops.where(ind_1[:, None, :] >= 0, pd_scores, 0.0)
+        pd_scores = ops.transpose(pd_scores, axes=(0, 2, 1))
 
-        bbox_scores = tf.where(mask_gt, pd_scores, 0.0)
+        bbox_scores = ops.where(mask_gt, pd_scores, 0.0)
 
-        pd_boxes = tf.repeat(
-            tf.expand_dims(pd_bboxes, axis=1), max_num_boxes, axis=1
+        pd_boxes = ops.repeat(
+            ops.expand_dims(pd_bboxes, axis=1), max_num_boxes, axis=1
         )
 
-        gt_boxes = tf.repeat(tf.expand_dims(gt_bboxes, axis=2), na, axis=2)
+        gt_boxes = ops.repeat(ops.expand_dims(gt_bboxes, axis=2), na, axis=2)
 
-        iou = tf.squeeze(
+        iou = ops.squeeze(
             compute_ciou(gt_boxes, pd_boxes, bounding_box_format="xyxy"),
             axis=-1,
         )
-        iou = tf.where(iou > 0, iou, 0.0)
+        iou = ops.where(iou > 0, iou, 0.0)
 
-        iou = tf.reshape(iou, (-1, max_num_boxes, na))
-        overlaps = tf.where(mask_gt, iou, 0.0)
+        iou = ops.reshape(iou, (-1, max_num_boxes, na))
+        overlaps = ops.where(mask_gt, iou, 0.0)
 
-        align_metric = tf.math.pow(bbox_scores, self.alpha) * tf.math.pow(
+        align_metric = ops.power(bbox_scores, self.alpha) * ops.power(
             overlaps, self.beta
         )
         return align_metric, overlaps
 
     def select_topk_candidates(self, metrics, topk_mask):
         """Selects the anchors with the top-k alignment metrics for each gt box.
 
@@ -339,75 +344,75 @@
             A Boolean Tensor of shape (batch_size, num_gt_boxes, num_anchors)
             representing whether each anchor is among the top-k anchors for a
             given gt box based on the alignment metric.
         """
 
         num_anchors = metrics.shape[-1]  # num_anchors
         # (b, max_num_boxes, topk)
-        topk_metrics, topk_idxs = tf.math.top_k(
-            metrics, self.max_anchor_matches
-        )
-        topk_mask = tf.tile(
-            tf.reduce_max(topk_metrics, axis=-1, keepdims=True) > self.epsilon,
+        topk_metrics, topk_idxs = ops.top_k(metrics, self.max_anchor_matches)
+        topk_mask = ops.tile(
+            ops.max(topk_metrics, axis=-1, keepdims=True) > self.epsilon,
             [1, 1, self.max_anchor_matches],
         )
 
         # (b, max_num_boxes, topk)
-        topk_idxs = tf.where(topk_mask, topk_idxs, 0)
-        is_in_topk = tf.zeros_like(metrics, dtype=tf.int64)
+        topk_idxs = ops.where(topk_mask, topk_idxs, 0)
+        is_in_topk = ops.zeros_like(metrics, dtype="int64")
 
         for it in range(self.max_anchor_matches):
-            is_in_topk += tf.one_hot(
-                topk_idxs[:, :, it], num_anchors, dtype=tf.int64
+            is_in_topk += ops.one_hot(
+                topk_idxs[:, :, it], num_anchors, dtype="int64"
             )
 
         # filter invalid bboxes
-        is_in_topk = tf.where(
-            is_in_topk > 1, tf.constant(0, tf.int64), is_in_topk
+        is_in_topk = ops.where(
+            is_in_topk > 1, ops.array(0, "int64"), is_in_topk
         )
-        return tf.cast(is_in_topk, metrics.dtype)
+        return ops.cast(is_in_topk, metrics.dtype)
 
     def get_targets(
         self, gt_labels, gt_bboxes, target_gt_idx, fg_mask, max_num_boxes
     ):
         """Computes target boxes and labels.
 
         Returns:
             A tuple of the following:
                 - A Float Tensor of shape (batch_size, num_anchors, 4)
                     representing target boxes each anchor..
                 - A Float Tensor of shape (batch_size, num_anchors, num_classes)
                     representing target classes for each anchor.
         """
 
-        batch_ind = tf.range(tf.shape(gt_labels)[0], dtype=tf.int64)[
-            ..., tf.newaxis
+        batch_ind = ops.arange(ops.shape(gt_labels)[0], dtype="int64")[
+            ..., None
         ]
-        target_gt_idx = target_gt_idx + batch_ind * max_num_boxes
+        target_gt_idx = target_gt_idx + batch_ind * ops.cast(
+            max_num_boxes, "int64"
+        )
 
-        gt_bboxes = tf.reshape(gt_bboxes, (-1, tf.shape(gt_bboxes)[1], 4))
+        gt_bboxes = keras.layers.Reshape((-1, 4))(gt_bboxes)
 
-        target_labels = tf.gather(
-            tf.reshape(tf.cast(gt_labels, tf.int64), (-1,)), target_gt_idx
+        target_labels = ops.take(
+            ops.reshape(ops.cast(gt_labels, "int64"), (-1,)), target_gt_idx
         )  # (b, num_anchors)
 
         # assigned target boxes, (b, max_num_boxes, 4) -> (b, num_anchors)
-        target_bboxes = tf.gather(
-            tf.reshape(gt_bboxes, (-1, 4)), target_gt_idx, axis=-2
+        target_bboxes = ops.take(
+            ops.reshape(gt_bboxes, (-1, 4)), target_gt_idx, axis=-2
         )
 
         # assigned target scores
-        target_labels = tf.math.maximum(target_labels, 0)
-        target_scores = tf.one_hot(
+        target_labels = ops.maximum(target_labels, 0)
+        target_scores = ops.one_hot(
             target_labels, self.num_classes
         )  # (b, num_anchors, num_classes)
-        fg_scores_mask = tf.repeat(
-            fg_mask[:, :, tf.newaxis], self.num_classes, axis=2
+        fg_scores_mask = ops.repeat(
+            fg_mask[:, :, None], self.num_classes, axis=2
         )  # (b, num_anchors, num_classes)
-        target_scores = tf.where(fg_scores_mask > 0, target_scores, 0)
+        target_scores = ops.where(fg_scores_mask > 0, target_scores, 0)
 
         return target_bboxes, target_scores
 
     def count_params(self):
         # The label encoder has no weights, so we short-circuit the weight
         # counting to avoid having to `build` this layer unnecessarily.
         return 0
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py

```diff
@@ -7,16 +7,16 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
-from keras import layers
+from keras_cv.backend import keras
+from keras_cv.backend import ops
 
 BATCH_NORM_EPSILON = 1e-3
 BATCH_NORM_MOMENTUM = 0.97
 
 
 # TODO(ianstenbit): Remove this method once we're using CSPDarkNet backbone
 # (Calls to it should be inlined in the detector head)
@@ -25,32 +25,32 @@
     output_channel,
     kernel_size=1,
     strides=1,
     activation="swish",
     name="conv_bn",
 ):
     if kernel_size > 1:
-        inputs = layers.ZeroPadding2D(
+        inputs = keras.layers.ZeroPadding2D(
             padding=kernel_size // 2, name=f"{name}_pad"
         )(inputs)
 
-    x = layers.Conv2D(
+    x = keras.layers.Conv2D(
         filters=output_channel,
         kernel_size=kernel_size,
         strides=strides,
         padding="valid",
         use_bias=False,
         name=f"{name}_conv",
     )(inputs)
-    x = layers.BatchNormalization(
+    x = keras.layers.BatchNormalization(
         momentum=BATCH_NORM_MOMENTUM,
         epsilon=BATCH_NORM_EPSILON,
         name=f"{name}_bn",
     )(x)
-    x = layers.Activation(activation, name=name)(x)
+    x = keras.layers.Activation(activation, name=name)(x)
     return x
 
 
 # TODO(ianstenbit): Remove this method once we're using CSPDarkNet backbone
 # Calls to it should instead call the CSP block from the DarkNet implementation.
 def apply_csp_block(
     inputs,
@@ -68,15 +68,15 @@
     pre = apply_conv_bn(
         inputs,
         hidden_channels * 2,
         kernel_size=1,
         activation=activation,
         name=f"{name}_pre",
     )
-    short, deep = tf.split(pre, 2, axis=channel_axis)
+    short, deep = ops.split(pre, 2, axis=channel_axis)
 
     out = [short, deep]
     for id in range(depth):
         deep = apply_conv_bn(
             deep,
             hidden_channels,
             kernel_size=3,
@@ -88,15 +88,15 @@
             hidden_channels,
             kernel_size=3,
             activation=activation,
             name=f"{name}_pre_{id}_2",
         )
         deep = (out[-1] + deep) if shortcut else deep
         out.append(deep)
-    out = tf.concat(out, axis=channel_axis)
+    out = ops.concatenate(out, axis=channel_axis)
     out = apply_conv_bn(
         out,
         channels,
         kernel_size=1,
         activation=activation,
         name=f"{name}_output",
     )
```

## keras_cv/utils/preprocessing.py

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import backend
 
 from keras_cv import core
+from keras_cv.backend import ops
 
 _TF_INTERPOLATION_METHODS = {
     "bilinear": tf.image.ResizeMethod.BILINEAR,
     "nearest": tf.image.ResizeMethod.NEAREST_NEIGHBOR,
     "bicubic": tf.image.ResizeMethod.BICUBIC,
     "area": tf.image.ResizeMethod.AREA,
     "lanczos3": tf.image.ResizeMethod.LANCZOS3,
@@ -372,18 +373,18 @@
             fill_mode=fill_mode.upper(),
             interpolation=interpolation.upper(),
         )
 
 
 def ensure_tensor(inputs, dtype=None):
     """Ensures the input is a Tensor, SparseTensor or RaggedTensor."""
-    if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):
-        inputs = tf.convert_to_tensor(inputs, dtype)
+    if not ops.is_tensor(inputs):
+        inputs = ops.convert_to_tensor(inputs, dtype)
     if dtype is not None and inputs.dtype != dtype:
-        inputs = tf.cast(inputs, dtype)
+        inputs = ops.cast(inputs, dtype)
     return inputs
 
 
 def check_fill_mode_and_interpolation(fill_mode, interpolation):
     if fill_mode not in {"reflect", "wrap", "constant", "nearest"}:
         raise NotImplementedError(
             " Want fillmode  to be one of `reflect`, `wrap`, "
```

## keras_cv/utils/resource_loader.py

```diff
@@ -18,15 +18,15 @@
 from __future__ import print_function
 
 import os
 import warnings
 
 import tensorflow as tf
 
-TF_VERSION_FOR_ABI_COMPATIBILITY = "2.11"
+TF_VERSION_FOR_ABI_COMPATIBILITY = "2.13"
 abi_warning_already_raised = False
 
 
 def get_project_root():
     """Returns project root folder."""
     return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
```

## keras_cv/utils/target_gather.py

```diff
@@ -8,20 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+from keras_cv.backend import ops
 
 
 def _target_gather(
-    targets: tf.Tensor,
-    indices: tf.Tensor,
+    targets,
+    indices,
     mask=None,
     mask_val=0.0,
 ):
     """A utility function wrapping tf.gather, which deals with:
      1) both batched and unbatched `targets`
      2) when unbatched `targets` have empty rows, the result will be filled
         with `mask_val`
@@ -42,84 +42,78 @@
     Returns:
      targets: [M, ...] or [batch_size, M, ...] Tensor representing
        selected targets.
 
      Raise:
        ValueError: If `targets` is higher than rank 3.
     """
-    targets_shape = targets.get_shape().as_list()
+    targets_shape = list(targets.shape)
     if len(targets_shape) > 3:
         raise ValueError(
             "`target_gather` does not support `targets` with rank "
             "larger than 3, got {}".format(len(targets.shape))
         )
 
     def _gather_unbatched(labels, match_indices, mask, mask_val):
         """Gather based on unbatched labels and boxes."""
-        num_gt_boxes = tf.shape(labels)[0]
+        num_gt_boxes = labels.shape[0]
 
         def _assign_when_rows_empty():
             if len(labels.shape) > 1:
                 mask_shape = [match_indices.shape[0], labels.shape[-1]]
             else:
                 mask_shape = [match_indices.shape[0]]
-            return tf.cast(mask_val, labels.dtype) * tf.ones(
+            return ops.cast(mask_val, labels.dtype) * ops.ones(
                 mask_shape, dtype=labels.dtype
             )
 
         def _assign_when_rows_not_empty():
-            targets = tf.gather(labels, match_indices)
+            targets = ops.take(labels, match_indices, axis=0)
             if mask is None:
                 return targets
             else:
-                masked_targets = tf.cast(mask_val, labels.dtype) * tf.ones_like(
-                    mask, dtype=labels.dtype
-                )
-                return tf.where(mask, masked_targets, targets)
+                masked_targets = ops.cast(
+                    mask_val, labels.dtype
+                ) * ops.ones_like(mask, dtype=labels.dtype)
+                return ops.where(mask, masked_targets, targets)
 
-        return tf.cond(
-            tf.greater(num_gt_boxes, 0),
-            _assign_when_rows_not_empty,
-            _assign_when_rows_empty,
-        )
+        if num_gt_boxes > 0:
+            return _assign_when_rows_not_empty()
+        else:
+            return _assign_when_rows_empty()
 
     def _gather_batched(labels, match_indices, mask, mask_val):
         """Gather based on batched labels."""
         batch_size = labels.shape[0]
         if batch_size == 1:
             if mask is not None:
                 result = _gather_unbatched(
-                    tf.squeeze(labels, axis=0),
-                    tf.squeeze(match_indices, axis=0),
-                    tf.squeeze(mask, axis=0),
+                    ops.squeeze(labels, axis=0),
+                    ops.squeeze(match_indices, axis=0),
+                    ops.squeeze(mask, axis=0),
                     mask_val,
                 )
             else:
                 result = _gather_unbatched(
-                    tf.squeeze(labels, axis=0),
-                    tf.squeeze(match_indices, axis=0),
+                    ops.squeeze(labels, axis=0),
+                    ops.squeeze(match_indices, axis=0),
                     None,
                     mask_val,
                 )
-            return tf.expand_dims(result, axis=0)
+            return ops.expand_dims(result, axis=0)
         else:
-            indices_shape = tf.shape(match_indices)
-            indices_dtype = match_indices.dtype
-            batch_indices = tf.expand_dims(
-                tf.range(indices_shape[0], dtype=indices_dtype), axis=-1
-            ) * tf.ones([1, indices_shape[-1]], dtype=indices_dtype)
-            gather_nd_indices = tf.stack(
-                [batch_indices, match_indices], axis=-1
+            targets = ops.take_along_axis(
+                labels, ops.expand_dims(match_indices, axis=-1), axis=1
             )
-            targets = tf.gather_nd(labels, gather_nd_indices)
+
             if mask is None:
                 return targets
             else:
-                masked_targets = tf.cast(mask_val, labels.dtype) * tf.ones_like(
-                    mask, dtype=labels.dtype
-                )
-                return tf.where(mask, masked_targets, targets)
+                masked_targets = ops.cast(
+                    mask_val, labels.dtype
+                ) * ops.ones_like(mask, dtype=labels.dtype)
+                return ops.where(mask, masked_targets, targets)
 
     if len(targets_shape) <= 2:
         return _gather_unbatched(targets, indices, mask, mask_val)
     elif len(targets_shape) == 3:
         return _gather_batched(targets, indices, mask, mask_val)
```

## keras_cv/utils/target_gather_test.py

```diff
@@ -8,110 +8,113 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
+from keras_cv.backend import ops
 from keras_cv.utils.target_gather import _target_gather
 
 
 class TargetGatherTest(tf.test.TestCase):
     def test_target_gather_boxes_batched(self):
-        target_boxes = tf.constant(
+        target_boxes = np.array(
             [[0, 0, 5, 5], [0, 5, 5, 10], [5, 0, 10, 5], [5, 5, 10, 10]]
         )
-        target_boxes = target_boxes[tf.newaxis, ...]
-        indices = tf.constant([[0, 2]], dtype=tf.int32)
-        expected_boxes = tf.constant([[0, 0, 5, 5], [5, 0, 10, 5]])
-        expected_boxes = expected_boxes[tf.newaxis, ...]
+        target_boxes = ops.expand_dims(target_boxes, axis=0)
+        indices = np.array([[0, 2]], dtype="int32")
+        expected_boxes = np.array([[0, 0, 5, 5], [5, 0, 10, 5]])
+        expected_boxes = ops.expand_dims(expected_boxes, axis=0)
         res = _target_gather(target_boxes, indices)
         self.assertAllClose(expected_boxes, res)
 
     def test_target_gather_boxes_unbatched(self):
-        target_boxes = tf.constant(
-            [[0, 0, 5, 5], [0, 5, 5, 10], [5, 0, 10, 5], [5, 5, 10, 10]]
+        target_boxes = np.array(
+            [[0, 0, 5, 5], [0, 5, 5, 10], [5, 0, 10, 5], [5, 5, 10, 10]],
+            "int32",
         )
-        indices = tf.constant([0, 2], dtype=tf.int32)
-        expected_boxes = tf.constant([[0, 0, 5, 5], [5, 0, 10, 5]])
+        indices = np.array([0, 2], dtype="int32")
+        expected_boxes = np.array([[0, 0, 5, 5], [5, 0, 10, 5]])
         res = _target_gather(target_boxes, indices)
         self.assertAllClose(expected_boxes, res)
 
     def test_target_gather_classes_batched(self):
-        target_classes = tf.constant([[1, 2, 3, 4]])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([[0, 2]], dtype=tf.int32)
-        expected_classes = tf.constant([[1, 3]])
-        expected_classes = expected_classes[..., tf.newaxis]
+        target_classes = np.array([[1, 2, 3, 4]])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([[0, 2]], dtype="int32")
+        expected_classes = np.array([[1, 3]])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_classes_unbatched(self):
-        target_classes = tf.constant([1, 2, 3, 4])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([0, 2], dtype=tf.int32)
-        expected_classes = tf.constant([1, 3])
-        expected_classes = expected_classes[..., tf.newaxis]
+        target_classes = np.array([1, 2, 3, 4])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([0, 2], dtype="int32")
+        expected_classes = np.array([1, 3])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_classes_batched_with_mask(self):
-        target_classes = tf.constant([[1, 2, 3, 4]])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([[0, 2]], dtype=tf.int32)
-        masks = tf.constant(([[False, True]]))
-        masks = masks[..., tf.newaxis]
+        target_classes = np.array([[1, 2, 3, 4]])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([[0, 2]], dtype="int32")
+        masks = np.array(([[False, True]]))
+        masks = ops.expand_dims(masks, axis=-1)
         # the second element is masked
-        expected_classes = tf.constant([[1, 0]])
-        expected_classes = expected_classes[..., tf.newaxis]
+        expected_classes = np.array([[1, 0]])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices, masks)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_classes_batched_with_mask_val(self):
-        target_classes = tf.constant([[1, 2, 3, 4]])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([[0, 2]], dtype=tf.int32)
-        masks = tf.constant(([[False, True]]))
-        masks = masks[..., tf.newaxis]
+        target_classes = np.array([[1, 2, 3, 4]])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([[0, 2]], dtype="int32")
+        masks = np.array(([[False, True]]))
+        masks = ops.expand_dims(masks, axis=-1)
         # the second element is masked
-        expected_classes = tf.constant([[1, -1]])
-        expected_classes = expected_classes[..., tf.newaxis]
+        expected_classes = np.array([[1, -1]])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices, masks, -1)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_classes_unbatched_with_mask(self):
-        target_classes = tf.constant([1, 2, 3, 4])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([0, 2], dtype=tf.int32)
-        masks = tf.constant([False, True])
-        masks = masks[..., tf.newaxis]
-        expected_classes = tf.constant([1, 0])
-        expected_classes = expected_classes[..., tf.newaxis]
+        target_classes = np.array([1, 2, 3, 4])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([0, 2], dtype="int32")
+        masks = np.array([False, True])
+        masks = ops.expand_dims(masks, axis=-1)
+        expected_classes = np.array([1, 0])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices, masks)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_with_empty_targets(self):
-        target_classes = tf.constant([])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([0, 2], dtype=tf.int32)
+        target_classes = np.array([])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([0, 2], dtype="int32")
         # return all 0s since input is empty
-        expected_classes = tf.constant([0, 0])
-        expected_classes = expected_classes[..., tf.newaxis]
+        expected_classes = np.array([0, 0])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_classes_multi_batch(self):
-        target_classes = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])
-        target_classes = target_classes[..., tf.newaxis]
-        indices = tf.constant([[0, 2], [1, 3]], dtype=tf.int32)
-        expected_classes = tf.constant([[1, 3], [6, 8]])
-        expected_classes = expected_classes[..., tf.newaxis]
+        target_classes = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])
+        target_classes = ops.expand_dims(target_classes, axis=-1)
+        indices = np.array([[0, 2], [1, 3]], dtype="int32")
+        expected_classes = np.array([[1, 3], [6, 8]])
+        expected_classes = ops.expand_dims(expected_classes, axis=-1)
         res = _target_gather(target_classes, indices)
         self.assertAllClose(expected_classes, res)
 
     def test_target_gather_invalid_rank(self):
-        targets = tf.random.normal([32, 2, 2, 2])
-        indices = tf.constant([0, 1], dtype=tf.int32)
+        targets = np.random.normal(size=[32, 2, 2, 2])
+        indices = np.array([0, 1], dtype="int32")
         with self.assertRaisesRegex(ValueError, "larger than 3"):
             _ = _target_gather(targets, indices)
```

## keras_cv/utils/to_numpy.py

```diff
@@ -10,19 +10,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import numpy as np
 import tensorflow as tf
 
+from keras_cv.backend import ops
+
 
 def to_numpy(x):
     if x is None:
         return None
     if isinstance(x, tf.RaggedTensor):
         x = x.to_tensor(-1)
-    if isinstance(x, tf.Tensor):
-        x = x.numpy()
-    if not isinstance(x, (np.ndarray, np.generic)):
-        x = np.array(x)
+    x = ops.convert_to_numpy(x)
     # Important for consistency when working with visualization utilities
     return np.ascontiguousarray(x)
```

## keras_cv/utils/train.py

```diff
@@ -9,15 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
+
+from keras_cv.backend import keras
 
 
 def scale_loss_for_distribution(loss_value):
     """Scales and returns the given loss value by the number of replicas."""
     num_replicas = tf.distribute.get_strategy().num_replicas_in_sync
     if num_replicas > 1:
         loss_value *= 1.0 / num_replicas
```

## Comparing `keras_cv-0.5.1.dist-info/LICENSE` & `keras_cv-0.6.0.dev0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `keras_cv-0.5.1.dist-info/METADATA` & `keras_cv-0.6.0.dev0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: keras-cv
-Version: 0.5.1
+Version: 0.6.0.dev0
 Summary: Industry-strength computer Vision extensions for Keras.
 Home-page: https://github.com/keras-team/keras-cv
 Author: Keras team
 Author-email: keras-cv@google.com
 License: Apache License 2.0
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3.7
```

## Comparing `keras_cv-0.5.1.dist-info/RECORD` & `keras_cv-0.6.0.dev0.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,39 @@
-keras_cv/__init__.py,sha256=6wteSYLAzYLXim8lP7-B-HWtDEcF3h52iHt7IbpxP2M,1182
-keras_cv/conftest.py,sha256=1GK8v29KpFjexH0wwxWEKCANkQAlrOTER6Jw_E9z6DM,2354
-keras_cv/version_check.py,sha256=LqMks3U07vEn9lmEZOoBvTgRl7_Nee-6rt_bkUP8VKM,1159
-keras_cv/version_check_test.py,sha256=rV0HuMJEAQOoE8embKlWKMLFMB8dY4WbYBxFdJ-jbjU,1362
+keras_cv/__init__.py,sha256=QYXLjySJnD2zo1YJUPSbroGzE_ZLwAw_A6-wlJ9Bf3E,1373
+keras_cv/conftest.py,sha256=ZBhXDj4kYqaw2OSB0HjgFdikezUQAAhRR8Mo5MW-kOE,2740
+keras_cv/version_check.py,sha256=wqYkE-q_MlUaJx8pwQ-cv5Vf-TiLgmWUAwo1xofOREk,1159
+keras_cv/version_check_test.py,sha256=E3dTosZelrwu1S9dGSClww-pfhEwYFQUACYxGMTL_x4,1363
+keras_cv/backend/__init__.py,sha256=kv8ethdK1T6gvUBiCCYCjm3KXmoiDAzcHBw5X-LjFhs,2921
+keras_cv/backend/config.py,sha256=oLe8GDv197YrAhpeWEqGt8YVsrnRZbKFp7aYd6GZhHA,2206
+keras_cv/backend/ops.py,sha256=n-Q2g0GKMKKKvq-el6vxQ4dDqFTthJRQsW8iGMNQzWQ,953
+keras_cv/backend/scope.py,sha256=dVX5IYlleSP4P-6vh0pGfIcTNL9dBjb_5zU3ZWR6idY,1987
+keras_cv/backend/tf_ops.py,sha256=lXSGBVOIahHtS8n65M2G4cK_4woknOyOVbKPjR1URpM,1595
 keras_cv/bounding_box/__init__.py,sha256=coBAs1UK0Mcj7FgEEFtse6TQWNZ8kQSys2axUoTq_34,1662
-keras_cv/bounding_box/converters.py,sha256=zdOwi1MtCv8E6WbXgnBRghxLDvJWCCrFZXrhSMFcCdI,18483
-keras_cv/bounding_box/converters_test.py,sha256=vhV7ip5QMgsL27zu5GWsSvRgv6z1jlRV0FyZ9mYD4Oo,7134
+keras_cv/bounding_box/converters.py,sha256=MyjUdDMxG3WP5ruxul4gW_VY5d56uPI1vDrEJg_oRvI,18316
+keras_cv/bounding_box/converters_test.py,sha256=30SanH5mWK8KamMJ1ZEsn90VIlmggbjnceDLAbRvWF0,7244
 keras_cv/bounding_box/ensure_tensor.py,sha256=6d7lbfv4-PKeY4yfpvphMClNdw2g49WohBGekzXXoHk,909
-keras_cv/bounding_box/ensure_tensor_test.py,sha256=y_8eepWMJ5Y2QRHOD8J3FDDT9hZoMTlDf_5Znehumvo,1443
+keras_cv/bounding_box/ensure_tensor_test.py,sha256=Iy8eIIijtVdyP4Wass4TewkVgDxe5wSJNxkcDxtmCUQ,1356
 keras_cv/bounding_box/formats.py,sha256=ClUExYPKQ9pEnPMhgmc48sC0PDpPnqHkzPq_GqlNuWs,4035
-keras_cv/bounding_box/iou.py,sha256=PCrhI5Lr5cBBW4xEs0WawVre27sYgEYq68UVLxYiIrE,9144
-keras_cv/bounding_box/iou_test.py,sha256=H4_6OzNVEQ-pbRestVmGds85R8mFZOS1Z9S0wj7Krsc,6130
-keras_cv/bounding_box/mask_invalid_detections.py,sha256=-kCZNPvMCWhCUSxF2GGSLxOfgzwDxxqCeLffgiWhGwE,3751
-keras_cv/bounding_box/mask_invalid_detections_test.py,sha256=tMexLag8SQ5BaWY8Ti8lJJYrnay0ovkTun0xc2dA06U,3901
-keras_cv/bounding_box/to_dense.py,sha256=GrDNFiDdJGIzJUIMOS0zjZFkfKf1J_XUjfwgxqo-PBE,3204
-keras_cv/bounding_box/to_dense_test.py,sha256=wjUtUn8AhyjRAHHpBLt3wFmwpXvy2CFc6qhL_orOXr4,1147
-keras_cv/bounding_box/to_ragged.py,sha256=w0idLsXjuySRvxtIdjuonfn1fAvyeSAGZJ0yZNcGieA,3014
-keras_cv/bounding_box/to_ragged_test.py,sha256=w5P8uupWyEcdGt8TG-OeTCc_UhJQPMHrlc-rKWabVxs,2713
-keras_cv/bounding_box/utils.py,sha256=JXbLVd1Qhqu6uJ-Pj9Ffx65q68yZi26M40T2gzAVSDM,7178
-keras_cv/bounding_box/utils_test.py,sha256=oqOp4QULsLwwO0F4bpZmc1QqIhi3N8qZgRWpwlfJqCI,5569
+keras_cv/bounding_box/iou.py,sha256=8FL9OxAUPddAJWn34Ba_DAkVZ0VV5Fh3cxIWufriBv0,9063
+keras_cv/bounding_box/iou_test.py,sha256=7kIZfaiqKjOcx_xScP0OaE5pYDsklgztH7lZ-aw9bXs,5692
+keras_cv/bounding_box/mask_invalid_detections.py,sha256=HSpZgormQ0GOYSXwVZgd9ypkZvogugTUbguJHRdWTKo,3728
+keras_cv/bounding_box/mask_invalid_detections_test.py,sha256=CdZo4aFtDja0bY2TmEF66_EzEzZcs64x3Ulpnqb9qg8,3265
+keras_cv/bounding_box/to_dense.py,sha256=p5Vefudy-vgZzYhzXcQB4lyHfdgvTJ4sBe4GDHlXvIA,3256
+keras_cv/bounding_box/to_dense_test.py,sha256=3jcgRCxAsbc2tGPJ2tlWcOMtW-_dN8JQKDWA3Qk6Tds,1192
+keras_cv/bounding_box/to_ragged.py,sha256=ylonVXk1EAj-z2n9lC9B5-kJXWzeJbwfUMXypdQztg0,3347
+keras_cv/bounding_box/to_ragged_test.py,sha256=05xPukPJD1Xeqor4kHZDEj2duqykH9NP0iFy_MJYkYU,3422
+keras_cv/bounding_box/utils.py,sha256=VV37v8y-t3ZAmhPukRUt9El4lASJlVadFhfpLbRNYSY,6947
+keras_cv/bounding_box/utils_test.py,sha256=e-XyavX7UKOQHNroye92UTaRtrPOMQmYdxVyRsL1vSM,5326
 keras_cv/bounding_box/validate_format.py,sha256=IrdFqLw5a-ENw3g1LTO5V-VFY30M1VA7ZKg-vaB0viM,3479
 keras_cv/bounding_box/validate_format_test.py,sha256=ruHJk2_ioqZnymkCBf8xCZPv07GlZ7IxP0B2oh6Q1O8,1581
 keras_cv/bounding_box_3d/__init__.py,sha256=hokR_WW7-_7UBm4ebdib6MjQpuDHoe1TU-fHFfi4ZPE,652
 keras_cv/bounding_box_3d/formats.py,sha256=_VLtI5w8ksveB-eJ4l1Dselw1C0fCF1fA4-myYbt28Y,1609
 keras_cv/callbacks/__init__.py,sha256=GaURZvzP51cgtturjtiGm0Kuov_0NsLWAMDvT7_ECB0,727
-keras_cv/callbacks/pycoco_callback.py,sha256=JWt8UcdesSoNSY9ZvS6NZqTBBvd2pPr1vjh5H9xgxqo,4693
-keras_cv/callbacks/pycoco_callback_test.py,sha256=l_kO7q_zDyUXuF59InM-33WKwztIeBWtaao6XXagKm4,3323
+keras_cv/callbacks/pycoco_callback.py,sha256=V_pcWl_SfJObDMZKlp5pmjqjOeoda8r_fAQEvaAndn4,5322
+keras_cv/callbacks/pycoco_callback_test.py,sha256=g42BeZQTVQEnGoSmPbCfV27TwDefmajAlbG3QNMVvJk,3326
 keras_cv/callbacks/waymo_evaluation_callback.py,sha256=VOfyEoWoyUsgzX47WH49Tn39lKjZCtTVCsK1D9c9awI,6910
 keras_cv/callbacks/waymo_evaluation_callback_test.py,sha256=A9eEyqeETWt_FIwSel43VRGWmGntxNfM7m4FNdtxOrY,3413
 keras_cv/core/__init__.py,sha256=CKYvfvZOXfcuH9XwYxt1XD-aR316P6mfXuYJSkHoAww,936
 keras_cv/core/factor_sampler/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/core/factor_sampler/constant_factor_sampler.py,sha256=yyn__-MmJAqVTJeExFvY6NVkSij0fO--bLVsp1hqvf8,1667
 keras_cv/core/factor_sampler/constant_factor_sampler_test.py,sha256=2BzWrYJXlQZLT6OJwqb52qtQOXHA8srLGvqBcPhu3gI,964
 keras_cv/core/factor_sampler/factor_sampler.py,sha256=PYBNGRE1SxHKZ6AdgP5TrxyAxc0pQmIIVfkfNFDQ1Bo,1343
@@ -52,141 +57,143 @@
 keras_cv/datasets/waymo/transformer_test.py,sha256=tmTtyO_B5ra3RFB6stssf2wTOatOmaceaeK2QcEXTb0,6947
 keras_cv/keypoint/__init__.py,sha256=r64NpD6b0BT0CS_hmpr_60Ah74pXgyYANss1x2bO2r0,783
 keras_cv/keypoint/converters.py,sha256=VKklqsoBwwtEmfwqOHBe0DCZc55orzbC1-xvC6V-YGk,6955
 keras_cv/keypoint/converters_test.py,sha256=qpgRbTtaH5oUe18sj9itOR1_mmU-4jcyXUnUDKMdZjc,5181
 keras_cv/keypoint/formats.py,sha256=b7vVRK9ePdC9lMeLNOlIRg_lnq8anuDsHu6o8Lq8XJU,1725
 keras_cv/keypoint/utils.py,sha256=H5SCMC0WWm8rclaAJr7pIHzitQuljbZ8FxCR4GYfLu8,1597
 keras_cv/keypoint/utils_test.py,sha256=_pcpWKS6zI5rj_vxM8PsaLRy7R2n5JA11hE4aaan25E,2000
-keras_cv/layers/__init__.py,sha256=_IjAl9WZ2BcIKZyQq8Ef81NB0MDlnNfFxz-BR_a9rlg,6103
+keras_cv/layers/__init__.py,sha256=CAOk1y34Am0Aq1HOKAW2SkJZteaOYAX7rJXcQEcSNOw,6195
 keras_cv/layers/feature_pyramid.py,sha256=KcNUCNRTcJwjXVbbvUUN4EY3oPQ4pZZbZ9hEMdnZqo4,8828
 keras_cv/layers/feature_pyramid_test.py,sha256=BwO_c3GFiITgxsdUJ4xN-EelrQkmoof-pvrOV-7XUGI,5125
-keras_cv/layers/fusedmbconv.py,sha256=MqLvyOR5wjs4kdlkWvPzdgFd1PogH5z2xJwh80xVDpk,8076
+keras_cv/layers/fusedmbconv.py,sha256=2XXP_xgDQRZTfQpo90y8SepF-V_EZ5T-K1dLdLUj7Zc,8171
 keras_cv/layers/fusedmbconv_test.py,sha256=aEfM3bhsqI2CY3TrxJOpYliCMxGnQSly7wz3_vNmpCg,2273
-keras_cv/layers/mbconv.py,sha256=6YbzUBk19egc7L4qcTSgezhYJ5mhWmz3ON761irZKfM,8349
+keras_cv/layers/mbconv.py,sha256=J-IaHzQVkG7hYM-mY_zlACYuKKUHZ84JA2YUbYXcG7U,8450
 keras_cv/layers/mbconv_test.py,sha256=gx0ylTG9BL1kopIh8EgIYpOILNQ8ck4Amp-skYKVzi0,2216
 keras_cv/layers/serialization_test.py,sha256=_3G017HBVBDrgkgAgI1M0wXy9oF4PCf6OQuqFTnm7Dc,12061
 keras_cv/layers/spatial_pyramid.py,sha256=moO2kwHzMORqv8KL25-dNStPXCKc6rqm0EsJLVreq3E,6281
 keras_cv/layers/spatial_pyramid_test.py,sha256=pcCIG967qGmE5VI7hfrln59VRCzk7-WySFpXU4zfrQU,1290
 keras_cv/layers/transformer_encoder.py,sha256=VG_j7yNYWfy0V8noJAXHOSiIGhW2FQ62g5az42Jk8aw,5249
 keras_cv/layers/transformer_encoder_test.py,sha256=OoZF6xLbZSrQC7L0COMyiXcow8pVFX256o6DsjvB2U4,2135
 keras_cv/layers/vit_layers.py,sha256=eDe99utjMz9CrZYtWmAVwj6u3Q1BKqwgjmRvbW2VQT8,7723
 keras_cv/layers/vit_layers_test.py,sha256=ykwjmxfMuCiJAYZf46W_0oQt-wEgQ8LRhWSYnQ30zvQ,2886
 keras_cv/layers/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/layers/object_detection/anchor_generator.py,sha256=nng-JAWtBjkkzH51LA_ArBWF5O011mvxvZsSHj1pJ0E,11406
-keras_cv/layers/object_detection/anchor_generator_test.py,sha256=P3A4cZEoKZM_15_YaM8x3KZjD5Q5DH5-qQGXM6JX1ac,6422
-keras_cv/layers/object_detection/box_matcher.py,sha256=OYNhkXpRr5SDMbD-MMI936RhO-v11sIMT4CYmSN-QkM,11483
-keras_cv/layers/object_detection/box_matcher_test.py,sha256=jR2m1KFh_9g875-qksmjZdsghlJSR7ppw3slJhLXq68,4939
-keras_cv/layers/object_detection/multi_class_non_max_suppression.py,sha256=_QoL0dolU5xv0J2Zz1_UdlD308m7ptXWq7nnwwQgdzE,5140
-keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py,sha256=yW5cuqoJ4xzhWfElEYiGD-XE6aRg5aLHDeR7eAiWV8U,1610
-keras_cv/layers/object_detection/roi_align.py,sha256=oQ9Jfgipum_MTSvBUIACXSJe-Hg5A35vy1RYdHewKJ4,16185
-keras_cv/layers/object_detection/roi_generator.py,sha256=SiVVMlrBtCuj55oBr9zkJWeYXX04p8kug4RwMWubjp4,9833
-keras_cv/layers/object_detection/roi_generator_test.py,sha256=gGwxAoefwEziGq3KYL4hYv_oV9qPRmcM6OQXCX4Omkk,9256
-keras_cv/layers/object_detection/roi_pool.py,sha256=cIYIH76gWvGtDKgmZ8NqUA4l-Sw58V9Xktp11wkN9gM,6427
-keras_cv/layers/object_detection/roi_pool_test.py,sha256=E4lKzwcuOG8y_2a6EykriLJG6Fngkk_X4GIJjdeFHfc,9712
-keras_cv/layers/object_detection/roi_sampler.py,sha256=w_0TA2Vtlgx-CGUIiEZRgg1zyn4sfiak9tUj29s43_M,9064
-keras_cv/layers/object_detection/roi_sampler_test.py,sha256=5T53jKZ9JahLBRE0CaxENTo1jTWSMY36lb8V_SIp-nw,11643
-keras_cv/layers/object_detection/rpn_label_encoder.py,sha256=AnMNPwSr_v9tPC5rfHO4--U24gL5BDZRS22kyGX9bpc,9272
-keras_cv/layers/object_detection/rpn_label_encoder_test.py,sha256=wH2PrfMfhp2HZ3f2QI-tmlmykC8schjLfsb2k6IcoIU,5631
+keras_cv/layers/object_detection/anchor_generator.py,sha256=U6Ekm9JxOD4-Y8--qst40PIYUT8yBpIKuly0b3mJG60,11281
+keras_cv/layers/object_detection/anchor_generator_test.py,sha256=P7L3ZMalHmR7c8NuX8f2-D6Kclha8-28KBCPuIVj1zs,6487
+keras_cv/layers/object_detection/box_matcher.py,sha256=Sx2WZlm0LSDCJSQYJMQqBP5Fepq0FIn5c4F3FsmDTLA,12057
+keras_cv/layers/object_detection/box_matcher_test.py,sha256=I6fgFvanQX22je1iniKhUKKx9du3iDYjjfk67DJF4PQ,4709
+keras_cv/layers/object_detection/multi_class_non_max_suppression.py,sha256=cMLSl4jvR7LnDw9u3hERArRsaK2eJvmj4hh_a9Dx1UI,5445
+keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py,sha256=5mOuc5vhXWkm-J9vOUDLNgCxZ-K0vmc6Bxsm_bFP2ws,1651
+keras_cv/layers/object_detection/non_max_suppression.py,sha256=Y7B1rwVD0aygMnY3U4RLtVRoBlyWxYStytx4yMkNK78,21573
+keras_cv/layers/object_detection/non_max_suppression_test.py,sha256=gzFu8oAXQu1tBXwB5PeJuZuW2_6_krCqmJvZfNA0yxA,2371
+keras_cv/layers/object_detection/roi_align.py,sha256=3WUnDzCCjv7MZbEl6_vBsPYClFKXLI7xY0_MPXe8MGE,16285
+keras_cv/layers/object_detection/roi_generator.py,sha256=sqRvwYYKNafdJc5TQTIe9sR1Em_4ciF2S-1Q4jSD07A,9934
+keras_cv/layers/object_detection/roi_generator_test.py,sha256=Sub-Cw3s_M2IrfvztIsgfr7P4bjXxBa7oVnrMdBvNnM,9297
+keras_cv/layers/object_detection/roi_pool.py,sha256=kfu8KwhYBdEdU7Dza0ocS6CCLboftzF4VEWf5VF_x9c,6525
+keras_cv/layers/object_detection/roi_pool_test.py,sha256=ioprUFU48N-oVbNJDvuIgbsimX49imNeccxb8oBodAI,9753
+keras_cv/layers/object_detection/roi_sampler.py,sha256=hjciKE8m4mJZciJZ_jBYgbru9_SNpxB6VvGFOnnk6M4,9164
+keras_cv/layers/object_detection/roi_sampler_test.py,sha256=0nQ12dO01hPYEzUbU_Iotvo7xId9JCwPrCbbVyDiZf8,11684
+keras_cv/layers/object_detection/rpn_label_encoder.py,sha256=gKdxnmFVsWTaboez-Mnxaa6LgPlap9hyIkJn245Y7IM,9377
+keras_cv/layers/object_detection/rpn_label_encoder_test.py,sha256=iHn_ak7LQvndX4DWxLGswy_hbEZ7cdvZyAC78eHrfxE,5672
 keras_cv/layers/object_detection/sampling.py,sha256=YCckkFqLp71zp85JZ1DZHGtqho8W7hE2fg7IYvmovsE,3426
-keras_cv/layers/object_detection/sampling_test.py,sha256=NoDiwuAm2WqPX4l_qPtPkVw4lM4sjGNC0XleNQ9LoWo,7277
+keras_cv/layers/object_detection/sampling_test.py,sha256=3Lni5RlV44LyL3lemjKSjPHWy1C25r7LkGfLOXYpomA,7318
 keras_cv/layers/object_detection_3d/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/layers/object_detection_3d/centernet_label_encoder.py,sha256=nUEL8w6C1q_6lk96M7CfcZLrD9LxvcQX-aQCBy8C-Mo,16098
 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py,sha256=WVkv-iNp984PZukuokvG2VaXOdqUwn5ywD_hUhVSRao,4743
 keras_cv/layers/object_detection_3d/heatmap_decoder.py,sha256=tCaA5MEmKvhERcrtVrDc0iVZvL73Y5_1ZxK2hmr18eY,8107
 keras_cv/layers/object_detection_3d/voxel_utils.py,sha256=AM1czJfqLqaXrRjHy8XROHJcXKAA-ZfTRPtoZBAHsEc,10029
 keras_cv/layers/object_detection_3d/voxel_utils_test.py,sha256=bFR-xIGe2fda3klY13bcz4ZJtLyT_T_ita1BzZjWf9k,2841
 keras_cv/layers/object_detection_3d/voxelization.py,sha256=iYTadgC2h3yNqnwg-fpa7G62kG5rIfKLSerzP1Bccv8,9159
 keras_cv/layers/object_detection_3d/voxelization_test.py,sha256=9b9MO1RhUZXZ_1-WLcGbgahL8XLYEslLpsZvFtmdkf8,4114
 keras_cv/layers/preprocessing/__init__.py,sha256=Xt9CHiVH4BNqiGu5iaYiDagJEGoyoXVjnLaPBfa5g9A,3914
-keras_cv/layers/preprocessing/aug_mix.py,sha256=TkkcX2Hnmp7SeHVT_q5dlchMZM38a_-Z3J8IVOZ9eMc,12754
+keras_cv/layers/preprocessing/aug_mix.py,sha256=_2SxfFGLvRcO2oHOLiedgfLxvGTVOfD-uZLvEd7c3LY,12761
 keras_cv/layers/preprocessing/aug_mix_test.py,sha256=qyGIT2A4BN0bC8bPYKyJi51Pl_ayEbDlX-EIuKHZmnQ,2580
 keras_cv/layers/preprocessing/auto_contrast.py,sha256=8TOYm11omxHoR6cdfVj0YjeIbdxLZYgkAY2uJUAe_p0,3509
 keras_cv/layers/preprocessing/auto_contrast_test.py,sha256=h9q40em7oa9MVH-J05MOYM9-9oXmCnJGQ9T_cwYfCa0,3331
-keras_cv/layers/preprocessing/base_image_augmentation_layer.py,sha256=_zz2ltzbTw555AkwRXcCMU24QBNiHdHD9ICLZIKni5E,20504
-keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py,sha256=FFcoKKdpAkgBNWYwvLHg-v6wTfZhyJFhtJeF9amUXEI,10953
+keras_cv/layers/preprocessing/base_image_augmentation_layer.py,sha256=7GjVH5_DLFA_r5nXhpHGU4Wbw-Hb5G61cGGM30x2y7o,21145
+keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py,sha256=SFWO1F2s357GUBW15FPTAb3bSAi6pGbpl3UVUk2NURQ,10074
 keras_cv/layers/preprocessing/channel_shuffle.py,sha256=Vl8do16_OhmFfNQht7-wT6sYxEjLBDLazvcFZdOPOOU,4559
 keras_cv/layers/preprocessing/channel_shuffle_test.py,sha256=Or2CYMmxNoJ2HWUMt7IanqmcHtQrfYCcljL6nwm-MXc,4065
-keras_cv/layers/preprocessing/cut_mix.py,sha256=d41F7OQPTwShcZIHxYq0lsq7eAhj8SPzDzDou87ldMs,5992
+keras_cv/layers/preprocessing/cut_mix.py,sha256=u6VlMyiNzbSOMFpqk50SqHvJlM4zTU-_FiliPjejMv0,6095
 keras_cv/layers/preprocessing/cut_mix_test.py,sha256=UuC7ZXugY5gFOF3iuWExf_AxgYsLELqamw9wlcdZsgU,5040
-keras_cv/layers/preprocessing/equalization.py,sha256=Go5iZ-2BqOhsNegljii_bH096HYUyKK4qQ0FDgyyOr0,4929
-keras_cv/layers/preprocessing/equalization_test.py,sha256=0MhbiO8wp_30YlkSoxdsZf7wawh20DVO3qx8zQqVyf8,2446
-keras_cv/layers/preprocessing/fourier_mix.py,sha256=Mg8nQymcoa_h5tCToKhwxyJZnW1qnIQ2Y-fw-KAS1QM,8025
+keras_cv/layers/preprocessing/equalization.py,sha256=n9JasiFQTLC_GGkZSz-hcAebdsTfMJBqsUDUjlFCrVk,4936
+keras_cv/layers/preprocessing/equalization_test.py,sha256=GX8Z7zaIOVW9ExbHhcnmx__IgW74BDIDjWWvfxuJ36M,2486
+keras_cv/layers/preprocessing/fourier_mix.py,sha256=GRHixJnTUBBsihs4zEnKsvFel2S79z5zwhMiHZnt1vU,8032
 keras_cv/layers/preprocessing/fourier_mix_test.py,sha256=2D6YCdpq120gB9uHkRItdN4v5jYgNbfN7699NtV7Jug,3447
-keras_cv/layers/preprocessing/grayscale.py,sha256=mM8jwsRqxsDb016u7LaOImUU823udd59xMu6Byj3GAc,3841
+keras_cv/layers/preprocessing/grayscale.py,sha256=joaDnCJzcITE-PuBMTGfF7pmB5pp-zy5evMit_wOmGs,3848
 keras_cv/layers/preprocessing/grayscale_test.py,sha256=JTVymTD9mBBh5Zu-Akkf9Pck9Rg-2kPwXcMmdRjN4Kc,2820
-keras_cv/layers/preprocessing/grid_mask.py,sha256=qeyFEi9_mb6YFHayr8MJoLxRKpQaJHqPD9xRvPpXj6o,9733
+keras_cv/layers/preprocessing/grid_mask.py,sha256=duJs59qt3P-bUm_qA7BFpILlESZfZxJyyUAeygZfQ-s,9710
 keras_cv/layers/preprocessing/grid_mask_test.py,sha256=RFsl45u6Yi_nG4lorqDQz6FJahRF1YzaZOcrXBsJmHU,3823
-keras_cv/layers/preprocessing/jittered_resize.py,sha256=R_FVyRid5n-DNh6KNbpkiMapdTpcMMKHIlI2uLfySWE,11349
+keras_cv/layers/preprocessing/jittered_resize.py,sha256=xWu18tgoDv-p6l3jRFdkfVu2c3R8wcM6WVmC3FQXfYU,11356
 keras_cv/layers/preprocessing/jittered_resize_test.py,sha256=1ahFxImZ_XDBnYT7v3BNa9VTlankWdsSgbLpH6enlEo,7360
-keras_cv/layers/preprocessing/mix_up.py,sha256=dUbD5TaBzl-TLADc1hbB6ZpA8ypNXxeMEJZ3gLFaRP8,7461
+keras_cv/layers/preprocessing/mix_up.py,sha256=lB4gCZHnbB6HqZa5FK5vPIDv8ztXWzyenfePzy-8dsI,7536
 keras_cv/layers/preprocessing/mix_up_test.py,sha256=G2boDs1ybCP7h2Jr2EMVwLgNcOtJvjCvfrSze6umdTw,6180
-keras_cv/layers/preprocessing/mosaic.py,sha256=u4ms7qWnSQ_a-Ms9KwlfVXQ84V-oxhBsEQFKd8ltijg,13424
-keras_cv/layers/preprocessing/mosaic_test.py,sha256=TbxJbwGqMkp3O_ilSuJNH1_gQC2-s7lj3HamHz41vX4,3726
-keras_cv/layers/preprocessing/posterization.py,sha256=z_i8-SeHd20hppTrWlzrtggPfgf8145ZGkVDJnaczIQ,4237
+keras_cv/layers/preprocessing/mosaic.py,sha256=lD9nhdCDD_6S1y4kGfiRK_9dLihAD26f_3TUVfoRPyo,13431
+keras_cv/layers/preprocessing/mosaic_test.py,sha256=G41r4mZDnb45OPxeBEnHiVx79gYWInAS4rzghomctZQ,3382
+keras_cv/layers/preprocessing/posterization.py,sha256=4G7Z3Ck1B9I2ppPAavyOEfa332Htpgsxg30_k-lwzEA,4244
 keras_cv/layers/preprocessing/posterization_test.py,sha256=GX4UvVllEYKcquYouF7n0MdCBtsz-1VRiWolN7CY5Mo,3792
 keras_cv/layers/preprocessing/ragged_image_test.py,sha256=Go_f7A5Pvokdv3_fB2lFrRspcC-IXzGW2n9vnqKU0LI,5101
-keras_cv/layers/preprocessing/rand_augment.py,sha256=GhSySn_Gv2WNlO0SP2a91lwjOTWNFhHXXxi43KICNTw,10805
+keras_cv/layers/preprocessing/rand_augment.py,sha256=lO6zBv3NL8KG3aFes9TobbMPRsJe7t494OhK7JyZXDU,10811
 keras_cv/layers/preprocessing/rand_augment_test.py,sha256=Zy-VCR9TXY2yF8ycMe2SGaWG5vT5rUMco1uglqg1Ndk,3758
-keras_cv/layers/preprocessing/random_apply.py,sha256=xcjzf2o3FtEgRvWwR5xJuCpX1OVO9tKqGWGYKHqBWOQ,5023
-keras_cv/layers/preprocessing/random_apply_test.py,sha256=jnZlTqYo9aXk7DhzkMIaOC5YefxnsLbre3O_Hjon1S0,4646
-keras_cv/layers/preprocessing/random_aspect_ratio.py,sha256=cSDj28L__z33lxtDyGRc2YzovjoSJmKaq3iInKPbq6o,4683
+keras_cv/layers/preprocessing/random_apply.py,sha256=gRaIG71ki2hD7aajowIe-uBjzT8_ld1DiUqLxDQ2j5c,5055
+keras_cv/layers/preprocessing/random_apply_test.py,sha256=IoNDueffBSy712-d9TjtOsrxVGtrW6OeGeMEnjMteh8,4652
+keras_cv/layers/preprocessing/random_aspect_ratio.py,sha256=1gLXkTUISJ_PF4lYzwSEUFb36KKusX5DHZyIvssfQeA,4690
 keras_cv/layers/preprocessing/random_aspect_ratio_test.py,sha256=R6kULrL5FggsQO88gYJhC0dIqz3FpDV6InLoL830aLw,2277
-keras_cv/layers/preprocessing/random_augmentation_pipeline.py,sha256=GmnTpiS7DW8vzbbs-PrFWN1VIIEH_kGSFYg6-t_Bojk,4754
-keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py,sha256=Y255n0YDCPFHd1rwFmKexba4PR6r5lQ3IUAzNjrS9oI,3340
-keras_cv/layers/preprocessing/random_brightness.py,sha256=YKj4hVwfm-5EPytYt2mOT241nnf3Dic2NU0C4QqvMpI,5241
+keras_cv/layers/preprocessing/random_augmentation_pipeline.py,sha256=-FtdGLb3BmFczRnBGiij_vodQuozOZGSuL2LucqrP8w,4761
+keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py,sha256=gfNiTjfjvoQGhfqV2qHcFQ6_wEPEKzm9JnLVRIeFJK4,3391
+keras_cv/layers/preprocessing/random_brightness.py,sha256=wa-dSEvI_rC4vXgnVs7SR2YRBmEzA7YD5S2i9N-z29A,5248
 keras_cv/layers/preprocessing/random_brightness_test.py,sha256=JMN9MdzDj1zoqP4ykKuPg_LMfz99TLF3IAsfRT88xzg,3337
-keras_cv/layers/preprocessing/random_channel_shift.py,sha256=5BpP2n-P7XQoD4l0f6b-o3i7qJry44WKj4HRxpZ906g,4396
+keras_cv/layers/preprocessing/random_channel_shift.py,sha256=umV6a2rc_AUfRTECFzhGiLJurAsuaiRPekCOV6QAVKo,4403
 keras_cv/layers/preprocessing/random_channel_shift_test.py,sha256=ZhLze0K1a-OM1i50F6F9SFzKVaBbUQWc3JeEs2Av-N0,3964
-keras_cv/layers/preprocessing/random_choice.py,sha256=49rSgIhgtOXpw_td1lMIIeX4xQ03UDLEBw-TqGXrCh8,4503
-keras_cv/layers/preprocessing/random_choice_test.py,sha256=rRJs_svcz6T7sx-WTXEMivjwKGQqv0wemOgkss1bns0,3316
-keras_cv/layers/preprocessing/random_color_degeneration.py,sha256=53KaEbV3Ele0x4yIc3zKRxVRH0QbbbZiiJdCoftOwbE,3392
+keras_cv/layers/preprocessing/random_choice.py,sha256=sdTg_k33hz6zWse6FFU9zpRo_CYg0bjN0FaFBk9wZFg,4510
+keras_cv/layers/preprocessing/random_choice_test.py,sha256=Fk8v0PZbWg-jCr52fQIX1v4wvKhoHezhLtWMRAoTKKQ,3367
+keras_cv/layers/preprocessing/random_color_degeneration.py,sha256=qkcNW9_Wxcr4EaWIZFxAilyjW5IxWWLjp2SUTvVHWFw,3399
 keras_cv/layers/preprocessing/random_color_degeneration_test.py,sha256=CGNpf979r5Zyfo05Ipgr4vBOwRKoj30atWLlSBtfBGs,2648
-keras_cv/layers/preprocessing/random_color_jitter.py,sha256=Z6TYcA-38KHwAUB57mZ1UsLpWwvNgCDyIIAr3KNowb8,6946
+keras_cv/layers/preprocessing/random_color_jitter.py,sha256=OB1SaCVCRCxO8oTVCu8G5GutsE3wNJWnlN4ESy4gmeA,6952
 keras_cv/layers/preprocessing/random_color_jitter_test.py,sha256=Z7aXMk74mCWWmXwFE46DButJpQiBPoP7yJUNqfk4QJU,3902
-keras_cv/layers/preprocessing/random_contrast.py,sha256=AcNGGlgJK-W-sg9gfJOUOo3uLzxORIEI4FLGlTw3zFM,4864
+keras_cv/layers/preprocessing/random_contrast.py,sha256=ExXERqLcEWV_amAy2NdRafPL2ONN3ruqxmMJyv__kZQ,4871
 keras_cv/layers/preprocessing/random_contrast_test.py,sha256=NVTvsRThc3WeKVz8-hMPuK5wsia_4Hkw8u0G4pzyXF8,2213
-keras_cv/layers/preprocessing/random_crop.py,sha256=e71xkckFIon5K-vWriIjLqFl3ML9WYrGeobwzkrhHL4,10846
-keras_cv/layers/preprocessing/random_crop_and_resize.py,sha256=uVwAHrc4B_Ls1amzy4LHjUMuHHapJaHG20PWQCNUUso,11178
-keras_cv/layers/preprocessing/random_crop_and_resize_test.py,sha256=9UgxIClGEAuJWF_6Ny9NMCkS3mf8Hjcoha35ehhykvs,10241
-keras_cv/layers/preprocessing/random_crop_test.py,sha256=KDSXAex569_MhVVxzRAN4Vc_rXmZgoWLOTk2WuaJG8Q,9856
-keras_cv/layers/preprocessing/random_cutout.py,sha256=BGA0uDt-Lxmmns1RzELY7mM_d03JVvEOAxCftUUniaY,7024
+keras_cv/layers/preprocessing/random_crop.py,sha256=n-uCsbiamlLiQBYz9Fd331EFE8A680VR-LM7LjhLLTY,10853
+keras_cv/layers/preprocessing/random_crop_and_resize.py,sha256=3e8KHR97GYOgEZBOkGXYSecATqrUQyA5ELt7_YN99C8,11185
+keras_cv/layers/preprocessing/random_crop_and_resize_test.py,sha256=9-fhrxrlZBfPq6H8d3n-rRWqz2MDVCRhGGDz0oV7rk8,10276
+keras_cv/layers/preprocessing/random_crop_test.py,sha256=37R3kl9RuKBgcay8bKw7juIdeInoW5m_-2trQv5e2G0,9814
+keras_cv/layers/preprocessing/random_cutout.py,sha256=9oO6q0hqvioL1XVsdU6Hv2eFyukIUhfApw72ghlSGT8,7031
 keras_cv/layers/preprocessing/random_cutout_test.py,sha256=9isiv_0Vj3nvZAXgxgFCoB-9iNFvlyAZ9Oiiqf-AR4s,4978
-keras_cv/layers/preprocessing/random_flip.py,sha256=R850U1PqpMhkuQ7v6vAaL4Yj_nrzo29GgfZhX-3tbkg,9003
-keras_cv/layers/preprocessing/random_flip_test.py,sha256=pjTDGOywZR19vItJ4rwNTyNbxIRB3w-8yJQpP5LUvFg,10994
-keras_cv/layers/preprocessing/random_gaussian_blur.py,sha256=Qi_xh53gz0mU3idr5pjKRdPzX07dAcPKJNeejpdQt8c,4576
+keras_cv/layers/preprocessing/random_flip.py,sha256=bBxZ9WlqbZJ6R5tKE7s8zgnzOvOkzze4A-UCpYcVj_M,9010
+keras_cv/layers/preprocessing/random_flip_test.py,sha256=DEM4toofUcuTG1NOrBIM0WC5z74quuseIJ1_hs_Dor4,10976
+keras_cv/layers/preprocessing/random_gaussian_blur.py,sha256=1Q4f6gNzt9wIIwobNd-2MvKvtvuLqiHeUQ7QYMXdFhw,4583
 keras_cv/layers/preprocessing/random_gaussian_blur_test.py,sha256=qKXMbClcRqadaO_4Zs97G-kWMTGta2tju9J04xeNOAQ,3245
-keras_cv/layers/preprocessing/random_hue.py,sha256=GohDofOhXEHu6Pb3mP4F4IwVxZmzgfqLik28H4Bozo0,5433
+keras_cv/layers/preprocessing/random_hue.py,sha256=y_irRgpYdk8zYCIrsq4l_lhsF_JMAmgZbA6hPgJvrkM,5440
 keras_cv/layers/preprocessing/random_hue_test.py,sha256=upvOGOXGsfdBkmNdkQ9E9_A0d1zInJjVDpe_BAIwIqc,4230
-keras_cv/layers/preprocessing/random_jpeg_quality.py,sha256=OJg6ldVju9p2SfEg3BzP3lpebEW4DI6S-jOlEf4EXfY,3021
+keras_cv/layers/preprocessing/random_jpeg_quality.py,sha256=5QD_Y7yy7WBzX1cl-M78LbxNkw8nGHdJ6J4T7ujCDP4,3028
 keras_cv/layers/preprocessing/random_jpeg_quality_test.py,sha256=C5MwpgOivDbwKJyh8iUjqXdBnpfdlaPDwTt6AVCumKc,1984
-keras_cv/layers/preprocessing/random_rotation.py,sha256=XHJBSulMI2GnqKUq2WDoxIoqc9Gy0gHwehqaRXxi_x0,12271
-keras_cv/layers/preprocessing/random_rotation_test.py,sha256=p6uwKrYEcirGUWiW-R9g-q-4UPiCNqcfwzqCWVQAQgY,7395
-keras_cv/layers/preprocessing/random_saturation.py,sha256=0nHv80-IaL_xdY3ftHLKsHYj6OoujRyhQwrUM9C6nUo,4951
-keras_cv/layers/preprocessing/random_saturation_test.py,sha256=ojtam1LOHjSBr89i3ZpOE2fQWSIR7o8PZsmk2HOMzyI,8248
-keras_cv/layers/preprocessing/random_sharpness.py,sha256=leperIQmL7LsDO-IOHJG3iX04hlBpvk4IYWzA4TPARY,5813
+keras_cv/layers/preprocessing/random_rotation.py,sha256=JGNZLopMY0KEVhvMUdVvuUGu8dO3eyJ4P-yhSXLTbF0,12297
+keras_cv/layers/preprocessing/random_rotation_test.py,sha256=1rZnMuohreEzdXhLEy0ni1S1Zhdw-fs6S66DNPi02uU,7251
+keras_cv/layers/preprocessing/random_saturation.py,sha256=Sxvk57XBojRkbN-DekNx1IVGErpcXlFIB3tpcUExaf4,4958
+keras_cv/layers/preprocessing/random_saturation_test.py,sha256=Y50vD4Xp8NSc1EznghAZi66lTrCQNwWHe5Fw2f7NTUo,8254
+keras_cv/layers/preprocessing/random_sharpness.py,sha256=1bBelthIgzKGMejKzzajUcdX9wHykaenNHqIp_9ssGs,5820
 keras_cv/layers/preprocessing/random_sharpness_test.py,sha256=0Il3x6WsuvI13U-UWglj_zvqYwnGG_wV8whCLJVDII8,2724
-keras_cv/layers/preprocessing/random_shear.py,sha256=sgnCm_TJkAzj9KVWEiItzA3KLfdBVVJYxuERLrZ3Sus,13111
+keras_cv/layers/preprocessing/random_shear.py,sha256=SQ37PX-zUZMTaVv9UquGiFeMsJ3IfY5T_fn5_G4fJrE,13118
 keras_cv/layers/preprocessing/random_shear_test.py,sha256=hDSaeI1uNB6YVV1_d54yhSkrZfPgqCh76oAaaAbc1ok,9331
-keras_cv/layers/preprocessing/random_translation.py,sha256=IGRJaJoqj3F9uqx996koQFv8pv7EwC6Ui5nc0SYVwAg,11148
+keras_cv/layers/preprocessing/random_translation.py,sha256=xDCRbuCmd4uBm-xzlcvyeYOvfnRDirvNJAGe7d5g0uY,11155
 keras_cv/layers/preprocessing/random_translation_test.py,sha256=14K-lrHIAZvKty3Je8VE0QUs_RfZ7Rb7VwNLf4mlWGw,8917
-keras_cv/layers/preprocessing/random_zoom.py,sha256=AsHgGYsffTG_ohlDsnzMZ4mlD1gh-8SQoNAiVD9Yd80,10340
+keras_cv/layers/preprocessing/random_zoom.py,sha256=dienYCMJf5MelBRab8IqfmkO6xrWXhUlKqmIiyyBr9s,10347
 keras_cv/layers/preprocessing/random_zoom_test.py,sha256=98ug5ZDgBI7Bq84s97tt22Y7WOA9I5r1j_qNaxNj2-Q,5859
-keras_cv/layers/preprocessing/repeated_augmentation.py,sha256=xtZy7qGY4hhrXKjzG6tCubGjOqaqVHtYoO5qjKozm-Q,4677
+keras_cv/layers/preprocessing/repeated_augmentation.py,sha256=fsYa3ILiGbxA45QsHY7yL72xpnTiuJbPiSkHnbH97Z8,4684
 keras_cv/layers/preprocessing/repeated_augmentation_test.py,sha256=HzlBGQOAv0rwGaYiVhpHmU50Q808zlrcpqBaYe2bBLE,1753
-keras_cv/layers/preprocessing/rescaling.py,sha256=hor0dxiHPGHqak6HIGKjqsDvd7XHHg2WuPmEy2Giz70,2766
+keras_cv/layers/preprocessing/rescaling.py,sha256=vZpfNHWCZq7-KIhTmVP648EZ7qNO7aGdJc_y8jUv1bs,2773
 keras_cv/layers/preprocessing/rescaling_test.py,sha256=G7pJGDkFGTSqnKxpLT9VoP4433uzQGo2y9LIud8SJdI,2123
-keras_cv/layers/preprocessing/resizing.py,sha256=Z45iRZQM1lskXrafaNwAcDu7WYhRenUnR2Ix9x7tVKE,15342
-keras_cv/layers/preprocessing/resizing_test.py,sha256=ZpfkIFRVOw9ss8uIJtMvsN_s2eV4oqodV-9V52J2kRc,12313
-keras_cv/layers/preprocessing/solarization.py,sha256=Spnuyvh3DinbV_g5cknlN7Pd5I3poQetZzJpT0EwDoU,6315
+keras_cv/layers/preprocessing/resizing.py,sha256=foglYfagyQOY4zv9kclDy1Jj68i3rORWlhfyO50V-Nw,15324
+keras_cv/layers/preprocessing/resizing_test.py,sha256=b--FLTq1VP06POX_QhXmEUampy1vXg80dXj5FLefhfk,12420
+keras_cv/layers/preprocessing/solarization.py,sha256=nBhGoXi45uzRMbU7l2H3UrNfIYvF9581ed5GBhhkEnA,6322
 keras_cv/layers/preprocessing/solarization_test.py,sha256=Ef4RgDgxYhrl7vOxlVjc3cbp1NF2-O74eiQyDHTzJ_c,3152
-keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py,sha256=x7ECy-QlzzZFeZ80CDp9-_3A2hKn6iqWSz1MwrhHlPg,19868
-keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py,sha256=LD1rg52LEsxEXlaUQh3fkii4DNywGr0EYbRqXuQoq9Q,20977
+keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py,sha256=HAR0teJqXVxl52qo4dibhQUUd5D9QBKHiUpCi7PaNVY,21021
+keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py,sha256=TDLO0r8i8LwXLX_WD_Ob0geBuR0ml1tG5B3T9e_-Zyc,20739
 keras_cv/layers/preprocessing/with_labels_test.py,sha256=nKTZKj40UhG28oV7z2rBBpIFgdGx97jwjSUgcR3SZjk,4773
-keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=XYb4YQyP-rUR-zIT5enD7TaybbA-9EYK3vXQ7Ibp0vc,5231
-keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=xdaI5yqWo9zTTVT1rT31ZGjCmJlOMljc-1WA_PY96Z4,4885
+keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=PC3FSM6Z2rK6II7Wnp4m0AxSizSmcK3jAtyGgMVTOz0,5237
+keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=0tlUmFBmMCS9WmBjKMx7NR0o9SrIsL1L6hveIRdBFYo,4852
 keras_cv/layers/preprocessing_3d/__init__.py,sha256=6_AQQLdMdzl63Ro_eEGOIeXe7Ilxjoezm5jaz1AOE7g,1904
 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py,sha256=4kp2UVUukA-PpE2BIxqnG6r5EIPVq6QSWK99i1bpGek,10608
 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py,sha256=XfLsnLcSPcBMIyyyaZO6aWiL0cKBcUsfLoUUI6fAAZU,4789
 keras_cv/layers/preprocessing_3d/input_format_test.py,sha256=MO5-1Me2sSkKMaJoAeLfjgBH6-jnI3f8g1fwj_r5TLc,3623
 keras_cv/layers/preprocessing_3d/waymo/__init__.py,sha256=4vu_MIXcDp6eo2TBan5WGw5AlGt13XhBn2cIF3VrQGc,172
 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py,sha256=KnDKn1UG8r4m5zA-7-_3NcUFiLLO7yCQ7P92V-7VbIE,5860
 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py,sha256=IKGJEAZlKEgimrPTFR2ajYvaCgUCRrEjs42scB7R08g,5063
@@ -211,93 +218,93 @@
 keras_cv/layers/preprocessing_3d/waymo/swap_background.py,sha256=K8DUmGQ2J4RQwXld46ES_6A6Wk7lgYS8zGeoj0cFXqU,6970
 keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py,sha256=svSIWtMDvu0kjeCfVdeJNz5mHP-DOOLv8j3uTc3bgK4,10691
 keras_cv/layers/regularization/__init__.py,sha256=puQMT3krCCm0-Z6_1N296zpAZGe5FSIv-6ILRtjkG-I,868
 keras_cv/layers/regularization/drop_path.py,sha256=ev56dTYJHlchvq4i4rfTBjgjFpJGMD97_WC6igt-4kE,2545
 keras_cv/layers/regularization/drop_path_test.py,sha256=xD2WretcOvKTSZNWwUTyPXh1XKjCPcBoYiP7hHBbXEk,2460
 keras_cv/layers/regularization/dropblock_2d.py,sha256=Z4ExqwQTyjFFZu1pW6caIrvWU0MnyreIzQVvnJ6qMJs,8831
 keras_cv/layers/regularization/dropblock_2d_test.py,sha256=fGYKqszc-Hd-FHuH2YK6hwXgiGGux25aDBW8BaUnnbw,3653
-keras_cv/layers/regularization/squeeze_excite.py,sha256=1Zwhgk6mJ4Bx5cptlnVXzMOYuY9hW-UXHu3tCUBpsTo,4906
+keras_cv/layers/regularization/squeeze_excite.py,sha256=MGOJVCAifTqOXNrgPAuBZuRkJJmFkxXZ0l1_CJQgVGo,4960
 keras_cv/layers/regularization/squeeze_excite_test.py,sha256=bK9QHsOJUe4ZIL0kaGCdjDbA9BP7LbnEkOcsIFhAzdM,1882
 keras_cv/layers/regularization/stochastic_depth.py,sha256=rRRFkhV9jPpKTLZhjUXaLMHvAePB5GsKxGnD1aiBMwE,2738
 keras_cv/layers/regularization/stochastic_depth_test.py,sha256=8ow6STkWtfuAUZGo7C1ExIFKyH32yhWDN0un9BtNXR4,1771
 keras_cv/losses/__init__.py,sha256=1gzlHvlAbdbI_ZFRyZuaW7NalhfTDpUdA7jwT9hMWEc,1036
-keras_cv/losses/centernet_box_loss.py,sha256=fG4Gg5i_GYTI2p9NDcKJavdl9kdeBEtjpddYjotQWm8,4854
-keras_cv/losses/centernet_box_loss_test.py,sha256=ypeCas7EODJmq-AzJ5MDjSj3K5wY1wli3gejWIewbzw,1459
-keras_cv/losses/ciou_loss.py,sha256=VueR_eGStcSfgeHNEiUEXkl9VHppOTDm3woJJ3tQaM4,3548
-keras_cv/losses/ciou_loss_test.py,sha256=2Vmu66GQR_T6gOelRs6jHY5sHlibyLrJdxesQAIKZug,3042
-keras_cv/losses/focal.py,sha256=QMmhZ8bInDwOgFCrLHYnzfVX0lqr-IdmjMcDTvOQoxM,4140
-keras_cv/losses/focal_test.py,sha256=LUKm7_GCCFKNM3xDnFtRCN20aMHYJeX6JQPWoXQMlm4,2486
-keras_cv/losses/giou_loss.py,sha256=8vJ9wCwNXYOifbDeCNOhZ4LveizR_EReykqG08KAZZA,7410
+keras_cv/losses/centernet_box_loss.py,sha256=PRu9wvlkaYjGgkb8xIALeV_VK1gEYhehNODhSnzakMY,4953
+keras_cv/losses/centernet_box_loss_test.py,sha256=lTqt9z9KSoFy1WDueuOSbO3Dt-IuUgl_4WH93YNDC5g,1488
+keras_cv/losses/ciou_loss.py,sha256=pJc3jdS8dlI3lYVyAr7GrflyInQVqS4tUNMdFevDLR0,3498
+keras_cv/losses/ciou_loss_test.py,sha256=-O3vdPXF2w58goabqG3PsrOFlqm-q_Gn-gsw999ZDk4,2719
+keras_cv/losses/focal.py,sha256=-IJ_ZDPuhFmRGgubDlFLoP4mAnXOBL0JfNGjWGPggyU,4250
+keras_cv/losses/focal_test.py,sha256=D6nZATHYDEgLyX905rzkbHo4WCt7-xbbTSKPQvaVWqY,2841
+keras_cv/losses/giou_loss.py,sha256=tmlfQdEuPDlIIDarTWeZ_Yj_wuieNwfQROsWNiVN7wE,7286
 keras_cv/losses/giou_loss_test.py,sha256=YklDUDDqzqh1fjyqfgfrP1fBeiNK1ue_JIyVM6FOtos,2541
-keras_cv/losses/iou_loss.py,sha256=pUnEwz_QcVHTnTgDSvnQz9oNcLzoKBYnxh2190gGDNQ,4921
-keras_cv/losses/iou_loss_test.py,sha256=md0cjQUuSTvkuFgw4RAFOY2Ptqzdb_pcLtTrsv4d4B8,2521
-keras_cv/losses/penalty_reduced_focal_loss.py,sha256=tMfcAclWiPyEeiGd6ux2z1Idt3V-n3pSzvikiOua30k,4283
-keras_cv/losses/penalty_reduced_focal_loss_test.py,sha256=V6aRLeDSxm3SmxVCuf6SDWAKtbTbP4xVhFCVe9088O4,3744
+keras_cv/losses/iou_loss.py,sha256=qeqU08ZiPWtAl7vsROWYMrosIQsuMJzDj9uhdVbazIc,4802
+keras_cv/losses/iou_loss_test.py,sha256=buMZUvvFoqLsn9xq51Q5QQhX2C1n6pgstmpa0KoTlhA,2364
+keras_cv/losses/penalty_reduced_focal_loss.py,sha256=XlMeD_Rgwonr_jI_eDb_-NIRdExVOxmT-whAMreqgrQ,4263
+keras_cv/losses/penalty_reduced_focal_loss_test.py,sha256=FAcvAtWRzoJUbJPWNFVvmWfYT1NrA0y-6XFs570sZR4,3460
 keras_cv/losses/serialization_test.py,sha256=xn8e8IB0DN5z3Q1YUTqtfzqC_XSeDBxhpHsglqFpMaE,2211
-keras_cv/losses/simclr_loss.py,sha256=V369MolbCdw4cYDTUSOC2pJbWVGQlH6dTaAGagTBw78,3468
-keras_cv/losses/simclr_loss_test.py,sha256=theY3nyff-eBEmkUdL7Q_PA_78aCLeL8U-e5nypZGUQ,2145
-keras_cv/losses/smooth_l1.py,sha256=TM_J-KT0GJpTp4xCGY3GXuQxFnfatic-YQfFK0cHNu4,1885
-keras_cv/losses/smooth_l1_test.py,sha256=1a0AjxK3AXPD760xoPnqjlbvV6xTyWuE_Xtya0c_RhU,1225
+keras_cv/losses/simclr_loss.py,sha256=xTWO9unhhN7SKhJ0B6nb6AZ0m2CWUOaoLa1wW3yyeKg,3714
+keras_cv/losses/simclr_loss_test.py,sha256=r6qmIeLFl8iZdKiCWCfELKxcdBrRr0amSzsV43gPpsY,2076
+keras_cv/losses/smooth_l1.py,sha256=x-VPZgXX_lruo0_Mn4p6XBG5wh7HCdQTxaZJt2e2i8I,1892
+keras_cv/losses/smooth_l1_test.py,sha256=VBm81hVXiweC5DCuJJkXMnxERcYOAoJWDJ53fVG85QA,1254
 keras_cv/metrics/__init__.py,sha256=X8Cshzprx-J3MNKQUgGCP4STOucojE5oUGFtM-lKisY,663
 keras_cv/metrics/coco/__init__.py,sha256=7ZP_01BNrYxXkrI7kCTLsrsIxuG5lYjHwS-HzlPz0r4,719
-keras_cv/metrics/coco/pycoco_wrapper.py,sha256=UxBRxijNGSz7dM1xlt7u7VN9tSA3tp5hmezv2GHGSuc,8421
+keras_cv/metrics/coco/pycoco_wrapper.py,sha256=Ez7NHuXL3cOTrQn20WWK-lvrQpMLm7v-ET2ZL37-D84,8149
 keras_cv/metrics/object_detection/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/metrics/object_detection/box_coco_metrics.py,sha256=4p1nPEXNRorARYtcr4CTacSBOyo7Fc5B6O1XusGnj30,10152
+keras_cv/metrics/object_detection/box_coco_metrics.py,sha256=XtCRI--U0Gz1rXX7gG5MFaJpJf9CPMGpHopDEIon2LA,10344
 keras_cv/metrics/object_detection/box_coco_metrics_test.py,sha256=VkBnTqexNQp-B0ae8xx2K3VKpImmhUy03B31VtjMvCI,9331
 keras_cv/models/__init__.py,sha256=bvCoed7A_RVZuWDWo-B9xVqWXTa_Q-kV4oO3g9tiVI0,4472
-keras_cv/models/task.py,sha256=i1UyQKvaBHfT28A4-x4-Em8wT6bnZx-gtkcz538kABc,7295
-keras_cv/models/utils.py,sha256=EVf7sMYHJWJ4ETwuVlle42U2otaDabZ4RAFr4WBk9Fk,1855
+keras_cv/models/task.py,sha256=wVM_xlwxS_EGUDgQwWmXYzFk5paaPXar9-wD37-LegE,7536
+keras_cv/models/utils.py,sha256=OLsZmhz4fpM54pmI-WG1v30q03UyQuBjaZbJtcezdH0,2020
 keras_cv/models/utils_test.py,sha256=ZXmz_ngH9DYrU-axCv3IYx7RCL-iwzC8Oc_uJ_Q_eQc,1133
 keras_cv/models/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/__internal__/unet.py,sha256=6V7gRexsVAHdweVeI13zCz4NONheX_wUchaF6j73C3E,5966
 keras_cv/models/__internal__/unet_test.py,sha256=snnjgT9KzqeL1kpsyphVTg15JBw6oudgTvQYJKylrEE,1693
 keras_cv/models/backbones/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/backbone.py,sha256=JsLD8ryc_Zr00OjapRnUB-KZ6IGFkyuoc-s-SYUApns,6535
+keras_cv/models/backbones/backbone.py,sha256=MtQYOMun1mHtoznkNGXR-AWS1o0UqPFxXhglXCHwRFo,6540
 keras_cv/models/backbones/backbone_presets.py,sha256=U2R3kTb3Fj5k2TbClRGizuUmsy8iUao0m6iaSKuwUHo,2236
 keras_cv/models/backbones/test_backbone_presets.py,sha256=fe5tweiyqijqo8-XOaA6723IZK9mEfP_ixEYeFDtYZA,821
 keras_cv/models/backbones/csp_darknet/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py,sha256=yf4uJL0AqzbUy5YmPNKJvTJpKFZ-6Vl0L-QWT_eXsh4,12495
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py,sha256=MeQjWWCcJLO65PlH0gNzrxEiL2542YEkEGKcS-gexs0,12526
 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py,sha256=FZrBXbztCR94xvAOJiAHck8uiwQBAZD9ULG7Dt70qQ8,6518
-keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py,sha256=0uQolv9drvvg2vQncrG7rpxkwR27UpJ6K164AzH2lMo,4028
-keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py,sha256=ODzZlZt4jFEbuISKE80vFtNj9MDz9dSkIyKwOzLyoZI,5644
-keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py,sha256=XnoHxxNt8PMVim-DRNI5synXqYcTia-rG0pjCOIAFqo,12186
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py,sha256=p43kiqaXOjh0GVHxyi_PTBbSJCK403CN6_8BrmhRaow,4120
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py,sha256=BJzfiHr1MtykQdpEaI2eRauYE8N0495iTBturQoPU5g,5576
+keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py,sha256=OQoqxmpe2IMEmqYLwIJKp5KktEwqV07ejgpsNVtOaeY,12164
 keras_cv/models/backbones/densenet/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/densenet/densenet_aliases.py,sha256=Zwf-6Yb3rk5juCYBAMR-wh0oT-N0xFt6lzaM2jLQeK0,4818
-keras_cv/models/backbones/densenet/densenet_backbone.py,sha256=HdMIR00V3eWMd1Bw0RaZ89jVnfHwAiYdG27tOkrLRrs,7928
+keras_cv/models/backbones/densenet/densenet_backbone.py,sha256=uo7grc_LnE58TFNa606_1Bb1HI4wFs7TAvCgkbZxEgQ,8042
 keras_cv/models/backbones/densenet/densenet_backbone_presets.py,sha256=13Bw7Q-k5_IMS-_2MDuXop5o7A3ED_lpEL7e-ZeemUs,3952
-keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py,sha256=URYk-s9faxmBCicXjx65xP8QLSyeSOPvD79cXGspXyI,3681
-keras_cv/models/backbones/densenet/densenet_backbone_test.py,sha256=m-VlHJi8Fc0wWHyprC3ypIO6JANS_mB4OwpQHsu2CBQ,4853
+keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py,sha256=j8GHPidVekmt2x6n4A7pgB-EDj2ITWPAu8-ckJ2slsk,3792
+keras_cv/models/backbones/densenet/densenet_backbone_test.py,sha256=-gtSrtOgSZ-03sC1pCqbGPAdtXm2Ba0g49NlgHjCluI,4764
 keras_cv/models/backbones/efficientnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py,sha256=y1WusTXHwRAAeqVFUzFG9JzaJjSiDxQNl_HDZshjU9g,9316
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py,sha256=B5lxQaPzYDkRL88V3zXHZOLXgEnaedWZLxqK6xWz3RE,12987
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py,sha256=2vRtuiEjvg45vG3HCxFdQadyVt3iSAZdMoWUF-KxUqE,13053
 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py,sha256=ycIWMYZa47gfzCh_Y3bEE4K7Sx0xyI6WRgGkRp2hPlw,19514
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py,sha256=xFa4fTwqQYLAfXg8iHyHMcwt1ZfNOPHlwm-c1AdrdSc,2299
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py,sha256=G3W8SuZdCGHDO31oKX7molTFArdOoIdyKphFlh98ahs,7678
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py,sha256=INmEM6OOqXtb7vR7QTFb2b3d-YkHPHj9jePtVAL-TQQ,2350
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py,sha256=Sl6Xuh8eKiIUzs2xtWL597h8lmbYKRwXo1Z9vIaZxmw,7619
 keras_cv/models/backbones/mobilenet_v3/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py,sha256=uKF4QOdvtsIVjLXaYmTZyRzWF85jyJgTC5vArtUvrD4,3931
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py,sha256=4aKYUG8Jkda_MPsbOGehtu95xHal-ddoveEo_-eNbzw,12382
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py,sha256=1WdAhi5e1why67ZSi0ZykFUF_RXIu4UnX__GlK0uCSM,12411
 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py,sha256=JuGNiVNnaAO-a6W6PRKOgGjtcCL_nQ--DW6D_beX9aw,6080
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py,sha256=joVC_BqMT1gedLevcC18_MlPpKWh3_VvkuosV6mYguM,2580
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py,sha256=MlNbqA-3tOKI5AyvOF3HLPRy3Drtqnv6hdPggN7NitA,3830
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py,sha256=yKNns5D0nGXNjt8QhnOh054xgTovz_QSc0VLX0AXN1E,2676
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py,sha256=6Nq0sgkQbXeFnQxhRX7NMZsNSzwJu2aP8wUP0-_Di0g,3825
 keras_cv/models/backbones/resnet_v1/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py,sha256=OTQ8L4lY9SYGAWAguJ1iPTCsJWP1l1dqxyOqpoGnsFk,6506
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py,sha256=A8WO1j70Th3DvrsPe55VMId9ovlRkx9ZECcxKnmFSt4,11614
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py,sha256=oXomM5Ze9zOFVfJMpxcp82lraJUFRStxvDS9kZaD8_s,11772
 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py,sha256=uOyWXpElHcRTx81YgsHTyBY0s0jhS5VsJgT5eUYrUUU,5551
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py,sha256=PJpWg44TkYC_x3OVG-jJX2uhmiaWYfgmlpghCkTnfpM,3550
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py,sha256=vm5nLcAKBzRXJd0t0LofNTlSyCIKJnThTviRGFHtXns,5815
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py,sha256=IViG4nDkgcQhGS8llexbfZ8jQeH-GRNym8DBqFVhLYY,3661
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py,sha256=BZSuYVpeSU1O2vxZz2YZjEWkRbcOZFIRV8Oln2_AVk4,5750
 keras_cv/models/backbones/resnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py,sha256=Mr1qd1Kua7Zg4zm1khifLL2eZSWWaRO9q4eb9tWqPo4,6666
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py,sha256=hAxgCEvYHmjVJu3kRyKgF4w8MMDJCu8hZa9WbIlaLz4,13017
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py,sha256=yI4UzfL20dRrbbVNyNMQuuRSqkQWOIUi5Oz5pfhJ2jk,13161
 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py,sha256=EdRbAgMOKA2d7XtfDSjSNhrML2GSUoD86PH0_ExVTKI,5617
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py,sha256=4CmM1ukbWtuLmIi5VmV52H28L5gedmlIsJM7qQxwg_Y,3723
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py,sha256=-6yujuia0APYNrT9KW9lPcvV1KeOTmiqXyl_2yH2Abw,5226
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py,sha256=xH2NCQAVCIOawMX7rc_sJBd7nOfZTJ-HjHyZEkeZC30,3827
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py,sha256=7ch_ZhhoJyWrQev3Im7sI1l9qAm-QT4waCldTz72hkY,5155
 keras_cv/models/classification/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/classification/image_classifier.py,sha256=5gD2fAYb1HvIg039DXXD0sxbBljLiW3uApqCBRu_3qQ,4645
+keras_cv/models/classification/image_classifier.py,sha256=FWxSMUZI1HSA2TFPaByf83JRcArj_mZhEAkn9qzr4FI,4644
 keras_cv/models/classification/image_classifier_presets.py,sha256=FDv2YJGhr0JXHCCNJV9ZSb5OzrJqwF3rPZg_uA_myII,8454
-keras_cv/models/classification/image_classifier_test.py,sha256=_Giogfe-bLrdAIZelY1-eXcEOcAFe9cwaRkN531166o,7220
+keras_cv/models/classification/image_classifier_test.py,sha256=M4LvidJXxuTwIDOMO37JBbUfQkvFowUfOWkYy9RYVOc,7401
 keras_cv/models/legacy/__init__.py,sha256=1ST822kpTlv-yJrfnHv6X_REqFAKQ8AiC-36Pow7kE8,4346
 keras_cv/models/legacy/convmixer.py,sha256=zF65eULXe1Jy2-iWsunOSX_lSDSwZMfqaxXw1B27ogY,14440
 keras_cv/models/legacy/convmixer_test.py,sha256=MCnQ2eEDwoSoESr50kUeJJZPosSM6CNRnx5QOoQnWXI,1835
 keras_cv/models/legacy/convnext.py,sha256=n1hJWSkLatMtciMQDCX5FG0eQpaykDMwvLErzAJageU,20264
 keras_cv/models/legacy/convnext_test.py,sha256=rBr9ka92Vd75jTkkYMCtyXvDkuFfitv9ItA3iU_L-mA,2456
 keras_cv/models/legacy/darknet.py,sha256=vo3TIrrp29sLJbyk0Vz_buCu6iu-Oa_JIbrIcq0cNiw,11050
 keras_cv/models/legacy/darknet_test.py,sha256=7QkF6BeAUTnEe1_pxmP6TwapjXKHKaGZqWIx27zCLK8,1823
@@ -318,39 +325,39 @@
 keras_cv/models/legacy/vgg19.py,sha256=qf87g78qYqAayPYR1DAYmy50ZsexBsI7OUQhXuktgF0,7103
 keras_cv/models/legacy/vgg19_test.py,sha256=ssWaUd6cTXuk9E0KIDd-211l_LP5tKGQROh8SHU3lvc,1892
 keras_cv/models/legacy/vit.py,sha256=cRFLOH-s5qcTD7qikyOqq_HpgGqF476_qFIop8HnTe4,26159
 keras_cv/models/legacy/vit_test.py,sha256=PxknM25tlUyhDujfC3UuHR1hL55zf9OyIKQAbjBO7Tc,2410
 keras_cv/models/legacy/weights.py,sha256=K7dt1GnBqp8QPYIwDpVwW4pYjo-9nm6UmiXzuapQMAA,8708
 keras_cv/models/legacy/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py,sha256=QW4vJw65ntFJzI3-rbz6p4DBBDcqyT_JALfmAwbvHog,23865
+keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py,sha256=Y0AyrpQ3XxatZaLcvSxVPmFlbL29NHIKFjCSdYJorJ4,23834
 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py,sha256=ngoVKO5r__PijXbKyhlrPkpjL7idBKLkkI9ne6T2bc4,3914
 keras_cv/models/legacy/segmentation/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/legacy/segmentation/deeplab.py,sha256=fXM6hHVSmtMhQr0bQH19OnaUDkJaanNDW3vHa0Nu-GM,12412
 keras_cv/models/legacy/segmentation/deeplab_test.py,sha256=mIgZ_08UdfjlxLcm61tJzZe2TgN2zYI5qmfRdfVz3iE,5686
 keras_cv/models/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/object_detection/__internal__.py,sha256=cjd5ZmGvv4OZ7BYySc2cvOFen2drOJgDNLPO2_edW1o,4329
-keras_cv/models/object_detection/__test_utils__.py,sha256=Eo0Vb9x12_33rXHlus3x6h7FLIhutbicZzp_X1RGQlQ,1904
+keras_cv/models/object_detection/__test_utils__.py,sha256=ENgNmjXwhT-92Vptq6JpaUALwp_Dc0bjbPFtb_LrOZo,1925
 keras_cv/models/object_detection/predict_utils.py,sha256=KOhXJW13_cP4KLP01fKJcUZh_wHTnRd67LoGR6S4OQw,3694
 keras_cv/models/object_detection/retinanet/__init__.py,sha256=IwrDTskFA_9eIuGebwJ4-0_B8KanlWA_PRyi3Q-2M58,898
-keras_cv/models/object_detection/retinanet/feature_pyramid.py,sha256=_wc9219mN25L8OpH8LJ7fleWSLEvCd7DQUB_7qPPFRU,2510
-keras_cv/models/object_detection/retinanet/prediction_head.py,sha256=-SACAmX69oyM6h4dqD5ySV-9-aKC84DhrvNFzbwHmXs,2841
-keras_cv/models/object_detection/retinanet/retinanet.py,sha256=RVjc8MFD-8OX29yA-jomQVEo1IR0IOJJIm-LGsZx2XU,24260
-keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py,sha256=1sB0cQRysPVi5vnnbseoBSmGgw_NNHG0Xd1hyYrYgtg,9759
-keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py,sha256=Vnr-OqPPphll3duyKmuhKYueSlgfDm2qBINzWw3OkaI,4594
+keras_cv/models/object_detection/retinanet/feature_pyramid.py,sha256=zulw-ql6-ZDYWCkucXA2lHfq8_ZtU1A0BVKjbTgFy68,3600
+keras_cv/models/object_detection/retinanet/prediction_head.py,sha256=vCfdqJpyE7y8OvKh3VKlsGO2mBkUdkb-LvadUMZ6Hxs,3231
+keras_cv/models/object_detection/retinanet/retinanet.py,sha256=iON7uX4ed4u-aRL2QVsrA3-uCr06jku3FE1KjP1-ecw,23601
+keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py,sha256=4OSKdV2ywuAcHg9VYKqiWar5VAohdtmk6E1Ra0L1lTE,10055
+keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py,sha256=0Pu02O9pO7qRSl8vqQ5eyiTVtoNeIXA4Dm6yTIVwXUs,4683
 keras_cv/models/object_detection/retinanet/retinanet_presets.py,sha256=k1G33_DMT_CXQy-F4GrEWGDeo9sJhzivMmDoPVGNQN4,1672
-keras_cv/models/object_detection/retinanet/retinanet_test.py,sha256=_eqvoWZ0AfcnsEJIX3Sbl2EsINvyHXUFI4g0vM-EvPs,11212
+keras_cv/models/object_detection/retinanet/retinanet_test.py,sha256=nkV-cH2Wp52lMKa6hsvAig2h8XgLHRQILnyTMOcbWrg,10810
 keras_cv/models/object_detection/yolo_v8/__init__.py,sha256=eUL-SD8orSL53eH7fPXWySgO2rHkJzkT3y8rL4llKtk,687
-keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py,sha256=z2llJGcJSi7sN8MpWjNuFwa3PZyVoeSULJvOEiD6JS8,7027
+keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py,sha256=oqjjhI-DoV0-LenwnFG6sMP4eJwvppxdPN9kLO51T_k,7111
 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py,sha256=LIXCwGg4xJHdibMFXB2m_KPQZLG7xbypFzZk1XfIsUc,6577
-keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py,sha256=JGRKLjRnkInmHjXiEpBlStAdzefv0j-NPhlR56-b8-0,23732
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py,sha256=yLpcxpC5JNQu69oY6bb5UcYczJSUJZN02P6jGEBrQl0,23491
 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py,sha256=Zqgwo0OkNWD2QyP2RItss733MvPXpXfteKMKQmGjgPM,1598
-keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py,sha256=a3PBLijH3NUBEJrO1tGL84HiAckEpIuorJbx0Jx3w0o,8675
-keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py,sha256=xQc73HuHaOK84_Z9l-518uCemcTiS6in6ackDoxRNps,16358
-keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py,sha256=DHmCqzwBwYi_lWuqLn8FUwGMNsXQ1I03pbzOucE3ktY,2884
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py,sha256=R1c1RquIbnW6QdKxMkaY8uhC7ZBcEPOyzVWcSHfTzvE,9404
+keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py,sha256=DZiJ1BaxEkITtlb5jXqreux5sNRd0Vg8xg7gzhxP7bI,16408
+keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py,sha256=1SMjhrOTy3GI1HiMjIKuPpFsq98oMyMP4BQ_SGft3zo,2934
 keras_cv/models/object_detection/yolox/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/object_detection/yolox/binary_crossentropy.py,sha256=HcGHiKXtAXufxvXy7FR6wOKdmXx0GODyH74xtUAJrzg,3419
 keras_cv/models/object_detection/yolox/layers/__init__.py,sha256=SRs1LNrQrUKFqBMxfq2syl2gdClaqXZdHEDJ7f35L2k,954
 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py,sha256=Qvlf1j8jBYpL9iZmK_6tWKgKbR1v0dmhqWmLdxH8eOg,6313
 keras_cv/models/object_detection/yolox/layers/yolox_head.py,sha256=mHO6fiwqQ-4ZZlUA2_uPnBeLFPLGk5nr-iKl8TNtprE,5423
 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py,sha256=6R9-KhIpJ7skYmmjz41vsNMKVokKAg9FrBN8oflUXts,1859
 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py,sha256=Zx10iyK4gULCV5RD97PuswQSSwbAqSr4Bp95I4VCroU,1923
@@ -389,26 +396,26 @@
 keras_cv/training/contrastive/simclr_trainer.py,sha256=H1WC5YqjlMF3yNnwRONkNWECTSWBOi-P63RxBWCmBAI,3199
 keras_cv/training/contrastive/simclr_trainer_test.py,sha256=yBFyhIs5APIXJIBfVMf3Yk2J8JJEdXm7Yc53rLWQzIw,1779
 keras_cv/utils/__init__.py,sha256=T29RuoUBnjaqMUw6d6_6PHf-SKQY0lvI7uOB7zyMw9Y,1408
 keras_cv/utils/conditional_imports.py,sha256=tm1Vn8ntsmw1eW2ewoZ-DRKTj2rTXipoHTamKrrI1pw,2080
 keras_cv/utils/conv_utils.py,sha256=bHkcdIJrC2XDByFQ6Lx52rKruXBJLkSq30erTSkeOko,2474
 keras_cv/utils/fill_utils.py,sha256=umSUsnPbqznINNhXjBRB-grPvdufXsbni8N4UuI_r8Y,3105
 keras_cv/utils/fill_utils_test.py,sha256=gJ8oq1yZIfBQyMpI61QnPKW4oujcJI19us_ghBVauHA,11265
-keras_cv/utils/preprocessing.py,sha256=yEXLCfwNnPlnnVD77hlmr3ZkZOtxlOIz3aAbz2pM21s,14465
+keras_cv/utils/preprocessing.py,sha256=jLHvQTMmsJYBv2UMz_TlARBSEYhvxU009GZK__ISIfE,14456
 keras_cv/utils/preprocessing_test.py,sha256=eFL6dlg9kFJ2zuHYutvz8qyNy9Skiqr8ZpLZql_7LPs,2303
 keras_cv/utils/python_utils.py,sha256=s8B82a4Pm_JZS9cwBSxh4htiRuu-VN4-wVhOA3lVr10,1803
-keras_cv/utils/resource_loader.py,sha256=LYsQkuTV30JJ1lIRfOdlhBhP2MrswTNMJ5p5YAgL1ts,2843
-keras_cv/utils/target_gather.py,sha256=b9gN5J6Y8fA1iuYAY3RtuUnrb79XfFO8rLjJGbHDvsM,4731
-keras_cv/utils/target_gather_test.py,sha256=-Gp8CBl1maluIpJbW7IT6djyOlIY7V6vCW2FkSMkvDo,5346
+keras_cv/utils/resource_loader.py,sha256=nVN8cxzdbWLZtCIaVJfl1WgjLT7Xk2lv2XnQK7qDTDo,2843
+keras_cv/utils/target_gather.py,sha256=eRtt0rz5bW0l9HFPpJawzxYNzB0AnOMLpoNnIzGUzrQ,4379
+keras_cv/utils/target_gather_test.py,sha256=HsS754XWrg0hAn3YbYjoyK8xIWCZJvDfW0koi9PZbSA,5491
 keras_cv/utils/test_utils.py,sha256=InZNiCk3HMZZ2PqTf1U_lThmXKYiPxcUhoYjxqxddAg,3628
-keras_cv/utils/to_numpy.py,sha256=Wa9tvz3RKEmIBdsaej5zzlADWkpVWSk_xL2lIAMOvMY,991
-keras_cv/utils/train.py,sha256=EsvRoaRvgeO-gaZVjQqTIBXCymOYC30MiaNg59y0G4Q,2813
+keras_cv/utils/to_numpy.py,sha256=01eTH9NjrUrl9SqM2XXeYrhGkCLXTsoaCvV8pEfsxBY,926
+keras_cv/utils/train.py,sha256=YQgJTPPeFpWIOiAjLA6Ngs9Y6HZ_1idccI3n3uc8Xnk,2820
 keras_cv/visualization/__init__.py,sha256=1R7vjZY2B7a0iPPNk94us8y-LVnGQPUtKe5Il_WYQfY,860
 keras_cv/visualization/draw_bounding_boxes.py,sha256=YqZGsARY1B47RRNsC75M50AUI0P-hCkCtNFGT5MLUH8,5497
 keras_cv/visualization/plot_bounding_box_gallery.py,sha256=yzuZVIBWLfhYqopbGPZjehPOaUYgrPdr-c2sxvl1jso,6120
 keras_cv/visualization/plot_image_gallery.py,sha256=96k2GOZzhMwkDAaIb-ZDqnUryI3JhUeQi-NmMy5hzW0,5899
 keras_cv/visualization/plot_segmentation_mask_gallery.py,sha256=hVFZXFmRXCkQ978cFgoJTueNLMrSjvKHfDiqrybzduE,4718
-keras_cv-0.5.1.dist-info/LICENSE,sha256=66xNyvXuVtDPh1pQjz-nT4O11zccCbCa8aU56W9m2-Q,11853
-keras_cv-0.5.1.dist-info/METADATA,sha256=8A6kYXcJY1Y4m5uKXeX8zmtaUETMHXjXHPbKIDj_FxI,10791
-keras_cv-0.5.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-keras_cv-0.5.1.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
-keras_cv-0.5.1.dist-info/RECORD,,
+keras_cv-0.6.0.dev0.dist-info/LICENSE,sha256=66xNyvXuVtDPh1pQjz-nT4O11zccCbCa8aU56W9m2-Q,11853
+keras_cv-0.6.0.dev0.dist-info/METADATA,sha256=fuoU5GqA_sI_445orhe4OH66mYUQsuP2zjWmr8fvZM4,10796
+keras_cv-0.6.0.dev0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+keras_cv-0.6.0.dev0.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
+keras_cv-0.6.0.dev0.dist-info/RECORD,,
```

