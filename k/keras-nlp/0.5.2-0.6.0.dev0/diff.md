# Comparing `tmp/keras-nlp-0.5.2.tar.gz` & `tmp/keras-nlp-0.6.0.dev0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "keras-nlp-0.5.2.tar", last modified: Thu May 11 22:32:34 2023, max compression
+gzip compressed data, was "keras-nlp-0.6.0.dev0.tar", last modified: Mon Jul 10 23:46:06 2023, max compression
```

## Comparing `keras-nlp-0.5.2.tar` & `keras-nlp-0.6.0.dev0.tar`

### file list

```diff
@@ -1,299 +1,334 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.536490 keras-nlp-0.5.2/
--rw-r--r--   0 runner    (1001) docker     (123)     5453 2023-05-11 22:32:34.536490 keras-nlp-0.5.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     4489 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/
--rw-r--r--   0 runner    (1001) docker     (123)      299 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/layers/
--rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      382 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/metrics/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/models/
--rw-r--r--   0 runner    (1001) docker     (123)     4389 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/samplers/
--rw-r--r--   0 runner    (1001) docker     (123)      731 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/samplers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp/src/
--rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1367 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/api_export.py
--rw-r--r--   0 runner    (1001) docker     (123)     3884 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.492489 keras-nlp-0.5.2/keras_nlp/src/layers/
--rw-r--r--   0 runner    (1001) docker     (123)     1528 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4414 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/cached_multi_head_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     2983 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/cached_multi_head_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6523 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/f_net_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5097 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/f_net_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8396 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     7281 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_head_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8902 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_mask_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     9626 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_mask_generator_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9411 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/multi_segment_packer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6745 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/multi_segment_packer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4767 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/position_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)    12708 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/position_embedding_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10875 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/random_deletion.py
--rw-r--r--   0 runner    (1001) docker     (123)     8024 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/random_deletion_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10619 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/random_swap.py
--rw-r--r--   0 runner    (1001) docker     (123)     8423 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/random_swap_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4445 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/sine_position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (123)     7087 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/sine_position_encoding_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7151 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/start_end_packer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5902 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/start_end_packer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5131 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/token_and_position_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     5625 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/token_and_position_embedding_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    15461 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    14352 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_decoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9189 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6421 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4077 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_layer_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/layers/transformer_layer_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.492489 keras-nlp-0.5.2/keras_nlp/src/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      846 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14273 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/bleu.py
--rw-r--r--   0 runner    (1001) docker     (123)     8636 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/bleu_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8278 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/edit_distance.py
--rw-r--r--   0 runner    (1001) docker     (123)    11046 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/edit_distance_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6925 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/perplexity.py
--rw-r--r--   0 runner    (1001) docker     (123)    11924 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/perplexity_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7699 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     4490 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_l.py
--rw-r--r--   0 runner    (1001) docker     (123)     7519 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_l_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5549 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_n.py
--rw-r--r--   0 runner    (1001) docker     (123)     8335 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_n_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.496489 keras-nlp-0.5.2/keras_nlp/src/models/
--rw-r--r--   0 runner    (1001) docker     (123)     4928 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.496489 keras-nlp-0.5.2/keras_nlp/src/models/albert/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10711 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4905 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7361 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5175 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5415 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7494 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6392 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5106 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7605 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6976 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5599 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7572 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4138 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3971 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4287 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.500489 keras-nlp-0.5.2/keras_nlp/src/models/bart/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9783 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5306 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    11717 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6968 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3129 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     5293 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7091 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5502 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4420 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3433 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.500489 keras-nlp-0.5.2/keras_nlp/src/models/bert/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8201 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4409 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6970 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     4561 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5441 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7616 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6044 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4482 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6952 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5947 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    12351 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     9650 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4113 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.508489 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7604 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4399 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8058 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5211 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5688 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7411 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6414 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4902 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7509 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6264 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6247 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7707 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5662 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4485 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8544 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/disentangled_attention_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    12467 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/disentangled_self_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     3290 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/relative_embedding.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.508489 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6776 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4178 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7682 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     4675 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5666 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7646 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5669 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4203 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6411 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5417 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4112 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7313 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4028 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3454 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.512489 keras-nlp-0.5.2/keras_nlp/src/models/f_net/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8144 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5900 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5115 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5330 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7547 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5671 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4746 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6389 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6263 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7125 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3441 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3957 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.516489 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10046 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5038 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    20815 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     6643 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5990 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7465 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6641 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4965 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6630 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3971 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.516489 keras-nlp-0.5.2/keras_nlp/src/models/opt/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9109 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5046 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    20464 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     6756 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6077 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7631 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6617 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5051 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5784 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     4190 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4964 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3812 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4698 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.520490 keras-nlp-0.5.2/keras_nlp/src/models/roberta/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6751 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7521 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5195 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5551 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7730 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6469 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4953 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5430 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_multi_segment_packer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7023 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6085 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3206 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     9612 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4553 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3806 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.520490 keras-nlp-0.5.2/keras_nlp/src/models/t5/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9107 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5247 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)    12518 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_multi_head_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     3402 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3945 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4615 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_transformer_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9815 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/task.py
--rw-r--r--   0 runner    (1001) docker     (123)     3183 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/task_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.524489 keras-nlp-0.5.2/keras_nlp/src/models/whisper/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/whisper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11120 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5936 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1418 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.524489 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3534 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     4413 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8058 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)     5005 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5762 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7801 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6372 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4676 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7498 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6131 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2881 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7461 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5737 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4609 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.528489 keras-nlp-0.5.2/keras_nlp/src/samplers/
--rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9266 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/beam_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4919 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/beam_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9566 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/contrastive_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     7167 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/contrastive_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2184 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/greedy_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4671 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/greedy_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2753 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/random_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4514 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/random_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6754 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/serialization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1785 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/serialization_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3053 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/top_k_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4680 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/top_k_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4408 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/top_p_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6016 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/samplers/top_p_sampler_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.528489 keras-nlp-0.5.2/keras_nlp/src/tests/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.528489 keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8078 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/docstring_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)     4960 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/docstring_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6808 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/fenced_docstring_lib.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.532490 keras-nlp-0.5.2/keras_nlp/src/tokenizers/
--rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    24894 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_pair_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6899 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_pair_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10662 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9955 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10488 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7592 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5223 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4048 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5335 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2325 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    13554 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    13896 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    18447 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10178 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7248 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7738 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.536490 keras-nlp-0.5.2/keras_nlp/src/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6226 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/keras_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2339 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/keras_utils_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9609 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/pipeline_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    23200 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/pipeline_model_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1804 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/python_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2828 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/python_utils_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2916 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/tf_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-05-11 22:32:24.000000 keras-nlp-0.5.2/keras_nlp/src/utils/tf_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.536490 keras-nlp-0.5.2/keras_nlp/tokenizers/
--rw-r--r--   0 runner    (1001) docker     (123)      769 2023-05-11 22:32:27.000000 keras-nlp-0.5.2/keras_nlp/tokenizers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-11 22:32:34.484489 keras-nlp-0.5.2/keras_nlp.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     5453 2023-05-11 22:32:34.000000 keras-nlp-0.5.2/keras_nlp.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    12867 2023-05-11 22:32:34.000000 keras-nlp-0.5.2/keras_nlp.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-11 22:32:34.000000 keras-nlp-0.5.2/keras_nlp.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-05-11 22:32:34.000000 keras-nlp-0.5.2/keras_nlp.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       10 2023-05-11 22:32:34.000000 keras-nlp-0.5.2/keras_nlp.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      602 2023-05-11 22:32:34.536490 keras-nlp-0.5.2/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     2924 2023-05-11 22:32:23.000000 keras-nlp-0.5.2/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.196850 keras-nlp-0.6.0.dev0/
+-rw-r--r--   0 runner    (1001) docker     (123)     5458 2023-07-10 23:46:06.196850 keras-nlp-0.6.0.dev0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     4489 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/
+-rw-r--r--   0 runner    (1001) docker     (123)      304 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)     1188 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      382 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/metrics/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     4293 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/samplers/
+-rw-r--r--   0 runner    (1001) docker     (123)      731 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/samplers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/src/
+-rw-r--r--   0 runner    (1001) docker     (123)     1129 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1379 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/api_export.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/
+-rw-r--r--   0 runner    (1001) docker     (123)     1237 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2276 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1446 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2142 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)      804 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/backend/random.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4103 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.164850 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6537 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/cached_multi_head_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/cached_multi_head_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7037 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/f_net_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3745 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/f_net_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9004 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/masked_lm_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5603 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/masked_lm_head_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3915 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/position_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9317 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/position_embedding_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3537 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/sine_position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4563 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/sine_position_encoding_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5482 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/token_and_position_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3860 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/token_and_position_embedding_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20778 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10900 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_decoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9511 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5034 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4117 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_layer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_layer_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.164850 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8765 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/masked_lm_mask_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7025 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/masked_lm_mask_generator_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12197 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/multi_segment_packer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6838 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/multi_segment_packer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/preprocessing_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10603 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_deletion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7106 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_deletion_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10344 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_swap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7403 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_swap_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8412 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/start_end_packer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5996 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/start_end_packer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.168850 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      846 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14531 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/bleu.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7336 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/bleu_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7882 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/edit_distance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8103 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/edit_distance_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7173 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/perplexity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9638 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/perplexity_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7983 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4446 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_l.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5114 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_l_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5508 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_n.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6495 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_n_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.168850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     5928 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.168850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10637 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4601 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7284 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5374 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5244 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7533 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6099 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4730 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7628 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6681 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5599 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7452 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4138 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3637 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4298 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.172850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9821 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4476 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10365 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6779 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4367 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5283 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21683 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_seq_2_seq_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10990 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6446 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8578 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_seq_2_seq_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4431 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3491 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.172850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8141 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4049 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6893 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4746 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5269 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7639 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5751 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4061 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6975 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5653 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12351 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9530 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4115 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3122 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.176850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7674 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4134 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8021 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5463 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5533 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7434 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4557 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7532 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5969 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6247 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7712 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5662 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4125 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8685 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/disentangled_attention_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12734 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/disentangled_self_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/relative_embedding.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.176850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6740 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7644 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4908 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5511 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7669 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5369 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3835 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5123 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4112 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7277 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4030 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3147 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.180850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8108 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3747 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5863 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5331 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5175 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7558 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5378 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4401 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6412 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5968 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7033 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3441 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3644 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10506 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/generative_task.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.180850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10044 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4602 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14774 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6903 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5695 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7032 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6659 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4670 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6630 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4215 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3663 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.180850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7512 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6306 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3827 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5849 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5739 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5128 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4724 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3257 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3685 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/rotary_embedding.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.184850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9107 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4617 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14439 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7098 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5782 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7198 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6635 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4756 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5784 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4185 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4380 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3504 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5040 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.184850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6748 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4065 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7472 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5422 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5396 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7753 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6175 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4587 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7067 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5790 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3206 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9565 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4553 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.184850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8859 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4965 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1287 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12494 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_multi_head_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3413 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3611 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5083 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_transformer_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11323 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/task.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3472 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/task_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.188850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10423 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_audio_feature_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3520 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_audio_feature_extractor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11489 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5610 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1686 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1353 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15086 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7513 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18308 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5320 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5453 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4264 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.188850 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3504 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4123 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8009 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5238 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7824 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6231 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4516 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7550 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5836 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2881 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7424 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5950 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4527 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.192850 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/
+-rw-r--r--   0 runner    (1001) docker     (123)     1235 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9549 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/beam_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5061 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/beam_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9837 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/contrastive_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7326 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/contrastive_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2213 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/greedy_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4810 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/greedy_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2912 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/random_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4634 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/random_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8688 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3383 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/serialization_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_k_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4819 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_k_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4619 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_p_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6184 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_p_sampler_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.192850 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.192850 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8078 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/docstring_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5013 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/docstring_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6808 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/fenced_docstring_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2157 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tests/test_case.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.192850 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/
+-rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24895 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_pair_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6679 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_pair_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10561 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9131 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10840 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7539 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5235 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4091 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2418 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13650 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12965 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19027 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10212 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7280 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7781 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.196850 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5849 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/keras_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2385 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/keras_utils_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10199 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/pipeline_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21612 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/pipeline_model_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1804 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/python_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2838 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/python_utils_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5826 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/tensor_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3797 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/keras_nlp/src/utils/tensor_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.196850 keras-nlp-0.6.0.dev0/keras_nlp/tokenizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      769 2023-07-10 23:46:00.000000 keras-nlp-0.6.0.dev0/keras_nlp/tokenizers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-10 23:46:06.160850 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     5458 2023-07-10 23:46:06.000000 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    14725 2023-07-10 23:46:06.000000 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-10 23:46:06.000000 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      130 2023-07-10 23:46:06.000000 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       10 2023-07-10 23:46:06.000000 keras-nlp-0.6.0.dev0/keras_nlp.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      602 2023-07-10 23:46:06.196850 keras-nlp-0.6.0.dev0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     2979 2023-07-10 23:45:53.000000 keras-nlp-0.6.0.dev0/setup.py
```

### Comparing `keras-nlp-0.5.2/PKG-INFO` & `keras-nlp-0.6.0.dev0/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: keras-nlp
-Version: 0.5.2
+Version: 0.6.0.dev0
 Summary: Industry-strength Natural Language Processing extensions for Keras.
 Home-page: https://github.com/keras-team/keras-nlp
 Author: Keras team
 Author-email: keras-nlp@google.com
 License: Apache License 2.0
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3
```

### Comparing `keras-nlp-0.5.2/README.md` & `keras-nlp-0.6.0.dev0/README.md`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/layers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/layers/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 """DO NOT EDIT.
 
 This file was autogenerated. Do not edit it by hand,
 since your modifications would be overwritten.
 """
 
 
-from keras_nlp.src.layers.cached_multi_head_attention import CachedMultiHeadAttention
-from keras_nlp.src.layers.f_net_encoder import FNetEncoder
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.random_deletion import RandomDeletion
-from keras_nlp.src.layers.random_swap import RandomSwap
-from keras_nlp.src.layers.sine_position_encoding import SinePositionEncoding
-from keras_nlp.src.layers.start_end_packer import StartEndPacker
-from keras_nlp.src.layers.token_and_position_embedding import TokenAndPositionEmbedding
-from keras_nlp.src.layers.transformer_decoder import TransformerDecoder
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.modeling.cached_multi_head_attention import CachedMultiHeadAttention
+from keras_nlp.src.layers.modeling.f_net_encoder import FNetEncoder
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.sine_position_encoding import SinePositionEncoding
+from keras_nlp.src.layers.modeling.token_and_position_embedding import TokenAndPositionEmbedding
+from keras_nlp.src.layers.modeling.transformer_decoder import TransformerDecoder
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.random_deletion import RandomDeletion
+from keras_nlp.src.layers.preprocessing.random_swap import RandomSwap
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
```

### Comparing `keras-nlp-0.5.2/keras_nlp/models/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/models/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,15 +45,14 @@
 from keras_nlp.src.models.opt.opt_causal_lm_preprocessor import OPTCausalLMPreprocessor
 from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
 from keras_nlp.src.models.roberta.roberta_classifier import RobertaClassifier
 from keras_nlp.src.models.roberta.roberta_masked_lm import RobertaMaskedLM
 from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import RobertaMaskedLMPreprocessor
-from keras_nlp.src.models.roberta.roberta_multi_segment_packer import RobertaMultiSegmentPacker
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_classifier import XLMRobertaClassifier
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm import XLMRobertaMaskedLM
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import XLMRobertaMaskedLMPreprocessor
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import XLMRobertaPreprocessor
```

### Comparing `keras-nlp-0.5.2/keras_nlp/samplers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/samplers/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -24,9 +24,9 @@
 from keras_nlp.src import metrics
 from keras_nlp.src import models
 from keras_nlp.src import samplers
 from keras_nlp.src import tokenizers
 from keras_nlp.src import utils
 
 # This is the global source of truth for the version number.
-__version__ = "0.5.2"
+__version__ = "0.6.0.dev0"
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/api_export.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/api_export.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,25 +10,25 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import types
 
-from tensorflow import keras
+from keras_nlp.src.backend import keras
 
 try:
     import namex
 except ImportError:
     namex = None
 
 
 def maybe_register_serializable(symbol):
     if isinstance(symbol, types.FunctionType) or hasattr(symbol, "get_config"):
-        keras.utils.register_keras_serializable(package="keras_nlp")(symbol)
+        keras.saving.register_keras_serializable(package="keras_nlp")(symbol)
 
 
 if namex:
 
     class keras_nlp_export(namex.export):
         def __init__(self, path):
             super().__init__(package="keras_nlp", path=path)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/conftest.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/conftest.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,20 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import os
-import sys
 
 import pytest
 import tensorflow as tf
-from packaging import version
-from tensorflow import keras
+
+from keras_nlp.src.backend import config as backend_config
+from keras_nlp.src.backend import keras
 
 
 @pytest.fixture(scope="session")
 def tpu_strategy():
     tpu_name = os.getenv("KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS")
     resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(
         tpu=tpu_name,
@@ -78,45 +78,57 @@
         "markers",
         "extra_large: mark test as being too large to run continuously",
     )
     config.addinivalue_line(
         "markers",
         "tpu: mark test as tpu test",
     )
+    config.addinivalue_line(
+        "markers",
+        "tf_only: mark test as a tf only test",
+    )
 
 
 def pytest_collection_modifyitems(config, items):
     run_extra_large_tests = config.getoption("--run_extra_large")
     # Run large tests for --run_extra_large or --run_large.
     run_large_tests = config.getoption("--run_large") or run_extra_large_tests
     run_tpu = config.getoption("--run_tpu")
 
     # Messages to annotate skipped tests with.
-    skip_xla = pytest.mark.skipif(
-        sys.platform == "darwin", reason="XLA unsupported on MacOS."
-    )
-    # Run Keras saving tests on 2.12 stable, nightlies and later releases.
-    skip_keras_saving_test = pytest.mark.skipif(
-        version.parse(tf.__version__) < version.parse("2.12.0-dev0"),
-        reason="keras_v3 format requires tf > 2.12.",
-    )
     skip_large = pytest.mark.skipif(
-        not run_large_tests, reason="need --run_large option to run"
+        not run_large_tests,
+        reason="need --run_large option to run",
     )
     skip_extra_large = pytest.mark.skipif(
-        not run_extra_large_tests, reason="need --run_extra_large option to run"
+        not run_extra_large_tests,
+        reason="need --run_extra_large option to run",
     )
     skip_tpu = pytest.mark.skipif(
-        not run_tpu, reason="need --run_tpu option to run"
+        not run_tpu,
+        reason="need --run_tpu option to run",
+    )
+    skip_tf_only = pytest.mark.skipif(
+        not backend_config.backend() == "tensorflow",
+        reason="tests only run on tf backend",
     )
     for item in items:
-        if "jit_compile_true" in item.name:
-            item.add_marker(skip_xla)
-        if "keras_format" in item.name:
-            item.add_marker(skip_keras_saving_test)
         if "large" in item.keywords:
             item.add_marker(skip_large)
         if "extra_large" in item.keywords:
             item.add_marker(skip_extra_large)
         if "tpu" in item.keywords:
             item.add_marker(skip_tpu)
+        if "tf_only" in item.keywords:
+            item.add_marker(skip_tf_only)
+
+
+# Disable traceback filtering for quicker debugging of tests failures.
+tf.debugging.disable_traceback_filtering()
+if backend_config.multi_backend():
+    keras.config.disable_traceback_filtering()
+
+# One off setup for dtensor tests.
+if not backend_config.multi_backend():
+    keras.backend.experimental.enable_tf_random_generator()
+    keras.utils.set_random_seed(1337)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,25 +8,31 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras_nlp.src.layers.cached_multi_head_attention import (
+from keras_nlp.src.layers.modeling.cached_multi_head_attention import (
     CachedMultiHeadAttention,
 )
-from keras_nlp.src.layers.f_net_encoder import FNetEncoder
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.random_deletion import RandomDeletion
-from keras_nlp.src.layers.random_swap import RandomSwap
-from keras_nlp.src.layers.sine_position_encoding import SinePositionEncoding
-from keras_nlp.src.layers.start_end_packer import StartEndPacker
-from keras_nlp.src.layers.token_and_position_embedding import (
+from keras_nlp.src.layers.modeling.f_net_encoder import FNetEncoder
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.sine_position_encoding import (
+    SinePositionEncoding,
+)
+from keras_nlp.src.layers.modeling.token_and_position_embedding import (
     TokenAndPositionEmbedding,
 )
-from keras_nlp.src.layers.transformer_decoder import TransformerDecoder
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.modeling.transformer_decoder import TransformerDecoder
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
+from keras_nlp.src.layers.preprocessing.random_deletion import RandomDeletion
+from keras_nlp.src.layers.preprocessing.random_swap import RandomSwap
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/cached_multi_head_attention_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/cached_multi_head_attention_test.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,74 +9,70 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for CachedMultiHeadAttention."""
 
-import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
-
-from keras_nlp.src.layers.cached_multi_head_attention import (
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling.cached_multi_head_attention import (
     CachedMultiHeadAttention,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class CachedMultiHeadAttentionTest(tf.test.TestCase, parameterized.TestCase):
+class CachedMultiHeadAttentionTest(TestCase):
     def test_valid_call(self):
         layer = CachedMultiHeadAttention(num_heads=2, key_dim=4)
-        x = tf.random.uniform(shape=[2, 2, 8])
+        x = ops.random.uniform(shape=(2, 2, 8))
         layer(query=x, value=x)
 
-    @parameterized.named_parameters(
-        ("graph", False),
-        ("eager", True),
-    )
-    def test_cache_call_is_correct(self, eager):
+    def test_cache_call_is_correct(self):
         batch_size = 2
         seq_len = 5
         num_heads = 2
         key_dim = 4
+        hidden_dim = num_heads * key_dim
+
+        input_shape = (batch_size, seq_len, hidden_dim)
+        x = ops.random.uniform(shape=input_shape)
+        input_cache = ops.zeros((batch_size, 2, seq_len, num_heads, key_dim))
+        # Use a causal mask.
+        mask = ops.tril(ops.ones((seq_len, seq_len)))
+        outputs = ops.zeros_like(x)
 
         layer = CachedMultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
-        dtype = layer.compute_dtype
-        x = tf.random.uniform(
-            shape=[batch_size, seq_len, num_heads * key_dim], dtype=dtype
-        )
-        cache = tf.zeros(
-            [batch_size, 2, seq_len, num_heads, key_dim], dtype=dtype
+        no_loop_outputs, no_loop_cache = layer(
+            x,
+            x,
+            cache=input_cache,
+            cache_update_index=0,
+            attention_mask=mask,
         )
-        # Use a causal mask.
-        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
-        outputs = tf.zeros_like(x)
 
-        def call(outputs, cache):
-            def loop_body(i, outputs, cache):
-                # Compute the rest tokens.
-                next_input = x[:, i : i + 1, :]
-                next_mask = mask[i : i + 1, :]
-                next_output, cache = layer(
-                    query=next_input,
-                    value=next_input,
-                    cache=cache,
-                    cache_index=i,
-                    attention_mask=next_mask,
-                )
-                outputs = dynamic_update_slice(outputs, next_output, [0, i, 0])
-                return i + 1, outputs, cache
+        def loop_body(i, outputs, cache):
+            # Compute the rest tokens.
+            next_input = ops.slice(x, (0, i, 0), (batch_size, 1, hidden_dim))
+            next_mask = ops.slice(mask, (i, 0), (1, seq_len))
+            next_output, cache = layer(
+                query=next_input,
+                value=next_input,
+                cache=cache,
+                cache_update_index=i,
+                attention_mask=next_mask,
+            )
+            outputs = ops.slice_update(outputs, [0, i, 0], next_output)
+            return i + 1, outputs, cache
 
-            _, outputs, cache = tf.while_loop(
+        def call(outputs, cache):
+            _, outputs, cache = ops.while_loop(
                 cond=lambda i, outputs, cache: i < seq_len,
                 body=loop_body,
                 loop_vars=[0, outputs, cache],
             )
             return outputs, cache
 
-        call = call if eager else tf.function(call)
-        output, cache = call(outputs, cache)
+        output, output_cache = call(outputs, input_cache)
 
-        no_loop_outputs, _ = layer(x, x, attention_mask=mask)
-        _, no_loop_cache = layer(x, x, cache=cache, attention_mask=mask)
         self.assertAllClose(output, no_loop_outputs)
-        self.assertAllClose(cache, no_loop_cache)
+        self.assertAllClose(output_cache, no_loop_cache)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/f_net_encoder.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/f_net_encoder.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,18 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """FNet encoder block implementation based on `keras.layers.Layer`."""
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.utils.keras_utils import clone_initializer
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
 @keras_nlp_export("keras_nlp.layers.FNetEncoder")
 class FNetEncoder(keras.layers.Layer):
     """FNet encoder.
 
     This class follows the architecture of FNet encoder layer in the
@@ -33,26 +34,28 @@
     the input. However, the padding masks are deleted, i.e., mixing of
     all tokens is done. This is because certain frequencies will be zeroed
     out if we apply padding masks in every encoder layer. Hence, we don't
     take padding mask as input in the call() function.
 
     Args:
         intermediate_dim: int. The hidden size of feedforward network.
-        dropout: float, defaults to 0. The dropout value, applied in the
-            feedforward network.
-        activation: string or `keras.activations`, defaults to "relu". The
+        dropout: float. The dropout value, applied in the
+            feedforward network. Defaults to `0.`.
+        activation: string or `keras.activations`. The
             activation function of feedforward network.
-        layer_norm_epsilon: float, defaults to 1e-5. The epsilon value in layer
-            normalization components.
-        kernel_initializer: "string" or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for the dense
-            layers.
-        bias_initializer: "string" or `keras.initializers` initializer,
-            defaults to "zeros". The bias initializer for the dense layers.
-        name: string, defaults to None. The name of the layer.
+            Defaults to `"relu"`.
+        layer_norm_epsilon: float. The epsilon value in layer
+            normalization components. Defaults to `1e-5`.
+        kernel_initializer: `str` or `keras.initializers` initializer.
+            The kernel initializer for the dense layers.
+            Defaults to `"glorot_uniform"`.
+        bias_initializer: "string" or `keras.initializers` initializer.
+            The bias initializer for the dense layers.
+            Defaults to `"zeros"`.
+        name: string. The name of the layer. Defaults to `None`.
         **kwargs: other keyword arguments.
 
     Examples:
 
     ```python
     # Create a single FNet encoder layer.
     encoder = keras_nlp.layers.FNetEncoder(
@@ -79,46 +82,54 @@
         activation="relu",
         layer_norm_epsilon=1e-5,
         kernel_initializer="glorot_uniform",
         bias_initializer="zeros",
         name=None,
         **kwargs
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(name=name, **kwargs)
         self.intermediate_dim = intermediate_dim
         self.dropout = dropout
         self.activation = keras.activations.get(activation)
         self.layer_norm_epsilon = layer_norm_epsilon
         self.kernel_initializer = keras.initializers.get(kernel_initializer)
         self.bias_initializer = keras.initializers.get(bias_initializer)
 
-    def build(self, input_shape):
+    def build(self, inputs_shape):
         # Create layers based on input shape.
-        feature_size = input_shape[-1]
+        feature_size = inputs_shape[-1]
 
         # Layer Norm layers.
         self._mixing_layer_norm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon
         )
+        self._mixing_layer_norm.build(inputs_shape)
         self._output_layer_norm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon
         )
+        self._output_layer_norm.build(inputs_shape)
 
         # Feedforward layers.
         self._intermediate_dense = keras.layers.Dense(
             self.intermediate_dim,
             activation=self.activation,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        self._intermediate_dense.build(inputs_shape)
         self._output_dense = keras.layers.Dense(
             feature_size,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        self._output_dense.build(
+            self._intermediate_dense.compute_output_shape(inputs_shape)
+        )
         self._output_dropout = keras.layers.Dropout(rate=self.dropout)
 
     def call(self, inputs):
         """Forward pass of the FNetEncoder.
 
         Args:
             inputs: a Tensor. The input data to TransformerEncoder, should be
@@ -171,7 +182,10 @@
                 "bias_initializer": keras.initializers.serialize(
                     self.bias_initializer
                 ),
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape):
+        return inputs_shape
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/f_net_encoder_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_encoder_test.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,138 +7,144 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for FNet Encoder."""
+"""Tests for Transformer Encoder."""
 
 import os
 
-import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.layers import f_net_encoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling import transformer_encoder
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetEncoderTest(tf.test.TestCase, parameterized.TestCase):
-    def test_valid_call(self):
-        encoder = f_net_encoder.FNetEncoder(intermediate_dim=4)
+class TransformerEncoderTest(TestCase):
+    @parameterized.named_parameters(
+        ("without_norm_first", False),
+        ("with_norm_first", True),
+    )
+    def test_valid_call(self, normalize_first):
+        encoder = transformer_encoder.TransformerEncoder(
+            intermediate_dim=4,
+            num_heads=2,
+            normalize_first=normalize_first,
+        )
         model = keras.Sequential(
             [
                 keras.Input(shape=(4, 6)),
                 encoder,
             ]
         )
-        input = tf.random.uniform(shape=[2, 4, 6])
+        input = ops.random.uniform(shape=[2, 4, 6])
         model(input)
 
+    def test_valid_call_with_mask(self):
+        encoder = transformer_encoder.TransformerEncoder(
+            intermediate_dim=4,
+            num_heads=2,
+        )
+        encoder.build([2, 4, 6])
+        input = ops.random.uniform(shape=[2, 4, 6])
+        mask = input[:, :, 0] < 0.5
+        encoder(input, mask)
+
     def test_get_config_and_from_config(self):
-        encoder = f_net_encoder.FNetEncoder(
+        encoder = transformer_encoder.TransformerEncoder(
             intermediate_dim=4,
+            num_heads=2,
             kernel_initializer="HeNormal",
             bias_initializer="Zeros",
+            normalize_first=True,
         )
+
         config = encoder.get_config()
+
         expected_config_subset = {
             "intermediate_dim": 4,
+            "num_heads": 2,
             "dropout": 0,
             "activation": "relu",
-            "layer_norm_epsilon": 1e-5,
+            "layer_norm_epsilon": 1e-05,
             "kernel_initializer": keras.initializers.serialize(
                 keras.initializers.HeNormal()
             ),
             "bias_initializer": keras.initializers.serialize(
                 keras.initializers.Zeros()
             ),
+            "normalize_first": True,
         }
+
         self.assertEqual(config, {**config, **expected_config_subset})
 
-        restored_encoder = f_net_encoder.FNetEncoder.from_config(
+        restored_encoder = transformer_encoder.TransformerEncoder.from_config(
             config,
         )
+
         self.assertEqual(
             restored_encoder.get_config(), {**config, **expected_config_subset}
         )
 
-    def test_value_error_when_invalid_kernel_initializer(self):
+    def test_value_error_when_invalid_kernel_inititalizer(self):
         with self.assertRaises(ValueError):
-            f_net_encoder.FNetEncoder(
+            transformer_encoder.TransformerEncoder(
                 intermediate_dim=4,
+                num_heads=2,
                 dropout=0.5,
                 kernel_initializer="Invalid",
             )
 
-    def test_one_training_step_of_f_net_encoder(self):
-        encoder = f_net_encoder.FNetEncoder(intermediate_dim=4)
+    def test_one_training_step_of_transformer_encoder(self):
+        encoder = transformer_encoder.TransformerEncoder(
+            intermediate_dim=4,
+            num_heads=2,
+        )
         inputs = keras.Input(shape=(4, 6))
         x = encoder(inputs)
         x = keras.layers.Dense(1, activation="sigmoid")(x)
         model = keras.Model(inputs=inputs, outputs=x)
 
-        data = tf.random.uniform(shape=[2, 4, 6])
-        label = tf.cast(data[:, :, 0] >= 0.5, dtype=tf.int32)
+        data = ops.random.uniform(shape=[2, 4, 6])
+        label = ops.random.uniform(minval=0, maxval=2, shape=[2, 4, 1])
 
-        loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)
+        loss = keras.losses.MeanSquaredError()
         optimizer = keras.optimizers.Adam()
-        with tf.GradientTape() as tape:
-            pred = model(data)
-            loss = loss_fn(label, pred)
-        grad = tape.gradient(loss, model.trainable_variables)
-        self.assertGreater(len(grad), 1)
-        optimizer.apply_gradients(zip(grad, model.trainable_variables))
-
-    def test_checkpointing_f_net_encoder(self):
-        encoder1 = f_net_encoder.FNetEncoder(
-            intermediate_dim=4,
-        )
+        model.compile(loss=loss, optimizer=optimizer)
+        loss = model.train_on_batch(x=data, y=label)
+        self.assertGreater(loss, 0)
 
-        encoder2 = f_net_encoder.FNetEncoder(
+    def test_mask_propagation(self):
+        encoder = transformer_encoder.TransformerEncoder(
             intermediate_dim=4,
+            num_heads=2,
         )
-        data = tf.random.uniform(shape=[2, 4, 6])
-        encoder1(data)
-        encoder2(data)
-        # The weights of encoder1 and encoder2 are different.
-        self.assertFalse(
-            all(
-                encoder1._output_dense.trainable_variables[0][0]
-                == encoder2._output_dense.trainable_variables[0][0]
-            )
-        )
-        checkpoint = tf.train.Checkpoint(encoder1)
-        checkpoint2 = tf.train.Checkpoint(encoder2)
-        temp_dir = self.get_temp_dir()
-        save_path = checkpoint.save(temp_dir)
-        checkpoint2.restore(save_path)
-
-        encoder1_output = encoder1(data)
-        encoder2_output = encoder2(data)
-        self.assertAllClose(encoder1_output, encoder2_output)
+        inputs = ops.random.uniform(shape=[1, 4, 6])
+        mask = ops.array([[True, True, False, False]])
+        inputs._keras_mask = mask
+        outputs = encoder(inputs)
+        self.assertAllEqual(outputs._keras_mask, mask)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = keras.Sequential(
             [
                 keras.Input(shape=(4, 6)),
-                f_net_encoder.FNetEncoder(
+                transformer_encoder.TransformerEncoder(
                     intermediate_dim=4,
+                    num_heads=2,
+                    normalize_first=True,
                 ),
             ]
         )
-        data = tf.random.uniform(shape=[2, 4, 6])
-        model(data)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
-        loaded_model = keras.models.load_model(path)
-
+        data = ops.random.uniform(shape=[2, 4, 6])
         model_output = model(data)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
+
+        loaded_model = keras.models.load_model(path)
         loaded_model_output = loaded_model(data)
         self.assertAllClose(model_output, loaded_model_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_head.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/masked_lm_head.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,18 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Masked Language Model (MaskedLM) head."""
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 
 
 @keras_nlp_export("keras_nlp.layers.MaskedLMHead")
 class MaskedLMHead(keras.layers.Layer):
     """Masked Language Model (MaskedLM) head.
 
     This layer takes two inputs:
@@ -52,42 +51,44 @@
             will be used to project a token embedding vector to a prediction
             over all input words, as described
             [here](https://arxiv.org/abs/1608.05859).
         intermediate_activation: The activation function of inner dense layer.
         activation: The activation function for the outputs of the layer.
             Usually either `None` (return logits), or `"softmax"`
             (return probabilities).
-        layer_norm_epsilon: float, defaults to 1e-5. The epsilon value in layer
-            normalization components.
-        kernel_initializer: string or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for
-            the dense and multiheaded attention layers.
-        bias_initializer: string or `keras.initializers` initializer,
-            defaults to "zeros". The bias initializer for
-            the dense and multiheaded attention layers.
-        name: string, defaults to None. The name of the layer.
+        layer_norm_epsilon: float. The epsilon value in layer
+            normalization components. Defaults to `1e-5`.
+        kernel_initializer: string or `keras.initializers` initializer.
+            The kernel initializer for the dense and multiheaded
+            attention layers. Defaults to `"glorot_uniform"`.
+        bias_initializer: string or `keras.initializers` initializer.
+            The bias initializer for the dense and multiheaded
+            attention layers. Defaults to `"zeros"`.
+        name: string. The name of the layer. Defaults to `None`.
         **kwargs: other keyword arguments.
 
     Examples:
 
     ```python
     batch_size = 32
     vocab_size = 100
     encoding_size = 32
     seq_length = 50
     mask_length = 10
 
     # Generate a random encoding.
-    encoded_tokens = tf.random.normal([batch_size, seq_length, encoding_size])
+    encoded_tokens = np.random.normal(
+        size=(batch_size, seq_length, encoding_size),
+    )
     # Generate random positions and labels
-    mask_positions = tf.random.uniform(
-        [batch_size, mask_length], maxval=seq_length, dtype="int32"
+    mask_positions = np.random.randint(
+        seq_length, size=(batch_size, mask_length),
     )
-    mask_ids = tf.random.uniform(
-        [batch_size, mask_length], maxval=vocab_size, dtype="int32"
+    mask_ids = np.random.randint(
+        vocab_size, size=(batch_size, mask_length),
     )
 
     # Predict an output word for each masked input token.
     mask_preds = keras_nlp.layers.MaskedLMHead(
         vocabulary_size=vocab_size,
         activation="softmax",
     )(encoded_tokens, mask_positions=mask_positions)
@@ -137,60 +138,65 @@
                     "`vocabulary_size` should match the first dimension of the "
                     "shape of `embedding_weights`. Received: "
                     f"`vocabulary_size={vocabulary_size}`, "
                     f"`embedding_weights.shape={shape}`"
                 )
             self.vocabulary_size = shape[0]
 
-    def build(self, input_shapes):
+    def build(self, inputs_shape, masked_positions_shape=None):
         if self.embedding_weights is not None:
             feature_size = self.embedding_weights.shape[-1]
         else:
-            feature_size = input_shapes[-1]
+            feature_size = inputs_shape[-1]
 
         self._dense = keras.layers.Dense(
             feature_size,
             activation=self.intermediate_activation,
             kernel_initializer=self.kernel_initializer,
             bias_initializer=self.bias_initializer,
         )
         self._layer_norm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon,
         )
+        if masked_positions_shape:
+            gather_length = masked_positions_shape[1]
+            shape = (inputs_shape[0], gather_length, inputs_shape[-1])
+            self._dense.build(shape)
+            shape = (inputs_shape[0], gather_length, feature_size)
+            self._layer_norm.build(shape)
         if self.embedding_weights is None:
             self._kernel = self.add_weight(
                 name="output_kernel",
                 shape=[feature_size, self.vocabulary_size],
                 initializer=self.kernel_initializer,
                 dtype=self.dtype,
             )
         self._bias = self.add_weight(
             name="output_bias",
             shape=[self.vocabulary_size],
             initializer=self.bias_initializer,
             dtype=self.dtype,
         )
 
-    def call(self, inputs, mask_positions):
+    def call(self, inputs, masked_positions):
         # Gather the encoded tokens at the masked indices.
-        x = tf.gather(inputs, mask_positions, axis=1, batch_dims=1)
+        masked_positions = ops.expand_dims(masked_positions, axis=-1)
+        x = ops.take_along_axis(inputs, masked_positions, axis=1)
 
         # Apply a trainable linear transformation and a layer norm.
         x = self._dense(x)
         x = self._layer_norm(x)
 
         # Transform encodings to vocabulary_size predictions.
         if self.embedding_weights is None:
-            outputs = tf.matmul(x, self._kernel)
+            kernel = self._kernel
         else:
-            outputs = tf.matmul(
-                x,
-                tf.cast(self.embedding_weights, self.compute_dtype),
-                transpose_b=True,
-            )
+            kernel = ops.cast(self.embedding_weights, self.compute_dtype)
+            kernel = ops.transpose(kernel)
+        outputs = ops.matmul(x, kernel)
         outputs = outputs + self._bias
 
         # Apply a final activation.
         if self.activation is not None:
             outputs = self.activation(outputs)
 
         return outputs
@@ -211,7 +217,12 @@
                 "bias_initializer": keras.initializers.serialize(
                     self.bias_initializer
                 ),
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape, masked_positions_shape):
+        output_shape = list(masked_positions_shape)
+        output_shape[-1] = self.vocabulary_size
+        return tuple(output_shape)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_head_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/masked_lm_head_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,56 +11,51 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Transformer Encoder."""
 
 import os
 
-import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling import masked_lm_head
+from keras_nlp.src.tests.test_case import TestCase
 
-from keras_nlp.src.layers import masked_lm_head
 
-
-class MaskedLMHeadTest(tf.test.TestCase, parameterized.TestCase):
+class MaskedLMHeadTest(TestCase):
     def test_valid_call(self):
         head = masked_lm_head.MaskedLMHead(
             vocabulary_size=100,
             activation="softmax",
         )
         encoded_tokens = keras.Input(shape=(10, 16))
         positions = keras.Input(shape=(5,), dtype="int32")
-        outputs = head(encoded_tokens, mask_positions=positions)
+        outputs = head(encoded_tokens, masked_positions=positions)
         model = keras.Model((encoded_tokens, positions), outputs)
 
-        token_data = tf.random.uniform(shape=(4, 10, 16))
-        position_data = tf.random.uniform(
-            shape=(4, 5), maxval=10, dtype="int32"
-        )
+        token_data = ops.random.uniform(shape=(4, 10, 16))
+        position_data = ops.random.randint(minval=0, maxval=10, shape=(4, 5))
         model((token_data, position_data))
 
     def test_valid_call_with_embedding_weights(self):
         embedding = keras.layers.Embedding(100, 16)
         embedding.build((4, 10))
         head = masked_lm_head.MaskedLMHead(
             vocabulary_size=100,
             embedding_weights=embedding.embeddings,
             activation="softmax",
         )
         # Use a difference "hidden dim" for the model than "embedding dim", we
         # need to support this in the layer.
         sequence = keras.Input(shape=(10, 32))
         positions = keras.Input(shape=(5,), dtype="int32")
-        outputs = head(sequence, mask_positions=positions)
+        outputs = head(sequence, masked_positions=positions)
         model = keras.Model((sequence, positions), outputs)
-        sequence_data = tf.random.uniform(shape=(4, 10, 32))
-        position_data = tf.random.uniform(
-            shape=(4, 5), maxval=10, dtype="int32"
-        )
+        sequence_data = ops.random.uniform(shape=(4, 10, 32))
+        position_data = ops.random.randint(minval=0, maxval=10, shape=(4, 5))
         model((sequence_data, position_data))
 
     def test_get_config_and_from_config(self):
         head = masked_lm_head.MaskedLMHead(
             vocabulary_size=100,
             kernel_initializer="HeNormal",
             bias_initializer="Zeros",
@@ -107,81 +102,40 @@
 
     def test_one_train_step(self):
         head = masked_lm_head.MaskedLMHead(
             vocabulary_size=100,
         )
         encoded_tokens = keras.Input(shape=(10, 16))
         positions = keras.Input(shape=(5,), dtype="int32")
-        outputs = head(encoded_tokens, mask_positions=positions)
+        outputs = head(encoded_tokens, masked_positions=positions)
         model = keras.Model((encoded_tokens, positions), outputs)
 
-        token_data = tf.random.uniform(shape=(4, 10, 16))
-        position_data = tf.random.uniform(
-            shape=(4, 5), maxval=10, dtype="int32"
-        )
-        label_data = tf.random.uniform(shape=(4, 5), maxval=100, dtype="int32")
+        token_data = ops.random.uniform(shape=(4, 10, 16))
+        position_data = ops.random.randint(minval=0, maxval=10, shape=(4, 5))
+        label_data = ops.random.randint(minval=0, maxval=2, shape=(4, 5, 1))
 
-        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
+        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
         optimizer = keras.optimizers.Adam()
-        with tf.GradientTape() as tape:
-            model((token_data, position_data))
-            pred = model((token_data, position_data))
-            loss = loss_fn(label_data, pred)
-        grad = tape.gradient(loss, model.trainable_variables)
-        self.assertGreater(len(grad), 1)
-        optimizer.apply_gradients(zip(grad, model.trainable_variables))
+        model.compile(loss=loss, optimizer=optimizer)
+        loss = model.train_on_batch(x=(token_data, position_data), y=label_data)
+        self.assertGreater(loss, 0)
 
-    def test_checkpointing(self):
-        head1 = masked_lm_head.MaskedLMHead(
-            vocabulary_size=100,
-            activation="softmax",
-        )
-        head2 = masked_lm_head.MaskedLMHead(
-            vocabulary_size=100,
-            activation="softmax",
-        )
-        token_data = tf.random.uniform(shape=(4, 10, 16))
-        position_data = tf.random.uniform(
-            shape=(4, 5), maxval=10, dtype="int32"
-        )
-        # The weights of head1 and head2 are different.
-        head1_output = head1(token_data, mask_positions=position_data)
-        head2_output = head2(token_data, mask_positions=position_data)
-        self.assertNotAllClose(head1_output, head2_output)
-
-        checkpoint = tf.train.Checkpoint(head1)
-        checkpoint2 = tf.train.Checkpoint(head2)
-        save_path = checkpoint.save(self.get_temp_dir())
-        checkpoint2.restore(save_path)
-
-        head1_output = head1(token_data, mask_positions=position_data)
-        head2_output = head2(token_data, mask_positions=position_data)
-        self.assertAllClose(head1_output, head2_output)
-
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         head = masked_lm_head.MaskedLMHead(
             vocabulary_size=100,
             activation="softmax",
         )
         encoded_tokens = keras.Input(shape=(10, 16))
         positions = keras.Input(shape=(5,), dtype="int32")
-        outputs = head(encoded_tokens, mask_positions=positions)
+        outputs = head(encoded_tokens, masked_positions=positions)
         model = keras.Model((encoded_tokens, positions), outputs)
 
-        token_data = tf.random.uniform(shape=(4, 10, 16))
-        position_data = tf.random.uniform(
-            shape=(4, 5), maxval=10, dtype="int32"
-        )
+        token_data = ops.random.uniform(shape=(4, 10, 16))
+        position_data = ops.random.randint(minval=0, maxval=10, shape=(4, 5))
         model_output = model((token_data, position_data))
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         restored_output = restored_model((token_data, position_data))
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_mask_generator.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/masked_lm_mask_generator.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,64 +9,68 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
 
 try:
     import tensorflow_text as tf_text
 except ImportError:
     tf_text = None
 
 
 @keras_nlp_export("keras_nlp.layers.MaskedLMMaskGenerator")
-class MaskedLMMaskGenerator(keras.layers.Layer):
+class MaskedLMMaskGenerator(PreprocessingLayer):
     """Layer that applies language model masking.
 
     This layer is useful for preparing inputs for masked language modeling
-    (MaskedLM) tasks. It follows the masking strategy described in the [original BERT
-    paper](https://arxiv.org/abs/1810.04805). Given tokenized text,
-    it randomly selects certain number of tokens for masking. Then for each
-    selected token, it has a chance (configurable) to be replaced by
+    (MaskedLM) tasks. It follows the masking strategy described in the
+    [original BERT paper](https://arxiv.org/abs/1810.04805). Given tokenized
+    text, it randomly selects certain number of tokens for masking. Then for
+    each selected token, it has a chance (configurable) to be replaced by
     "mask token" or random token, or stay unchanged.
 
-    Users should use this layer with `tf.data` to generate masks.
+    Input data should be passed as tensors, `tf.RaggedTensor`s, or lists. For
+    batched input, inputs should be a list of lists or a rank two tensor. For
+    unbatched inputs, each element should be a list or a rank one tensor.
+
+    This layer can be used with `tf.data` to generate dynamic masks on the fly
+    during training.
 
     Args:
         vocabulary_size: int, the size of the vocabulary.
         mask_selection_rate: float, the probability of a token is selected for
             masking.
         mask_token_id: int. The id of mask token.
-        mask_selection_length: int, defaults to None. Maximum number of tokens
+        mask_selection_length: int. Maximum number of tokens
             selected for  masking in each sequence. If set, the output
             `mask_positions`, `mask_ids` and `mask_weights` will be padded
-            to dense tensors of length `mask_selection_length`,
-            otherwise the output will be a RaggedTensor.
-        unselectable_token_ids: A list of tokens, defaults to [0] (the default
-            `padding_token_id`).
-        mask_token_rate: float, defaults to 0.8. `mask_token_rate` must be
+            to dense tensors of length `mask_selection_length`, otherwise
+            the output will be a RaggedTensor. Defaults to `None`.
+        unselectable_token_ids: A list of tokens id that should not be
+            considered eligible for masking. By default, we assume `0`
+            corresponds to a padding token and ignore it. Defaults to `[0]`.
+        mask_token_rate: float. `mask_token_rate` must be
             between 0 and 1 which indicates how often the mask_token is
-            substituted for tokens selected for masking.
-        random_token_rate: float, defaults to 0.1. `random_token_rate` must be
+            substituted for tokens selected for masking. Defaults to `0.8`.
+        random_token_rate: float. `random_token_rate` must be
             between 0 and 1 which indicates how often a random token is
-            substituted for tokens selected for masking. Default is 0.1.
+            substituted for tokens selected for masking.
             Note: mask_token_rate + random_token_rate <= 1,  and for
             (1 - mask_token_rate - random_token_rate), the token will not be
-            changed.
-
-    Input:
-        A 1D integer tensor of shape [sequence_length] or a 2D integer tensor
-        of shape [batch_size, sequence_length], or a 2D integer RaggedTensor.
-        Represents the sequence to mask.
+            changed. Defaults to `0.1`.
 
     Returns:
         A Dict with 4 keys:
             token_ids: Tensor or RaggedTensor, has the same type and shape of
                 input. Sequence after getting masked.
             mask_positions: Tensor, or RaggedTensor if `mask_selection_length`
                 is None. The positions of token_ids getting masked.
@@ -85,40 +89,39 @@
     masker = keras_nlp.layers.MaskedLMMaskGenerator(
         vocabulary_size=10,
         mask_selection_rate=0.2,
         mask_token_id=0,
         mask_selection_length=5
     )
     # Dense input.
-    masker(tf.constant([1, 2, 3, 4, 5]))
+    masker([1, 2, 3, 4, 5])
 
     # Ragged input.
-    masker(tf.ragged.constant([[1, 2], [1, 2, 3, 4]]))
+    masker([[1, 2], [1, 2, 3, 4]])
     ```
 
     Masking a batch that contains special tokens.
     ```python
     pad_id, cls_id, sep_id, mask_id = 0, 1, 2, 3
-    batch = tf.constant([
+    batch = [
         [cls_id,   4,    5,      6, sep_id,    7,    8, sep_id, pad_id, pad_id],
         [cls_id,   4,    5, sep_id,      6,    7,    8,      9, sep_id, pad_id],
-    ])
+    ]
 
     masker = keras_nlp.layers.MaskedLMMaskGenerator(
         vocabulary_size = 10,
         mask_selection_rate = 0.2,
         mask_selection_length = 5,
         mask_token_id = mask_id,
         unselectable_token_ids = [
             cls_id,
             sep_id,
             pad_id,
         ]
     )
-
     masker(batch)
     ```
     """
 
     def __init__(
         self,
         vocabulary_size,
@@ -129,14 +132,15 @@
         mask_token_rate=0.8,
         random_token_rate=0.1,
         **kwargs,
     ):
         assert_tf_text_installed(self.__class__.__name__)
 
         super().__init__(**kwargs)
+
         self.vocabulary_size = vocabulary_size
         self.unselectable_token_ids = unselectable_token_ids
         self.mask_selection_rate = mask_selection_rate
         self.mask_selection_length = mask_selection_length
         self.mask_token_rate = mask_token_rate
         self.random_token_rate = random_token_rate
 
@@ -160,47 +164,39 @@
             self.vocabulary_size,
             self.mask_token_id,
             mask_token_rate=self.mask_token_rate,
             random_token_rate=self.random_token_rate,
         )
 
     def call(self, inputs):
-        input_is_ragged = isinstance(inputs, tf.RaggedTensor)
-        input_is_1d = inputs.shape.rank == 1
-        if input_is_1d:
-            # If inputs is of rank 1, we manually add the batch axis.
-            inputs = inputs[tf.newaxis, :]
-        if not input_is_ragged:
-            # `tf_text.mask_language_model` requires a ragged tensor, so
-            # convert dense to ragged.
-            inputs = tf.RaggedTensor.from_tensor(inputs)
+        inputs, unbatched, rectangular = convert_to_ragged_batch(inputs)
 
         (
             token_ids,
             mask_positions,
             mask_ids,
         ) = tf_text.mask_language_model(
             inputs,
             item_selector=self._random_selector,
             mask_values_chooser=self._mask_values_chooser,
         )
 
-        if not input_is_ragged:
+        if rectangular:
             # If we converted the input from dense to ragged, convert back.
             token_ids = token_ids.to_tensor()
 
         mask_weights = tf.ones_like(mask_positions, self.compute_dtype)
         # If `mask_selection_length` is set, convert to dense.
         if self.mask_selection_length:
-            target_shape = tf.cast([-1, self.mask_selection_length], tf.int64)
+            target_shape = tf.cast([-1, self.mask_selection_length], "int64")
             mask_positions = mask_positions.to_tensor(shape=target_shape)
             mask_ids = mask_ids.to_tensor(shape=target_shape)
             mask_weights = mask_weights.to_tensor(shape=target_shape)
 
-        if input_is_1d:
+        if unbatched:
             # If inputs is 1D, we format the output to be 1D as well.
             token_ids = tf.squeeze(token_ids, axis=0)
             mask_positions = tf.squeeze(mask_positions, axis=0)
             mask_ids = tf.squeeze(mask_ids, axis=0)
             mask_weights = tf.squeeze(mask_weights, axis=0)
 
         return {
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/masked_lm_mask_generator_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/masked_lm_mask_generator_test.py`

 * *Files 25% similar despite different names*

```diff
@@ -8,20 +8,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class MaskedLMMaskGeneratorTest(tf.test.TestCase):
+class MaskedLMMaskGeneratorTest(TestCase):
     def setUp(self):
         super().setUp()
         self.VOCAB = [
             "[UNK]",
             "[MASK]",
             "[RANDOM]",
             "[CLS]",
@@ -34,155 +38,90 @@
             "welcome",
             "to",
             "keras",
         ]
         self.mask_token_id = self.VOCAB.index("[MASK]")
         self.vocabulary_size = len(self.VOCAB)
 
-    def test_mask_ragged_tensor(self):
+    def test_mask_ragged(self):
         masked_lm_masker = MaskedLMMaskGenerator(
             vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=0.5,
-            mask_selection_length=5,
+            mask_selection_rate=1,
+            mask_selection_length=4,
             mask_token_id=self.mask_token_id,
             mask_token_rate=1,
             random_token_rate=0,
         )
-        inputs = tf.ragged.constant([[5, 3, 2], [1, 2, 3, 4, 5]])
-        outputs = masked_lm_masker(inputs)
-        token_ids, mask_positions, mask_ids = (
-            outputs["token_ids"],
-            outputs["mask_positions"],
-            outputs["mask_ids"],
-        )
-        self.assertEqual(type(token_ids), type(inputs))
-        self.assertAllEqual(token_ids.shape, inputs.shape)
-        self.assertAllEqual(mask_positions.shape, mask_ids.shape)
-
-        # Test all selected token_ids are correctly masked.
-        masked_values = tf.gather(
-            token_ids,
-            mask_positions,
-            batch_dims=1,
-        )
-        self.assertEqual(tf.reduce_mean(masked_values), self.mask_token_id)
+        inputs = [[5, 3, 2], [1, 2, 3, 4]]
+        x = masked_lm_masker(inputs)
+        self.assertAllEqual(x["token_ids"], [[1, 1, 1], [1, 1, 1, 1]])
+        self.assertAllEqual(x["mask_positions"], [[0, 1, 2, 0], [0, 1, 2, 3]])
+        self.assertAllEqual(x["mask_ids"], [[5, 3, 2, 0], [1, 2, 3, 4]])
 
-    def test_mask_tensor(self):
+    def test_mask_dense(self):
         masked_lm_masker = MaskedLMMaskGenerator(
             vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=0.5,
-            mask_selection_length=5,
+            mask_selection_rate=1,
+            mask_selection_length=4,
             mask_token_id=self.mask_token_id,
             mask_token_rate=1,
             random_token_rate=0,
         )
-        inputs = tf.random.uniform(
-            shape=[5, 10],
-            maxval=len(self.VOCAB),
-            dtype=tf.int32,
-        )
-        outputs = masked_lm_masker(inputs)
-        token_ids, mask_positions, mask_ids = (
-            outputs["token_ids"],
-            outputs["mask_positions"],
-            outputs["mask_ids"],
-        )
-        self.assertEqual(type(token_ids), type(inputs))
-        self.assertAllEqual(token_ids.shape, inputs.shape)
-        self.assertAllEqual(mask_positions.shape, mask_ids.shape)
-        # Test all selected token_ids are correctly masked.
-        masked_values = tf.gather(
-            token_ids,
-            mask_positions,
-            batch_dims=1,
-        )
-        self.assertEqual(tf.reduce_mean(masked_values), self.mask_token_id)
+        inputs = [[5, 3, 2, 4], [1, 2, 3, 4]]
+        x = masked_lm_masker(inputs)
+        self.assertAllEqual(x["token_ids"], [[1, 1, 1, 1], [1, 1, 1, 1]])
+        self.assertAllEqual(x["mask_positions"], [[0, 1, 2, 3], [0, 1, 2, 3]])
+        self.assertAllEqual(x["mask_ids"], [[5, 3, 2, 4], [1, 2, 3, 4]])
 
-    def test_mask_1d_input(self):
+    def test_unbatched(self):
         masked_lm_masker = MaskedLMMaskGenerator(
             vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=0.5,
-            mask_selection_length=5,
+            mask_selection_rate=1,
+            mask_selection_length=4,
             mask_token_id=self.mask_token_id,
             mask_token_rate=1,
             random_token_rate=0,
         )
-        inputs = tf.constant([1, 2, 3, 4, 5])
-        outputs = masked_lm_masker(inputs)
-        self.assertAllEqual(outputs["token_ids"].shape, inputs.shape)
+        inputs = [5, 3, 2, 4]
+        x = masked_lm_masker(inputs)
+        self.assertAllEqual(x["token_ids"], [1, 1, 1, 1])
+        self.assertAllEqual(x["mask_positions"], [0, 1, 2, 3])
+        self.assertAllEqual(x["mask_ids"], [5, 3, 2, 4])
 
-    def test_number_of_masked_position_as_expected(self):
-        mask_selection_rate = 0.5
-        mask_selection_length = 5
+    def test_random_replacement(self):
         masked_lm_masker = MaskedLMMaskGenerator(
-            vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=mask_selection_rate,
+            vocabulary_size=10_000,
+            mask_selection_rate=1,
+            mask_selection_length=4,
             mask_token_id=self.mask_token_id,
-            unselectable_token_ids=None,
-        )
-        inputs = tf.ragged.constant(
-            [[0, 1, 2], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4]]
-        )
-        outputs = masked_lm_masker(inputs)
-        expected_number_of_masked_tokens = tf.cast(
-            tf.math.ceil(
-                tf.cast(inputs.row_lengths(), dtype=tf.float32)
-                * mask_selection_rate,
-            ),
-            dtype=tf.int64,
-        )
-
-        self.assertAllEqual(
-            outputs["mask_positions"].row_lengths(),
-            expected_number_of_masked_tokens,
+            mask_token_rate=0,
+            random_token_rate=1,
         )
+        inputs = [5, 3, 2, 4]
+        x = masked_lm_masker(inputs)
+        self.assertNotAllEqual(x["token_ids"], [1, 1, 1, 1])
+        self.assertAllEqual(x["mask_positions"], [0, 1, 2, 3])
+        self.assertAllEqual(x["mask_ids"], [5, 3, 2, 4])
 
+    def test_number_of_masked_position_as_expected(self):
+        mask_selection_rate = 0.5
+        mask_selection_length = 5
+        inputs = [[0, 1, 2], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4]]
         # Cap the number of masked tokens at 0, so we can test if
         # mask_selection_length takes effect.
         mask_selection_length = 0
         masked_lm_masker = MaskedLMMaskGenerator(
             vocabulary_size=self.vocabulary_size,
             mask_selection_rate=mask_selection_rate,
             mask_token_id=self.mask_token_id,
             mask_selection_length=mask_selection_length,
         )
         outputs = masked_lm_masker(inputs)
         self.assertEqual(tf.reduce_sum(outputs["mask_positions"]), 0)
 
-    def test_apply_random_token_not_mask(self):
-        masked_lm_masker = MaskedLMMaskGenerator(
-            vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=0.5,
-            mask_token_id=self.mask_token_id,
-            mask_token_rate=0,
-            random_token_rate=1,
-        )
-        inputs = tf.random.uniform(
-            shape=[5, 10],
-            maxval=len(self.VOCAB),
-            dtype=tf.int32,
-        )
-        outputs = masked_lm_masker(inputs)
-        tokens_ids, mask_positions, mask_ids = (
-            outputs["token_ids"],
-            outputs["mask_positions"],
-            outputs["mask_ids"],
-        )
-        self.assertAllEqual(tokens_ids.shape, inputs.shape)
-        self.assertAllEqual(
-            mask_positions.row_lengths(), mask_ids.row_lengths()
-        )
-        masked_values = tf.gather(
-            tokens_ids,
-            mask_positions,
-            batch_dims=1,
-        )
-        # Verify that selected tokens_ids are replaced by random tokens_ids.
-        self.assertNotEqual(tf.reduce_mean(masked_values), self.mask_token_id)
-
     def test_invalid_mask_token(self):
         with self.assertRaisesRegex(ValueError, "Mask token id should be*"):
             _ = MaskedLMMaskGenerator(
                 vocabulary_size=self.vocabulary_size,
                 mask_selection_rate=0.5,
                 mask_token_id=self.vocabulary_size,
                 mask_selection_length=5,
@@ -198,18 +137,17 @@
             mask_selection_rate=1,
             mask_token_id=self.mask_token_id,
             mask_selection_length=5,
             unselectable_token_ids=unselectable_token_ids,
             mask_token_rate=1,
             random_token_rate=0,
         )
-        inputs = tf.convert_to_tensor([unselectable_token_ids])
-        outputs = masked_lm_masker(inputs)
+        outputs = masked_lm_masker([unselectable_token_ids])
         # Verify that no token is masked out.
-        self.assertEqual(tf.reduce_sum(outputs["mask_positions"]), 0)
+        self.assertEqual(np.sum(np.array(outputs["mask_weights"])), 0)
 
     def test_config(self):
         unselectable_token_ids = [
             self.vocabulary_size - 1,
             self.vocabulary_size - 2,
         ]
         masked_lm_masker = MaskedLMMaskGenerator(
@@ -224,35 +162,17 @@
             "vocabulary_size": self.vocabulary_size,
             "unselectable_token_ids": unselectable_token_ids,
         }
         self.assertDictContainsSubset(expected_config, config)
 
         # Test cloned masked_lm_masker can be run.
         cloned_masked_lm_masker = MaskedLMMaskGenerator.from_config(config)
-        inputs = tf.ragged.constant(
-            [[0, 1, 2], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4]]
-        )
+        inputs = [[5, 3, 2], [1, 2, 3, 4]]
         cloned_masked_lm_masker(inputs)
 
-    def test_graph_mode_execution(self):
-        masked_lm_masker = MaskedLMMaskGenerator(
-            vocabulary_size=self.vocabulary_size,
-            mask_selection_rate=0.5,
-            mask_token_id=self.mask_token_id,
-            mask_selection_length=5,
-        )
-
-        @tf.function
-        def masker(inputs):
-            return masked_lm_masker(inputs)
-
-        masker(tf.constant([1, 2, 3]))
-        masker(tf.constant([[1, 2, 3], [1, 2, 3]]))
-        masker(tf.ragged.constant([[3, 5, 7, 7], [4, 6, 7, 5]]))
-
     def test_with_tf_data(self):
         ds = tf.data.Dataset.from_tensor_slices(
             tf.ones((100, 10), dtype="int32")
         )
         masked_lm_masker = MaskedLMMaskGenerator(
             vocabulary_size=self.vocabulary_size,
             mask_selection_rate=0.5,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/multi_segment_packer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_deletion.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,228 +7,262 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-"""BERT token packing layer."""
+import random
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
-
-try:
-    import tensorflow_text as tf_text
-except ImportError:
-    tf_text = None
-
-
-@keras_nlp_export("keras_nlp.layers.MultiSegmentPacker")
-class MultiSegmentPacker(keras.layers.Layer):
-    """Packs multiple sequences into a single fixed width model input.
-
-    This layer packs multiple input sequences into a single fixed width sequence
-    containing start and end delimeters, forming an dense input suitable for a
-    classification task for BERT and BERT-like models.
-
-    Takes as input a list or tuple of token segments. The layer will process
-    inputs as follows:
-     - Truncate all input segments to fit within `sequence_length` according to
-       the `truncate` strategy.
-     - Concatenate all input segments, adding a single `start_value` at the
-       start of the entire sequence, and multiple `end_value`s at the end of
-       each segment.
-     - Pad the resulting sequence to `sequence_length` using `pad_tokens`.
-     - Calculate a separate tensor of "segment ids", with integer type and the
-       same shape as the packed token output, where each integer index of the
-       segment the token originated from. The segment id of the `start_value`
-       is always 0, and the segment id of each `end_value` is the segment that
-       precedes it.
-
-    Input should be either a `tf.RaggedTensor` or a dense `tf.Tensor`, and
-    either rank-1 or rank-2.
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
+from keras_nlp.src.utils.tensor_utils import is_string_dtype
+
+
+@keras_nlp_export("keras_nlp.layers.RandomDeletion")
+class RandomDeletion(PreprocessingLayer):
+    """Augments input by randomly deleting tokens.
+
+    This layer comes in handy when you need to generate new data using deletion
+    augmentation as described in the paper [EDA: Easy Data Augmentation
+    Techniques for Boosting Performance on Text Classification Tasks]
+    (https://arxiv.org/pdf/1901.11196.pdf). The layer expects the inputs to be
+    pre-split into token level inputs. This allows control over the level of
+    augmentation, you can split by character for character level swaps, or by
+    word for word level swaps.
+
+    Input data should be passed as tensors, `tf.RaggedTensor`s, or lists. For
+    batched input, inputs should be a list of lists or a rank two tensor. For
+    unbatched inputs, each element should be a list or a rank one tensor.
 
     Args:
-        sequence_length: The desired output length.
-        start_value: The id or token that is to be placed at the start of each
-            sequence (called "[CLS]" for BERT). The dtype must match the dtype
-            of the input tensors to the layer.
-        end_value: The id or token that is to be placed at the end of each
-            input segment (called "[SEP]" for BERT). The dtype much match the
-            dtype of the input tensors to the layer.
-        pad_value: The id or token that is to be placed into the unused
-            positions after the last segment in the sequence
-            (called "[PAD]" for BERT).
-        truncate: The algorithm to truncate a list of batched segments to fit a
-            per-example length limit. The value can be either `round_robin` or
-            `waterfall`:
-                - `"round_robin"`: Available space is assigned one token at a
-                    time in a round-robin fashion to the inputs that still need
-                    some, until the limit is reached.
-                - `"waterfall"`: The allocation of the budget is done using a
-                    "waterfall" algorithm that allocates quota in a
-                    left-to-right manner and fills up the buckets until we run
-                    out of budget. It support arbitrary number of segments.
-
-    Returns:
-        A tuple with two elements. The first is the dense, packed token
-        sequence. The second is an integer tensor of the same shape, containing
-        the segment ids.
+        rate: The probability of a token being chosen for deletion.
+        max_deletions: The maximum number of tokens to delete.
+        skip_list: A list of token values that should not be considered
+            candidates for deletion.
+        skip_fn: A function that takes as input a scalar tensor token and
+            returns as output a scalar tensor True/False value. A value of
+            True indicates that the token should not be considered a
+            candidate for deletion. This function must be tracable--it
+            should consist of tensorflow operations.
+        skip_py_fn: A function that takes as input a python token value and
+            returns as output `True` or `False`. A value of True
+            indicates that should not be considered a candidate for deletion.
+            Unlike the `skip_fn` argument, this argument need not be
+            tracable--it can be any python function.
+        seed: A seed for the random number generator.
 
     Examples:
 
-    *Pack a single input for classification.*
-    >>> seq1 = tf.constant([1, 2, 3, 4])
-    >>> packer = keras_nlp.layers.MultiSegmentPacker(
-    ...     8, start_value=101, end_value=102)
-    >>> packer(seq1)
-    (<tf.Tensor: shape=(8,), dtype=int32,
-        numpy=array([101, 1, 2, 3, 4, 102, 0, 0], dtype=int32)>,
-     <tf.Tensor: shape=(8,), dtype=int32,
-        numpy=array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>)
-
-    *Pack multiple inputs for classification.*
-    >>> seq1 = tf.constant([1, 2, 3, 4])
-    >>> seq2 = tf.constant([11, 12, 13, 14])
-    >>> packer = keras_nlp.layers.MultiSegmentPacker(
-    ...     8, start_value=101, end_value=102)
-    >>> packer((seq1, seq2))
-    (<tf.Tensor: shape=(8,), dtype=int32,
-        numpy=array([101,   1,   2,   3, 102,  11,  12, 102], dtype=int32)>,
-     <tf.Tensor: shape=(8,), dtype=int32,
-        numpy=array([0, 0, 0, 0, 0, 1, 1, 1], dtype=int32)>)
-
-    Reference:
-        [Devlin et al., 2018](https://arxiv.org/abs/1810.04805).
+    Word level usage.
+    >>> keras.utils.set_random_seed(1337)
+    >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
+    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4, seed=42)
+    >>> augmented=augmenter(inputs)
+    >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
+    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'I like', b'and'],
+    dtype=object)>
+
+    Character level usage.
+    >>> keras.utils.set_random_seed(1337)
+    >>> inputs=tf.strings.unicode_split(["Hey Dude", "Speed Up"], "UTF-8")
+    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4, seed=42)
+    >>> augmented=augmenter(inputs)
+    >>> tf.strings.reduce_join(augmented, axis=-1)
+    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'H Dude', b'pedUp'],
+    dtype=object)>
+
+    Usage with skip_list.
+    >>> keras.utils.set_random_seed(1337)
+    >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
+    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4,
+    ...     skip_list=["Keras", "Tensorflow"], seed=42)
+    >>> augmented=augmenter(inputs)
+    >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
+    <tf.Tensor: shape=(2,), dtype=string,
+    numpy=array([b'I like', b'Keras Tensorflow'], dtype=object)>
+
+    Usage with skip_fn.
+    >>> def skip_fn(word):
+    ...     return tf.strings.regex_full_match(word, r"\\pP")
+    >>> keras.utils.set_random_seed(1337)
+    >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
+    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4,
+    ...     skip_fn=skip_fn, seed=42)
+    >>> augmented=augmenter(inputs)
+    >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
+    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'I like', b'and'],
+    dtype=object)>
+
+    Usage with skip_py_fn.
+    >>> def skip_py_fn(word):
+    ...     return len(word) < 4
+    >>> keras.utils.set_random_seed(1337)
+    >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
+    >>> augmenter=RandomDeletion(rate=0.4,
+    ...     skip_py_fn=skip_py_fn, seed=42)
+    >>> augmented=augmenter(inputs)
+    >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
+    <tf.Tensor: shape=(2,), dtype=string,
+    numpy=array([b'Hey I', b'and Tensorflow'], dtype=object)>
     """
 
     def __init__(
         self,
-        sequence_length,
-        start_value,
-        end_value,
-        pad_value=None,
-        truncate="round_robin",
+        rate,
+        max_deletions=None,
+        skip_list=None,
+        skip_fn=None,
+        skip_py_fn=None,
+        seed=None,
+        name=None,
+        dtype="int32",
         **kwargs,
     ):
-        assert_tf_text_installed(self.__class__.__name__)
-
-        super().__init__(**kwargs)
-        self.sequence_length = sequence_length
-        if truncate not in ("round_robin", "waterfall"):
+        if not is_integer_dtype(dtype) and not is_string_dtype(dtype):
             raise ValueError(
-                "Only 'round_robin' and 'waterfall' algorithms are "
-                "supported. Received %s" % truncate
+                "Output dtype must be an integer type or a string. "
+                f"Received: dtype={dtype}"
             )
-        self.truncate = truncate
-        self.start_value = start_value
-        self.end_value = end_value
-        self.pad_value = pad_value
 
-    def get_config(self):
-        config = super().get_config()
-        config.update(
-            {
-                "sequence_length": self.sequence_length,
-                "start_value": self.start_value,
-                "end_value": self.end_value,
-                "pad_value": self.pad_value,
-                "truncate": self.truncate,
-            }
-        )
-        return config
+        super().__init__(dtype=dtype, name=name, **kwargs)
 
-    def _sanitize_inputs(self, inputs):
-        """Force inputs to a list of rank 2 ragged tensors."""
-        # Sanitize inputs.
-        if not isinstance(inputs, (list, tuple)):
-            inputs = [inputs]
-        if not inputs:
-            raise ValueError("At least one input is required for packing")
-        input_ranks = [x.shape.rank for x in inputs]
-        if not all(0 < rank < 3 for rank in input_ranks):
+        self.rate = rate
+        self.max_deletions = max_deletions
+        self.seed = random.randint(1, 1e9) if seed is None else seed
+        self._generator = tf.random.Generator.from_seed(self.seed)
+        self.skip_list = skip_list
+        self.skip_fn = skip_fn
+        self.skip_py_fn = skip_py_fn
+        if self.max_deletions is not None and self.max_deletions < 0:
             raise ValueError(
-                "All inputs for packing must have rank 1 or 2. "
-                f"Received input ranks: {input_ranks}"
+                "max_deletions must be non-negative."
+                f"Received max_deletions={max_deletions}."
             )
-        if None in input_ranks or len(set(input_ranks)) > 1:
+
+        if self.rate > 1 or self.rate < 0:
             raise ValueError(
-                "All inputs for packing must have the same rank. "
-                f"Received input ranks: {input_ranks}"
+                "Rate must be between 0 and 1 (both inclusive)."
+                f"Received: rate={rate}"
+            )
+
+        if [self.skip_list, self.skip_fn, self.skip_py_fn].count(None) < 2:
+            raise ValueError(
+                "Exactly one of `skip_list`, `skip_fn`, `skip_py_fn` must be "
+                "provided."
             )
-        return inputs
 
-    def _convert_dense(self, x):
-        """Converts inputs to rank 2 ragged tensors."""
-        if isinstance(x, tf.Tensor):
-            return tf.RaggedTensor.from_tensor(x)
-        else:
-            return x
-
-    def _trim_inputs(self, inputs):
-        """Trim inputs to desired length."""
-        num_special_tokens = len(inputs) + 1
-        if self.truncate == "round_robin":
-            return tf_text.RoundRobinTrimmer(
-                self.sequence_length - num_special_tokens
-            ).trim(inputs)
-        elif self.truncate == "waterfall":
-            return tf_text.WaterfallTrimmer(
-                self.sequence_length - num_special_tokens
-            ).trim(inputs)
-        else:
-            raise ValueError("Unsupported truncate: %s" % self.truncate)
-
-    def _combine_inputs(self, segments):
-        """Combine inputs with start and end values added."""
-        dtype = segments[0].dtype
-        batch_size = segments[0].nrows()
-        start_value = tf.convert_to_tensor(self.start_value, dtype=dtype)
-        end_value = tf.convert_to_tensor(self.end_value, dtype=dtype)
-
-        start_column = tf.fill((batch_size, 1), start_value)
-        end_column = tf.fill((batch_size, 1), end_value)
-        ones_column = tf.ones_like(start_column, dtype=tf.int32)
-
-        segments_to_combine = [start_column]
-        segment_ids_to_combine = [ones_column * 0]
-        for i, seg in enumerate(segments):
-            # Combine all segments adding end tokens.
-            segments_to_combine.append(seg)
-            segments_to_combine.append(end_column)
-
-            # Combine segment ids accounting for end tokens.
-            segment_ids_to_combine.append(tf.ones_like(seg, dtype=tf.int32) * i)
-            segment_ids_to_combine.append(ones_column * i)
-
-        token_ids = tf.concat(segments_to_combine, 1)
-        segment_ids = tf.concat(segment_ids_to_combine, 1)
-        return token_ids, segment_ids
+        if self.skip_list:
+            self.StaticHashTable = tf.lookup.StaticHashTable(
+                tf.lookup.KeyValueTensorInitializer(
+                    tf.convert_to_tensor(self.skip_list),
+                    tf.convert_to_tensor([True] * len(self.skip_list)),
+                ),
+                default_value=False,
+            )
 
     def call(self, inputs):
-        inputs = self._sanitize_inputs(inputs)
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
+
+        skip_masks = None
+        if self.skip_list:
+            skip_masks = self.StaticHashTable.lookup(inputs.flat_values)
+        elif self.skip_fn:
+            skip_masks = tf.map_fn(
+                self.skip_fn, inputs.flat_values, fn_output_signature="bool"
+            )
+        elif self.skip_py_fn:
+
+            def string_fn(token):
+                return self.skip_py_fn(token.numpy().decode("utf-8"))
 
-        # If rank 1, add a batch dim.
-        rank_1 = inputs[0].shape.rank == 1
-        if rank_1:
-            inputs = [tf.expand_dims(x, 0) for x in inputs]
-        inputs = [self._convert_dense(x) for x in inputs]
-
-        segments = self._trim_inputs(inputs)
-        token_ids, segment_ids = self._combine_inputs(segments)
-        # Pad to dense tensor output.
-        shape = tf.cast([-1, self.sequence_length], tf.int64)
-        token_ids = token_ids.to_tensor(
-            shape=shape, default_value=self.pad_value
+            def int_fn(token):
+                return self.skip_py_fn(token.numpy())
+
+            py_fn = string_fn if inputs.dtype == tf.string else int_fn
+
+            skip_masks = tf.map_fn(
+                lambda x: tf.py_function(py_fn, [x], "bool"),
+                inputs.flat_values,
+                fn_output_signature="bool",
+            )
+
+        positions_flat = tf.range(tf.size(inputs.flat_values))
+        positions = inputs.with_flat_values(positions_flat)
+        if skip_masks is not None:
+            skip_masks = tf.logical_not(skip_masks)
+            skip_masks.set_shape([None])
+            positions = tf.ragged.boolean_mask(
+                positions, inputs.with_flat_values(skip_masks)
+            )
+
+        # Figure out how many we are going to select.
+        token_counts = tf.cast(positions.row_lengths(), "float32")
+        num_to_select = tf.random.stateless_binomial(
+            shape=tf.shape(token_counts),
+            seed=self._generator.make_seeds()[:, 0],
+            counts=token_counts,
+            probs=self.rate,
         )
-        segment_ids = segment_ids.to_tensor(shape=shape)
-        # Remove the batch dim if added.
-        if rank_1:
-            token_ids = tf.squeeze(token_ids, 0)
-            segment_ids = tf.squeeze(segment_ids, 0)
+        if self.max_deletions is not None:
+            num_to_select = tf.math.minimum(num_to_select, self.max_deletions)
+        num_to_select = tf.cast(num_to_select, "int64")
+
+        # Shuffle and trim to items that are going to be selected.
+        def _shuffle_and_trim(x):
+            positions, top_n = x
+            shuffled = tf.random.shuffle(positions, seed=self.seed)
+            return shuffled[:top_n]
+
+        selected_for_mask = tf.map_fn(
+            _shuffle_and_trim,
+            (positions, num_to_select),
+            fn_output_signature=tf.RaggedTensorSpec(
+                ragged_rank=positions.ragged_rank - 1, dtype=positions.dtype
+            ),
+        )
+        selected_for_mask.flat_values.set_shape([None])
+
+        # Construct the mask which is a boolean RT
+        # Scatter 0's to positions that have been selector for deletion.
+        update_values = tf.zeros_like(selected_for_mask.flat_values, "int32")
+        update_indices = selected_for_mask.flat_values
+        update_indices = tf.expand_dims(update_indices, -1)
+        update_indices = tf.cast(update_indices, "int32")
+        mask_flat = tf.ones_like(inputs.flat_values, dtype="int32")
+        mask_flat = tf.tensor_scatter_nd_update(
+            mask_flat, update_indices, update_values
+        )
+        mask = tf.cast(inputs.with_flat_values(mask_flat), "bool")
+
+        inputs = tf.ragged.boolean_mask(inputs, mask)
+
+        if unbatched:
+            inputs = tf.squeeze(inputs, axis=0)
+
+        return inputs
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "rate": self.rate,
+                "max_deletions": self.max_deletions,
+                "seed": self.seed,
+                "skip_list": self.skip_list,
+                "skip_fn": self.skip_fn,
+                "skip_py_fn": self.skip_py_fn,
+            }
+        )
+        return config
 
-        return (token_ids, segment_ids)
+    def compute_output_shape(self, inputs_shape):
+        inputs_shape = list(inputs_shape)
+        inputs_shape[-1] = None
+        return tuple(inputs_shape)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/multi_segment_packer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/multi_segment_packer_test.py`

 * *Files 25% similar despite different names*

```diff
@@ -7,183 +7,198 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for Transformer Decoder."""
+"""Tests for multi-segment packing."""
 
-import os
+import numpy as np
 
-import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
+from keras_nlp.src.tests.test_case import TestCase
 
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
 
-
-class MultiSegmentPackerTest(tf.test.TestCase, parameterized.TestCase):
+class MultiSegmentPackerTest(TestCase):
     def test_trim_single_input_ints(self):
-        input_data = tf.range(3, 10)
-        packer = MultiSegmentPacker(8, start_value=1, end_value=2)
-        output = packer(input_data)
-        self.assertAllEqual(
-            output, ([1, 3, 4, 5, 6, 7, 8, 2], [0, 0, 0, 0, 0, 0, 0, 0])
+        input_data = np.arange(3, 10)
+        packer = MultiSegmentPacker(
+            sequence_length=8, start_value=1, end_value=2
         )
+        token_ids, segment_ids = packer(input_data)
+        self.assertAllEqual(token_ids, [1, 3, 4, 5, 6, 7, 8, 2])
+        self.assertAllEqual(segment_ids, [0, 0, 0, 0, 0, 0, 0, 0])
 
     def test_trim_single_input_strings(self):
-        input_data = tf.constant(["a", "b", "c", "d"])
-        packer = MultiSegmentPacker(5, start_value="[CLS]", end_value="[SEP]")
-        output = packer(input_data)
-        self.assertAllEqual(
-            output, (["[CLS]", "a", "b", "c", "[SEP]"], [0, 0, 0, 0, 0])
+        input_data = np.array(["a", "b", "c", "d"])
+        packer = MultiSegmentPacker(
+            sequence_length=5, start_value="[CLS]", end_value="[SEP]"
         )
+        token_ids, segment_ids = packer(input_data)
+        self.assertAllEqual(token_ids, ["[CLS]", "a", "b", "c", "[SEP]"])
+        self.assertAllEqual(segment_ids, [0, 0, 0, 0, 0])
 
     def test_trim_multiple_inputs_round_robin(self):
-        seq1 = tf.constant(["a", "b", "c"])
-        seq2 = tf.constant(["x", "y", "z"])
+        seq1 = ["a", "b", "c"]
+        seq2 = ["x", "y", "z"]
         packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="round_robin"
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            truncate="round_robin",
         )
-        output = packer([seq1, seq2])
+        token_ids, segment_ids = packer([seq1, seq2])
         self.assertAllEqual(
-            output,
-            (
-                ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
-                [0, 0, 0, 0, 1, 1, 1],
-            ),
+            token_ids, ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"]
         )
+        self.assertAllEqual(segment_ids, [0, 0, 0, 0, 1, 1, 1])
 
     def test_trim_multiple_inputs_waterfall(self):
-        seq1 = tf.constant(["a", "b", "c"])
-        seq2 = tf.constant(["x", "y", "z"])
+        seq1 = ["a", "b", "c"]
+        seq2 = ["x", "y", "z"]
         packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="waterfall"
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            truncate="waterfall",
         )
-        output = packer([seq1, seq2])
+        token_ids, segment_ids = packer([seq1, seq2])
         self.assertAllEqual(
-            output,
-            (
-                ["[CLS]", "a", "b", "c", "[SEP]", "x", "[SEP]"],
-                [0, 0, 0, 0, 0, 1, 1],
-            ),
+            token_ids, ["[CLS]", "a", "b", "c", "[SEP]", "x", "[SEP]"]
         )
+        self.assertAllEqual(segment_ids, [0, 0, 0, 0, 0, 1, 1])
 
     def test_trim_batched_inputs_round_robin(self):
-        seq1 = tf.constant([["a", "b", "c"], ["a", "b", "c"]])
-        seq2 = tf.constant([["x", "y", "z"], ["x", "y", "z"]])
+        seq1 = [["a", "b", "c"], ["a", "b", "c"]]
+        seq2 = [["x", "y", "z"], ["x", "y", "z"]]
         packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="round_robin"
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            truncate="round_robin",
         )
-        output = packer([seq1, seq2])
+        token_ids, segment_ids = packer([seq1, seq2])
         self.assertAllEqual(
-            output,
-            (
-                [
-                    ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
-                    ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
-                ],
-                [
-                    [0, 0, 0, 0, 1, 1, 1],
-                    [0, 0, 0, 0, 1, 1, 1],
-                ],
-            ),
+            token_ids,
+            [
+                ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
+                ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
+            ],
+        )
+        self.assertAllEqual(
+            segment_ids,
+            [
+                [0, 0, 0, 0, 1, 1, 1],
+                [0, 0, 0, 0, 1, 1, 1],
+            ],
         )
 
     def test_trim_batched_inputs_waterfall(self):
-        seq1 = tf.ragged.constant([["a", "b", "c"], ["a", "b"]])
-        seq2 = tf.constant([["x", "y", "z"], ["x", "y", "z"]])
+        seq1 = [["a", "b", "c"], ["a", "b"]]
+        seq2 = [["x", "y", "z"], ["x", "y", "z"]]
         packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="waterfall"
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            truncate="waterfall",
         )
-        output = packer([seq1, seq2])
+        token_ids, segment_ids = packer([seq1, seq2])
         self.assertAllEqual(
-            output,
-            (
-                [
-                    ["[CLS]", "a", "b", "c", "[SEP]", "x", "[SEP]"],
-                    ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
-                ],
-                [
-                    [0, 0, 0, 0, 0, 1, 1],
-                    [0, 0, 0, 0, 1, 1, 1],
-                ],
-            ),
+            token_ids,
+            [
+                ["[CLS]", "a", "b", "c", "[SEP]", "x", "[SEP]"],
+                ["[CLS]", "a", "b", "[SEP]", "x", "y", "[SEP]"],
+            ],
+        )
+        self.assertAllEqual(
+            segment_ids,
+            [
+                [0, 0, 0, 0, 0, 1, 1],
+                [0, 0, 0, 0, 1, 1, 1],
+            ],
         )
 
     def test_pad_inputs(self):
-        seq1 = tf.constant(["a"])
-        seq2 = tf.constant(["x"])
+        seq1 = ["a"]
+        seq2 = ["x"]
         packer = MultiSegmentPacker(
             6, start_value="[CLS]", end_value="[SEP]", pad_value="[PAD]"
         )
-        output = packer([seq1, seq2])
+        token_ids, segment_ids = packer([seq1, seq2])
         self.assertAllEqual(
-            output,
-            (
-                ["[CLS]", "a", "[SEP]", "x", "[SEP]", "[PAD]"],
-                [0, 0, 0, 1, 1, 0],
-            ),
+            token_ids,
+            ["[CLS]", "a", "[SEP]", "x", "[SEP]", "[PAD]"],
         )
+        self.assertAllEqual(segment_ids, [0, 0, 0, 1, 1, 0])
 
     def test_pad_batched_inputs(self):
-        seq1 = tf.ragged.constant([["a"], ["a"]])
-        seq2 = tf.ragged.constant([["x"], ["x", "y"]])
+        seq1 = [["a"], ["a"]]
+        seq2 = [["x"], ["x", "y"]]
         packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", pad_value="[PAD]"
-        )
-        output = packer([seq1, seq2])
-        self.assertAllEqual(
-            output,
-            (
-                [
-                    ["[CLS]", "a", "[SEP]", "x", "[SEP]", "[PAD]", "[PAD]"],
-                    ["[CLS]", "a", "[SEP]", "x", "y", "[SEP]", "[PAD]"],
-                ],
-                [
-                    [0, 0, 0, 1, 1, 0, 0],
-                    [0, 0, 0, 1, 1, 1, 0],
-                ],
-            ),
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            pad_value="[PAD]",
+        )
+        token_ids, segment_ids = packer([seq1, seq2])
+        self.assertAllEqual(
+            token_ids,
+            [
+                ["[CLS]", "a", "[SEP]", "x", "[SEP]", "[PAD]", "[PAD]"],
+                ["[CLS]", "a", "[SEP]", "x", "y", "[SEP]", "[PAD]"],
+            ],
+        )
+        self.assertAllEqual(
+            segment_ids,
+            [
+                [0, 0, 0, 1, 1, 0, 0],
+                [0, 0, 0, 1, 1, 1, 0],
+            ],
+        )
+
+    def test_list_special_tokens(self):
+        seq1 = [["a", "b"], ["a", "b"]]
+        seq2 = [["x", "y"], ["x"]]
+        packer = MultiSegmentPacker(
+            8,
+            start_value="<s>",
+            end_value="</s>",
+            sep_value=["</s>", "</s>"],
+            pad_value="<pad>",
+            truncate="round_robin",
+        )
+        token_ids, segment_ids = packer([seq1, seq2])
+        self.assertAllEqual(
+            token_ids,
+            [
+                ["<s>", "a", "b", "</s>", "</s>", "x", "y", "</s>"],
+                ["<s>", "a", "b", "</s>", "</s>", "x", "</s>", "<pad>"],
+            ],
+        )
+        self.assertAllEqual(
+            segment_ids,
+            [
+                [0, 0, 0, 0, 0, 1, 1, 1],
+                [0, 0, 0, 0, 0, 1, 1, 0],
+            ],
         )
 
     def test_config(self):
-        seq1 = tf.ragged.constant([["a", "b", "c"], ["a", "b"]])
-        seq2 = tf.ragged.constant([["x", "y", "z"], ["x", "y", "z"]])
+        seq1 = [["a", "b", "c"], ["a", "b"]]
+        seq2 = [["x", "y", "z"], ["x", "y", "z"]]
         original_packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="waterfall"
+            sequence_length=7,
+            start_value="[CLS]",
+            end_value="[SEP]",
+            truncate="waterfall",
         )
         cloned_packer = MultiSegmentPacker.from_config(
             original_packer.get_config()
         )
-        self.assertAllEqual(
-            original_packer([seq1, seq2]),
-            cloned_packer([seq1, seq2]),
-        )
-
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
-        seq1 = tf.ragged.constant([["a", "b", "c"], ["a", "b"]])
-        seq2 = tf.ragged.constant([["x", "y", "z"], ["x", "y", "z"]])
-        packer = MultiSegmentPacker(
-            7, start_value="[CLS]", end_value="[SEP]", truncate="waterfall"
-        )
-        inputs = (
-            keras.Input(dtype="string", ragged=True, shape=(None,)),
-            keras.Input(dtype="string", ragged=True, shape=(None,)),
-        )
-        outputs = packer(inputs)
-        model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
-        restored_model = keras.models.load_model(path)
-        self.assertAllEqual(
-            model((seq1, seq2)),
-            restored_model((seq1, seq2)),
-        )
+        token_ids, segment_ids = original_packer([seq1, seq2])
+        cloned_token_ids, cloned_segment_ids = cloned_packer([seq1, seq2])
+        self.assertAllEqual(token_ids, cloned_token_ids)
+        self.assertAllEqual(segment_ids, cloned_segment_ids)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/position_embedding.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/position_embedding.py`

 * *Files 20% similar despite different names*

```diff
@@ -10,48 +10,41 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Position embedding implementation based on `keras.layers.Layer`."""
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 
 
 @keras_nlp_export("keras_nlp.layers.PositionEmbedding")
 class PositionEmbedding(keras.layers.Layer):
     """A layer which learns a position embedding for inputs sequences.
 
     This class assumes that in the input tensor, the last dimension corresponds
     to the features, and the dimension before the last corresponds to the
     sequence.
 
-    This layer optionally accepts `tf.RaggedTensor`s as inputs to process
-    batches of sequences of different lengths. The one ragged dimension must be
-    the dimension that corresponds to the sequence, that is, the penultimate
-    dimension.
-
     This layer does not supporting masking, but can be combined with a
     `keras.layers.Embedding` for padding mask support.
 
     Args:
         sequence_length: The maximum length of the dynamic sequence.
         initializer: The initializer to use for the embedding weights. Defaults
             to `"glorot_uniform"`.
         seq_axis: The axis of the input tensor where we add the embeddings.
 
     Examples:
 
     Called directly on input.
     >>> layer = keras_nlp.layers.PositionEmbedding(sequence_length=10)
-    >>> layer(tf.zeros((8, 10, 16))).shape
-    TensorShape([8, 10, 16])
+    >>> layer(np.zeros((8, 10, 16)))
 
     Combine with a token embedding.
     ```python
     seq_length = 50
     vocab_size = 5000
     embed_dim = 128
     inputs = keras.Input(shape=(seq_length,))
@@ -91,46 +84,32 @@
             }
         )
         return config
 
     def build(self, input_shape):
         feature_size = input_shape[-1]
         self.position_embeddings = self.add_weight(
-            "embeddings",
+            name="embeddings",
             shape=[self.sequence_length, feature_size],
             initializer=self.initializer,
             trainable=True,
         )
 
         super().build(input_shape)
 
     def call(self, inputs, start_index=0):
-        if isinstance(inputs, tf.RaggedTensor):
-            bounding_shape = inputs.bounding_shape()
-            position_embeddings = self._trim_and_broadcast_position_embeddings(
-                bounding_shape,
-                start_index,
-            )
-            # then apply row lengths to recreate the same ragged shape as inputs
-            return tf.RaggedTensor.from_tensor(
-                position_embeddings,
-                inputs.nested_row_lengths(),
-            )
-        else:
-            return self._trim_and_broadcast_position_embeddings(
-                tf.shape(inputs),
-                start_index,
-            )
-
-    def _trim_and_broadcast_position_embeddings(self, shape, start_index):
+        shape = ops.shape(inputs)
         feature_length = shape[-1]
         sequence_length = shape[-2]
         # trim to match the length of the input sequence, which might be less
         # than the sequence_length of the layer.
-        position_embeddings = tf.slice(
-            self.position_embeddings,
+        position_embeddings = ops.convert_to_tensor(self.position_embeddings)
+        position_embeddings = ops.slice(
+            position_embeddings,
             (start_index, 0),
             (sequence_length, feature_length),
         )
-        # then broadcast to add the missing dimensions to match "shape"
-        return tf.broadcast_to(position_embeddings, shape)
+        return ops.broadcast_to(position_embeddings, shape)
+
+    def compute_output_shape(self, input_shape):
+        return input_shape
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/position_embedding_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/position_embedding_test.py`

 * *Files 19% similar despite different names*

```diff
@@ -11,43 +11,42 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for position embedding layer."""
 
 import os
 
-import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
-
-from keras_nlp.src.layers import position_embedding
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling import position_embedding
+from keras_nlp.src.tests.test_case import TestCase
 
 
 def custom_init(shape, dtype=None):
     count = 1
     for length in shape:
         count *= length
-    return tf.reshape(tf.range(count, dtype=dtype), shape)
+    return ops.reshape(ops.arange(count, dtype=dtype), shape)
 
 
-class PositionEmbeddingLayerTest(tf.test.TestCase, parameterized.TestCase):
+class PositionEmbeddingTest(TestCase):
     def test_static_layer_output_shape(self):
         # Create a 3-dimensional input (the first dimension is implicit).
         sequence_length = 21
         feature_size = 30
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=sequence_length
         )
         input_tensor = keras.Input(shape=(sequence_length, feature_size))
         output_tensor = test_layer(input_tensor)
 
         # When using static position embedding shapes, the output is expected
         # to be the same as the input shape in all dimensions save batch.
-        expected_output_shape = [None, sequence_length, feature_size]
-        self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+        expected_output_shape = (None, sequence_length, feature_size)
+        self.assertEqual(expected_output_shape, output_tensor.shape)
         # The output dtype for this layer should match the compute dtype.
         self.assertEqual(test_layer.compute_dtype, output_tensor.dtype)
 
     def test_more_than_3_dimensions_static(self):
         # Create a 4-dimensional input (the first dimension is implicit).
         sequence_length = 21
         feature_size = 30
@@ -57,21 +56,21 @@
         input_tensor = keras.Input(
             shape=(feature_size, sequence_length, feature_size)
         )
         output_tensor = test_layer(input_tensor)
 
         # When using static position embedding shapes, the output is expected
         # to be the same as the input shape in all dimensions save batch.
-        expected_output_shape = [
+        expected_output_shape = (
             None,
             feature_size,
             sequence_length,
             feature_size,
-        ]
-        self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+        )
+        self.assertEqual(expected_output_shape, output_tensor.shape)
         # The output dtype for this layer should match the compute dtype.
         self.assertEqual(test_layer.compute_dtype, output_tensor.dtype)
 
     def test_float16_dtype(self):
         # Create a 3-dimensional input (the first dimension is implicit).
         sequence_length = 21
         feature_size = 30
@@ -79,50 +78,50 @@
             sequence_length=sequence_length, dtype="float16"
         )
         input_tensor = keras.Input(shape=(sequence_length, feature_size))
         output_tensor = test_layer(input_tensor)
 
         # When using static position embedding shapes, the output is expected
         # to be the same as the input shape in all dimensions save batch.
-        expected_output_shape = [None, sequence_length, feature_size]
-        self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
-        # The default output dtype for this layer should be tf.float32.
-        self.assertEqual(tf.float16, output_tensor.dtype)
+        expected_output_shape = (None, sequence_length, feature_size)
+        self.assertEqual(expected_output_shape, output_tensor.shape)
+        # The default output dtype for this layer should be "float32".
+        self.assertEqual("float16", output_tensor.dtype)
 
     def test_dynamic_layer_output_shape(self):
         max_sequence_length = 40
         feature_size = 30
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length
         )
         # Create a 3-dimensional input (the first dimension is implicit).
         input_tensor = keras.Input(shape=(None, feature_size))
         output_tensor = test_layer(input_tensor)
 
         # When using dynamic position embedding shapes, the output is expected
         # to be the same as the input shape in all dimensions - but may be None
         # if the input shape is None there.
-        expected_output_shape = [None, None, feature_size]
-        self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+        expected_output_shape = (None, None, feature_size)
+        self.assertEqual(expected_output_shape, output_tensor.shape)
 
     def test_more_than_3_dimensions_dynamic(self):
         max_sequence_length = 60
         feature_size = 30
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length
         )
         # Create a 4-dimensional input (the first dimension is implicit).
         input_tensor = keras.Input(shape=(None, None, feature_size))
         output_tensor = test_layer(input_tensor)
 
         # When using dynamic position embedding shapes, the output is expected
         # to be the same as the input shape in all dimensions - but may be None
         # if the input shape is None there.
-        expected_output_shape = [None, None, None, feature_size]
-        self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+        expected_output_shape = (None, None, None, feature_size)
+        self.assertEqual(expected_output_shape, output_tensor.shape)
 
     def test_dynamic_layer_slicing(self):
         max_sequence_length = 40
         feature_size = 30
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length
         )
@@ -136,15 +135,15 @@
         # should trigger a down-slice.
         input_length = 17
         # Note: This test explicitly uses a batch size of 1. This is to get
         # around Keras' restriction on Model invocations: inputs are expected to
         # have the same batch cardinality as outputs. In practice, this layer
         # should be used inside a model, where it can be projected when added to
         # another tensor.
-        input_data = tf.ones(shape=[1, input_length, feature_size])
+        input_data = ops.ones(shape=[1, input_length, feature_size])
         output_data = model.predict(input_data)
 
         self.assertAllEqual([1, input_length, feature_size], output_data.shape)
 
     def test_callable_initializer(self):
         max_sequence_length = 4
         feature_size = 3
@@ -153,172 +152,74 @@
             initializer=custom_init,
         )
         inputs = keras.Input(shape=(max_sequence_length, feature_size))
         outputs = test_layer(inputs)
         model = keras.Model(inputs=inputs, outputs=outputs)
 
         batch_size = 2
-        data = tf.zeros(shape=[batch_size, max_sequence_length, feature_size])
+        data = ops.zeros(shape=[batch_size, max_sequence_length, feature_size])
         model(data)
         model_output = model.predict(data)
-        expected_output = tf.broadcast_to(
-            tf.reshape(
-                tf.range(max_sequence_length * feature_size),
+        expected_output = ops.broadcast_to(
+            ops.reshape(
+                ops.arange(max_sequence_length * feature_size),
                 [max_sequence_length, feature_size],
             ),
             [batch_size, max_sequence_length, feature_size],
         )
         self.assertAllClose(model_output, expected_output)
 
-    def test_ragged_tensor_with_3_dimensions(self):
-        max_sequence_length = 4
-        feature_size = 2
-        test_layer = position_embedding.PositionEmbedding(
-            sequence_length=max_sequence_length,
-            initializer=custom_init,
-        )
-        # Create a 3-dimensional ragged input (the first dimension is implicit).
-        input_tensor = keras.Input(
-            shape=(None, feature_size), dtype=tf.float32, ragged=True
-        )
-        output_tensor = test_layer(input_tensor)
-        model = keras.Model(input_tensor, output_tensor)
-
-        input_data = tf.ragged.constant(
-            [
-                [[1.0, 1.0], [1.0, 1.0]],
-                [],
-                [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],
-                [[1.0, 1.0]],
-            ],
-            ragged_rank=1,
-            inner_shape=(2,),
-        )
-        expected_output_data = tf.ragged.constant(
-            [
-                [[0.0, 1.0], [2.0, 3.0]],
-                [],
-                [[0.0, 1.0], [2.0, 3.0], [4.0, 5.0]],
-                [[0.0, 1.0]],
-            ],
-            ragged_rank=1,
-            inner_shape=(2,),
-        )
-        output_data = model.predict(input_data)
-        self.assertAllClose(output_data, expected_output_data)
-
-    def test_ragged_tensor_with_4_dimensions(self):
-        max_sequence_length = 4
-        feature_size = 2
-        test_layer = position_embedding.PositionEmbedding(
-            sequence_length=max_sequence_length,
-            initializer=custom_init,
-        )
-        # Create a 4-dimensional ragged input (the first dimension is implicit).
-        input_tensor = keras.Input(
-            shape=(None, None, feature_size), dtype=tf.float32, ragged=True
-        )
-        output_tensor = test_layer(input_tensor)
-        model = keras.Model(input_tensor, output_tensor)
-
-        input_data = tf.ragged.constant(
-            [
-                [
-                    [[1.0, 1.0], [1.0, 1.0]],
-                    [],
-                ],
-                [
-                    [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],
-                    [[1.0, 1.0]],
-                ],
-            ],
-            ragged_rank=2,
-            inner_shape=(2,),
-        )
-        expected_output_data = tf.ragged.constant(
-            [
-                [
-                    [[0.0, 1.0], [2.0, 3.0]],
-                    [],
-                ],
-                [
-                    [[0.0, 1.0], [2.0, 3.0], [4.0, 5.0]],
-                    [[0.0, 1.0]],
-                ],
-            ],
-            ragged_rank=2,
-            inner_shape=(2,),
-        )
-        output_data = model.predict(input_data)
-        self.assertAllClose(output_data, expected_output_data)
-
     def test_one_training_step(self):
         max_sequence_length = 4
         feature_size = 3
+        inputs = keras.Input(shape=(max_sequence_length, feature_size))
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length
         )
-        inputs = keras.Input(shape=(max_sequence_length, feature_size))
         outputs = test_layer(inputs)
         model = keras.Model(inputs=inputs, outputs=outputs)
 
         batch_size = 2
-        data = tf.random.uniform(
+        data = ops.random.uniform(
+            shape=[batch_size, max_sequence_length, feature_size]
+        )
+        label = ops.random.uniform(
             shape=[batch_size, max_sequence_length, feature_size]
         )
-        label = tf.random.uniform(shape=[max_sequence_length, feature_size])
 
-        loss_fn = keras.losses.MeanSquaredError()
+        loss = keras.losses.MeanSquaredError()
         optimizer = keras.optimizers.Adam()
-        with tf.GradientTape() as tape:
-            pred = model(data)
-            loss = loss_fn(label, pred)
-        grad = tape.gradient(loss, model.trainable_variables)
-        self.assertEqual(len(grad), 1)
-
-        trainable_variables_before = tf.Variable(model.trainable_variables[0])
-        optimizer.apply_gradients(zip(grad, model.trainable_variables))
-        self.assertNotAllClose(
-            trainable_variables_before, model.trainable_variables[0]
-        )
+        model.compile(loss=loss, optimizer=optimizer)
+        loss = model.train_on_batch(x=data, y=label)
+        self.assertGreater(loss, 0)
 
     def test_get_config_and_from_config(self):
         max_sequence_length = 40
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length,
             initializer="zeros",
         )
         config = test_layer.get_config()
         restored = position_embedding.PositionEmbedding.from_config(config)
         self.assertEqual(restored.get_config(), config)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         max_sequence_length = 4
         feature_size = 6
         test_layer = position_embedding.PositionEmbedding(
             sequence_length=max_sequence_length
         )
         inputs = keras.Input(shape=(max_sequence_length, feature_size))
         outputs = test_layer(inputs)
         model = keras.Model(inputs=inputs, outputs=outputs)
 
-        data = tf.zeros(shape=[2, max_sequence_length, feature_size])
+        data = ops.zeros(shape=[2, max_sequence_length, feature_size])
         model(data)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         loaded_model = keras.models.load_model(path)
 
         model_output = model.predict(data)
         loaded_model_output = loaded_model.predict(data)
         self.assertAllClose(model_output, loaded_model_output)
 
-
-if __name__ == "__main__":
-    tf.test.main()
-
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/random_deletion.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_swap.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,266 +10,255 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import random
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
+from keras_nlp.src.utils.tensor_utils import is_string_dtype
 
 
-@keras_nlp_export("keras_nlp.layers.RandomDeletion")
-class RandomDeletion(keras.layers.Layer):
-    """Augments input by randomly deleting tokens.
+@keras_nlp_export("keras_nlp.layers.RandomSwap")
+class RandomSwap(PreprocessingLayer):
+    """Augments input by randomly swapping words.
 
-    This layer comes in handy when you need to generate new data using deletion
-    augmentation as described in the paper [EDA: Easy Data Augmentation
+    This layer comes in handy when you need to generate new data using swap
+    augmentations as described in the paper [EDA: Easy Data Augmentation
     Techniques for Boosting Performance on Text Classification Tasks]
     (https://arxiv.org/pdf/1901.11196.pdf). The layer expects the inputs to be
     pre-split into token level inputs. This allows control over the level of
     augmentation, you can split by character for character level swaps, or by
     word for word level swaps.
 
-    Input should be either a `tf.RaggedTensor` or a dense `tf.Tensor`, and
-    either rank-1 or rank-2.
+    Input data should be passed as tensors, `tf.RaggedTensor`s, or lists. For
+    batched input, inputs should be a list of lists or a rank two tensor. For
+    unbatched inputs, each element should be a list or a rank one tensor.
 
     Args:
-        rate: The probability of a token being chosen for deletion.
-        max_deletions: The maximum number of tokens to delete.
+        rate: The probability of a given token being chosen to be swapped
+            with another random token.
+        max_swaps: The maximum number of swaps to be performed.
         skip_list: A list of token values that should not be considered
             candidates for deletion.
         skip_fn: A function that takes as input a scalar tensor token and
             returns as output a scalar tensor True/False value. A value of
             True indicates that the token should not be considered a
             candidate for deletion. This function must be tracable--it
             should consist of tensorflow operations.
         skip_py_fn: A function that takes as input a python token value and
             returns as output `True` or `False`. A value of True
             indicates that should not be considered a candidate for deletion.
             Unlike the `skip_fn` argument, this argument need not be
             tracable--it can be any python function.
         seed: A seed for the random number generator.
 
+
     Examples:
 
     Word level usage.
     >>> keras.utils.set_random_seed(1337)
     >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
-    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4, seed=42)
+    >>> augmenter=keras_nlp.layers.RandomSwap(rate=0.4, seed=42)
     >>> augmented=augmenter(inputs)
     >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'I like', b'and'],
-    dtype=object)>
+    <tf.Tensor: shape=(2,), dtype=string,
+    numpy=array([b'like I Hey', b'and Keras Tensorflow'], dtype=object)>
 
     Character level usage.
     >>> keras.utils.set_random_seed(1337)
     >>> inputs=tf.strings.unicode_split(["Hey Dude", "Speed Up"], "UTF-8")
-    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4, seed=42)
+    >>> augmenter=keras_nlp.layers.RandomSwap(rate=0.4, seed=42)
     >>> augmented=augmenter(inputs)
     >>> tf.strings.reduce_join(augmented, axis=-1)
-    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'H Dude', b'pedUp'],
-    dtype=object)>
+    <tf.Tensor: shape=(2,), dtype=string,
+    numpy=array([b'deD yuHe', b'SUede pp'], dtype=object)>
 
     Usage with skip_list.
     >>> keras.utils.set_random_seed(1337)
     >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
-    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4,
-    ...     skip_list=["Keras", "Tensorflow"], seed=42)
+    >>> augmenter=keras_nlp.layers.RandomSwap(rate=0.4,
+    ...     skip_list=["Keras"], seed=42)
     >>> augmented=augmenter(inputs)
     >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
     <tf.Tensor: shape=(2,), dtype=string,
-    numpy=array([b'I like', b'Keras Tensorflow'], dtype=object)>
+    numpy=array([b'like I Hey', b'Keras and Tensorflow'], dtype=object)>
 
     Usage with skip_fn.
     >>> def skip_fn(word):
-    ...     return tf.strings.regex_full_match(word, r"\\pP")
+    ...     return tf.strings.regex_full_match(word, r"[I, a].*")
     >>> keras.utils.set_random_seed(1337)
     >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
-    >>> augmenter=keras_nlp.layers.RandomDeletion(rate=0.4,
-    ...     skip_fn=skip_fn, seed=42)
+    >>> augmenter=keras_nlp.layers.RandomSwap(rate=0.9, max_swaps=3,
+    ...     skip_fn=skip_fn, seed=11)
     >>> augmented=augmenter(inputs)
     >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'I like', b'and'],
-    dtype=object)>
+    <tf.Tensor: shape=(2,), dtype=string,
+    numpy=array([b'like I Hey', b'Keras and Tensorflow'], dtype=object)>
 
     Usage with skip_py_fn.
     >>> def skip_py_fn(word):
     ...     return len(word) < 4
     >>> keras.utils.set_random_seed(1337)
-    >>> inputs=tf.strings.split(["Hey I like", "Keras and Tensorflow"])
-    >>> augmenter=RandomDeletion(rate=0.4,
-    ...     skip_py_fn=skip_py_fn, seed=42)
+    >>> inputs=tf.strings.split(["He was drifting along", "With the wind"])
+    >>> augmenter=keras_nlp.layers.RandomSwap(rate=0.8, max_swaps=2,
+    ...     skip_py_fn=skip_py_fn, seed=15)
     >>> augmented=augmenter(inputs)
     >>> tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-    <tf.Tensor: shape=(2,), dtype=string,
-    numpy=array([b'Hey I', b'and Tensorflow'], dtype=object)>
+    <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'He was along drifting',
+    b'wind the With'], dtype=object)>
     """
 
     def __init__(
         self,
         rate,
-        max_deletions=None,
+        max_swaps=None,
         skip_list=None,
         skip_fn=None,
         skip_py_fn=None,
         seed=None,
         name=None,
+        dtype="int32",
         **kwargs,
     ):
-        # Check dtype and provide a default.
-        if "dtype" not in kwargs or kwargs["dtype"] is None:
-            kwargs["dtype"] = tf.int32
-        else:
-            dtype = tf.dtypes.as_dtype(kwargs["dtype"])
-            if not dtype.is_integer and dtype != tf.string:
-                raise ValueError(
-                    "Output dtype must be one of `'string'`, `'int32'`, and "
-                    f"`'int64'`. Received: dtype={dtype}"
-                )
+        if not is_integer_dtype(dtype) and not is_string_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type or a string. "
+                f"Received: dtype={dtype}"
+            )
+
+        super().__init__(name=name, dtype=dtype, **kwargs)
 
-        super().__init__(name=name, **kwargs)
         self.rate = rate
-        self.max_deletions = max_deletions
+        self.max_swaps = max_swaps
         self.seed = random.randint(1, 1e9) if seed is None else seed
         self._generator = tf.random.Generator.from_seed(self.seed)
         self.skip_list = skip_list
         self.skip_fn = skip_fn
         self.skip_py_fn = skip_py_fn
-        if self.max_deletions is not None and self.max_deletions < 0:
+        if self.max_swaps is not None and self.max_swaps < 0:
             raise ValueError(
-                "max_deletions must be non-negative."
-                f"Received max_deletions={max_deletions}."
-            )
-
-        if self.rate > 1 or self.rate < 0:
-            raise ValueError(
-                "Rate must be between 0 and 1 (both inclusive)."
-                f"Received: rate={rate}"
+                "max_swaps must be non-negative."
+                f"Received max_swaps={max_swaps}."
             )
 
         if [self.skip_list, self.skip_fn, self.skip_py_fn].count(None) < 2:
             raise ValueError(
-                "Exactly one of `skip_list`, `skip_fn`, `skip_py_fn` must be "
+                "Exactly one of skip_list, skip_fn, skip_py_fn must be "
                 "provided."
             )
 
         if self.skip_list:
             self.StaticHashTable = tf.lookup.StaticHashTable(
                 tf.lookup.KeyValueTensorInitializer(
                     tf.convert_to_tensor(self.skip_list),
                     tf.convert_to_tensor([True] * len(self.skip_list)),
                 ),
                 default_value=False,
             )
 
     def call(self, inputs):
-        if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):
-            inputs = tf.convert_to_tensor(inputs)
-
-        input_is_1d = False
-        if inputs.shape.rank < 1 or inputs.shape.rank > 2:
-            raise ValueError(
-                "Input must either be rank 1 or rank 2. Received input with "
-                f"rank={inputs.shape.rank}"
-            )
-        elif inputs.shape.rank == 1:
-            input_is_1d = True
-            # Add a new axis at the beginning.
-            inputs = tf.expand_dims(inputs, axis=0)
-        if isinstance(inputs, tf.Tensor):
-            # Convert to ragged tensor.
-            inputs = tf.RaggedTensor.from_tensor(inputs)
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
 
         skip_masks = None
         if self.skip_list:
             skip_masks = self.StaticHashTable.lookup(inputs.flat_values)
         elif self.skip_fn:
             skip_masks = tf.map_fn(
-                self.skip_fn, inputs.flat_values, fn_output_signature=tf.bool
+                self.skip_fn, inputs.flat_values, fn_output_signature="bool"
             )
         elif self.skip_py_fn:
 
             def string_fn(token):
                 return self.skip_py_fn(token.numpy().decode("utf-8"))
 
             def int_fn(token):
                 return self.skip_py_fn(token.numpy())
 
             py_fn = string_fn if inputs.dtype == tf.string else int_fn
 
             skip_masks = tf.map_fn(
-                lambda x: tf.py_function(py_fn, [x], tf.bool),
+                lambda x: tf.py_function(py_fn, [x], "bool"),
                 inputs.flat_values,
-                fn_output_signature=tf.bool,
+                fn_output_signature="bool",
             )
 
-        positions_flat = tf.range(tf.size(inputs.flat_values))
-        positions = inputs.with_flat_values(positions_flat)
+        positions = tf.ragged.range(inputs.row_lengths())
+
         if skip_masks is not None:
             skip_masks = tf.logical_not(skip_masks)
             skip_masks.set_shape([None])
             positions = tf.ragged.boolean_mask(
                 positions, inputs.with_flat_values(skip_masks)
             )
-
         # Figure out how many we are going to select.
         token_counts = tf.cast(positions.row_lengths(), "float32")
         num_to_select = tf.random.stateless_binomial(
             shape=tf.shape(token_counts),
             seed=self._generator.make_seeds()[:, 0],
             counts=token_counts,
             probs=self.rate,
         )
-        if self.max_deletions is not None:
-            num_to_select = tf.math.minimum(num_to_select, self.max_deletions)
+        if self.max_swaps is not None:
+            num_to_select = tf.math.minimum(num_to_select, self.max_swaps)
+        num_to_select = tf.math.minimum(
+            num_to_select, tf.cast(positions.row_lengths(), "int32")
+        )
         num_to_select = tf.cast(num_to_select, "int64")
 
-        # Shuffle and trim to items that are going to be selected.
-        def _shuffle_and_trim(x):
-            positions, top_n = x
-            shuffled = tf.random.shuffle(positions, seed=self.seed)
-            return shuffled[:top_n]
-
-        selected_for_mask = tf.map_fn(
-            _shuffle_and_trim,
-            (positions, num_to_select),
+        def _swap(x):
+            positions, inputs, num_to_select = x
+            for _ in range(num_to_select):
+                index = tf.random.stateless_uniform(
+                    shape=[2],
+                    minval=0,
+                    maxval=tf.size(positions),
+                    dtype="int32",
+                    seed=self._generator.make_seeds()[:, 0],
+                )
+                index1, index2 = positions[index[0]], positions[index[1]]
+                # swap items at the sampled indices with each other
+                inputs = tf.tensor_scatter_nd_update(
+                    inputs,
+                    [[index1], [index2]],
+                    [inputs[index2], inputs[index1]],
+                )
+            return inputs
+
+        swapped = tf.map_fn(
+            _swap,
+            (positions, inputs, num_to_select),
             fn_output_signature=tf.RaggedTensorSpec(
-                ragged_rank=positions.ragged_rank - 1, dtype=positions.dtype
+                ragged_rank=positions.ragged_rank - 1, dtype=inputs.dtype
             ),
         )
-        selected_for_mask.flat_values.set_shape([None])
-
-        # Construct the mask which is a boolean RT
-        # Scatter 0's to positions that have been selector for deletion.
-        update_values = tf.zeros_like(selected_for_mask.flat_values, "int32")
-        update_indices = selected_for_mask.flat_values
-        update_indices = tf.expand_dims(update_indices, -1)
-        update_indices = tf.cast(update_indices, "int32")
-        mask_flat = tf.ones_like(inputs.flat_values, dtype="int32")
-        mask_flat = tf.tensor_scatter_nd_update(
-            mask_flat, update_indices, update_values
-        )
-        mask = tf.cast(inputs.with_flat_values(mask_flat), "bool")
+        swapped.flat_values.set_shape([None])
 
-        inputs = tf.ragged.boolean_mask(inputs, mask)
-
-        if input_is_1d:
-            inputs = tf.squeeze(inputs, axis=0)
-
-        return inputs
+        if unbatched:
+            swapped = tf.squeeze(swapped, axis=0)
+        return swapped
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "rate": self.rate,
-                "max_deletions": self.max_deletions,
+                "max_swaps": self.max_swaps,
                 "seed": self.seed,
                 "skip_list": self.skip_list,
                 "skip_fn": self.skip_fn,
                 "skip_py_fn": self.skip_py_fn,
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape):
+        inputs_shape = list(inputs_shape)
+        inputs_shape[-1] = None
+        return tuple(inputs_shape)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/random_deletion_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_deletion_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,90 +10,87 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for RandomDeletion Layer."""
 
 import tensorflow as tf
-from tensorflow import keras
 
-from keras_nlp.src.layers.random_deletion import RandomDeletion
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.preprocessing.random_deletion import RandomDeletion
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RandomDeletionTest(tf.test.TestCase):
+class RandomDeletionTest(TestCase):
     def test_shape_and_output_from_word_deletion(self):
         keras.utils.set_random_seed(1337)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         augmenter = RandomDeletion(rate=0.4, max_deletions=1, seed=42)
         augmented = augmenter(split)
-        output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"I like", b"and Tensorflow"]
+        output = [
+            tf.strings.reduce_join(x, separator=" ", axis=-1) for x in augmented
+        ]
+        exp_output = ["I like", "and Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
     def test_shape_and_output_from_character_swaps(self):
         keras.utils.set_random_seed(1337)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.unicode_split(inputs, "UTF-8")
         augmenter = RandomDeletion(rate=0.4, max_deletions=1, seed=42)
         augmented = augmenter(split)
-        output = tf.strings.reduce_join(augmented, axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"Hey I lie", b"Keras and Tensoflow"]
+        output = [tf.strings.reduce_join(x, axis=-1) for x in augmented]
+        exp_output = ["Hey I lie", "Keras and Tensoflow"]
         self.assertAllEqual(output, exp_output)
 
     def test_with_integer_tokens(self):
         keras.utils.set_random_seed(1337)
         inputs = tf.constant([[1, 2], [3, 4]])
         augmenter = RandomDeletion(rate=0.4, max_deletions=4, seed=42)
         output = augmenter(inputs)
-        self.assertAllEqual(output.to_tensor().shape[0], inputs.shape[0])
         exp_output = [[2], [4]]
         self.assertAllEqual(output, exp_output)
 
     def test_skip_options(self):
         keras.utils.set_random_seed(1337)
         augmenter = RandomDeletion(
             rate=0.4, max_deletions=1, seed=42, skip_list=["Tensorflow", "like"]
         )
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"I like", b"and Tensorflow"]
+        exp_output = ["I like", "and Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             if word == "Tensorflow" or word == "like":
                 return True
             return False
 
         augmenter = RandomDeletion(
             rate=0.4, max_deletions=1, seed=42, skip_fn=skip_fn
         )
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"Hey like", b"Keras Tensorflow"]
+        exp_output = ["Hey like", "Keras Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
         def skip_py_fn(word):
             if word == "Tensorflow" or word == "like":
                 return True
             return False
 
         augmenter = RandomDeletion(
             rate=0.4, max_deletions=1, seed=42, skip_py_fn=skip_py_fn
         )
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"Hey like", b"Keras Tensorflow"]
+        exp_output = ["Hey like", "Keras Tensorflow"]
 
     def test_get_config_and_from_config(self):
         augmenter = RandomDeletion(rate=0.4, max_deletions=1, seed=42)
 
         expected_config_subset = {"max_deletions": 1, "rate": 0.4, "seed": 42}
 
         config = augmenter.get_config()
@@ -115,15 +112,15 @@
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
 
-        exp_output = [[b"I", b"like"], [b"Keras", b"and", b"Tensorflow"]]
+        exp_output = [["I", "like"], ["Keras", "and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             return tf.strings.regex_full_match(word, r"\pP")
 
         def skip_py_fn(word):
             return len(word) < 4
@@ -131,68 +128,56 @@
         augmenter = RandomDeletion(
             rate=0.8, max_deletions=1, seed=42, skip_fn=skip_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
-        exp_output = [[b"I", b"like"], [b"and", b"Tensorflow"]]
+        exp_output = [["I", "like"], ["and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
         augmenter = RandomDeletion(
             rate=0.8, max_deletions=1, seed=42, skip_py_fn=skip_py_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
-        exp_output = [[b"Hey", b"I", b"like"], [b"and", b"Tensorflow"]]
+        exp_output = [["Hey", "I", "like"], ["and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
     def test_batch_first_augment_second(self):
         keras.utils.set_random_seed(1337)
         augmenter = RandomDeletion(rate=0.4, max_deletions=1, seed=42)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(5).map(augmenter)
         output = ds.take(1).get_single_element()
 
-        exp_output = [[b"I", b"like"], [b"and", b"Tensorflow"]]
+        exp_output = [["I", "like"], ["and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             return tf.strings.regex_full_match(word, r"\pP")
 
         def skip_py_fn(word):
             return len(word) < 4
 
         augmenter = RandomDeletion(
             rate=0.8, max_deletions=1, seed=42, skip_fn=skip_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(5).map(augmenter)
         output = ds.take(1).get_single_element()
-        exp_output = [[b"I", b"like"], [b"and", b"Tensorflow"]]
+        exp_output = [["I", "like"], ["and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
         augmenter = RandomDeletion(
             rate=0.8, max_deletions=1, seed=42, skip_py_fn=skip_py_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(5).map(augmenter)
         output = ds.take(1).get_single_element()
-        exp_output = [[b"Hey", b"I", b"like"], [b"and", b"Tensorflow"]]
+        exp_output = [["Hey", "I", "like"], ["and", "Tensorflow"]]
         self.assertAllEqual(output, exp_output)
 
-    def test_functional_model(self):
-        keras.utils.set_random_seed(1337)
-        input_data = tf.constant(["Hey I like", "Keras and Tensorflow"])
-        augmenter = RandomDeletion(rate=0.4, max_deletions=1, seed=42)
-        inputs = tf.keras.Input(dtype="string", shape=())
-        outputs = augmenter(tf.strings.split(inputs))
-        model = tf.keras.Model(inputs, outputs)
-        model_output = model(input_data)
-        self.assertAllEqual(
-            model_output, [[b"I", b"like"], [b"and", b"Tensorflow"]]
-        )
-
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/random_swap_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/random_swap_test.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,90 +10,85 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for RandomSwaps Layer."""
 
 import tensorflow as tf
-from tensorflow import keras
 
-from keras_nlp.src.layers.random_swap import RandomSwap
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.preprocessing.random_swap import RandomSwap
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RandomSwapTest(tf.test.TestCase):
+class RandomSwapTest(TestCase):
     def test_shape_and_output_from_word_swap(self):
         keras.utils.set_random_seed(1337)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         augmenter = RandomSwap(rate=0.7, max_swaps=3, seed=42)
         augmented = augmenter(split)
-        output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"like I Hey", b"Tensorflow Keras and"]
+        output = [
+            tf.strings.reduce_join(x, separator=" ", axis=-1) for x in augmented
+        ]
+        exp_output = ["like I Hey", "Tensorflow Keras and"]
         self.assertAllEqual(output, exp_output)
 
     def test_shape_and_output_from_character_swap(self):
         keras.utils.set_random_seed(1337)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.unicode_split(inputs, "UTF-8")
         augmenter = RandomSwap(rate=0.7, max_swaps=6, seed=42)
         augmented = augmenter(split)
-        output = tf.strings.reduce_join(augmented, axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"yli I eHke", b"seaad rnK Tensolrfow"]
+        output = [tf.strings.reduce_join(x, axis=-1) for x in augmented]
+        exp_output = ["yli I eHke", "seaad rnK Tensolrfow"]
         self.assertAllEqual(output, exp_output)
 
     def test_with_integer_tokens(self):
         keras.utils.set_random_seed(1337)
         inputs = tf.constant([[1, 2, 3], [4, 5, 6]])
         augmenter = RandomSwap(rate=0.7, max_swaps=6, seed=42)
         output = augmenter(inputs)
-        self.assertAllEqual(
-            output.to_tensor().shape, tf.convert_to_tensor(inputs).shape
-        )
         exp_output = [[3, 2, 1], [6, 4, 5]]
         self.assertAllEqual(output, exp_output)
 
     def test_skip_options(self):
         keras.utils.set_random_seed(1337)
         augmenter = RandomSwap(
             rate=0.9, max_swaps=3, seed=11, skip_list=["Tensorflow", "like"]
         )
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"I Hey like", b"Keras and Tensorflow"]
+        exp_output = ["I Hey like", "Keras and Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             if word == "Tensorflow" or word == "like":
                 return True
             return False
 
         augmenter = RandomSwap(rate=0.9, max_swaps=3, seed=11, skip_fn=skip_fn)
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"I Hey like", b"Keras and Tensorflow"]
+        exp_output = ["I Hey like", "Keras and Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
         def skip_py_fn(word):
             if word == "Tensorflow" or word == "like":
                 return True
             return False
 
         augmenter = RandomSwap(
             rate=0.9, max_swaps=3, seed=11, skip_py_fn=skip_py_fn
         )
         augmented = augmenter(split)
         output = tf.strings.reduce_join(augmented, separator=" ", axis=-1)
-        self.assertAllEqual(output.shape, tf.convert_to_tensor(inputs).shape)
-        exp_output = [b"I Hey like", b"Keras and Tensorflow"]
+        exp_output = ["I Hey like", "Keras and Tensorflow"]
         self.assertAllEqual(output, exp_output)
 
     def test_get_config_and_from_config(self):
         augmenter = RandomSwap(rate=0.4, max_swaps=3, seed=42)
 
         expected_config_subset = {"rate": 0.4, "max_swaps": 3, "seed": 42}
 
@@ -116,16 +111,16 @@
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"like", b"I", b"Hey"],
-            [b"and", b"Tensorflow", b"Keras"],
+            ["like", "I", "Hey"],
+            ["and", "Tensorflow", "Keras"],
         ]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             # Regex to match words starting with I or a
             return tf.strings.regex_full_match(word, r"[I, a].*")
 
@@ -134,43 +129,43 @@
 
         augmenter = RandomSwap(rate=0.7, max_swaps=5, seed=11, skip_fn=skip_fn)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"like", b"I", b"Hey"],
-            [b"Keras", b"and", b"Tensorflow"],
+            ["like", "I", "Hey"],
+            ["Keras", "and", "Tensorflow"],
         ]
         self.assertAllEqual(output, exp_output)
 
         augmenter = RandomSwap(
             rate=0.7, max_swaps=2, seed=42, skip_py_fn=skip_py_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.map(augmenter)
         ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"Hey", b"I", b"like"],
-            [b"Tensorflow", b"Keras", b"and"],
+            ["Hey", "I", "like"],
+            ["Tensorflow", "Keras", "and"],
         ]
         self.assertAllEqual(output, exp_output)
 
     def test_batch_first_augment_second(self):
         keras.utils.set_random_seed(1337)
         augmenter = RandomSwap(rate=0.7, max_swaps=2, seed=42)
         inputs = ["Hey I like", "Keras and Tensorflow"]
         split = tf.strings.split(inputs)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(2).map(augmenter)
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"like", b"I", b"Hey"],
-            [b"Tensorflow", b"Keras", b"and"],
+            ["like", "I", "Hey"],
+            ["Tensorflow", "Keras", "and"],
         ]
         self.assertAllEqual(output, exp_output)
 
         def skip_fn(word):
             # Regex to match words starting with I
             return tf.strings.regex_full_match(word, r"[I].*")
 
@@ -178,38 +173,24 @@
             return len(word) < 2
 
         augmenter = RandomSwap(rate=0.7, max_swaps=2, seed=42, skip_fn=skip_fn)
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(2).map(augmenter)
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"Hey", b"I", b"like"],
-            [b"and", b"Keras", b"Tensorflow"],
+            ["Hey", "I", "like"],
+            ["and", "Keras", "Tensorflow"],
         ]
         self.assertAllEqual(output, exp_output)
 
         augmenter = RandomSwap(
             rate=0.7, max_swaps=2, seed=42, skip_py_fn=skip_py_fn
         )
         ds = tf.data.Dataset.from_tensor_slices(split)
         ds = ds.batch(2).map(augmenter)
         output = ds.take(1).get_single_element()
         exp_output = [
-            [b"Hey", b"I", b"like"],
-            [b"and", b"Keras", b"Tensorflow"],
+            ["Hey", "I", "like"],
+            ["and", "Keras", "Tensorflow"],
         ]
         self.assertAllEqual(output, exp_output)
 
-    def test_functional_model(self):
-        keras.utils.set_random_seed(1337)
-        input_data = tf.constant(["Hey I like", "Keras and Tensorflow"])
-        augmenter = RandomSwap(rate=0.7, max_swaps=2, seed=42)
-        inputs = tf.keras.Input(dtype="string", shape=())
-        outputs = augmenter(tf.strings.split(inputs))
-        model = tf.keras.Model(inputs, outputs)
-        model_output = model(input_data)
-        exp_output = [
-            [b"like", b"I", b"Hey"],
-            [b"Tensorflow", b"Keras", b"and"],
-        ]
-        self.assertAllEqual(model_output, exp_output)
-
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/sine_position_encoding.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/sine_position_encoding.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,18 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Sinusoidal position embedding layer."""
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 
 
 @keras_nlp_export("keras_nlp.layers.SinePositionEncoding")
 class SinePositionEncoding(keras.layers.Layer):
     """Sinusoidal positional encoding layer.
 
     This layer calculates the position encoding as a mix of sine and cosine
@@ -29,31 +28,26 @@
     in [Attention is All You Need](https://arxiv.org/abs/1706.03762).
 
     Takes as input an embedded token tensor. The input must have shape
     [batch_size, sequence_length, feature_size]. This layer will return a
     positional encoding the same size as the embedded token tensor, which
     can be added directly to the embedded token tensor.
 
-    This layer optionally accepts `tf.RaggedTensor`s as inputs to process
-    batches of sequences of different lengths. The one ragged dimension must be
-    the dimension that corresponds to the sequence, that is, the penultimate
-    dimension.
-
     Args:
         max_wavelength: The maximum angular wavelength of the sine/cosine
             curves, as described in Attention is All You Need. Defaults to
-            10000.
+            `10000`.
 
     Examples:
     ```python
     # create a simple embedding layer with sinusoidal positional encoding
     seq_len = 100
     vocab_size = 1000
     embedding_dim = 32
-    inputs = keras.Input((seq_len,), dtype=tf.float32)
+    inputs = keras.Input((seq_len,), dtype="float32")
     embedding = keras.layers.Embedding(
         input_dim=vocab_size, output_dim=embedding_dim
     )(inputs)
     positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)
     outputs = embedding + positional_encoding
     ```
 
@@ -66,56 +60,40 @@
         max_wavelength=10000,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.max_wavelength = max_wavelength
 
     def call(self, inputs):
-        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency
-        # with other layers.
-        if isinstance(inputs, tf.RaggedTensor):
-            bounding_shape = inputs.bounding_shape()
-            position_embeddings = (
-                self._compute_trim_and_broadcast_position_embeddings(
-                    bounding_shape,
-                )
-            )
-            # then apply row lengths to recreate the same ragged shape as inputs
-            return tf.RaggedTensor.from_tensor(
-                position_embeddings,
-                inputs.nested_row_lengths(),
-            )
-        else:
-            return self._compute_trim_and_broadcast_position_embeddings(
-                tf.shape(inputs),
-            )
-
-    def _compute_trim_and_broadcast_position_embeddings(self, shape):
+        shape = ops.shape(inputs)
         seq_length = shape[-2]
         hidden_size = shape[-1]
-        position = tf.cast(tf.range(seq_length), self.compute_dtype)
-        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)
-        timescales = tf.pow(
+        position = ops.cast(ops.arange(seq_length), self.compute_dtype)
+        min_freq = ops.cast(1 / self.max_wavelength, dtype=self.compute_dtype)
+        timescales = ops.power(
             min_freq,
-            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)
-            / tf.cast(hidden_size, self.compute_dtype),
+            ops.cast(2 * (ops.arange(hidden_size) // 2), self.compute_dtype)
+            / ops.cast(hidden_size, self.compute_dtype),
         )
-        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)
+        angles = ops.expand_dims(position, 1) * ops.expand_dims(timescales, 0)
         # even indices are sine, odd are cosine
-        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)
+        cos_mask = ops.cast(ops.arange(hidden_size) % 2, self.compute_dtype)
         sin_mask = 1 - cos_mask
         # embedding shape is [seq_length, hidden_size]
         positional_encodings = (
-            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask
+            ops.sin(angles) * sin_mask + ops.cos(angles) * cos_mask
         )
 
-        return tf.broadcast_to(positional_encodings, shape)
+        return ops.broadcast_to(positional_encodings, shape)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "max_wavelength": self.max_wavelength,
             }
         )
         return config
 
+    def compute_output_shape(self, input_shape):
+        return input_shape
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/start_end_packer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/start_end_packer.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,44 +8,48 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Start End Packer implementation based on `keras.layers.Layer`."""
-
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
 
 
 @keras_nlp_export("keras_nlp.layers.StartEndPacker")
-class StartEndPacker(keras.layers.Layer):
+class StartEndPacker(PreprocessingLayer):
     """Adds start and end tokens to a sequence and pads to a fixed length.
 
     This layer is useful when tokenizing inputs for tasks like translation,
     where each sequence should include a start and end marker. It should
     be called after tokenization. The layer will first trim inputs to fit, then
     add start/end tokens, and finally pad, if necessary, to `sequence_length`.
 
-    Input should be either a `tf.RaggedTensor` or a dense `tf.Tensor`, and
-    either rank-1 or rank-2.
+    Input data should be passed as tensors, `tf.RaggedTensor`s, or lists. For
+    batched input, inputs should be a list of lists or a rank two tensor. For
+    unbatched inputs, each element should be a list or a rank one tensor.
 
     Args:
         sequence_length: int. The desired output length.
-        start_value: int/str. The ID or token that is to be placed at the start
-            of each sequence. The dtype must match the dtype of the input
-            tensors to the layer. If None, no start value will be added.
-        end_value: int/str. The ID or token that is to be placed at the end of
-            each input segment. The dtype must match the dtype of the input
-            tensors to the layer. If None, no end value will be added.
+        start_value: int/str/list/tuple. The ID(s) or token(s) that are to be
+            placed at the start of each sequence. The dtype must match the dtype
+            of the input tensors to the layer. If `None`, no start value will be
+            added.
+        end_value: int/str/list/tuple. The ID(s) or token(s) that are to be
+            placed at the end of each input segment. The dtype must match the
+            dtype of the input tensors to the layer. If `None`, no end value
+            will be added.
         pad_value: int/str. The ID or token that is to be placed into the
-            unused positions after the last segment in the sequence. If None,
+            unused positions after the last segment in the sequence. If `None`,
             0 or "" will be added depending on the dtype of the input tensor.
         return_padding_mask: bool. Whether to return a boolean padding mask of
             all locations that are filled in with the `pad_value`.
 
     Call arguments:
         inputs: A `tf.Tensor`, `tf.RaggedTensor`, or list of python strings.
         sequence_length: Pass to override the configured `sequence_length` of
@@ -54,53 +58,63 @@
             input.
         add_end_value: Pass `False` to not append an end value for this
             input.
 
     Examples:
 
     Unbatched input (int).
-    >>> input_data = tf.constant([5, 6, 7])
+    >>> inputs = [5, 6, 7]
     >>> start_end_packer = keras_nlp.layers.StartEndPacker(
     ...     sequence_length=7, start_value=1, end_value=2,
     ... )
-    >>> start_end_packer(input_data)
-    <tf.Tensor: shape=(7,), dtype=int32, numpy=
-    array([1, 5, 6, 7, 2, 0, 0], dtype=int32)>
+    >>> outputs = start_end_packer(inputs)
+    >>> np.array(outputs)
+    array([1, 5, 6, 7, 2, 0, 0], dtype=int32)
 
     Batched input (int).
-    >>> input_data = tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11, 12, 13, 14]])
+    >>> inputs = [[5, 6, 7], [8, 9, 10, 11, 12, 13, 14]]
     >>> start_end_packer = keras_nlp.layers.StartEndPacker(
     ...     sequence_length=6, start_value=1, end_value=2,
     ... )
-    >>> start_end_packer(input_data)
-    <tf.Tensor: shape=(2, 6), dtype=int32, numpy=
+    >>> outputs = start_end_packer(inputs)
+    >>> np.array(outputs)
     array([[ 1,  5,  6,  7,  2,  0],
-           [ 1,  8,  9, 10, 11,  2]], dtype=int32)>
+           [ 1,  8,  9, 10, 11,  2]], dtype=int32)
 
     Unbatched input (str).
-    >>> input_data = tf.constant(["this", "is", "fun"])
+    >>> inputs = tf.constant(["this", "is", "fun"])
     >>> start_end_packer = keras_nlp.layers.StartEndPacker(
     ...     sequence_length=6, start_value="<s>", end_value="</s>",
     ...     pad_value="<pad>"
     ... )
-    >>> start_end_packer(input_data)
-    <tf.Tensor: shape=(6,), dtype=string, numpy=
-    array([b'<s>', b'this', b'is', b'fun', b'</s>', b'<pad>'], dtype=object)>
+    >>> outputs = start_end_packer(inputs)
+    >>> np.array(outputs).astype("U")
+    array(['<s>', 'this', 'is', 'fun', '</s>', '<pad>'], dtype='<U5')
 
     Batched input (str).
-    >>> input_data = tf.ragged.constant([["this", "is", "fun"], ["awesome"]])
+    >>> inputs = tf.ragged.constant([["this", "is", "fun"], ["awesome"]])
     >>> start_end_packer = keras_nlp.layers.StartEndPacker(
     ...     sequence_length=6, start_value="<s>", end_value="</s>",
     ...     pad_value="<pad>"
     ... )
-    >>> start_end_packer(input_data)
-    <tf.Tensor: shape=(2, 6), dtype=string, numpy=
-    array([[b'<s>', b'this', b'is', b'fun', b'</s>', b'<pad>'],
-           [b'<s>', b'awesome', b'</s>', b'<pad>', b'<pad>', b'<pad>']],
-          dtype=object)>
+    >>> outputs = start_end_packer(inputs)
+    >>> np.array(outputs).astype("U")
+    array([['<s>', 'this', 'is', 'fun', '</s>', '<pad>'],
+           ['<s>', 'awesome', '</s>', '<pad>', '<pad>', '<pad>']], dtype='<U7')
+
+    Multiple start tokens.
+    >>> inputs = tf.ragged.constant([["this", "is", "fun"], ["awesome"]])
+    >>> start_end_packer = keras_nlp.layers.StartEndPacker(
+    ...     sequence_length=6, start_value=["</s>", "<s>"], end_value="</s>",
+    ...     pad_value="<pad>"
+    ... )
+    >>> outputs = start_end_packer(inputs)
+    >>> np.array(outputs).astype("U")
+    array([['</s>', '<s>', 'this', 'is', 'fun', '</s>'],
+           ['</s>', '<s>', 'awesome', '</s>', '<pad>', '<pad>']], dtype='<U7')
     """
 
     def __init__(
         self,
         sequence_length,
         start_value=None,
         end_value=None,
@@ -108,78 +122,95 @@
         return_padding_mask=False,
         name=None,
         **kwargs,
     ):
         super().__init__(name=name, **kwargs)
 
         self.sequence_length = sequence_length
+
+        # Maintain private copies for config purposes.
+        self._start_value = start_value
+        self._end_value = end_value
+
+        def check_special_value_type(value, value_name):
+            if isinstance(value, (int, str)):
+                return [value]
+            if value and not isinstance(value, (list, tuple)):
+                raise ValueError(
+                    f"{value_name} should be of type int/str/list/tuple."
+                    f"Received type: `{type(value)}`."
+                )
+            return value
+
+        start_value = check_special_value_type(start_value, "start_value")
+        end_value = check_special_value_type(end_value, "end_value")
+
         self.start_value = start_value
         self.end_value = end_value
+
         self.pad_value = pad_value
         self.return_padding_mask = return_padding_mask
 
     def call(
         self,
         inputs,
         sequence_length=None,
         add_start_value=True,
         add_end_value=True,
     ):
-        x = inputs  # Intermediate result.
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
 
-        if not isinstance(x, (tf.Tensor, tf.RaggedTensor)):
-            x = tf.convert_to_tensor(x)
-
-        input_is_1d = x.shape.rank == 1
-        if x.shape.rank < 1 or x.shape.rank > 2:
-            raise ValueError(
-                "Input must either be rank 1 or rank 2. Received input with "
-                f"rank={x.shape.rank}"
-            )
-        if input_is_1d:
-            # Add a new axis at the beginning.
-            x = tf.expand_dims(x, axis=0)
-        if isinstance(x, tf.Tensor):
-            # Convert to ragged tensor.
-            x = tf.RaggedTensor.from_tensor(x)
+        x = inputs  # Intermediate result.
 
         batch_size = tf.shape(x)[0]
         sequence_length = sequence_length or self.sequence_length
+        dtype = inputs.dtype
 
         # Concatenate start and end tokens.
         if add_start_value and self.start_value is not None:
-            start_token_id_tensor = tf.fill((batch_size, 1), self.start_value)
+            start_value = tf.convert_to_tensor(self.start_value, dtype=dtype)
+            start_token_id_tensor = tf.repeat(
+                start_value[tf.newaxis, :], repeats=batch_size, axis=0
+            )
             x = tf.concat([start_token_id_tensor, x], axis=-1)
         if add_end_value and self.end_value is not None:
-            end_token_id_tensor = tf.fill((batch_size, 1), self.end_value)
+            end_value = tf.convert_to_tensor(self.end_value, dtype=dtype)
+            end_token_id_tensor = tf.repeat(
+                end_value[tf.newaxis, :], repeats=batch_size, axis=0
+            )
             # Trim to leave room for end token.
-            x = x[..., : sequence_length - 1]
+            x = x[..., : sequence_length - len(self.end_value)]
             x = tf.concat([x, end_token_id_tensor], axis=-1)
 
         # Pad to desired length.
         outputs = x.to_tensor(
             default_value=self.pad_value,
             shape=(batch_size, sequence_length),
         )
-        outputs = tf.squeeze(outputs, axis=0) if input_is_1d else outputs
+        outputs = tf.squeeze(outputs, axis=0) if unbatched else outputs
 
         if self.return_padding_mask:
-            mask = tf.ones_like(x, dtype=tf.bool)
+            mask = tf.ones_like(x, dtype="bool")
             mask = mask.to_tensor(shape=(batch_size, sequence_length))
-            mask = tf.squeeze(mask, axis=0) if input_is_1d else mask
+            mask = tf.squeeze(mask, axis=0) if unbatched else mask
             return outputs, mask
 
         return outputs
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "sequence_length": self.sequence_length,
-                "start_value": self.start_value,
-                "end_value": self.end_value,
+                "start_value": self._start_value,
+                "end_value": self._end_value,
                 "pad_value": self.pad_value,
                 "return_padding_mask": self.return_padding_mask,
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape):
+        inputs_shape = list(inputs_shape)
+        inputs_shape[-1] = self.sequence_length
+        return tuple(inputs_shape)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/start_end_packer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/start_end_packer_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,127 +12,130 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for Start End Packer layer."""
 
 
 import tensorflow as tf
-from tensorflow import keras
 
-from keras_nlp.src.layers.start_end_packer import StartEndPacker
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class StartEndPackerTest(tf.test.TestCase):
+class StartEndPackerTest(TestCase):
     def test_dense_input(self):
-        input_data = tf.constant([5, 6, 7])
+        input_data = [5, 6, 7]
         start_end_packer = StartEndPacker(sequence_length=5)
         output = start_end_packer(input_data)
         expected_output = [5, 6, 7, 0, 0]
         self.assertAllEqual(output, expected_output)
 
     def test_dense_2D_input(self):
-        input_data = tf.constant([[5, 6, 7]])
+        input_data = [[5, 6, 7]]
         start_end_packer = StartEndPacker(sequence_length=5)
         output = start_end_packer(input_data)
         expected_output = [[5, 6, 7, 0, 0]]
         self.assertAllEqual(output, expected_output)
 
     def test_ragged_input(self):
-        input_data = tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11]])
+        input_data = [[5, 6, 7], [8, 9, 10, 11]]
         start_end_packer = StartEndPacker(sequence_length=5)
         output = start_end_packer(input_data)
         expected_output = [[5, 6, 7, 0, 0], [8, 9, 10, 11, 0]]
         self.assertAllEqual(output, expected_output)
 
-    def test_ragged_input_error(self):
-        input_data = tf.ragged.constant([[[5, 6, 7], [8, 9, 10, 11]]])
-        start_end_packer = StartEndPacker(sequence_length=5)
-        with self.assertRaisesRegex(
-            ValueError,
-            "Input must either be rank 1 or rank 2. Received input with "
-            "rank=3",
-        ):
-            start_end_packer(input_data)
-
     def test_start_end_token(self):
-        input_data = tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11]])
+        input_data = [[5, 6, 7], [8, 9, 10, 11]]
         start_end_packer = StartEndPacker(
             sequence_length=6, start_value=1, end_value=2
         )
         output = start_end_packer(input_data)
         expected_output = [[1, 5, 6, 7, 2, 0], [1, 8, 9, 10, 11, 2]]
         self.assertAllEqual(output, expected_output)
 
+    def test_multiple_start_end_tokens(self):
+        input_data = [[5, 6, 7], [8, 9, 10, 11, 12, 13]]
+        start_end_packer = StartEndPacker(
+            sequence_length=8,
+            start_value=[1, 2],
+            end_value=[3, 4],
+            pad_value=0,
+        )
+        output = start_end_packer(input_data)
+        expected_output = [[1, 2, 5, 6, 7, 3, 4, 0], [1, 2, 8, 9, 10, 11, 3, 4]]
+        self.assertAllEqual(output, expected_output)
+
     def test_start_end_padding_value(self):
-        input_data = tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11]])
+        input_data = [[5, 6, 7], [8, 9, 10, 11]]
         start_end_packer = StartEndPacker(
             sequence_length=7, start_value=1, end_value=2, pad_value=3
         )
         output = start_end_packer(input_data)
         expected_output = [[1, 5, 6, 7, 2, 3, 3], [1, 8, 9, 10, 11, 2, 3]]
         self.assertAllEqual(output, expected_output)
 
     def test_end_token_value_during_truncation(self):
-        input_data = tf.ragged.constant([[5, 6], [8, 9, 10, 11, 12, 13]])
+        input_data = [[5, 6], [8, 9, 10, 11, 12, 13]]
         start_end_packer = StartEndPacker(
             sequence_length=5, start_value=1, end_value=2, pad_value=0
         )
         output = start_end_packer(input_data)
         expected_output = [[1, 5, 6, 2, 0], [1, 8, 9, 10, 2]]
         self.assertAllEqual(output, expected_output)
 
     def test_string_input(self):
-        input_data = tf.ragged.constant(
-            [["KerasNLP", "is", "awesome"], ["amazing"]]
-        )
+        input_data = [["KerasNLP", "is", "awesome"], ["amazing"]]
         start_end_packer = StartEndPacker(
             sequence_length=5,
             start_value="[START]",
             end_value="[END]",
             pad_value="[PAD]",
         )
         output = start_end_packer(input_data)
         expected_output = [
             ["[START]", "KerasNLP", "is", "awesome", "[END]"],
             ["[START]", "amazing", "[END]", "[PAD]", "[PAD]"],
         ]
         self.assertAllEqual(output, expected_output)
 
-    def test_functional_model(self):
-        input_data = tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11]])
+    def test_string_input_with_multiple_special_values(self):
+        input_data = [["KerasNLP", "is", "awesome"], ["amazing"]]
         start_end_packer = StartEndPacker(
-            sequence_length=7, start_value=1, end_value=2, pad_value=3
+            sequence_length=6,
+            start_value=["[END]", "[START]"],
+            end_value="[END]",
+            pad_value="[PAD]",
         )
+        output = start_end_packer(input_data)
+        expected_output = [
+            ["[END]", "[START]", "KerasNLP", "is", "awesome", "[END]"],
+            ["[END]", "[START]", "amazing", "[END]", "[PAD]", "[PAD]"],
+        ]
+        self.assertAllEqual(output, expected_output)
 
-        inputs = keras.Input(dtype=tf.int32, shape=())
-        outputs = start_end_packer(inputs)
-        model = keras.Model(inputs, outputs)
-        model_output = model(input_data)
-
-        expected_output = [[1, 5, 6, 7, 2, 3, 3], [1, 8, 9, 10, 11, 2, 3]]
-        self.assertAllEqual(model_output, expected_output)
+    def test_special_token_dtype_error(self):
+        with self.assertRaises(ValueError):
+            StartEndPacker(sequence_length=5, start_value=1.0)
 
     def test_batch(self):
         start_end_packer = StartEndPacker(
             sequence_length=7, start_value=1, end_value=2, pad_value=3
         )
 
         ds = tf.data.Dataset.from_tensor_slices(
             tf.ragged.constant([[5, 6, 7], [8, 9, 10, 11]])
         )
         ds = ds.batch(2).map(start_end_packer)
         output = ds.take(1).get_single_element()
 
         exp_output = [[1, 5, 6, 7, 2, 3, 3], [1, 8, 9, 10, 11, 2, 3]]
-
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_call_overrides(self):
-        x = tf.constant([5, 6, 7])
+        x = [5, 6, 7]
         packer = StartEndPacker(start_value=1, end_value=2, sequence_length=4)
         self.assertAllEqual(packer(x), [1, 5, 6, 2])
         self.assertAllEqual(packer(x, add_start_value=False), [5, 6, 7, 2])
         self.assertAllEqual(packer(x, add_end_value=False), [1, 5, 6, 7])
         self.assertAllEqual(packer(x, sequence_length=2), [1, 2])
 
     def test_get_config(self):
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/token_and_position_embedding.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/token_and_position_embedding.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,18 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Creates an Embedding Layer and adds Positional Embeddings"""
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
 from keras_nlp.src.utils.keras_utils import clone_initializer
 
 
 @keras_nlp_export("keras_nlp.layers.TokenAndPositionEmbedding")
 class TokenAndPositionEmbedding(keras.layers.Layer):
     """A layer which sums a token and position embedding.
 
@@ -44,15 +43,15 @@
             model need to support masking or an exception will be raised.
             If mask_zero` is set to True, as a consequence, index 0 cannot be
             used in the vocabulary
             (input_dim should equal size of vocabulary + 1).
 
     Examples:
     ```python
-    inputs = tf.ones(shape=(1, 50), dtype=tf.int64)
+    inputs = np.ones(shape=(1, 50), dtype="int32")
     embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(
         vocabulary_size=10_000,
         sequence_length=50,
         embedding_dim=128,
     )
     outputs = embedding_layer(inputs)
     ```
@@ -100,14 +99,20 @@
             sequence_length=sequence_length,
             initializer=clone_initializer(self.embeddings_initializer),
             name="position_embedding"
             + str(keras.backend.get_uid("position_embedding")),
         )
         self.supports_masking = self.token_embedding.supports_masking
 
+    def build(self, input_shape):
+        input_shape = tuple(input_shape)
+        self.token_embedding.build(input_shape)
+        self.position_embedding.build(input_shape + (self.embedding_dim,))
+        self.built = True
+
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "vocabulary_size": self.vocabulary_size,
                 "sequence_length": self.sequence_length,
                 "embedding_dim": self.embedding_dim,
@@ -127,7 +132,10 @@
         )
         outputs = embedded_tokens + embedded_positions
         return outputs
 
     def compute_mask(self, inputs, mask=None):
         return self.token_embedding.compute_mask(inputs, mask=mask)
 
+    def compute_output_shape(self, input_shape):
+        return tuple(input_shape) + (self.embedding_dim,)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/transformer_decoder_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_decoder_test.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,23 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Transformer Decoder."""
 
 import os
 
-import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
-from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
 
-from keras_nlp.src.layers import transformer_decoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling import transformer_decoder
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class TransformerDecoderTest(tf.test.TestCase, parameterized.TestCase):
+class TransformerDecoderTest(TestCase):
     @parameterized.named_parameters(
         ("without_norm_first", False),
         ("with_norm_first", True),
     )
     def test_valid_call(self, normalize_first):
         encoder_input = keras.Input(shape=[4, 6])
         decoder_input = keras.Input(shape=[4, 6])
@@ -37,16 +37,16 @@
             normalize_first=normalize_first,
         )
         output = decoder(decoder_input, encoder_input)
         model = keras.Model(
             inputs=[decoder_input, encoder_input],
             outputs=output,
         )
-        encoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
+        encoder_sequence = ops.random.uniform(shape=[2, 4, 6])
+        decoder_sequence = ops.random.uniform(shape=[2, 4, 6])
         model([decoder_sequence, encoder_sequence])
 
     @parameterized.named_parameters(
         ("without_norm_first", False),
         ("with_norm_first", True),
     )
     def test_valid_call_without_cross_attention(self, normalize_first):
@@ -57,20 +57,20 @@
             normalize_first=normalize_first,
         )
         output = decoder(decoder_input)
         model = keras.Model(
             inputs=decoder_input,
             outputs=output,
         )
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
+        decoder_sequence = ops.random.uniform(shape=[2, 4, 6])
         model(decoder_sequence)
 
     def test_invalid_calls(self):
-        encoder_input = keras.Input(shape=[4, 6])
-        decoder_input = keras.Input(shape=[4, 6])
+        encoder_input = ops.zeros((2, 4, 6))
+        decoder_input = ops.zeros((2, 4, 6))
 
         # with cross-attention.
         decoder = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=2,
         )
         decoder(decoder_input, encoder_input)
@@ -127,250 +127,161 @@
                 intermediate_dim=4,
                 num_heads=2,
                 dropout=0.5,
                 kernel_initializer="Invalid",
             )
 
     def test_one_training_step_of_transformer_with_cross_attention(self):
-        class MyModel(keras.Model):
-            def __init__(self):
-                super().__init__()
-                self._decoder = transformer_decoder.TransformerDecoder(
-                    intermediate_dim=4, num_heads=2
-                )
-                self._dense = keras.layers.Dense(1, activation="sigmoid")
-
-            def call(self, decoder_input, encoder_output):
-                x = self._decoder(decoder_input, encoder_output)
-                return self._dense(x)
-
-        model = MyModel()
-
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        encoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        label = tf.cast(decoder_sequence[:, :, 0] >= 0.5, dtype=tf.int32)
-
-        loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)
-        optimizer = keras.optimizers.Adam()
-        with tf.GradientTape() as tape:
-            pred = model(decoder_sequence, encoder_sequence)
-            loss = loss_fn(label, pred)
-        grad = tape.gradient(loss, model.trainable_variables)
-        self.assertGreater(len(grad), 1)
-        optimizer.apply_gradients(zip(grad, model.trainable_variables))
-
-    def test_one_training_step_of_transformer_without_cross_attention(self):
-        class MyModel(keras.Model):
-            def __init__(self):
-                super().__init__()
-                self._decoder = transformer_decoder.TransformerDecoder(
-                    intermediate_dim=4,
-                    num_heads=2,
-                )
-                self._dense = keras.layers.Dense(1, activation="sigmoid")
-
-            def call(self, decoder_input):
-                x = self._decoder(decoder_input)
-                return self._dense(x)
-
-        model = MyModel()
+        decoder_input = keras.Input(shape=(4, 6))
+        encoder_input = keras.Input(shape=(4, 6))
+        decoder = transformer_decoder.TransformerDecoder(
+            intermediate_dim=4, num_heads=2
+        )
+        outputs = decoder(decoder_input, encoder_input)
+        outputs = keras.layers.Dense(10, activation="softmax")(outputs)
+        model = keras.Model((decoder_input, encoder_input), outputs)
 
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        label = tf.cast(decoder_sequence[:, :, 0] >= 0.5, dtype=tf.int32)
+        decoder_sequence = ops.random.uniform(shape=(2, 4, 6))
+        encoder_sequence = ops.random.uniform(shape=(2, 4, 6))
+        label = ops.random.randint(minval=0, maxval=10, shape=(2, 4, 1))
 
-        loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)
+        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
         optimizer = keras.optimizers.Adam()
-        with tf.GradientTape() as tape:
-            pred = model(decoder_sequence)
-            loss = loss_fn(label, pred)
-        grad = tape.gradient(loss, model.trainable_variables)
-        self.assertGreater(len(grad), 1)
-        optimizer.apply_gradients(zip(grad, model.trainable_variables))
-
-    def test_checkpointing_transformer_decoder(self):
-        decoder1 = transformer_decoder.TransformerDecoder(
-            intermediate_dim=4,
-            num_heads=2,
-        )
-        decoder2 = transformer_decoder.TransformerDecoder(
-            intermediate_dim=4,
-            num_heads=2,
+        model.compile(loss=loss, optimizer=optimizer)
+        loss = model.train_on_batch(
+            x=(decoder_sequence, encoder_sequence), y=label
         )
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        encoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        decoder1(decoder_sequence, encoder_sequence)
-        decoder2(decoder_sequence, encoder_sequence)
-        # The weights of decoder1 and decoder2 are different.
-        self.assertNotAllClose(
-            decoder1.trainable_variables[0][0],
-            decoder2.trainable_variables[0][0],
-        )
-        checkpoint = tf.train.Checkpoint(decoder1)
-        checkpoint2 = tf.train.Checkpoint(decoder2)
-        temp_dir = self.get_temp_dir()
-        save_path = checkpoint.save(temp_dir)
-        checkpoint2.restore(save_path)
-
-        decoder1_output = decoder1(decoder_sequence, encoder_sequence)
-        decoder2_output = decoder2(decoder_sequence, encoder_sequence)
-        self.assertAllClose(decoder1_output, decoder2_output)
+        self.assertGreater(loss, 0)
 
-    def test_checkpointing_transformer_decoder_without_cross_attention(self):
-        decoder1 = transformer_decoder.TransformerDecoder(
-            intermediate_dim=4,
-            num_heads=2,
+    def test_one_training_step_of_transformer_without_cross_attention(self):
+        decoder_input = keras.Input(shape=(4, 6))
+        decoder = transformer_decoder.TransformerDecoder(
+            intermediate_dim=4, num_heads=2
         )
+        outputs = decoder(decoder_input)
+        outputs = keras.layers.Dense(10, activation="softmax")(outputs)
+        model = keras.Model(decoder_input, outputs)
 
-        decoder2 = transformer_decoder.TransformerDecoder(
-            intermediate_dim=4,
-            num_heads=2,
-        )
+        decoder_sequence = ops.random.uniform(shape=(2, 4, 6))
+        label = ops.random.randint(minval=0, maxval=10, shape=(2, 4, 1))
 
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        decoder1(decoder_sequence)
-        decoder2(decoder_sequence)
-        # The weights of decoder1 and decoder2 are different.
-        self.assertNotAllClose(
-            decoder1.trainable_variables[0][0],
-            decoder2.trainable_variables[0][0],
-        )
-        checkpoint = tf.train.Checkpoint(decoder1)
-        checkpoint2 = tf.train.Checkpoint(decoder2)
-        temp_dir = self.get_temp_dir()
-        save_path = checkpoint.save(temp_dir)
-        checkpoint2.restore(save_path)
-
-        decoder1_output = decoder1(decoder_sequence)
-        decoder2_output = decoder2(decoder_sequence)
-        self.assertAllClose(decoder1_output, decoder2_output)
+        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
+        optimizer = keras.optimizers.Adam()
+        model.compile(loss=loss, optimizer=optimizer)
+        loss = model.train_on_batch(x=decoder_sequence, y=label)
+        self.assertGreater(loss, 0)
 
     def test_mask_propagation(self):
         decoder = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=2,
         )
-        decoder_sequence = tf.random.uniform(shape=[1, 4, 6])
-        encoder_sequence = tf.random.uniform(shape=[1, 4, 6])
-        mask = tf.constant([[True, True, False, False]])
+        decoder_sequence = ops.random.uniform(shape=[1, 4, 6])
+        encoder_sequence = ops.random.uniform(shape=[1, 4, 6])
+        mask = ops.array([[True, True, False, False]])
         decoder_sequence._keras_mask = mask
         outputs = decoder(decoder_sequence, encoder_sequence)
         self.assertAllEqual(outputs._keras_mask, mask)
 
     def test_mask_propagation_without_cross_attention(self):
         decoder = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=2,
         )
-        decoder_sequence = tf.random.uniform(shape=[1, 4, 6])
-        mask = tf.constant([[True, True, False, False]])
+        decoder_sequence = ops.random.uniform(shape=[1, 4, 6])
+        mask = ops.array([[True, True, False, False]])
         decoder_sequence._keras_mask = mask
         outputs = decoder(decoder_sequence)
         self.assertAllEqual(outputs._keras_mask, mask)
 
-    @parameterized.named_parameters(
-        ("graph", False),
-        ("eager", True),
-    )
-    def test_cached_decoding_is_correct(self, eager):
+    def test_cache_call_is_correct(self):
         batch_size = 2
         seq_len = 5
         num_heads = 2
-        head_dim = 4
+        key_dim = 4
+        hidden_dim = num_heads * key_dim
+
+        input_shape = (batch_size, seq_len, hidden_dim)
+        x = ops.random.uniform(shape=input_shape)
+        input_cache = ops.zeros((batch_size, 2, seq_len, num_heads, key_dim))
+        outputs = ops.zeros_like(x)
 
         layer = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=num_heads,
         )
-        dtype = layer.compute_dtype
-        x = tf.random.uniform(
-            shape=[batch_size, seq_len, num_heads * head_dim], dtype=dtype
+        no_loop_outputs, no_loop_cache = layer(
+            x,
+            self_attention_cache=input_cache,
+            self_attention_cache_update_index=0,
         )
-        cache = tf.zeros(
-            [batch_size, 2, seq_len, num_heads, head_dim], dtype=dtype
-        )
-        outputs = tf.zeros_like(x)
 
-        def call(outputs, cache):
-            def loop_body(i, outputs, cache):
-                # Compute the rest tokens.
-                next_input = x[:, i : i + 1, :]
-                next_output, cache = layer(
-                    decoder_sequence=next_input,
-                    cache=cache,
-                    cache_index=i,
-                )
-                outputs = dynamic_update_slice(outputs, next_output, [0, i, 0])
-                return i + 1, outputs, cache
+        def loop_body(i, outputs, cache):
+            # Compute the rest tokens.
+            next_input = ops.slice(x, (0, i, 0), (batch_size, 1, hidden_dim))
+            next_output, cache = layer(
+                decoder_sequence=next_input,
+                self_attention_cache=cache,
+                self_attention_cache_update_index=i,
+            )
+            outputs = ops.slice_update(outputs, [0, i, 0], next_output)
+            return i + 1, outputs, cache
 
-            _, outputs, cache = tf.while_loop(
+        def call(outputs, cache):
+            _, outputs, cache = ops.while_loop(
                 cond=lambda i, outputs, cache: i < seq_len,
                 body=loop_body,
                 loop_vars=[0, outputs, cache],
             )
             return outputs, cache
 
-        call = call if eager else tf.function(call)
-        output, cache = call(outputs, cache)
+        output, output_cache = call(outputs, input_cache)
 
-        no_loop_outputs = layer(x)
-        _, no_loop_cache = layer(x, cache=cache)
         self.assertAllClose(output, no_loop_outputs)
-        self.assertAllClose(cache, no_loop_cache)
+        self.assertAllClose(output_cache, no_loop_cache)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         encoder_input = keras.Input(shape=[4, 6])
         decoder_input = keras.Input(shape=[4, 6])
         decoder = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=2,
             normalize_first=True,
         )
         output = decoder(encoder_input, decoder_input)
         model = keras.Model(
             inputs=[decoder_input, encoder_input],
             outputs=output,
         )
-        encoder_sequence = tf.random.uniform(shape=[2, 4, 6])
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
+        encoder_sequence = ops.random.uniform(shape=[2, 4, 6])
+        decoder_sequence = ops.random.uniform(shape=[2, 4, 6])
         model([decoder_sequence, encoder_sequence])
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         loaded_model = keras.models.load_model(path)
         model_output = model([decoder_sequence, encoder_sequence])
         loaded_model_output = loaded_model([decoder_sequence, encoder_sequence])
         self.assertAllClose(model_output, loaded_model_output)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model_without_cross_attention(self, save_format, filename):
+    def test_saved_model_without_cross_attention(self):
         decoder_input = keras.Input(shape=[4, 6])
         decoder = transformer_decoder.TransformerDecoder(
             intermediate_dim=4,
             num_heads=2,
             normalize_first=True,
         )
         output = decoder(decoder_input)
         model = keras.Model(
             inputs=decoder_input,
             outputs=output,
         )
-        decoder_sequence = tf.random.uniform(shape=[2, 4, 6])
+        decoder_sequence = ops.random.uniform(shape=[2, 4, 6])
         model(decoder_sequence)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         loaded_model = keras.models.load_model(path)
 
         model_output = model(decoder_sequence)
         loaded_model_output = loaded_model(decoder_sequence)
         self.assertAllClose(model_output, loaded_model_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/transformer_encoder.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_encoder.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,20 +10,19 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Transformer encoder block implementation based on `keras.layers.Layer`."""
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.utils.keras_utils import clone_initializer
 
-from keras_nlp.src.layers.transformer_layer_utils import (  # isort:skip
+from keras_nlp.src.layers.modeling.transformer_layer_utils import (  # isort:skip
     merge_padding_and_attention_mask,
 )
 
 
 @keras_nlp_export("keras_nlp.layers.TransformerEncoder")
 class TransformerEncoder(keras.layers.Layer):
     """Transformer encoder.
@@ -38,47 +37,50 @@
     [guide](https://keras.io/guides/understanding_masking_and_padding/)
     for more details.
 
     Args:
         intermediate_dim: int, the hidden size of feedforward network.
         num_heads: int, the number of heads in the
             `keras.layers.MultiHeadAttention` layer.
-        dropout: float, defaults to 0. the dropout value, shared by
+        dropout: float. the dropout value, shared by
             `keras.layers.MultiHeadAttention` and feedforward network.
-        activation: string or `keras.activations`, defaults to "relu". the
+            Defaults to `0.`.
+        activation: string or `keras.activations`. the
             activation function of feedforward network.
-        layer_norm_epsilon: float, defaults to 1e-5. The epsilon value in layer
-            normalization components.
-        kernel_initializer: string or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for
-            the dense and multiheaded attention layers.
-        bias_initializer: string or `keras.initializers` initializer,
-            defaults to "zeros". The bias initializer for
-            the dense and multiheaded attention layers.
-        normalize_first: bool. Defaults to False. If True, the inputs to the
+            Defaults to `"relu"`.
+        layer_norm_epsilon: float. The epsilon value in layer
+            normalization components. Defaults to `1e-5`.
+        kernel_initializer: string or `keras.initializers` initializer.
+            The kernel initializer for the dense and multiheaded
+            attention layers. Defaults to `"glorot_uniform"`.
+        bias_initializer: string or `keras.initializers` initializer.
+            The bias initializer for the dense and multiheaded
+            attention layers. Defaults to `"zeros"`.
+        normalize_first: bool. If True, the inputs to the
             attention layer and the intermediate dense layer  are normalized
             (similar to GPT-2). If set to False, outputs of attention layer and
             intermediate dense layer are normalized (similar to BERT).
-        name: string, defaults to None. The name of the layer.
+            Defaults to `False`.
+        name: string. The name of the layer. Defaults to `None`.
         **kwargs: other keyword arguments.
 
     Examples:
 
     ```python
     # Create a single transformer encoder layer.
     encoder = keras_nlp.layers.TransformerEncoder(
         intermediate_dim=64, num_heads=8)
 
     # Create a simple model containing the encoder.
-    input = keras.Input(shape=[10, 64])
+    input = keras.Input(shape=(10, 64))
     output = encoder(input)
     model = keras.Model(inputs=input, outputs=output)
 
     # Call encoder on the inputs.
-    input_data = tf.random.uniform(shape=[2, 10, 64])
+    input_data = np.random.uniform(size=(2, 10, 64))
     output = model(input_data)
     ```
 
     References:
      - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)
     """
 
@@ -91,78 +93,81 @@
         layer_norm_epsilon=1e-05,
         kernel_initializer="glorot_uniform",
         bias_initializer="zeros",
         normalize_first=False,
         name=None,
         **kwargs
     ):
-        # Work around for model saving
-        self._input_shape = kwargs.pop("build_input_shape", None)
-
         super().__init__(name=name, **kwargs)
         self.intermediate_dim = intermediate_dim
         self.num_heads = num_heads
         self.dropout = dropout
         self.activation = keras.activations.get(activation)
         self.layer_norm_epsilon = layer_norm_epsilon
         self.kernel_initializer = keras.initializers.get(kernel_initializer)
         self.bias_initializer = keras.initializers.get(bias_initializer)
         self.normalize_first = normalize_first
-        self._built = False
         self.supports_masking = True
 
-        if self._input_shape is not None:
-            self._build(self._input_shape)
-
-    def _build(self, input_shape):
-        # Create layers based on input shape.
-        self._built = True
-        self._input_shape = input_shape
+    def build(self, inputs_shape):
         # Infer the dimension of our hidden feature size from the build shape.
-        hidden_dim = input_shape[-1]
+        hidden_dim = inputs_shape[-1]
         # Attention head size is `hidden_dim` over the number of heads.
         key_dim = int(hidden_dim // self.num_heads)
 
         # Self attention layers.
         self._self_attention_layer = keras.layers.MultiHeadAttention(
             num_heads=self.num_heads,
             key_dim=key_dim,
             dropout=self.dropout,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
-        self._self_attention_layer._build_from_signature(
-            query=input_shape,
-            value=input_shape,
-        )
+        if hasattr(self._self_attention_layer, "_build_from_signature"):
+            self._self_attention_layer._build_from_signature(
+                query=inputs_shape,
+                value=inputs_shape,
+            )
+        else:
+            self._self_attention_layer.build(
+                query_shape=inputs_shape,
+                value_shape=inputs_shape,
+            )
         self._self_attention_layernorm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon,
         )
+        self._self_attention_layernorm.build(inputs_shape)
         self._self_attention_dropout = keras.layers.Dropout(
             rate=self.dropout,
         )
 
         # Feedforward layers.
         self._feedforward_layernorm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon,
         )
+        self._feedforward_layernorm.build(inputs_shape)
         self._feedforward_intermediate_dense = keras.layers.Dense(
             self.intermediate_dim,
             activation=self.activation,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        self._feedforward_intermediate_dense.build(inputs_shape)
         self._feedforward_output_dense = keras.layers.Dense(
             hidden_dim,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        intermediate_shape = list(inputs_shape)
+        intermediate_shape[-1] = self.intermediate_dim
+        self._feedforward_output_dense.build(tuple(intermediate_shape))
         self._feedforward_dropout = keras.layers.Dropout(
             rate=self.dropout,
         )
+        self.built = True
 
     def call(self, inputs, padding_mask=None, attention_mask=None):
         """Forward pass of the TransformerEncoder.
 
         Args:
             inputs: a Tensor. The input data to TransformerEncoder, should be
                 of shape [batch_size, sequence_length, hidden_dim].
@@ -172,18 +177,14 @@
             attention_mask: a boolean Tensor. Customized mask used to mask out
                 certain tokens. `attention_mask` should have shape
                 [batch_size, sequence_length, sequence_length].
 
         Returns:
             A Tensor of the same shape as the `inputs`.
         """
-
-        if not self._built:
-            self._build(inputs.shape)
-
         x = inputs  # Intermediate result.
 
         # Compute self attention mask.
         self_attention_mask = merge_padding_and_attention_mask(
             inputs, padding_mask, attention_mask
         )
 
@@ -226,12 +227,14 @@
                 "kernel_initializer": keras.initializers.serialize(
                     self.kernel_initializer
                 ),
                 "bias_initializer": keras.initializers.serialize(
                     self.bias_initializer
                 ),
                 "normalize_first": self.normalize_first,
-                "build_input_shape": self._input_shape,
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape):
+        return inputs_shape
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/transformer_layer_utils.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_layer_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,31 +10,32 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """ Utility functions for `TransformerEncoder` and `TransformerDecoder`."""
 
-import tensorflow as tf
 from absl import logging
 
+from keras_nlp.src.backend import ops
+
 
 def _check_masks_shapes(inputs, padding_mask, attention_mask):
     mask = padding_mask
     if hasattr(inputs, "_keras_mask") and mask is None:
         mask = inputs._keras_mask
     if mask is not None:
-        if mask._rank() != 2:
+        if len(mask.shape) != 2:
             raise ValueError(
                 "`padding_mask` should have shape "
                 "(batch_size, target_length). "
                 f"Received shape `{mask.shape}`."
             )
     if attention_mask is not None:
-        if attention_mask._rank() != 3:
+        if len(attention_mask.shape) != 3:
             raise ValueError(
                 "`attention_mask` should have shape "
                 "(batch_size, target_length, source_length). "
                 f"Received shape `{mask.shape}`."
             )
 
 
@@ -52,18 +53,18 @@
             five positions of the key/value tensors.
 
     Return:
         A causal attention mask with shape
         `(batch_size, output_length, input_length)` that can be passed to a
         attention layer.
     """
-    i = tf.range(output_length)[:, tf.newaxis] + cache_index
-    j = tf.range(input_length)
-    mask = tf.cast(i >= j, dtype="int32")[tf.newaxis, :, :]
-    return tf.broadcast_to(mask, (batch_size, output_length, input_length))
+    i = ops.expand_dims(ops.arange(output_length), axis=1) + cache_index
+    j = ops.arange(input_length)
+    mask = ops.expand_dims(ops.cast(i >= j, dtype="int32"), axis=0)
+    return ops.broadcast_to(mask, (batch_size, output_length, input_length))
 
 
 def merge_padding_and_attention_mask(
     inputs,
     padding_mask,
     attention_mask,
 ):
@@ -91,16 +92,16 @@
             logging.warning(
                 "You are explicitly setting `padding_mask` while the `inputs` "
                 "have built-in mask, so the built-in mask is ignored."
             )
     if mask is not None:
         # Add an axis for broadcasting, the attention mask should be 2D
         # (not including the batch axis).
-        mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
+        mask = ops.cast(ops.expand_dims(mask, axis=1), "int32")
     if attention_mask is not None:
-        attention_mask = tf.cast(attention_mask, dtype=tf.int32)
+        attention_mask = ops.cast(attention_mask, "int32")
         if mask is None:
             return attention_mask
         else:
-            return tf.minimum(mask, attention_mask)
+            return ops.minimum(mask, attention_mask)
     return mask
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/layers/transformer_layer_utils_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/transformer_layer_utils_test.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,55 +8,49 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
+import keras_nlp.src.layers.modeling.transformer_layer_utils as utils
+from keras_nlp.src.backend import ops
+from keras_nlp.src.tests.test_case import TestCase
 
-import keras_nlp.src.layers.transformer_layer_utils as utils
 
-
-class TransformerEncoderTest(tf.test.TestCase):
+class TransformerLayerUtilsTest(TestCase):
     def test_compute_causal_mask(self):
         mask = utils.compute_causal_mask(1, 2, 2)
-        self.assertTrue((mask.numpy() == [[1, 0], [1, 1]]).all())
+        self.assertAllEqual(mask, [[[1, 0], [1, 1]]])
 
     def test_merge_padding_and_attention_mask(self):
-        padding_mask = tf.convert_to_tensor([[1, 1, 0]])
-        attention_mask = tf.convert_to_tensor(
-            [[[0, 0, 1], [0, 1, 0], [1, 0, 0]]]
-        )
-        inputs = tf.random.uniform(shape=[1, 3, 2])
+        padding_mask = ops.array([[1, 1, 0]])
+        attention_mask = ops.array([[[0, 0, 1], [0, 1, 0], [1, 0, 0]]])
+        inputs = ops.random.uniform(shape=[1, 3, 2])
         merged_mask = utils.merge_padding_and_attention_mask(
             inputs,
             padding_mask,
             attention_mask,
         )
-        self.assertTrue(
-            (merged_mask.numpy() == [[0, 0, 0], [0, 1, 0], [1, 0, 0]]).all()
-        )
+        self.assertAllEqual(merged_mask, [[[0, 0, 0], [0, 1, 0], [1, 0, 0]]])
 
     def test_bad_mask_shapes(self):
         with self.assertRaises(ValueError):
-            padding_mask = tf.convert_to_tensor([[[1, 1, 0], [1, 0, 0]]])
-            attention_mask = tf.convert_to_tensor(
-                [[0, 0, 1], [0, 1, 0], [1, 0, 0]]
-            )
-            inputs = tf.random.uniform(shape=[1, 3, 2])
+            padding_mask = ops.array([[[1, 1, 0], [1, 0, 0]]])
+            attention_mask = ops.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]])
+            inputs = ops.random.uniform(shape=[1, 3, 2])
             utils.merge_padding_and_attention_mask(
                 inputs,
                 padding_mask,
                 attention_mask,
             )
 
         with self.assertRaises(ValueError):
-            padding_mask = tf.convert_to_tensor([[1, 1, 0]])
-            attention_mask = tf.convert_to_tensor([[0, 0, 1], [1, 0, 0]])
-            inputs = tf.random.uniform(shape=[1, 3, 2])
+            padding_mask = ops.array([[1, 1, 0]])
+            attention_mask = ops.array([[0, 0, 1], [1, 0, 0]])
+            inputs = ops.random.uniform(shape=[1, 3, 2])
             utils.merge_padding_and_attention_mask(
                 inputs,
                 padding_mask,
                 attention_mask,
             )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/bleu.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/bleu.py`

 * *Files 3% similar despite different names*

```diff
@@ -14,18 +14,20 @@
 
 """BLEU metric implementation."""
 
 import collections
 import math
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.utils.tf_utils import tensor_to_list
+from keras_nlp.src.backend import keras
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
+from keras_nlp.src.utils.tensor_utils import is_floating_dtype
+from keras_nlp.src.utils.tensor_utils import tensor_to_list
 
 REPLACE_SUBSTRINGS = [
     ("<skipped>", ""),
     ("-\n", ""),
     ("\n", " "),
     ("&quot;", '"'),
     ("&amp;", "&"),
@@ -81,77 +83,82 @@
             (of any shape), and tokenizes the strings in the tensor. If the
             tokenizer is not specified, the default tokenizer is used. The
             default tokenizer replicates the behaviour of SacreBLEU's
             `"tokenizer_13a"` tokenizer
             (https://github.com/mjpost/sacrebleu/blob/v2.1.0/sacrebleu/tokenizers/tokenizer_13a.py).
         max_order: int. The maximum n-gram order to use. For example, if
             `max_order` is set to 3, unigrams, bigrams, and trigrams will be
-            considered. Defaults to 4.
+            considered. Defaults to `4`.
         smooth: bool. Whether to apply Lin et al. 2004 smoothing to the BLEU
             score. Adds 1 to the matched n-gram count (i.e., numerator) and 1
             to the total n-gram count (i.e., denominator) for every order while
-            calculating precision. Defaults to False.
+            calculating precision. Defaults to `False`.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-               not specified, it defaults to tf.float32.
+               not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     References:
         - [Papineni et al., 2002](https://aclanthology.org/P02-1040/)
         - [SacreBLEU](https://github.com/mjpost/sacrebleu)
         - [Lin et al., 2004](https://aclanthology.org/P04-1077/)
     """
 
     def __init__(
         self,
         tokenizer=None,
         max_order=4,
         smooth=False,
-        dtype=None,
+        dtype="float32",
         name="bleu",
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(name=name, dtype=dtype, **kwargs)
 
-        if not tf.as_dtype(self.dtype).is_floating:
+        if not is_floating_dtype(dtype):
             raise ValueError(
                 "`dtype` must be a floating point type. "
                 f"Received: dtype={dtype}"
             )
 
         self.tokenizer = tokenizer
         self.max_order = max_order
         self.smooth = smooth
 
         self._matches = self.add_weight(
             shape=(self.max_order,),
-            name="bleu_matches",
             initializer="zeros",
             dtype=self.dtype,
+            name="bleu_matches",
         )
         self._possible_matches = self.add_weight(
             shape=(self.max_order,),
-            name="bleu_possible_matches",
             initializer="zeros",
             dtype=self.dtype,
+            name="bleu_possible_matches",
         )
         self._translation_length = self.add_weight(
-            name="bleu_translation_length",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="bleu_translation_length",
         )
         self._reference_length = self.add_weight(
-            name="bleu_reference_length",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="bleu_reference_length",
         )
         self._bleu = self.add_weight(
-            name="bleu",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="bleu",
         )
 
     def _tokenizer(self, inputs):
         """
         Tokenizes the input strings. By default, replicates the behaviour of
         SacreBLEU's default tokenizer, namely, `tokenizer_13a`.
         """
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/bleu_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/bleu_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,23 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for Bleu."""
-
+import pytest
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.metrics.bleu import Bleu
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.byte_tokenizer import ByteTokenizer
 
 
-class BleuTest(tf.test.TestCase):
+@pytest.mark.tf_only
+class BleuTest(TestCase):
     def test_initialization(self):
         bleu = Bleu()
         result = bleu.result()
 
         self.assertEqual(result, 0.0)
 
     def test_scalar_input(self):
@@ -33,43 +35,43 @@
         y_true = [
             "He eats a sweet apple.",
             "He is eating a tasty apple, isn't he?",
         ]
         y_pred = "He He He eats sweet apple which is a fruit."
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.212, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.212, delta=1e-3)
 
     def test_1d_list_input(self):
         bleu = Bleu()
         y_true = [
             ["He eats a sweet apple."],
             ["Silicon Valley is one of my favourite shows!"],
         ]
         y_pred = [
             "He He He eats sweet apple which is a fruit.",
             "I love Silicon Valley, it's one of my favourite shows.",
         ]
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.243, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.243, delta=1e-3)
 
     def test_2d_list_input(self):
         bleu = Bleu()
         y_true = [
             [["He eats a sweet apple."]],
             [["Silicon Valley is one of my favourite shows!"]],
         ]
         y_pred = [
             ["He He He eats sweet apple which is a fruit."],
             ["I love Silicon Valley, it's one of my favourite shows."],
         ]
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.243, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.243, delta=1e-3)
 
     def test_1d_tensor_input(self):
         bleu = Bleu()
         y_true = tf.ragged.constant(
             [
                 ["He eats a sweet apple."],
                 ["Silicon Valley is one of my favourite shows!"],
@@ -79,15 +81,15 @@
             [
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.243, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.243, delta=1e-3)
 
     def test_2d_tensor_input(self):
         bleu = Bleu()
         y_true = tf.constant(
             [
                 [["He eats a sweet apple."]],
                 [["Silicon Valley is one of my favourite shows!"]],
@@ -97,15 +99,15 @@
             [
                 ["He He He eats sweet apple which is a fruit."],
                 ["I love Silicon Valley, it's one of my favourite shows."],
             ]
         )
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.243, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.243, delta=1e-3)
 
     def test_custom_tokenizer(self):
         byte_tokenizer = ByteTokenizer()
         bleu = Bleu(tokenizer=byte_tokenizer)
         y_true = tf.ragged.constant(
             [
                 ["He eats a sweet apple."],
@@ -116,15 +118,15 @@
             [
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.609, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.609, delta=1e-3)
 
     def test_different_order(self):
         bleu = Bleu(max_order=5)
         y_true = tf.ragged.constant(
             [
                 ["He eats a sweet apple."],
                 ["Silicon Valley is one of my favourite shows!"],
@@ -134,37 +136,36 @@
             [
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
 
         bleu_val = bleu(y_true, y_pred)
-        self.assertAlmostEqual(bleu_val.numpy(), 0.188, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.188, delta=1e-3)
 
     def test_model_compile(self):
         inputs = keras.Input(shape=(), dtype="string")
-        outputs = tf.identity(inputs)
+        outputs = keras.layers.Identity()(inputs)
         model = keras.Model(inputs, outputs)
-
         model.compile(metrics=[Bleu()])
 
-        x = tf.constant(
+        y_pred = x = tf.constant(
             [
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
         y = tf.constant(
             [
                 ["He eats a sweet apple."],
                 ["Silicon Valley is one of my favourite shows!"],
             ]
         )
 
-        output = model.evaluate(x, y, return_dict=True)
+        output = model.compute_metrics(x, y, y_pred, sample_weight=None)
         self.assertAlmostEqual(output["bleu"], 0.243, delta=1e-3)
 
     def test_reset_state(self):
         bleu = Bleu()
         y_true = tf.ragged.constant(
             [
                 ["He eats a sweet apple."],
@@ -176,15 +177,15 @@
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
 
         bleu.update_state(y_true, y_pred)
         bleu_val = bleu.result()
-        self.assertNotEqual(bleu_val.numpy(), 0.0)
+        self.assertNotEqual(bleu_val, 0.0)
 
         bleu.reset_state()
         bleu_val = bleu.result()
         self.assertEqual(bleu_val, 0.0)
 
     def test_update_state(self):
         bleu = Bleu()
@@ -199,59 +200,22 @@
                 "He He He eats sweet apple which is a fruit.",
                 "I love Silicon Valley, it's one of my favourite shows.",
             ]
         )
 
         bleu.update_state(y_true_1, y_pred_1)
         bleu_val = bleu.result()
-        self.assertAlmostEqual(bleu_val.numpy(), 0.243, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.243, delta=1e-3)
 
         y_true_2 = tf.constant(["Virat Kohli is the GOAT."])
         y_pred_2 = tf.constant("Virat Kohli is the greatest of all time!")
 
         bleu.update_state(y_true_2, y_pred_2)
         bleu_val = bleu.result()
-        self.assertAlmostEqual(bleu_val.numpy(), 0.26, delta=1e-3)
-
-    def test_merge_state_normalize(self):
-        bleu_1 = Bleu(smooth=True)
-        bleu_2 = Bleu(smooth=True)
-
-        y_true_1 = tf.ragged.constant(
-            [
-                ["He eats a sweet apple."],
-                ["Silicon Valley is one of my favourite shows!"],
-            ]
-        )
-        y_pred_1 = tf.constant(
-            [
-                "He He He eats sweet apple which is a fruit.",
-                "I love Silicon Valley, it's one of my favourite shows.",
-            ]
-        )
-
-        y_true_2 = tf.constant(["Virat Kohli is the GOAT."])
-        y_pred_2 = tf.constant("Virat Kohli is the greatest of all time!")
-
-        y_true_3 = tf.constant([["Watching Test cricket is so much fun."]])
-        y_pred_3 = tf.constant(["Test is the best format in cricket."])
-
-        bleu_1.update_state(y_true_1, y_pred_1)
-        bleu_1.update_state(y_true_2, y_pred_2)
-        bleu_val = bleu_1.result()
-        self.assertAlmostEqual(bleu_val.numpy(), 0.293, delta=1e-3)
-
-        bleu_2.update_state(y_true_3, y_pred_3)
-        bleu_val = bleu_2.result()
-        self.assertAlmostEqual(bleu_val.numpy(), 0.202, delta=1e-3)
-
-        merged_bleu = Bleu(smooth=True)
-        merged_bleu.merge_state([bleu_1, bleu_2])
-        bleu_val = merged_bleu.result()
-        self.assertAlmostEqual(bleu_val.numpy(), 0.495, delta=1e-3)
+        self.assertAlmostEqual(bleu_val, 0.26, delta=1e-3)
 
     def test_get_config(self):
         byte_tokenizer = ByteTokenizer()
         bleu = Bleu(
             tokenizer=byte_tokenizer,
             max_order=8,
             smooth=True,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/edit_distance.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/edit_distance.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,17 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Edit Distance metric."""
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
+from keras_nlp.src.utils.tensor_utils import is_floating_dtype
 
 
 @keras_nlp_export("keras_nlp.metrics.EditDistance")
 class EditDistance(keras.metrics.Metric):
     """Edit Distance metric.
 
     This class implements the edit distance metric, sometimes called
@@ -43,15 +45,15 @@
     Args:
         normalize: bool. If True, the computed number of operations
             (substitutions + deletions + insertions) across all samples is
             divided by the aggregate number of tokens in all reference texts. If
             False, number of operations are calculated for every sample, and
             averaged over all the samples.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-            not specified, it defaults to tf.float32.
+            not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     References:
         - [Morris et al.](https://www.researchgate.net/publication/221478089)
 
     Examples:
@@ -93,65 +95,54 @@
     ... ])
     >>> y_pred = tf.strings.split([
     ...     "the cat was found under the bed",
     ...     "it is sunny but with a hint of cloud cover",
     ... ])
     >>> edit_distance(y_true, y_pred)
     <tf.Tensor: shape=(), dtype=float32, numpy=0.73333335>
-
-    Pass the metric to `model.compile()`.
-    >>> inputs = keras.Input(shape=(None,), dtype="string")
-    >>> outputs = tf.strings.lower(inputs)
-    >>> model = keras.Model(inputs, outputs)
-    >>> model.compile(metrics=[keras_nlp.metrics.EditDistance()])
-    >>> x = tf.strings.split(
-    ...     tf.constant(
-    ...         ["the tiny little cat was found under the big funny bed"]
-    ...     )
-    ... )
-    >>> y = tf.strings.split(["the cat was found under the bed"])
-    >>> y = tf.strings.split(["the cat was found under the bed"])
-    >>> output = model.evaluate(y, x, return_dict=True)
-    >>> output["edit_distance"]
-    0.3636363744735718
     """
 
     def __init__(
         self,
         normalize=True,
-        dtype=None,
+        dtype="float32",
         name="edit_distance",
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(name=name, dtype=dtype, **kwargs)
 
-        if not tf.as_dtype(self.dtype).is_floating:
+        if not is_floating_dtype(dtype):
             raise ValueError(
                 "`dtype` must be a floating point type. "
                 f"Received: dtype={dtype}"
             )
 
         self.normalize = normalize
 
         self._aggregate_unnormalized_edit_distance = self.add_weight(
-            name="aggregate_unnormalized_edit_distance",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="aggregate_unnormalized_edit_distance",
         )
         if normalize:
             self._aggregate_reference_length = self.add_weight(
-                name="aggregate_reference_length",
+                shape=(),
                 initializer="zeros",
                 dtype=self.dtype,
+                name="aggregate_reference_length",
             )
         else:
             self._number_of_samples = self.add_weight(
-                name="number_of_samples",
+                shape=(),
                 initializer="zeros",
                 dtype=self.dtype,
+                name="number_of_samples",
             )
 
     def update_state(self, y_true, y_pred, sample_weight=None):
         def validate_and_fix_rank(inputs, tensor_name):
             if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):
                 inputs = tf.ragged.constant(inputs)
 
@@ -193,15 +184,15 @@
             if not self.normalize:
                 self._number_of_samples.assign_add(tf.cast(1, dtype=self.dtype))
             return 0
 
         _ = tf.map_fn(
             fn=calculate_edit_distance,
             elems=(y_true, y_pred),
-            fn_output_signature=tf.int8,
+            fn_output_signature="int8",
         )
 
     def result(self):
         if self.normalize:
             if self._aggregate_reference_length == 0:
                 return 0.0
             return (
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/edit_distance_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/edit_distance_test.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,59 +9,61 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for EditDistance."""
-
+import pytest
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.metrics.edit_distance import EditDistance
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class EditDistanceTest(tf.test.TestCase):
+@pytest.mark.tf_only
+class EditDistanceTest(TestCase):
     def test_initialization(self):
         edit_distance = EditDistance()
         result = edit_distance.result()
 
         self.assertEqual(result, 0.0)
 
     def test_1d_list_input_normalize(self):
         edit_distance = EditDistance()
         y_true = "the tiny little cat was found under the big funny bed".split()
         y_pred = "the cat was found under the bed".split()
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.364, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.364, delta=1e-3)
 
     def test_2d_list_input_normalize(self):
         edit_distance = EditDistance()
         y_true = [
             "the tiny little cat was found under the big funny bed".split(),
             "it is sunny today".split(),
         ]
         y_pred = [
             "the cat was found under the bed".split(),
             "it is sunny but with a hint of cloud cover".split(),
         ]
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.733, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.733, delta=1e-3)
 
     def test_rank_1_tensor_input_normalize(self):
         edit_distance = EditDistance()
         y_true = tf.strings.split(
             "the tiny little cat was found under the big funny bed"
         )
         y_pred = tf.strings.split("the cat was found under the bed")
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.364, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.364, delta=1e-3)
 
     def test_rank_2_tensor_input_normalize(self):
         edit_distance = EditDistance()
         y_true = tf.strings.split(
             [
                 "the tiny little cat was found under the big funny bed",
                 "it is sunny today",
@@ -71,25 +73,25 @@
             [
                 "the cat was found under the bed",
                 "it is sunny but with a hint of cloud cover",
             ]
         )
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.733, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.733, delta=1e-3)
 
     def test_rank_1_tensor_input_normalize_false(self):
         edit_distance = EditDistance(normalize=False)
         y_true = tf.strings.split(
             "the tiny little cat was found under the big funny bed"
         )
         y_pred = tf.strings.split("the cat was found under the bed")
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 4.0, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 4.0, delta=1e-3)
 
     def test_rank_2_tensor_input_normalize_false(self):
         edit_distance = EditDistance(normalize=False)
         y_true = tf.strings.split(
             [
                 "the tiny little cat was found under the big funny bed",
                 "it is sunny today",
@@ -99,46 +101,44 @@
             [
                 "the cat was found under the bed",
                 "it is sunny but with a hint of cloud cover",
             ]
         )
 
         edit_distance_val = edit_distance(y_true, y_pred)
-        self.assertAlmostEqual(edit_distance_val.numpy(), 5.5, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 5.5, delta=1e-3)
 
     def test_model_compile_normalize(self):
         inputs = keras.Input(shape=(None,), dtype="string")
-        outputs = tf.strings.lower(inputs)
+        outputs = keras.layers.Identity()(inputs)
         model = keras.Model(inputs, outputs)
 
         model.compile(metrics=[EditDistance()])
 
-        x = tf.strings.split(
+        y_pred = x = tf.strings.split(["the cat was found under the bed"])
+        y = tf.strings.split(
             ["the tiny little cat was found under the big funny bed"]
         )
-        y = tf.strings.split(["the cat was found under the bed"])
-
-        output = model.evaluate(y, x, return_dict=True)
 
+        output = model.compute_metrics(x, y, y_pred, sample_weight=None)
         self.assertAlmostEqual(output["edit_distance"], 0.364, delta=1e-3)
 
     def test_model_compile_normalize_false(self):
         inputs = keras.Input(shape=(None,), dtype="string")
-        outputs = tf.strings.lower(inputs)
+        outputs = keras.layers.Identity()(inputs)
         model = keras.Model(inputs, outputs)
 
         model.compile(metrics=[EditDistance(normalize=False)])
 
-        x = tf.strings.split(
+        y_pred = x = tf.strings.split(["the cat was found under the bed"])
+        y = tf.strings.split(
             ["the tiny little cat was found under the big funny bed"]
         )
-        y = tf.strings.split(["the cat was found under the bed"])
-
-        output = model.evaluate(y, x, return_dict=True)
 
+        output = model.compute_metrics(x, y, y_pred, sample_weight=None)
         self.assertAlmostEqual(output["edit_distance"], 4.0, delta=1e-3)
 
     def test_reset_state_normalize(self):
         edit_distance = EditDistance()
         y_true = tf.strings.split(
             [
                 "the tiny little cat was found under the big funny bed",
@@ -150,15 +150,15 @@
                 "the cat was found under the bed",
                 "it is sunny but with a hint of cloud cover",
             ]
         )
 
         edit_distance.update_state(y_true, y_pred)
         edit_distance_val = edit_distance.result()
-        self.assertNotEqual(edit_distance_val.numpy(), 0.0)
+        self.assertNotEqual(edit_distance_val, 0.0)
 
         edit_distance.reset_state()
         edit_distance_val = edit_distance.result()
         self.assertEqual(edit_distance_val, 0.0)
 
     def test_update_state_normalize(self):
         edit_distance = EditDistance()
@@ -173,22 +173,22 @@
                 "the cat was found under the bed",
                 "it is sunny but with a hint of cloud cover",
             ]
         )
 
         edit_distance.update_state(y_true_1, y_pred_1)
         edit_distance_val = edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.733, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.733, delta=1e-3)
 
         y_true_2 = tf.strings.split(["what is your favourite show"])
         y_pred_2 = tf.strings.split(["my favourite show is silicon valley"])
 
         edit_distance.update_state(y_true_2, y_pred_2)
         edit_distance_val = edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.85, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 0.85, delta=1e-3)
 
     def test_update_state_normalize_false(self):
         edit_distance = EditDistance(normalize=False)
         y_true_1 = tf.strings.split(
             [
                 "the tiny little cat was found under the big funny bed",
                 "it is sunny today",
@@ -199,101 +199,27 @@
                 "the cat was found under the bed",
                 "it is sunny but with a hint of cloud cover",
             ]
         )
 
         edit_distance.update_state(y_true_1, y_pred_1)
         edit_distance_val = edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 5.5, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 5.5, delta=1e-3)
 
         y_true_2 = tf.strings.split(["what is your favourite show"])
         y_pred_2 = tf.strings.split(["my favourite show is silicon valley"])
 
         edit_distance.update_state(y_true_2, y_pred_2)
         edit_distance_val = edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 5.667, delta=1e-3)
-
-    def test_merge_state_normalize(self):
-        edit_distance_1 = EditDistance()
-        edit_distance_2 = EditDistance()
-
-        y_true_1 = tf.strings.split(
-            [
-                "the tiny little cat was found under the big funny bed",
-                "it is sunny today",
-            ]
-        )
-        y_pred_1 = tf.strings.split(
-            [
-                "the cat was found under the bed",
-                "it is sunny but with a hint of cloud cover",
-            ]
-        )
-
-        y_true_2 = tf.strings.split(["what is your favourite show"])
-        y_pred_2 = tf.strings.split(["my favourite show is silicon valley"])
-
-        y_true_3 = tf.strings.split(["lorem ipsum dolor sit amet"])
-        y_pred_3 = tf.strings.split(["lorem ipsum is simply dummy text"])
-
-        edit_distance_1.update_state(y_true_1, y_pred_1)
-        edit_distance_1.update_state(y_true_2, y_pred_2)
-        edit_distance_val = edit_distance_1.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.85, delta=1e-3)
-
-        edit_distance_2.update_state(y_true_3, y_pred_3)
-        edit_distance_val = edit_distance_2.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.8, delta=1e-3)
-
-        merged_edit_distance = EditDistance()
-        merged_edit_distance.merge_state([edit_distance_1, edit_distance_2])
-        edit_distance_val = merged_edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 0.84, delta=1e-3)
-
-    def test_merge_state_normalize_false(self):
-        edit_distance_1 = EditDistance(normalize=False)
-        edit_distance_2 = EditDistance(normalize=False)
-
-        y_true_1 = tf.strings.split(
-            [
-                "the tiny little cat was found under the big funny bed",
-                "it is sunny today",
-            ]
-        )
-        y_pred_1 = tf.strings.split(
-            [
-                "the cat was found under the bed",
-                "it is sunny but with a hint of cloud cover",
-            ]
-        )
-
-        y_true_2 = tf.strings.split(["what is your favourite show"])
-        y_pred_2 = tf.strings.split(["my favourite show is silicon valley"])
-
-        y_true_3 = tf.strings.split(["lorem ipsum dolor sit amet"])
-        y_pred_3 = tf.strings.split(["lorem ipsum is simply dummy text"])
-
-        edit_distance_1.update_state(y_true_1, y_pred_1)
-        edit_distance_1.update_state(y_true_2, y_pred_2)
-        edit_distance_val = edit_distance_1.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 5.667, delta=1e-3)
-
-        edit_distance_2.update_state(y_true_3, y_pred_3)
-        edit_distance_val = edit_distance_2.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 4.0, delta=1e-3)
-
-        merged_edit_distance = EditDistance(normalize=False)
-        merged_edit_distance.merge_state([edit_distance_1, edit_distance_2])
-        edit_distance_val = merged_edit_distance.result()
-        self.assertAlmostEqual(edit_distance_val.numpy(), 5.25, delta=1e-3)
+        self.assertAlmostEqual(edit_distance_val, 5.667, delta=1e-3)
 
     def test_get_config(self):
         rouge = EditDistance(
             normalize=False,
-            dtype=tf.float32,
+            dtype="float32",
             name="edit_distance_test",
         )
 
         config = rouge.get_config()
         expected_config_subset = {
             "normalize": False,
         }
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/perplexity.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/perplexity.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,17 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Perplexity metric."""
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
+from keras_nlp.src.utils.tensor_utils import is_floating_dtype
 
 
 @keras_nlp_export("keras_nlp.metrics.Perplexity")
 class Perplexity(keras.metrics.Metric):
     """Perplexity metric.
 
     This class implements the perplexity metric. In short, this class calculates
@@ -34,94 +36,100 @@
             tensor of probabilities.
         mask_token_id: int. ID of the token to be masked. If provided, the mask
             is computed for this class. Note that if this field is provided, and
             if the `sample_weight` field in `update_state()` is also provided,
             we will compute the final `sample_weight` as the element-wise
             product of the mask and the `sample_weight`.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-               not specified, it defaults to tf.float32.
+               not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     Examples:
 
     1. Calculate perplexity by calling update_state() and result().
     1.1. `sample_weight`, and `mask_token_id` are not provided.
     >>> tf.random.set_seed(42)
     >>> perplexity = keras_nlp.metrics.Perplexity(name="perplexity")
     >>> target = tf.random.uniform(
-    ...     shape=[2, 5],  maxval=10, dtype=tf.int32, seed=42)
+    ...     shape=[2, 5],  maxval=10, dtype="int32", seed=42)
     >>> logits = tf.random.uniform(shape=(2, 5, 10), seed=42)
     >>> perplexity.update_state(target, logits)
     >>> perplexity.result()
     <tf.Tensor: shape=(), dtype=float32, numpy=11.8781595>
 
     1.2. `sample_weight` specified (masking token with ID 0).
     >>> tf.random.set_seed(42)
     >>> perplexity = keras_nlp.metrics.Perplexity(name="perplexity")
     >>> target = tf.random.uniform(
-    ...     shape=[2, 5],  maxval=10, dtype=tf.int32, seed=42)
+    ...     shape=[2, 5],  maxval=10, dtype="int32", seed=42)
     >>> logits = tf.random.uniform(shape=(2, 5, 10), seed=42)
     >>> sample_weight = tf.cast(
-    ...     tf.math.logical_not(tf.equal(target, 0)), tf.float32)
+    ...     tf.math.logical_not(tf.equal(target, 0)), "float32")
     >>> perplexity.update_state(target, logits, sample_weight)
     >>> perplexity.result()
     <tf.Tensor: shape=(), dtype=float32, numpy=13.1128>
 
     2. Call perplexity directly.
     >>> tf.random.set_seed(42)
     >>> perplexity = keras_nlp.metrics.Perplexity(name="perplexity")
     >>> target = tf.random.uniform(
-    ...     shape=[2, 5],  maxval=10, dtype=tf.int32, seed=42)
+    ...     shape=[2, 5],  maxval=10, dtype="int32", seed=42)
     >>> logits = tf.random.uniform(shape=(2, 5, 10), seed=42)
     >>> perplexity(target, logits)
     <tf.Tensor: shape=(), dtype=float32, numpy=11.8781595>
 
     3. Provide the padding token ID and let the class compute the mask on its
        own.
     >>> tf.random.set_seed(42)
     >>> perplexity = keras_nlp.metrics.Perplexity(
     ...     name="perplexity", mask_token_id=0)
     >>> target = tf.random.uniform(
-    ...     shape=[2, 5],  maxval=10, dtype=tf.int32, seed=42)
+    ...     shape=[2, 5],  maxval=10, dtype="int32", seed=42)
     >>> logits = tf.random.uniform(shape=(2, 5, 10), seed=42)
     >>> perplexity(target, logits)
     <tf.Tensor: shape=(), dtype=float32, numpy=13.1128>
     """
 
     def __init__(
         self,
         from_logits=False,
         mask_token_id=None,
-        dtype=None,
+        dtype="float32",
         name="perplexity",
         **kwargs,
     ):
-        super().__init__(name=name, dtype=dtype, **kwargs)
+        assert_tf_backend(self.__class__.__name__)
 
-        if not tf.as_dtype(self.dtype).is_floating:
+        if not is_floating_dtype(dtype):
             raise ValueError(
                 "`dtype` must be a floating point type. "
                 f"Received: dtype={dtype}"
             )
 
+        super().__init__(name=name, dtype=dtype, **kwargs)
+
         self._crossentropy = keras.losses.SparseCategoricalCrossentropy(
             from_logits=from_logits, reduction="sum"
         )
 
         self.from_logits = from_logits
         self.mask_token_id = mask_token_id
 
         self._aggregate_crossentropy = self.add_weight(
-            name="aggregate_crossentropy",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="aggregate_crossentropy",
         )
         self._number_of_samples = self.add_weight(
-            name="number_of_samples", initializer="zeros", dtype=self.dtype
+            shape=(),
+            initializer="zeros",
+            dtype=self.dtype,
+            name="number_of_samples",
         )
 
     def update_state(self, y_true, y_pred, sample_weight=None):
         # y_true shape: (batch_size, seq_len)
         # y_pred shape: (batch_size, seq_len, vocab_size)
         y_true = tf.cast(y_true, self.dtype)
         y_pred = tf.cast(y_pred, self.dtype)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/perplexity_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/perplexity_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,24 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for Perplexity."""
-
+import pytest
 import tensorflow as tf
 
 from keras_nlp.src.metrics.perplexity import Perplexity
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class PerplexityTest(tf.test.TestCase):
+@pytest.mark.tf_only
+class PerplexityTest(TestCase):
     def test_vars_after_initializing_class(self):
         perplexity = Perplexity()
-        self.assertEqual(perplexity.result().numpy(), 0.0)
+        self.assertEqual(perplexity.result(), 0.0)
 
     def test_from_logits_without_masking(self):
         perplexity = Perplexity(from_logits=True)
         y_true = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred = tf.constant(
             [
                 [
@@ -39,15 +41,15 @@
                     [1.163, 1.943, 1.761, 1.497],
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
 
         perplexity_val = perplexity(y_true, y_pred)
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.6542, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.6542, delta=1e-3)
 
     def test_from_logits_with_sample_weight(self):
         perplexity = Perplexity(from_logits=True)
 
         y_true = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred = tf.constant(
             [
@@ -59,18 +61,18 @@
                 [
                     [1.363, 1.726, 1.898, 2.582],
                     [1.163, 1.943, 1.761, 1.497],
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
-        sample_wt = tf.cast(y_true != 0, tf.int32)
+        sample_wt = tf.cast(y_true != 0, "int32")
 
         perplexity_val = perplexity(y_true, y_pred, sample_wt)
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.8789, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.8789, delta=1e-3)
 
     def test_from_logits_with_mask_token_id(self):
         perplexity = Perplexity(from_logits=True, mask_token_id=0)
 
         y_true = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred = tf.constant(
             [
@@ -84,15 +86,15 @@
                     [1.163, 1.943, 1.761, 1.497],
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
 
         perplexity_val = perplexity(y_true, y_pred)
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.8789, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.8789, delta=1e-3)
 
     def test_from_logits_with_mask_token_id_and_sample_weight(self):
         perplexity = Perplexity(from_logits=True, mask_token_id=0)
 
         y_true = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred = tf.constant(
             [
@@ -107,15 +109,15 @@
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
         sample_weight = tf.constant([[0.5, 0.1, 0.9], [1, 0.7, 0.5]])
 
         perplexity_val = perplexity(y_true, y_pred, sample_weight)
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.9442, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.9442, delta=1e-3)
 
     def test_two_inputs_from_logits(self):
         perplexity = Perplexity(from_logits=True, mask_token_id=0)
 
         y_true_1 = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred_1 = tf.constant(
             [
@@ -129,15 +131,15 @@
                     [1.163, 1.943, 1.761, 1.497],
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
 
         perplexity_val = perplexity(y_true_1, y_pred_1)
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.8789, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.8789, delta=1e-3)
 
         y_true_2 = tf.constant([[2, 0, 0], [1, 2, 3]])
         y_pred_2 = tf.constant(
             [
                 [
                     [2.887, 0.885, 2.973, 2.582],
                     [0.3838, 2.629, 1.91, 1.802],
@@ -147,15 +149,15 @@
                     [1.623, 2.784, 0.2109, 2.66],
                     [2.395, 2.01, 0.252, 1.828],
                     [0.4482, 2.629, 0.9697, 0.998],
                 ],
             ]
         )
         perplexity_val = perplexity(y_true_2, y_pred_2)
-        self.assertAlmostEqual(perplexity_val.numpy(), 3.9998, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 3.9998, delta=1e-3)
 
     def test_from_probs_with_sample_weight(self):
         perplexity = Perplexity(from_logits=False)
 
         y_true = tf.constant([[1, 3, 0], [2, 1, 3]])
         y_pred = tf.constant(
             [
@@ -169,15 +171,15 @@
                     [1.163, 1.943, 1.761, 1.497],
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
         y_prob = tf.nn.softmax(y_pred, axis=-1)
 
-        sample_wt = tf.cast(y_true != 0, tf.int32)
+        sample_wt = tf.cast(y_true != 0, "int32")
 
         perplexity_val = perplexity(y_true, y_prob, sample_wt)
         self.assertAlmostEqual(perplexity_val, 2.8789, delta=1e-3)
 
     def test_from_probs_with_pad_token(self):
         perplexity = Perplexity(from_logits=False, mask_token_id=0)
 
@@ -243,15 +245,15 @@
                     [2.766, 1.453, 2.61, 2.805],
                 ],
             ]
         )
 
         perplexity.update_state(y_true_1, y_pred_1)
         perplexity_val = perplexity.result()
-        self.assertAlmostEqual(perplexity_val.numpy(), 2.8789, delta=1e-3)
+        self.assertAlmostEqual(perplexity_val, 2.8789, delta=1e-3)
 
         y_true_2 = tf.constant([[2, 0, 0], [1, 2, 3]])
         y_pred_2 = tf.constant(
             [
                 [
                     [2.887, 0.885, 2.973, 2.582],
                     [0.3838, 2.629, 1.91, 1.802],
@@ -265,92 +267,23 @@
             ]
         )
 
         perplexity.update_state(y_true_2, y_pred_2)
         perplexity_val = perplexity.result()
         self.assertAlmostEqual(perplexity_val, 3.9998, delta=1e-3)
 
-    def test_merge_state(self):
-        perplexity_1 = Perplexity(from_logits=True, mask_token_id=0)
-        perplexity_2 = Perplexity(from_logits=True, mask_token_id=0)
-
-        y_true_1 = tf.constant([[1, 3, 0], [2, 1, 3]])
-        y_pred_1 = tf.constant(
-            [
-                [
-                    [1.034, 4.797, 2.82, 1.154],
-                    [2.258, 1.591, 1.811, 1.852],
-                    [3.216, 1.037, 0.3662, 2.7],
-                ],
-                [
-                    [1.363, 1.726, 1.898, 2.582],
-                    [1.163, 1.943, 1.761, 1.497],
-                    [2.766, 1.453, 2.61, 2.805],
-                ],
-            ]
-        )
-
-        y_true_2 = tf.constant([[2, 0, 0], [1, 2, 3]])
-        y_pred_2 = tf.constant(
-            [
-                [
-                    [2.887, 0.885, 2.973, 2.582],
-                    [0.3838, 2.629, 1.91, 1.802],
-                    [0.2578, 1.081, 1.125, 2.773],
-                ],
-                [
-                    [1.623, 2.784, 0.2109, 2.66],
-                    [2.395, 2.01, 0.252, 1.828],
-                    [0.4482, 2.629, 0.9697, 0.998],
-                ],
-            ]
-        )
-
-        y_true_3 = tf.constant([[1, 3, 1], [2, 2, 1]])
-        y_pred_3 = tf.constant(
-            [
-                [
-                    [0.7383, 0.882, 0.7295, 2.64],
-                    [0.867, 1.588, 2.291, 0.967],
-                    [0.0908, 1.453, 1.5, 0.0703],
-                ],
-                [
-                    [2.783, 0.9785, 2.664, 0.507],
-                    [0.741, 1.535, 2.16, 2.531],
-                    [2.863, 1.591, 1.403, 0.885],
-                ],
-            ]
-        )
-
-        perplexity_1.update_state(y_true_1, y_pred_1)
-        perplexity_1.update_state(y_true_2, y_pred_2)
-        self.assertAlmostEqual(
-            perplexity_1.result().numpy(), 3.9998, delta=1e-3
-        )
-
-        perplexity_2.update_state(y_true_3, y_pred_3)
-        self.assertAlmostEqual(
-            perplexity_2.result().numpy(), 4.4303, delta=1e-3
-        )
-
-        merged_perplexity = Perplexity(from_logits=True, mask_token_id=0)
-        merged_perplexity.merge_state([perplexity_1, perplexity_2])
-        self.assertAlmostEqual(
-            merged_perplexity.result().numpy(), 4.1385, delta=1e-3
-        )
-
     def test_get_config(self):
         perplexity = Perplexity(
             from_logits=True,
             mask_token_id=0,
-            dtype=tf.float32,
+            dtype="float32",
             name="perplexity_test",
         )
         config = perplexity.get_config()
         expected_config = {
             "from_logits": True,
             "mask_token_id": 0,
-            "dtype": tf.float32,
+            "dtype": "float32",
             "name": "perplexity_test",
         }
         self.assertEqual(config, expected_config)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_base.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_base.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,17 +14,19 @@
 
 """ROUGE metric."""
 
 
 import types
 
 import tensorflow as tf
-from tensorflow import keras
 
-from keras_nlp.src.utils.tf_utils import tensor_to_string_list
+from keras_nlp.src.backend import keras
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
+from keras_nlp.src.utils.tensor_utils import is_floating_dtype
+from keras_nlp.src.utils.tensor_utils import tensor_to_list
 
 try:
     from rouge_score import rouge_scorer
 except ImportError:
     rouge_scorer = None
 
 
@@ -35,44 +37,46 @@
     and ROUGE-L.
 
     Note on input shapes:
     For `y_true` and `y_pred`, this class supports scalar values and batch
     inputs of shapes `()`, `(batch_size,)` and `(batch_size, 1)`.
 
     Args:
-        variant: string. One of "rougeN", "rougeL". Defaults to
-            "rouge2". For "rougeN", N lies in the range [1, 9].
+        variant: string. One of "rougeN", "rougeL". For "rougeN", N lies in
+            the range [1, 9]. Defaults to `"rouge2"`.
         use_stemmer: bool. Whether Porter Stemmer should be used to strip word
-            suffixes to improve matching. Defaults to False.
+            suffixes to improve matching. Defaults to `False`.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-            not specified, it defaults to tf.float32.
+            not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     References:
         - [Lin et al., 2004](https://aclanthology.org/W04-1013/)
     """
 
     def __init__(
         self,
         variant="rouge2",
         use_stemmer=False,
-        dtype=None,
+        dtype="float32",
         name="rouge",
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(name=name, dtype=dtype, **kwargs)
 
         if rouge_scorer is None:
             raise ImportError(
                 f"{self.__class__.__name__} requires the `rouge_score` "
                 "package. Please install it with `pip install rouge-score`."
             )
 
-        if not tf.as_dtype(self.dtype).is_floating:
+        if not is_floating_dtype(dtype):
             raise ValueError(
                 "`dtype` must be a floating point type. "
                 f"Received: dtype={dtype}"
             )
 
         if variant not in tuple(
             ("rouge" + str(order) for order in range(1, 10))
@@ -90,31 +94,37 @@
         # of rouge_scorer have released a new version.
         self._rouge_scorer = rouge_scorer.RougeScorer(
             rouge_types=[self.variant],
             use_stemmer=use_stemmer,
         )
 
         self._rouge_precision = self.add_weight(
-            name="rouge_precision",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="rouge_precision",
         )
         self._rouge_recall = self.add_weight(
-            name="rouge_recall",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="rouge_recall",
         )
         self._rouge_f1_score = self.add_weight(
-            name="rouge_f1_score",
+            shape=(),
             initializer="zeros",
             dtype=self.dtype,
+            name="rouge_f1_score",
         )
 
         self._number_of_samples = self.add_weight(
-            name="number_of_samples", initializer="zeros", dtype=self.dtype
+            shape=(),
+            initializer="zeros",
+            dtype=self.dtype,
+            name="number_of_samples",
         )
 
     def __new__(cls, *args, **kwargs):
         # Temporary workaround for Keras bug with dictionary return types.
         # Wraps `result()` with a python dictionary that also supports variable
         # assignment. We have to do this with __new__ because the base metric
         # class wraps the `results()` method.
@@ -163,16 +173,16 @@
 
         y_true = validate_and_fix_rank(y_true, "y_true")
         y_pred = validate_and_fix_rank(y_pred, "y_pred")
 
         batch_size = tf.shape(y_true)[0]
 
         def calculate_rouge_score(reference, hypothesis):
-            reference = tensor_to_string_list(reference)
-            hypothesis = tensor_to_string_list(hypothesis)
+            reference = tensor_to_list(reference)
+            hypothesis = tensor_to_list(hypothesis)
             score = self._rouge_scorer.score(reference, hypothesis)[
                 self.variant
             ]
             return tf.cast(
                 tf.constant([score.precision, score.recall, score.fmeasure]),
                 dtype=self.dtype,
             )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_l.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_l.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,17 +29,17 @@
 
     Note on input shapes:
     For `y_true` and `y_pred`, this class supports scalar values and batch
     inputs of shapes `()`, `(batch_size,)` and `(batch_size, 1)`.
 
     Args:
         use_stemmer: bool. Whether Porter Stemmer should be used to strip word
-            suffixes to improve matching. Defaults to False.
+            suffixes to improve matching. Defaults to `False`.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-               not specified, it defaults to tf.float32.
+               not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     References:
         - [Lin et al., 2004](https://aclanthology.org/W04-1013/)
 
     Examples:
@@ -98,35 +98,32 @@
     ...     ]
     ... )
     >>> rouge_l(y_true, y_pred)["f1_score"]
     <tf.Tensor: shape=(), dtype=float32, numpy=0.80748665>
 
     3. Pass the metric to `model.compile()`.
     >>> inputs = keras.Input(shape=(), dtype='string')
-    >>> outputs = tf.strings.lower(inputs)
+    >>> outputs = keras.layers.Identity()(inputs)
     >>> model = keras.Model(inputs, outputs)
     >>> model.compile(metrics=[keras_nlp.metrics.RougeL()])
-    >>> x = tf.constant(["HELLO THIS IS FUN"])
+    >>> y_pred = x = tf.constant(["hello this is fun"])
     >>> y = tf.constant(["hello this is awesome"])
-    >>> metric_dict = model.evaluate(x, y, return_dict=True)
-    >>> metric_dict["f1_score"]
-     0.75
+    >>> model.compute_metrics(x, y, y_pred, sample_weight=None)["f1_score"]
+    0.75
     """
 
     def __init__(
         self,
         use_stemmer=False,
-        dtype=None,
         name="rouge-l",
         **kwargs,
     ):
         super().__init__(
             variant="rougeL",
             use_stemmer=use_stemmer,
-            dtype=dtype,
             name=name,
             **kwargs,
         )
 
     def get_config(self):
         config = super().get_config()
         del config["variant"]
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_l_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_l_test.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,70 +9,58 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for RougeL."""
-
+import pytest
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.metrics.rouge_l import RougeL
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RougeLTest(tf.test.TestCase):
-    def setUp(self):
-        super().setUp()
-
-        def assertDictAlmostEqual(d1, d2, delta=1e-3, typecast_to_numpy=True):
-            for key, val in d1.items():
-                if typecast_to_numpy:
-                    val = val.numpy()
-                self.assertAlmostEqual(val, d2[key], delta=delta)
-
-        def assertDictAllValuesNotEqual(d1, d2):
-            for key, val in d1.items():
-                self.assertNotEqual(val, d2[key])
-
-        self.assertDictAlmostEqual = assertDictAlmostEqual
-        self.assertDictAllValuesNotEqual = assertDictAllValuesNotEqual
-
+@pytest.mark.tf_only
+class RougeLTest(TestCase):
     def test_initialization(self):
         rouge = RougeL()
         result = rouge.result()
 
         self.assertDictEqual(
-            result, {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
+            result,
+            {"precision": 0.0, "recall": 0.0, "f1_score": 0.0},
         )
 
     def test_string_input(self):
         rouge = RougeL(use_stemmer=False)
         y_true = "the tiny little cat was found under the big funny bed"
         y_pred = "the cat was under the bed"
 
         rouge_val = rouge(y_true, y_pred)
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 1.0, "recall": 0.545, "f1_score": 0.706}
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 1.0, "recall": 0.545454, "f1_score": 0.705882},
         )
 
     def test_string_list_input(self):
         rouge = RougeL(use_stemmer=False)
         y_true = [
             "the tiny little cat was found under the big funny bed",
             "i really love contributing to KerasNLP",
         ]
         y_pred = [
             "the cat was under the bed",
             "i love contributing to KerasNLP",
         ]
 
         rouge_val = rouge(y_true, y_pred)
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 1.0, "recall": 0.689, "f1_score": 0.807}
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 1.0, "recall": 0.689393, "f1_score": 0.807486},
         )
 
     def test_tensor_input(self):
         rouge = RougeL(use_stemmer=False)
         y_true = tf.constant(
             [
                 "the tiny little cat was found under the big funny bed",
@@ -80,16 +68,17 @@
             ]
         )
         y_pred = tf.constant(
             ["the cat was under the bed", "i love contributing to KerasNLP"]
         )
 
         rouge_val = rouge(y_true, y_pred)
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 1.0, "recall": 0.689, "f1_score": 0.807}
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 1.0, "recall": 0.689393, "f1_score": 0.807486},
         )
 
     def test_rank_2_input(self):
         rouge = RougeL(use_stemmer=False)
         y_true = tf.constant(
             [
                 ["the tiny little cat was found under the big funny bed"],
@@ -97,34 +86,17 @@
             ]
         )
         y_pred = tf.constant(
             [["the cat was under the bed"], ["i love contributing to KerasNLP"]]
         )
 
         rouge_val = rouge(y_true, y_pred)
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 1.0, "recall": 0.689, "f1_score": 0.807}
-        )
-
-    def test_model_compile(self):
-        inputs = keras.Input(shape=(), dtype="string")
-        outputs = tf.strings.lower(inputs)
-        model = keras.Model(inputs, outputs)
-
-        model.compile(metrics=[RougeL()])
-
-        x = tf.constant(["HELLO THIS IS FUN"])
-        y = tf.constant(["hello this is awesome"])
-
-        output = model.evaluate(x, y, return_dict=True)
-        del output["loss"]
-        self.assertDictAlmostEqual(
-            output,
-            {"precision": 0.75, "recall": 0.75, "f1_score": 0.75},
-            typecast_to_numpy=False,
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 1.0, "recall": 0.689393, "f1_score": 0.807486},
         )
 
     def test_reset_state(self):
         rouge = RougeL()
         y_true = tf.constant(
             ["hey, this is great fun", "i love contributing to KerasNLP"]
         )
@@ -133,22 +105,24 @@
                 "great fun indeed",
                 "KerasNLP is awesome, i love contributing to it",
             ]
         )
 
         rouge.update_state(y_true, y_pred)
         rouge_val = rouge.result()
-        self.assertDictAllValuesNotEqual(
-            rouge_val, {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
+        self.assertNotAllClose(
+            rouge_val,
+            {"precision": 0.0, "recall": 0.0, "f1_score": 0.0},
         )
 
         rouge.reset_state()
         rouge_val = rouge.result()
         self.assertDictEqual(
-            rouge_val, {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
+            rouge_val,
+            {"precision": 0.0, "recall": 0.0, "f1_score": 0.0},
         )
 
     def test_update_state(self):
         rouge = RougeL()
         y_true_1 = tf.constant(
             [
                 "the tiny little cat was found under the big funny bed",
@@ -157,71 +131,33 @@
         )
         y_pred_1 = tf.constant(
             ["the cat was under the bed", "i love contributing to KerasNLP"]
         )
 
         rouge.update_state(y_true_1, y_pred_1)
         rouge_val = rouge.result()
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 1.0, "recall": 0.689, "f1_score": 0.807}
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 1.0, "recall": 0.689393, "f1_score": 0.807486},
         )
 
         y_true_2 = tf.constant(["what is your favourite show"])
         y_pred_2 = tf.constant(["my favourite show is silicon valley"])
 
         rouge.update_state(y_true_2, y_pred_2)
         rouge_val = rouge.result()
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 0.778, "recall": 0.593, "f1_score": 0.66}
-        )
-
-    def test_merge_state(self):
-        rouge_1 = RougeL()
-        rouge_2 = RougeL()
-
-        y_true_1 = tf.constant(
-            [
-                "the tiny little cat was found under the big funny bed",
-                "i really love contributing to KerasNLP",
-            ]
-        )
-        y_pred_1 = tf.constant(
-            ["the cat was under the bed", "i love contributing to KerasNLP"]
-        )
-
-        y_true_2 = tf.constant(["what is your favourite show"])
-        y_pred_2 = tf.constant(["my favourite show is silicon valley"])
-
-        y_true_3 = tf.constant(["lorem ipsum dolor sit amet"])
-        y_pred_3 = tf.constant(["lorem ipsum is simply dummy text"])
-
-        rouge_1.update_state(y_true_1, y_pred_1)
-        rouge_1.update_state(y_true_2, y_pred_2)
-        rouge_val = rouge_1.result()
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 0.778, "recall": 0.593, "f1_score": 0.66}
-        )
-
-        rouge_2.update_state(y_true_3, y_pred_3)
-        rouge_val = rouge_2.result()
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 0.333, "recall": 0.4, "f1_score": 0.364}
-        )
-
-        merged_rouge = RougeL()
-        merged_rouge.merge_state([rouge_1, rouge_2])
-        rouge_val = merged_rouge.result()
-        self.assertDictAlmostEqual(
-            rouge_val, {"precision": 0.667, "recall": 0.545, "f1_score": 0.586}
+        self.assertAllClose(
+            rouge_val,
+            {"precision": 0.777777, "recall": 0.592929, "f1_score": 0.659536},
         )
 
     def test_get_config(self):
         rouge = RougeL(
             use_stemmer=True,
-            dtype=tf.float32,
+            dtype="float32",
             name="rouge_l_test",
         )
 
         config = rouge.get_config()
         expected_config_subset = {
             "use_stemmer": True,
         }
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/metrics/rouge_n.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/metrics/rouge_n.py`

 * *Files 6% similar despite different names*

```diff
@@ -29,19 +29,19 @@
 
     Note on input shapes:
     For `y_true` and `y_pred`, this class supports scalar values and batch
     inputs of shapes `()`, `(batch_size,)` and `(batch_size, 1)`.
 
     Args:
         order: The order of n-grams which are to be matched. It should lie in
-            range [1, 9]. Defaults to 2.
+            range [1, 9]. Defaults to `2`.
         use_stemmer: bool. Whether Porter Stemmer should be used to strip word
-            suffixes to improve matching. Defaults to False.
+            suffixes to improve matching. Defaults to `False`.
         dtype: string or tf.dtypes.Dtype. Precision of metric computation. If
-               not specified, it defaults to tf.float32.
+               not specified, it defaults to `"float32"`.
         name: string. Name of the metric instance.
         **kwargs: Other keyword arguments.
 
     References:
         - [Lin et al., 2004](https://aclanthology.org/W04-1013/)
 
     Examples:
@@ -117,42 +117,39 @@
     ...     ]
     ... )
     >>> rouge_n(y_true, y_pred)["f1_score"]
     <tf.Tensor: shape=(), dtype=float32, numpy=0.2857143>
 
     3. Pass the metric to `model.compile()`.
     >>> inputs = keras.Input(shape=(), dtype='string')
-    >>> outputs = tf.strings.lower(inputs)
+    >>> outputs = keras.layers.Identity()(inputs)
     >>> model = keras.Model(inputs, outputs)
     >>> model.compile(metrics=[keras_nlp.metrics.RougeN()])
-    >>> x = tf.constant(["HELLO THIS IS FUN"])
+    >>> y_pred = x = tf.constant(["hello this is fun"])
     >>> y = tf.constant(["hello this is awesome"])
-    >>> metric_dict = model.evaluate(x, y, return_dict=True)
-    >>> metric_dict["f1_score"]
+    >>> model.compute_metrics(x, y, y_pred, sample_weight=None)["f1_score"]
     0.6666666865348816
     """
 
     def __init__(
         self,
         order=2,
         use_stemmer=False,
-        dtype=None,
         name="rouge-n",
         **kwargs,
     ):
         if order not in range(1, 10):
             raise ValueError(
                 "Invalid `order` value. Should lie in the range [1, 9]."
                 f"Received order={order}"
             )
 
         super().__init__(
             variant=f"rouge{order}",
             use_stemmer=use_stemmer,
-            dtype=dtype,
             name=name,
             **kwargs,
         )
 
         self.order = order
 
     def get_config(self):
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,14 +16,21 @@
 from keras_nlp.src.models.albert.albert_classifier import AlbertClassifier
 from keras_nlp.src.models.albert.albert_masked_lm import AlbertMaskedLM
 from keras_nlp.src.models.albert.albert_masked_lm_preprocessor import (
     AlbertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.models.bart.bart_backbone import BartBackbone
+from keras_nlp.src.models.bart.bart_preprocessor import BartPreprocessor
+from keras_nlp.src.models.bart.bart_seq_2_seq_lm import BartSeq2SeqLM
+from keras_nlp.src.models.bart.bart_seq_2_seq_lm_preprocessor import (
+    BartSeq2SeqLMPreprocessor,
+)
+from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
 from keras_nlp.src.models.bert.bert_backbone import BertBackbone
 from keras_nlp.src.models.bert.bert_classifier import BertClassifier
 from keras_nlp.src.models.bert.bert_masked_lm import BertMaskedLM
 from keras_nlp.src.models.bert.bert_masked_lm_preprocessor import (
     BertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
@@ -67,14 +74,16 @@
 from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
 from keras_nlp.src.models.gpt2.gpt2_causal_lm import GPT2CausalLM
 from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
     GPT2CausalLMPreprocessor,
 )
 from keras_nlp.src.models.gpt2.gpt2_preprocessor import GPT2Preprocessor
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.models.gpt_neo_x.gpt_neo_x_backbone import GPTNeoXBackbone
+from keras_nlp.src.models.gpt_neo_x.gpt_neo_x_tokenizer import GPTNeoXTokenizer
 from keras_nlp.src.models.opt.opt_backbone import OPTBackbone
 from keras_nlp.src.models.opt.opt_causal_lm import OPTCausalLM
 from keras_nlp.src.models.opt.opt_causal_lm_preprocessor import (
     OPTCausalLMPreprocessor,
 )
 from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
@@ -82,14 +91,22 @@
 from keras_nlp.src.models.roberta.roberta_classifier import RobertaClassifier
 from keras_nlp.src.models.roberta.roberta_masked_lm import RobertaMaskedLM
 from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import (
     RobertaMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.models.t5.t5_backbone import T5Backbone
+from keras_nlp.src.models.t5.t5_tokenizer import T5Tokenizer
+from keras_nlp.src.models.whisper.whisper_audio_feature_extractor import (
+    WhisperAudioFeatureExtractor,
+)
+from keras_nlp.src.models.whisper.whisper_backbone import WhisperBackbone
+from keras_nlp.src.models.whisper.whisper_preprocessor import WhisperPreprocessor
+from keras_nlp.src.models.whisper.whisper_tokenizer import WhisperTokenizer
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_classifier import (
     XLMRobertaClassifier,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm import (
     XLMRobertaMaskedLM,
 )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/modeling/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_backbone.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """ALBERT backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 from keras_nlp.src.models.albert.albert_presets import backbone_presets
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def albert_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
@@ -74,21 +72,17 @@
             embeddings.
         num_segments: int. The number of types that the 'segment_ids' input can
             take.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]]),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Randomly initialized ALBERT encoder
     model = keras_nlp.models.AlbertBackbone(
         vocabulary_size=30000,
         num_layers=12,
         num_heads=12,
@@ -161,15 +155,15 @@
         x = keras.layers.Add()(
             (token_embedding, position_embedding, segment_embedding)
         )
         x = keras.layers.LayerNormalization(
             name="embeddings_layer_norm",
             axis=-1,
             epsilon=1e-12,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="embeddings_dropout",
         )(x)
 
         # Project the embedding to `hidden_dim`.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_backbone_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,38 +13,39 @@
 # limitations under the License.
 """Test for ALBERT backbone model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertBackboneTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertBackboneTest(TestCase):
     def setUp(self):
         self.backbone = AlbertBackbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             num_groups=1,
             num_inner_repetitions=1,
             embedding_dim=16,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
         )
         self.batch_size = 8
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "segment_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "segment_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_valid_call_albert(self):
@@ -53,27 +54,27 @@
     def test_name(self):
         # Check default name passed through
         self.assertRegexpMatches(self.backbone.name, "albert_backbone")
 
     def test_variable_sequence_length_call_albert(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "segment_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "segment_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
             self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
     def test_error_for_invalid_num_groups(self):
         with self.assertRaises(ValueError):
             self.model = AlbertBackbone(
                 vocabulary_size=10,
@@ -82,40 +83,34 @@
                 num_groups=2,
                 num_inner_repetitions=1,
                 embedding_dim=4,
                 hidden_dim=64,
                 intermediate_dim=128,
             )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, AlbertBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(
             model_output["pooled_output"], restored_output["pooled_output"]
         )
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class AlbertBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = AlbertBackbone(
                 vocabulary_size=10,
                 num_layers=2,
                 num_heads=2,
                 num_groups=1,
@@ -123,17 +118,17 @@
                 embedding_dim=16,
                 hidden_dim=2,
                 intermediate_dim=2,
                 max_sequence_length=4,
             )
 
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "segment_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((8, 128), dtype="int32"),
+            "segment_ids": ops.ones((8, 128), dtype="int32"),
+            "padding_mask": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_classifier.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,23 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """ALBERT classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
 from keras_nlp.src.models.albert.albert_backbone import albert_kernel_initializer
 from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
 from keras_nlp.src.models.albert.albert_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.AlbertClassifier")
 class AlbertClassifier(Task):
     """An end-to-end ALBERT model for classification tasks
 
@@ -46,17 +44,18 @@
 
     Args:
         backbone: A `keras_nlp.models.AlertBackbone` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.AlbertPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
+        activation: Optional `str` or callable. The
             activation function to use on the model outputs. Set
             `activation="softmax"` to return output probabilities.
+            Defaults to `None`.
         dropout: float. The dropout probability value, applied after the dense
             layer.
 
     Examples:
 
      Raw string data.
     ```python
@@ -82,21 +81,17 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.AlbertClassifier.from_preset(
         "albert_base_en_uncased",
         num_classes=4,
@@ -181,21 +176,22 @@
         self._backbone = backbone
         self._preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.dropout = dropout
 
         # Default compilation
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_classifier_test.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,144 +1,146 @@
-# Copyright 2022 The KerasNLP Authors
+# Copyright 2023 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for ALBERT classification model."""
+"""Tests for RoBERTa classification model."""
 
-import io
 import os
 
 import pytest
-import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
-from keras_nlp.src.models.albert.albert_classifier import AlbertClassifier
-from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
-from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
+from keras_nlp.src.models.roberta.roberta_classifier import RobertaClassifier
+from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
+from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertClassifierTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaClassifierTest(TestCase):
     def setUp(self):
-        # Setup model
-
-        bytes_io = io.BytesIO()
-        vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the earth is round"]
-        )
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=vocab_data.as_numpy_iterator(),
-            model_writer=bytes_io,
-            vocab_size=10,
-            model_type="WORD",
-            pad_id=0,
-            unk_id=1,
-            bos_id=2,
-            eos_id=3,
-            pad_piece="<pad>",
-            unk_piece="<unk>",
-            bos_piece="[CLS]",
-            eos_piece="[SEP]",
-            user_defined_symbols="[MASK]",
-        )
-        self.proto = bytes_io.getvalue()
-
-        tokenizer = AlbertTokenizer(proto=self.proto)
-
-        self.preprocessor = AlbertPreprocessor(
-            tokenizer=tokenizer,
+        self.vocab = {
+            "<s>": 0,
+            "<pad>": 1,
+            "</s>": 2,
+            "air": 3,
+            "plane": 4,
+            "at": 5,
+            "port": 6,
+            "koh": 7,
+            "li": 8,
+            "is": 9,
+            "the": 10,
+            "best": 11,
+            "<mask>": 12,
+        }
+
+        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
+        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
+        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
+        merges += ["pla ne"]
+        self.merges = merges
+        self.preprocessor = RobertaPreprocessor(
+            RobertaTokenizer(vocabulary=self.vocab, merges=self.merges),
             sequence_length=5,
         )
-        self.backbone = AlbertBackbone(
+        self.backbone = RobertaBackbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
             num_heads=2,
-            embedding_dim=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-
-        self.classifier = AlbertClassifier(
+        self.classifier = RobertaClassifier(
             self.backbone,
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
+            hidden_dim=4,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        # Setup data.
+        self.raw_batch = [
+            " airplane at airport",
+            " the airplane is the best",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
     def test_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
-            loss="sparse_categorical_crossentropy",
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = RobertaClassifier(
+            self.backbone,
+            num_classes=2,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = RobertaClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            hidden_dim=4,
+            name="test",
+            trainable=False,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saving_model(self, save_format, filename):
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saved_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
-        # Check we got the real object back
-        self.assertIsInstance(restored_model, AlbertClassifier)
+        # Check we got the real object back.
+        self.assertIsInstance(restored_model, RobertaClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,26 +12,24 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """ALBERT masked LM model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
 from keras_nlp.src.models.albert.albert_backbone import albert_kernel_initializer
 from keras_nlp.src.models.albert.albert_masked_lm_preprocessor import (
     AlbertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.albert.albert_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.AlbertMaskedLM")
 class AlbertMaskedLM(Task):
     """An end-to-end ALBERT model for the masked language modeling task.
 
@@ -79,22 +77,18 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create preprocessed batch where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2)),
-        "segment_ids": tf.constant([[0, 0, 0, 0, 0, 0, 0, 0]] * 2, shape=(2, 8))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 0, 0, 0]] * 2),
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.AlbertMaskedLM.from_preset(
         "albert_base_en_uncased",
         preprocessor=None,
@@ -131,16 +125,16 @@
 
         self.backbone = backbone
         self.preprocessor = preprocessor
 
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return AlbertBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_preprocessor.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 """ALBERT masked language model preprocessor layer."""
 
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
 from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
 @keras_nlp_export("keras_nlp.models.AlbertMaskedLMPreprocessor")
 class AlbertMaskedLMPreprocessor(AlbertPreprocessor):
     """ALBERT preprocessing for the masked language modeling task.
@@ -44,23 +46,24 @@
     Args:
         tokenizer: A `keras_nlp.models.AlbertTokenizer` instance.
         sequence_length: The length of the packed inputs.
         mask_selection_rate: The probability an input token will be dynamically
             masked.
         mask_selection_length: The maximum number of masked tokens supported
             by the layer.
-        mask_token_rate: float, defaults to 0.8. `mask_token_rate` must be
+        mask_token_rate: float. `mask_token_rate` must be
             between 0 and 1 which indicates how often the mask_token is
             substituted for tokens selected for masking.
-        random_token_rate: float, defaults to 0.1. `random_token_rate` must be
+            Defaults to `0.8`.
+        random_token_rate: float. `random_token_rate` must be
             between 0 and 1 which indicates how often a random token is
             substituted for tokens selected for masking. Default is 0.1.
             Note: mask_token_rate + random_token_rate <= 1,  and for
             (1 - mask_token_rate - random_token_rate), the token will not be
-            changed.
+            changed. Defaults to `0.1`.
         truncate: string. The algorithm to truncate a list of batched segments
             to fit within `sequence_length`. The value can be either
             `round_robin` or `waterfall`:
                 - `"round_robin"`: Available space is assigned one token at a
                     time in a round-robin fashion to the inputs that still need
                     some, until the limit is reached.
                 - `"waterfall"`: The allocation of the budget is done using a
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_preprocessor_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,24 +15,24 @@
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.albert.albert_masked_lm_preprocessor import (
     AlbertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertMaskedLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
 
         bytes_io = io.BytesIO()
         sentencepiece.SentencePieceTrainer.train(
@@ -140,36 +140,31 @@
             x["padding_mask"], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
         )
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_masked_lm_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -15,26 +15,26 @@
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
 from keras_nlp.src.models.albert.albert_masked_lm import AlbertMaskedLM
 from keras_nlp.src.models.albert.albert_masked_lm_preprocessor import (
     AlbertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertMaskedLMTest(TestCase):
     def setUp(self):
         # Setup model.
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round", "an eagle flew"]
         )
 
         bytes_io = io.BytesIO()
@@ -81,22 +81,20 @@
             preprocessor=self.preprocessor,
         )
         self.masked_lm_no_preprocessing = AlbertMaskedLM(
             self.backbone,
             preprocessor=None,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "quick brown fox",
-                "eagle flew over fox",
-                "the eagle flew quick",
-                "a brown eagle",
-            ]
-        )
+        self.raw_batch = [
+            "quick brown fox",
+            "eagle flew over fox",
+            "the eagle flew quick",
+            "a brown eagle",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
@@ -119,25 +117,19 @@
         self.masked_lm.preprocessor = None
         self.masked_lm.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.masked_lm.fit(self.preprocessed_dataset)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, AlbertMaskedLM)
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output, atol=0.01, rtol=0.01)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_preprocessor.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """ALBERT preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
 from keras_nlp.src.models.albert.albert_presets import backbone_presets
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_preprocessor_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,29 +9,28 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for ALBERT preprocessor layer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertPreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -155,34 +154,29 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_presets_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,25 +10,26 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
 from keras_nlp.src.models.albert.albert_classifier import AlbertClassifier
 from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
 from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class AlbertPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertPresetSmokeTest(TestCase):
     """
     A smoke test for ALBERT presets we run continuously.
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/albert/albert_presets_test.py --run_large`
     """
 
     def test_tokenizer_output(self):
@@ -48,31 +49,31 @@
         expected_outputs = [2, 13, 1, 3]
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = AlbertClassifier.from_preset(
             "albert_base_en_uncased",
             num_classes=2,
             load_weights=load_weights,
         )
         # We don't assert output values, as the head weights are random.
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "segment_ids": tf.constant([[0, 0, 0, 0]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "segment_ids": ops.array([[0, 0, 0, 0]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = AlbertClassifier.from_preset(
             "albert_base_en_uncased",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -80,17 +81,17 @@
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[2, 13, 1, 3]]),
-            "segment_ids": tf.constant([[0, 0, 0, 0]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[2, 13, 1, 3]]),
+            "segment_ids": ops.array([[0, 0, 0, 0]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = AlbertBackbone.from_preset(
             "albert_base_en_uncased", load_weights=load_weights
         )
         outputs = model(input_data)
         if load_weights:
             outputs = outputs["sequence_output"][0, 0, :5]
@@ -117,15 +118,15 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("albert_base_en_uncased_clowntown", **kwargs)
 
 
 @pytest.mark.extra_large
-class AlbertPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
     This tests every ALBERT preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/albert/albert_presets_test.py --run_extra_large`
     """
 
@@ -134,58 +135,54 @@
     )
     def test_load_albert(self, load_weights):
         for preset in AlbertBackbone.presets:
             model = AlbertBackbone.from_preset(
                 preset, load_weights=load_weights
             )
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
-                ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_albert_classifier(self, load_weights):
         for preset in AlbertClassifier.presets:
             classifier = AlbertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["This quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_albert_classifier_without_preprocessing(self, load_weights):
         for preset in AlbertClassifier.presets:
             classifier = AlbertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 preprocessor=None,
                 load_weights=load_weights,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
-                ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in AlbertTokenizer.presets:
             tokenizer = AlbertTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_tokenizer.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/albert/albert_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_tokenizer_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,109 +8,102 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for ALBERT tokenizer."""
-
+"""Tests for T5 tokenizer."""
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.t5.t5_tokenizer import T5Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class AlbertTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class T5TokenizerTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=12,
+            vocab_size=11,
             model_type="WORD",
+            bos_id=-1,
             pad_id=0,
-            unk_id=1,
-            bos_id=2,
-            eos_id=3,
+            eos_id=1,
+            unk_id=2,
             pad_piece="<pad>",
+            eos_piece="</s>",
             unk_piece="<unk>",
-            bos_piece="[CLS]",
-            eos_piece="[SEP]",
             user_defined_symbols="[MASK]",
         )
         self.proto = bytes_io.getvalue()
 
-        self.tokenizer = AlbertTokenizer(proto=self.proto)
+        self.tokenizer = T5Tokenizer(proto=self.proto)
 
     def test_tokenize(self):
         input_data = "the quick brown fox"
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [5, 10, 6, 8])
+        self.assertAllEqual(output, [4, 9, 5, 7])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant(["the quick brown fox", "the earth is round"])
+        input_data = ["the quick brown fox", "the earth is round"]
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [[5, 10, 6, 8], [5, 7, 9, 11]])
+        self.assertAllEqual(output, [[4, 9, 5, 7], [4, 6, 8, 10]])
 
     def test_detokenize(self):
-        input_data = tf.constant([[5, 10, 6, 8]])
+        input_data = [[4, 9, 5, 7]]
         output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["the quick brown fox"]))
+        self.assertEqual(output, ["the quick brown fox"])
 
     def test_vocabulary_size(self):
-        tokenizer = AlbertTokenizer(proto=self.proto)
-        self.assertEqual(tokenizer.vocabulary_size(), 12)
+        tokenizer = T5Tokenizer(proto=self.proto)
+        self.assertEqual(tokenizer.vocabulary_size(), 11)
 
     def test_errors_missing_special_tokens(self):
         bytes_io = io.BytesIO()
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=iter(["abc"]),
             model_writer=bytes_io,
             vocab_size=5,
             pad_id=-1,
             eos_id=-1,
             bos_id=-1,
         )
         with self.assertRaises(ValueError):
-            AlbertTokenizer(proto=bytes_io.getvalue())
+            T5Tokenizer(proto=bytes_io.getvalue())
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/backbone.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,21 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Base class for Backbone models."""
 
 import os
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class Backbone(keras.Model):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
     @property
     def token_embedding(self):
         """A `keras.layers.Embedding` instance for embedding token ids."""
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_backbone.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,30 +12,28 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """BART backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.transformer_decoder import TransformerDecoder
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.transformer_decoder import TransformerDecoder
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.bart.bart_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def bart_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class BartBackbone(Backbone):
     """BART encoder-decoder network.
 
     This class implements a Transformer-based encoder-decoder model as
     described in
     ["BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"](https://arxiv.org/abs/1910.13461).
 
@@ -62,21 +60,21 @@
             can consume. If None, `max_sequence_length` uses the value from
             sequence length. This determines the variable shape for positional
             embeddings.
 
     Examples:
     ```python
     input_data = {
-        "encoder_token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "encoder_padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
+        "encoder_token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "encoder_padding_mask": np.array(
+            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]
         ),
-        "decoder_token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "decoder_padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], shape=(1, 12)
+        "decoder_token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "decoder_padding_mask": np.array(
+            [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]
         ),
     }
 
     # Pretrained BART encoder.
     model = keras_nlp.models.BartBackbone.from_preset("bart_base_en")
     model(input_data)
 
@@ -136,20 +134,22 @@
         position_embedding = PositionEmbedding(
             initializer=bart_kernel_initializer(),
             sequence_length=max_sequence_length,
             name="encoder_position_embedding",
         )(token_embedding)
 
         # Sum, normalize and apply dropout to embeddings.
-        x = keras.layers.Add()((token_embedding, position_embedding))
+        x = keras.layers.Add(name="encoder_embeddings_add")(
+            (token_embedding, position_embedding)
+        )
         x = keras.layers.LayerNormalization(
             name="encoder_embeddings_layer_norm",
             axis=-1,
             epsilon=1e-5,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="encoder_embeddings_dropout",
         )(x)
 
         # Apply successive transformer encoder blocks.
@@ -176,20 +176,22 @@
         position_embedding = PositionEmbedding(
             initializer=bart_kernel_initializer(),
             sequence_length=max_sequence_length,
             name="decoder_position_embedding",
         )(token_embedding)
 
         # Sum, normalize and apply dropout to embeddings.
-        x = keras.layers.Add()((token_embedding, position_embedding))
+        x = keras.layers.Add(name="decoder_embeddings_add")(
+            (token_embedding, position_embedding)
+        )
         x = keras.layers.LayerNormalization(
             name="decoder_embeddings_layer_norm",
             axis=-1,
             epsilon=1e-5,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="decoder_embeddings_dropout",
         )(x)
 
         # Apply successive transformer decoder blocks.
@@ -200,15 +202,14 @@
                 dropout=dropout,
                 activation=lambda x: keras.activations.gelu(
                     x, approximate=False
                 ),
                 layer_norm_epsilon=1e-5,
                 kernel_initializer=bart_kernel_initializer(),
                 name=f"transformer_decoder_layer_{i}",
-                has_cross_attention=True,
             )
             x = transformer_decoder_layer(
                 decoder_sequence=x,
                 encoder_sequence=encoder_output,
                 decoder_padding_mask=decoder_padding_mask,
                 encoder_padding_mask=encoder_padding_mask,
             )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_backbone_test.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,108 +7,112 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Test for BART backbone models."""
+"""Test for Whisper backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.bart.bart_backbone import BartBackbone
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.whisper.whisper_backbone import WhisperBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BartBackboneTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class WhisperBackboneTest(TestCase):
     def setUp(self):
-        self.model = BartBackbone(
-            vocabulary_size=1000,
+        self.backbone = WhisperBackbone(
+            vocabulary_size=10,
             num_layers=2,
             num_heads=2,
-            hidden_dim=64,
-            intermediate_dim=128,
-            max_sequence_length=128,
+            hidden_dim=2,
+            intermediate_dim=4,
+            max_encoder_sequence_length=6,
+            max_decoder_sequence_length=6,
         )
-        self.batch_size = 8
         self.input_batch = {
-            "encoder_token_ids": tf.ones(
-                (self.batch_size, self.model.max_sequence_length), dtype="int32"
-            ),
-            "encoder_padding_mask": tf.ones(
-                (self.batch_size, self.model.max_sequence_length), dtype="int32"
-            ),
-            "decoder_token_ids": tf.ones(
-                (self.batch_size, self.model.max_sequence_length), dtype="int32"
-            ),
-            "decoder_padding_mask": tf.ones(
-                (self.batch_size, self.model.max_sequence_length), dtype="int32"
-            ),
+            "encoder_features": ops.ones((2, 5, 80), dtype="float32"),
+            "decoder_token_ids": ops.ones((2, 5), dtype="int32"),
+            "decoder_padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
-    def test_valid_call_bart(self):
-        self.model(self.input_batch)
+    def test_valid_call_whisper(self):
+        self.backbone(self.input_batch)
+
+    def test_token_embedding(self):
+        output = self.backbone.token_embedding(
+            self.input_batch["decoder_token_ids"]
+        )
+        self.assertEqual(output.shape, (2, 5, 2))
 
+    def test_name(self):
         # Check default name passed through
-        self.assertRegexpMatches(self.model.name, "bart_backbone")
+        self.assertRegexpMatches(self.backbone.name, "whisper_backbone")
 
-    def test_variable_sequence_length_call_bart(self):
-        for seq_length in (25, 50, 75):
+    def test_variable_sequence_length_call_whisper(self):
+        for seq_length in (2, 3, 4):
             input_data = {
-                "encoder_token_ids": tf.ones(
-                    (self.batch_size, seq_length), dtype="int32"
+                "encoder_features": ops.ones(
+                    (2, seq_length, 80), dtype="float32"
                 ),
-                "encoder_padding_mask": tf.ones(
-                    (self.batch_size, seq_length), dtype="int32"
-                ),
-                "decoder_token_ids": tf.ones(
-                    (self.batch_size, seq_length), dtype="int32"
-                ),
-                "decoder_padding_mask": tf.ones(
-                    (self.batch_size, seq_length), dtype="int32"
+                "decoder_token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "decoder_padding_mask": ops.ones(
+                    (2, seq_length), dtype="int32"
                 ),
             }
-            self.model(input_data)
+            self.backbone(input_data)
+
+    def test_predict(self):
+        self.backbone.predict(self.input_batch)
+        self.backbone.predict(self.input_dataset)
+
+    def test_serialization(self):
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
+        )
+        self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
+
+    def test_key_projection_bias_absence(self):
+        # Check only for the first encoder layer and first decoder layer.
+        self.assertIsNone(
+            self.backbone.get_layer(
+                "transformer_encoder_layer_0"
+            )._self_attention_layer._key_dense.bias
+        )
+        self.assertIsNone(
+            self.backbone.get_layer(
+                "transformer_decoder_layer_0"
+            )._self_attention_layer._key_dense.bias
+        )
+        self.assertIsNone(
+            self.backbone.get_layer(
+                "transformer_decoder_layer_0"
+            )._cross_attention_layer._key_dense.bias
+        )
 
-    @parameterized.named_parameters(
-        ("jit_compile_false", False), ("jit_compile_true", True)
-    )
-    def test_compile(self, jit_compile):
-        self.model.compile(jit_compile=jit_compile)
-        self.model.predict(self.input_batch)
-
-    @parameterized.named_parameters(
-        ("jit_compile_false", False), ("jit_compile_true", True)
-    )
-    def test_compile_batched_ds(self, jit_compile):
-        self.model.compile(jit_compile=jit_compile)
-        self.model.predict(self.input_dataset)
-
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
-        model_output = self.model(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.model.save(path, save_format=save_format, **kwargs)
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saved_model(self):
+        model_output = self.backbone(self.input_batch)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, BartBackbone)
+        self.assertIsInstance(restored_model, WhisperBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(
             model_output["encoder_sequence_output"],
             restored_output["encoder_sequence_output"],
         )
@@ -116,32 +120,45 @@
             model_output["decoder_sequence_output"],
             restored_output["decoder_sequence_output"],
         )
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class BartBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class WhisperBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
-            self.model = BartBackbone(
-                vocabulary_size=1000,
+            self.backbone = WhisperBackbone(
+                vocabulary_size=10,
                 num_layers=2,
                 num_heads=2,
-                hidden_dim=64,
-                intermediate_dim=128,
-                max_sequence_length=128,
+                hidden_dim=2,
+                intermediate_dim=4,
+                max_encoder_sequence_length=6,
+                max_decoder_sequence_length=6,
             )
+
         self.input_batch = {
-            "encoder_token_ids": tf.ones((8, 128), dtype="int32"),
-            "encoder_padding_mask": tf.ones((8, 128), dtype="int32"),
-            "decoder_token_ids": tf.ones((8, 128), dtype="int32"),
-            "decoder_padding_mask": tf.ones((8, 128), dtype="int32"),
+            "encoder_features": ops.ones(
+                (
+                    8,
+                    self.backbone.max_encoder_sequence_length,
+                    80,
+                ),
+                dtype="int32",
+            ),
+            "decoder_token_ids": ops.ones(
+                (8, self.backbone.max_decoder_sequence_length), dtype="int32"
+            ),
+            "decoder_padding_mask": ops.ones(
+                (8, self.backbone.max_decoder_sequence_length), dtype="int32"
+            ),
         }
+
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
-        self.model.compile()
-        self.model.predict(self.input_dataset)
+        self.backbone.compile()
+        self.backbone.predict(self.input_dataset)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_preprocessor.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,29 +11,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """BART preprocessor layer."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
 from keras_nlp.src.models.bart.bart_presets import backbone_presets
 from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 from keras_nlp.src.utils.python_utils import classproperty
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class BartPreprocessor(Preprocessor):
     """A BART preprocessing layer which tokenizes and packs inputs.
 
     This preprocessing layer will do three things:
 
      1. Tokenize both encoder inputs and decoder inputs using the `tokenizer`.
         Both inputs can contain only one segment.
@@ -42,24 +40,14 @@
         `"encoder_padding_mask"`, `"decoder_token_ids"`, `"decoder_padding_mask"`
         that can be passed directly to a BART model.
 
     Args:
         tokenizer: A `keras_nlp.models.BartTokenizer` instance.
         encoder_sequence_length: The length of the packed encoder inputs.
         decoder_sequence_length: The length of the packed decoder inputs.
-        truncate: string. The algorithm to truncate a list of batched segments
-            to fit within `sequence_length`. The value can be either
-            `round_robin` or `waterfall`:
-                - `"round_robin"`: Available space is assigned one token at a
-                    time in a round-robin fashion to the inputs that still need
-                    some, until the limit is reached.
-                - `"waterfall"`: The allocation of the budget is done using a
-                    "waterfall" algorithm that allocates quota in a
-                    left-to-right manner and fills up the buckets until we run
-                    out of budget. It supports an arbitrary number of segments.
 
     Call arguments:
         x: A dictionary with `encoder_text` and `decoder_text` as its keys.
             Each value in the dictionary should be a tensor of single string
             sequences. Inputs may be batched or unbatched. Raw python inputs
             will be converted to tensors.
         y: Any label data. Will be passed through unaltered.
@@ -145,48 +133,49 @@
     """
 
     def __init__(
         self,
         tokenizer,
         encoder_sequence_length=1024,
         decoder_sequence_length=1024,
-        truncate="round_robin",
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.tokenizer = tokenizer
 
-        self.encoder_packer = MultiSegmentPacker(
-            start_value=self.tokenizer.start_token_id,
-            end_value=self.tokenizer.end_token_id,
-            pad_value=self.tokenizer.pad_token_id,
-            truncate=truncate,
+        # TODO: Use `MultiSegmentPacker` instead of `StartEndPacker` once we
+        # want to move to multi-segment packing and have improved
+        # `MultiSegmentPacker`'s performance.
+        self.encoder_packer = StartEndPacker(
+            start_value=tokenizer.start_token_id,
+            end_value=tokenizer.end_token_id,
+            pad_value=tokenizer.pad_token_id,
             sequence_length=encoder_sequence_length,
+            return_padding_mask=True,
         )
+
         # The decoder is packed a bit differently; the format is as follows:
         # `[end_token_id, start_token_id, tokens..., end_token_id, padding...]`.
-        # Hence, we pass `sequence_length - 1` to the packer.
-        self.decoder_packer = MultiSegmentPacker(
-            start_value=self.tokenizer.start_token_id,
+        self.decoder_packer = StartEndPacker(
+            start_value=[
+                self.tokenizer.end_token_id,
+                self.tokenizer.start_token_id,
+            ],
             end_value=self.tokenizer.end_token_id,
             pad_value=self.tokenizer.pad_token_id,
-            truncate=truncate,
-            sequence_length=decoder_sequence_length - 1,
+            sequence_length=decoder_sequence_length,
+            return_padding_mask=True,
         )
-        # Maintain a private copy of `decoder_sequence_length` for config
-        # purposes.
-        self._decoder_sequence_length = decoder_sequence_length
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "encoder_sequence_length": self.encoder_packer.sequence_length,
-                "decoder_sequence_length": self._decoder_sequence_length,
-                "truncate": self.encoder_packer.truncate,
+                "decoder_sequence_length": self.decoder_packer.sequence_length,
             }
         )
         return config
 
     def call(self, x, y=None, sample_weight=None):
         if not (
             isinstance(x, dict)
@@ -206,39 +195,29 @@
         if len(encoder_text) > 1 or len(decoder_text) > 1:
             raise ValueError(
                 '`BARTPreprocessor` requires both `"encoder_text"` and '
                 f'`"decoder_text"` to contain only one segment, but received '
                 f"{len(encoder_text)} and {len(decoder_text)}, respectively."
             )
 
-        encoder_inputs = [self.tokenizer(segment) for segment in encoder_text]
-        encoder_token_ids, _ = self.encoder_packer(encoder_inputs)
-
-        decoder_inputs = [self.tokenizer(segment) for segment in decoder_text]
-        decoder_token_ids, _ = self.decoder_packer(decoder_inputs)
+        encoder_inputs = self.tokenizer(encoder_text[0])
+        encoder_token_ids, encoder_padding_mask = self.encoder_packer(
+            encoder_inputs
+        )
 
-        # Append `end_token_id` to the beginning of `decoder_token_ids`.
-        input_is_1d = decoder_token_ids.shape.rank == 1
-        if input_is_1d:
-            decoder_token_ids = decoder_token_ids[tf.newaxis, :]
-        batch_size = tf.shape(decoder_token_ids)[0]
-        end_token_ids = tf.fill((batch_size, 1), self.tokenizer.end_token_id)
-        decoder_token_ids = tf.concat(
-            [end_token_ids, decoder_token_ids], axis=1
+        decoder_inputs = self.tokenizer(decoder_text[0])
+        decoder_token_ids, decoder_padding_mask = self.decoder_packer(
+            decoder_inputs
         )
-        if input_is_1d:
-            decoder_token_ids = tf.squeeze(decoder_token_ids, axis=0)
 
         x = {
             "encoder_token_ids": encoder_token_ids,
-            "encoder_padding_mask": encoder_token_ids
-            != self.tokenizer.pad_token_id,
+            "encoder_padding_mask": encoder_padding_mask,
             "decoder_token_ids": decoder_token_ids,
-            "decoder_padding_mask": decoder_token_ids
-            != self.tokenizer.pad_token_id,
+            "decoder_padding_mask": decoder_padding_mask,
         }
 
         return pack_x_y_sample_weight(x, y, sample_weight)
 
     @classproperty
     def tokenizer_cls(cls):
         return BartTokenizer
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_preprocessor_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,27 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for BART preprocessor layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bart.bart_preprocessor import BartPreprocessor
 from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BartPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class BartPreprocessorTest(TestCase):
     def setUp(self):
         vocab = {
             "<s>": 0,
             "<pad>": 1,
             "</s>": 2,
             "air": 3,
             "plane": 4,
@@ -157,43 +156,42 @@
             ),
         }
 
         with self.assertRaises(ValueError):
             self.preprocessor(input_data)
 
     def test_serialization(self):
-        new_preprocessor = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.preprocessor)
         )
         self.assertEqual(
             new_preprocessor.get_config(), self.preprocessor.get_config()
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = {
-            "encoder_text": tf.constant(" airplane at airport"),
-            "decoder_text": tf.constant(" kohli is the best"),
+            "encoder_text": tf.constant([" airplane at airport"]),
+            "decoder_text": tf.constant([" kohli is the best"]),
         }
 
         inputs = {
-            "encoder_text": keras.Input(dtype="string", shape=()),
-            "decoder_text": keras.Input(dtype="string", shape=()),
+            "encoder_text": keras.Input(
+                dtype="string", name="encoder_text", shape=()
+            ),
+            "decoder_text": keras.Input(
+                dtype="string", name="decoder_text", shape=()
+            ),
         }
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs=inputs, outputs=outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
 
         model_output = model(input_data)
         restored_model_output = restored_model(input_data)
 
         self.assertAllClose(
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_presets.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,68 +7,64 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""BART model preset configurations."""
+"""FNet model preset configurations."""
 
 backbone_presets = {
-    "bart_base_en": {
+    "f_net_base_en": {
         "metadata": {
             "description": (
-                "6-layer BART model where case is maintained. "
-                "Trained on BookCorpus, English Wikipedia and CommonCrawl."
+                "12-layer FNet model where case is maintained. "
+                "Trained on the C4 dataset."
             ),
-            "params": 139417344,
-            "official_name": "BART",
-            "path": "bart",
-            "model_card": "https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md",
+            "params": 82861056,
+            "official_name": "FNet",
+            "path": "f_net",
+            "model_card": "https://github.com/google-research/google-research/blob/master/f_net/README.md",
         },
         "config": {
-            "vocabulary_size": 50265,
-            "num_layers": 6,
-            "num_heads": 12,
+            "vocabulary_size": 32000,
+            "num_layers": 12,
             "hidden_dim": 768,
             "intermediate_dim": 3072,
             "dropout": 0.1,
-            "max_sequence_length": 1024,
+            "max_sequence_length": 512,
+            "num_segments": 4,
         },
         "preprocessor_config": {},
-        "weights_url": "https://storage.googleapis.com/keras-nlp/models/bart_base_en/v1/model.h5",
-        "weights_hash": "5b59403f0cafafbd89680e0785791163",
-        "vocabulary_url": "https://storage.googleapis.com/keras-nlp/models/bart_base_en/v1/vocab.json",
-        "vocabulary_hash": "be4d3c6f3f5495426b2c03b334334354",
-        "merges_url": "https://storage.googleapis.com/keras-nlp/models/bart_base_en/v1/merges.txt",
-        "merges_hash": "75a37753dd7a28a2c5df80c28bf06e4e",
+        "weights_url": "https://storage.googleapis.com/keras-nlp/models/f_net_base_en/v1/model.h5",
+        "weights_hash": "35db90842b85a985a0e54c86c00746fe",
+        "spm_proto_url": "https://storage.googleapis.com/keras-nlp/models/f_net_base_en/v1/vocab.spm",
+        "spm_proto_hash": "71c5f4610bef1daf116998a113a01f3d",
     },
-    "bart_large_en": {
+    "f_net_large_en": {
         "metadata": {
             "description": (
-                "12-layer BART model where case is maintained. "
-                "Trained on BookCorpus, English Wikipedia and CommonCrawl."
+                "24-layer FNet model where case is maintained. "
+                "Trained on the C4 dataset."
             ),
-            "params": 406287360,
-            "official_name": "BART",
-            "path": "bart",
-            "model_card": "https://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md",
+            "params": 236945408,
+            "official_name": "FNet",
+            "path": "f_net",
+            "model_card": "https://github.com/google-research/google-research/blob/master/f_net/README.md",
         },
         "config": {
-            "vocabulary_size": 50265,
-            "num_layers": 12,
-            "num_heads": 16,
+            "vocabulary_size": 32000,
+            "num_layers": 24,
             "hidden_dim": 1024,
             "intermediate_dim": 4096,
             "dropout": 0.1,
-            "max_sequence_length": 1024,
+            "max_sequence_length": 512,
+            "num_segments": 4,
         },
         "preprocessor_config": {},
-        "weights_url": "https://storage.googleapis.com/keras-nlp/models/bart_large_en/v1/model.h5",
-        "weights_hash": "6bfe7e591af8c5699ce6f9f18753af9a",
-        "vocabulary_url": "https://storage.googleapis.com/keras-nlp/models/bart_large_en/v1/vocab.json",
-        "vocabulary_hash": "cf410ee085c5c69c957bb1f6d8456596",
-        "merges_url": "https://storage.googleapis.com/keras-nlp/models/bart_large_en/v1/merges.txt",
-        "merges_hash": "75a37753dd7a28a2c5df80c28bf06e4e",
+        "weights_url": "https://storage.googleapis.com/keras-nlp/models/f_net_large_en/v1/model.h5",
+        "weights_hash": "7ae4a3faa67ff054f8cecffb5619f779",
+        "spm_proto_url": "https://storage.googleapis.com/keras-nlp/models/f_net_large_en/v1/vocab.spm",
+        "spm_proto_hash": "71c5f4610bef1daf116998a113a01f3d",
     },
 }
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_presets_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,37 +9,39 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # Copyright 2023 The KerasNLP Authors
 #
+from keras_nlp.src.backend import ops
+from keras_nlp.src.tests.test_case import TestCase
+
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_nlp.src.models.bart.bart_backbone import BartBackbone
 from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
 
 
 @pytest.mark.large
-class BartPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class BartPresetSmokeTest(TestCase):
     """
     A smoke test for BART presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/bart/bart_presets_test.py --run_large`
     """
 
@@ -52,18 +54,18 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "encoder_token_ids": tf.constant([[0, 133, 2119, 2]]),
-            "encoder_padding_mask": tf.constant([[1, 1, 1, 1]]),
-            "decoder_token_ids": tf.constant([[0, 7199, 14, 2119, 2]]),
-            "decoder_padding_mask": tf.constant([[1, 1, 1, 1, 1]]),
+            "encoder_token_ids": ops.array([[0, 133, 2119, 2]]),
+            "encoder_padding_mask": ops.array([[1, 1, 1, 1]]),
+            "decoder_token_ids": ops.array([[0, 7199, 14, 2119, 2]]),
+            "decoder_padding_mask": ops.array([[1, 1, 1, 1, 1]]),
         }
         model = BartBackbone.from_preset(
             "bart_base_en", load_weights=load_weights
         )
         outputs = model(input_data)
         if load_weights:
             encoder_output = outputs["encoder_sequence_output"][0, 0, :5]
@@ -94,15 +96,15 @@
     def test_unknown_preset_error(self, cls):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("bart_base_en_clowntown")
 
 
 @pytest.mark.extra_large
-class BartPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class BartPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every BART preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/bart/bart_presets_test.py --run_extra_large`
     """
@@ -110,28 +112,28 @@
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_bart(self, load_weights):
         for preset in BartBackbone.presets:
             model = BartBackbone.from_preset(preset, load_weights=load_weights)
             input_data = {
-                "encoder_token_ids": tf.random.uniform(
+                "encoder_token_ids": ops.random.uniform(
                     shape=(1, 1024),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=model.vocabulary_size,
                 ),
-                "encoder_padding_mask": tf.constant(
+                "encoder_padding_mask": ops.array(
                     [1] * 768 + [0] * 256, shape=(1, 1024)
                 ),
-                "decoder_token_ids": tf.random.uniform(
+                "decoder_token_ids": ops.random.uniform(
                     shape=(1, 1024),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=model.vocabulary_size,
                 ),
-                "decoder_padding_mask": tf.constant(
+                "decoder_padding_mask": ops.array(
                     [1] * 489 + [0] * 535, shape=(1, 1024)
                 ),
             }
             model(input_data)
 
     def test_load_tokenizers(self):
         for preset in BartTokenizer.presets:
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor.py`

 * *Files 20% similar despite different names*

```diff
@@ -7,182 +7,181 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-"""BART Seq2Seq LM preprocessor layer."""
-
 from absl import logging
-from tensorflow import keras
 
-from keras_nlp.src.models.bart.bart_preprocessor import BartPreprocessor
+from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
+from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
-class BartSeq2SeqLMPreprocessor(BartPreprocessor):
-    """BART Seq2Seq LM preprocessor.
-
-    This layer is used as preprocessor for seq2seq tasks using the BART model.
-    This class subclasses `keras_nlp.models.BartPreprocessor` and keeps most of
-    its functionality. It has two changes from the superclass:
-
-     1. Sets the `y` (label) and `sample_weights` fields by shifting the
-        decoder input sequence one step towards the left. Both these fields are
-        inferred internally, and any passed values will be ignored.
-     2. Drops the last token from the decoder input sequence as it does not have
-        a successor.
+@keras_nlp_export("keras_nlp.models.FNetMaskedLMPreprocessor")
+class FNetMaskedLMPreprocessor(FNetPreprocessor):
+    """FNet preprocessing for the masked language modeling task.
+
+    This preprocessing layer will prepare inputs for a masked language modeling
+    task. It is primarily intended for use with the
+    `keras_nlp.models.FNetMaskedLM` task model. Preprocessing will occur in
+    multiple steps.
+
+    1. Tokenize any number of input segments using the `tokenizer`.
+    2. Pack the inputs together with the appropriate `"<s>"`, `"</s>"` and
+      `"<pad>"` tokens, i.e., adding a single `"<s>"` at the start of the
+      entire sequence, `"</s></s>"` between each segment,
+      and a `"</s>"` at the end of the entire sequence.
+    3. Randomly select non-special tokens to mask, controlled by
+      `mask_selection_rate`.
+    4. Construct a `(x, y, sample_weight)` tuple suitable for training with a
+      `keras_nlp.models.FNetMaskedLM` task model.
 
     Args:
-        tokenizer: A `keras_nlp.models.BartTokenizer` instance.
-        encoder_sequence_length: The length of the packed encoder inputs.
-        decoder_sequence_length: The length of the packed decoder inputs.
+        tokenizer: A `keras_nlp.models.FNetTokenizer` instance.
+        sequence_length: The length of the packed inputs.
+        mask_selection_rate: The probability an input token will be dynamically
+            masked.
+        mask_selection_length: The maximum number of masked tokens supported
+            by the layer.
+        mask_token_rate: float. `mask_token_rate` must be
+            between 0 and 1 which indicates how often the mask_token is
+            substituted for tokens selected for masking. Defaults to `0.8`.
+        random_token_rate: float. `random_token_rate` must be
+            between 0 and 1 which indicates how often a random token is
+            substituted for tokens selected for masking.
+            Note: mask_token_rate + random_token_rate <= 1,  and for
+            (1 - mask_token_rate - random_token_rate), the token will not be
+            changed. Defaults to `0.1`.
         truncate: string. The algorithm to truncate a list of batched segments
             to fit within `sequence_length`. The value can be either
             `round_robin` or `waterfall`:
                 - `"round_robin"`: Available space is assigned one token at a
                     time in a round-robin fashion to the inputs that still need
                     some, until the limit is reached.
                 - `"waterfall"`: The allocation of the budget is done using a
                     "waterfall" algorithm that allocates quota in a
                     left-to-right manner and fills up the buckets until we run
                     out of budget. It supports an arbitrary number of segments.
 
-    Call arguments:
-        x: A dictionary with `encoder_text` and `decoder_text` as its keys.
-            Each value in the dictionary should be a tensor of single string
-            sequences. Inputs may be batched or unbatched. Raw python inputs
-            will be converted to tensors.
-        y: Label data. Should always be `None` as the layer generates labels by
-            shifting the decoder input sequence one step to the left.
-        sample_weight: Label weights. Should always be `None` as the layer
-            generates label weights by shifting the padding mask one step to the
-            left.
-
     Examples:
 
-    Directly calling the layer on data
+    Directly calling the layer on data.
     ```python
-    preprocessor = keras_nlp.models.BartPreprocessor.from_preset("bart_base_en")
-
-    # Preprocess unbatched inputs.
-    inputs = {
-        "encoder_text": "The fox was sleeping.",
-        "decoder_text": "The fox was awake."
-    }
-    preprocessor(inputs)
-
-    # Preprocess batched inputs.
-    inputs = {
-        "encoder_text": ["The fox was sleeping.", "The lion was quiet."],
-        "decoder_text": ["The fox was awake.", "The lion was roaring."]
-    }
-    preprocessor(inputs)
-
-    # Custom vocabulary.
-    vocab = {
-        "<s>": 0,
-        "<pad>": 1,
-        "</s>": 2,
-        "after": 5,
-        "noon": 6,
-        "sun": 7,
-    }
-    merges = [" a", " s", " n", "e r", "n o", "o n", "s u", "a f", "no on"]
-    merges += ["su n", "af t", "aft er"]
-
-    tokenizer = keras_nlp.models.BartTokenizer(
-        vocabulary=vocab,
-        merges=merges,
-    )
-    preprocessor = keras_nlp.models.BartPreprocessor(
-        tokenizer=tokenizer,
-        encoder_sequence_length=20,
-        decoder_sequence_length=10,
+    # Load the preprocessor from a preset.
+    preprocessor = keras_nlp.models.FNetMaskedLMPreprocessor.from_preset(
+        "f_net_base_en"
     )
-    inputs = {
-        "encoder_text": "The fox was sleeping.",
-        "decoder_text": "The fox was awake."
-    }
-    preprocessor(inputs)
+
+    # Tokenize and mask a single sentence.
+    preprocessor("The quick brown fox jumped.")
+
+    # Tokenize and mask a batch of single sentences.
+    preprocessor(["The quick brown fox jumped.", "Call me Ishmael."])
+
+    # Tokenize and mask sentence pairs.
+    # In this case, always convert input to tensors before calling the layer.
+    first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
+    second = tf.constant(["The fox tripped.", "Oh look, a whale."])
+    preprocessor((first, second))
     ```
 
     Mapping with `tf.data.Dataset`.
     ```python
-    preprocessor = keras_nlp.models.BartPreprocessor.from_preset("bart_base_en")
+    preprocessor = keras_nlp.models.FNetMaskedLMPreprocessor.from_preset(
+        "f_net_base_en"
+    )
+
+    first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
+    second = tf.constant(["The fox tripped.", "Oh look, a whale."])
 
     # Map single sentences.
-    features = {
-        "encoder_text": tf.constant(
-            ["The fox was sleeping.", "The lion was quiet."]
-        ),
-        "decoder_text": tf.constant(
-            ["The fox was awake.", "The lion was roaring."]
-        )
-    }
-    ds = tf.data.Dataset.from_tensor_slices(features)
+    ds = tf.data.Dataset.from_tensor_slices(first)
     ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
+
+    # Alternatively, you can create a preprocessor from your own vocabulary.
+    vocab_data = tf.data.Dataset.from_tensor_slices(
+        ["the quick brown fox", "the earth is round"]
+    )
+
+    # Map sentence pairs.
+    ds = tf.data.Dataset.from_tensor_slices((first, second))
+    # Watch out for tf.data's default unpacking of tuples here!
+    # Best to invoke the `preprocessor` directly in this case.
+    ds = ds.map(
+        lambda first, second: preprocessor(x=(first, second)),
+        num_parallel_calls=tf.data.AUTOTUNE,
+    )
     ```
     """
 
     def __init__(
         self,
         tokenizer,
-        encoder_sequence_length,
-        decoder_sequence_length,
+        sequence_length=512,
         truncate="round_robin",
-        **kwargs
+        mask_selection_rate=0.15,
+        mask_selection_length=96,
+        mask_token_rate=0.8,
+        random_token_rate=0.1,
+        **kwargs,
     ):
-        # Since we truncate the last token from `decoder_token_ids`, we need to
-        # forcefully set the `decoder_sequence_length` to one greater than the
-        # value passed.
         super().__init__(
-            tokenizer=tokenizer,
-            encoder_sequence_length=encoder_sequence_length,
-            decoder_sequence_length=decoder_sequence_length + 1,
+            tokenizer,
+            sequence_length=sequence_length,
             truncate=truncate,
-            **kwargs
+            **kwargs,
         )
 
-        # Maintain a private copy of `decoder_sequence_length` for config
-        # purposes.
-        self._decoder_sequence_length = decoder_sequence_length
+        self.masker = MaskedLMMaskGenerator(
+            mask_selection_rate=mask_selection_rate,
+            mask_selection_length=mask_selection_length,
+            mask_token_rate=mask_token_rate,
+            random_token_rate=random_token_rate,
+            vocabulary_size=tokenizer.vocabulary_size(),
+            mask_token_id=tokenizer.mask_token_id,
+            unselectable_token_ids=[
+                tokenizer.cls_token_id,
+                tokenizer.sep_token_id,
+                tokenizer.pad_token_id,
+            ],
+        )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
-                "encoder_sequence_length": self.encoder_packer.sequence_length,
-                "decoder_sequence_length": self._decoder_sequence_length,
-                "truncate": self.encoder_packer.truncate,
+                "mask_selection_rate": self.masker.mask_selection_rate,
+                "mask_selection_length": self.masker.mask_selection_length,
+                "mask_token_rate": self.masker.mask_token_rate,
+                "random_token_rate": self.masker.random_token_rate,
             }
         )
         return config
 
     def call(self, x, y=None, sample_weight=None):
         if y is not None or sample_weight is not None:
             logging.warning(
-                "`BartSeq2SeqLMPreprocessor` infers `y` and `sample_weight` "
-                "from the provided input data, i.e., `x`. However, non-`None`"
-                "values have been passed for `y` or `sample_weight` or both. "
-                "These values will be ignored."
+                f"{self.__class__.__name__} generates `y` and `sample_weight` "
+                "based on your input data, but your data already contains `y` "
+                "or `sample_weight`. Your `y` and `sample_weight` will be "
+                "ignored."
             )
-
         x = super().call(x)
-        decoder_token_ids = x.pop("decoder_token_ids")
-        decoder_padding_mask = x.pop("decoder_padding_mask")
-
-        # The last token does not have a next token. Hence, we truncate it.
+        token_ids, segment_ids = (
+            x["token_ids"],
+            x["segment_ids"],
+        )
+        masker_outputs = self.masker(token_ids)
         x = {
-            **x,
-            "decoder_token_ids": decoder_token_ids[..., :-1],
-            "decoder_padding_mask": decoder_padding_mask[..., :-1],
+            "token_ids": masker_outputs["token_ids"],
+            "segment_ids": segment_ids,
+            "mask_positions": masker_outputs["mask_positions"],
         }
-        # Target `y` will be the decoder input sequence shifted one step to the
-        # left (i.e., the next token).
-        y = decoder_token_ids[..., 1:]
-        sample_weight = decoder_padding_mask[..., 1:]
+        y = masker_outputs["mask_ids"]
+        sample_weight = masker_outputs["mask_weights"]
         return pack_x_y_sample_weight(x, y, sample_weight)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_preprocessor_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,156 +7,121 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-"""Tests for BART preprocessor layer."""
+"""Tests for DistilBERT preprocessor layer."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.bart.bart_seq_2_seq_lm_preprocessor import (
-    BartSeq2SeqLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
+    DistilBertPreprocessor,
+)
+from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
+    DistilBertTokenizer,
 )
-from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BartSeq2SeqLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class DistilBertPreprocessorTest(TestCase):
     def setUp(self):
-        vocab = {
-            "<s>": 0,
-            "<pad>": 1,
-            "</s>": 2,
-            "air": 3,
-            "plane": 4,
-            "at": 5,
-            "port": 6,
-            "koh": 7,
-            "li": 8,
-            "is": 9,
-            "the": 10,
-            "best": 11,
-            "<mask>": 12,
-        }
-
-        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
-        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
-        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
-        merges += ["pla ne"]
-
-        self.preprocessor = BartSeq2SeqLMPreprocessor(
-            tokenizer=BartTokenizer(
-                vocabulary=vocab,
-                merges=merges,
-            ),
-            encoder_sequence_length=10,
-            decoder_sequence_length=9,
+        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
+        self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
+        self.vocab += ["the", "quick", "brown", "fox"]
+        self.preprocessor = DistilBertPreprocessor(
+            DistilBertTokenizer(vocabulary=self.vocab),
+            sequence_length=8,
         )
 
     def test_tokenize_strings(self):
-        input_data = {
-            "encoder_text": " airplane at airport",
-            "decoder_text": " kohli is the best",
-        }
-
-        x_out, y_out, sw_out = self.preprocessor(input_data)
-        self.assertAllEqual(
-            x_out["encoder_token_ids"], [0, 3, 4, 5, 3, 6, 2, 1, 1, 1]
-        )
-        self.assertAllEqual(
-            x_out["encoder_padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]
-        )
-        self.assertAllEqual(
-            x_out["decoder_token_ids"], [2, 0, 7, 8, 9, 10, 11, 2, 1]
-        )
-        self.assertAllEqual(
-            x_out["decoder_padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1, 0]
-        )
-        self.assertAllEqual(y_out, [0, 7, 8, 9, 10, 11, 2, 1, 1])
-        self.assertAllEqual(sw_out, [1, 1, 1, 1, 1, 1, 1, 0, 0])
+        input_data = "THE QUICK BROWN FOX."
+        output = self.preprocessor(input_data)
+        self.assertAllEqual(output["token_ids"], [2, 5, 6, 7, 8, 1, 3, 0])
+        self.assertAllEqual(output["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
 
     def test_tokenize_list_of_strings(self):
-        input_data = {
-            "encoder_text": [" airplane at airport"] * 4,
-            "decoder_text": [" kohli is the best"] * 4,
-        }
-
-        x_out, y_out, sw_out = self.preprocessor(input_data)
+        # We should handle a list of strings as as batch.
+        input_data = ["THE QUICK BROWN FOX."] * 4
+        output = self.preprocessor(input_data)
+        self.assertAllEqual(output["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
+        self.assertAllEqual(
+            output["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
+        )
+
+    def test_tokenize_labeled_batch(self):
+        x = tf.constant(["THE QUICK BROWN FOX."] * 4)
+        y = tf.constant([1] * 4)
+        sw = tf.constant([1.0] * 4)
+        x_out, y_out, sw_out = self.preprocessor(x, y, sw)
+        self.assertAllEqual(x_out["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
+        self.assertAllEqual(
+            x_out["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
+        )
+        self.assertAllEqual(y_out, y)
+        self.assertAllEqual(sw_out, sw)
+
+    def test_tokenize_labeled_dataset(self):
+        x = tf.constant(["THE QUICK BROWN FOX."] * 4)
+        y = tf.constant([1] * 4)
+        sw = tf.constant([1.0] * 4)
+        ds = tf.data.Dataset.from_tensor_slices((x, y, sw))
+        ds = ds.map(self.preprocessor)
+        x_out, y_out, sw_out = ds.batch(4).take(1).get_single_element()
+        self.assertAllEqual(x_out["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
+        self.assertAllEqual(
+            x_out["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
+        )
+        self.assertAllEqual(y_out, y)
+        self.assertAllEqual(sw_out, sw)
+
+    def test_tokenize_multiple_sentences(self):
+        sentence_one = tf.constant("THE QUICK")
+        sentence_two = tf.constant("BROWN FOX.")
+        output = self.preprocessor((sentence_one, sentence_two))
+        self.assertAllEqual(output["token_ids"], [2, 5, 6, 3, 7, 8, 1, 3])
+        self.assertAllEqual(output["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1])
+
+    def test_tokenize_multiple_batched_sentences(self):
+        sentence_one = tf.constant(["THE QUICK"] * 4)
+        sentence_two = tf.constant(["BROWN FOX."] * 4)
+        # The first tuple or list is always interpreted as an enumeration of
+        # separate sequences to concatenate.
+        output = self.preprocessor((sentence_one, sentence_two))
+        self.assertAllEqual(output["token_ids"], [[2, 5, 6, 3, 7, 8, 1, 3]] * 4)
         self.assertAllEqual(
-            x_out["encoder_token_ids"], [[0, 3, 4, 5, 3, 6, 2, 1, 1, 1]] * 4
+            output["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 1]] * 4
         )
-        self.assertAllEqual(
-            x_out["encoder_padding_mask"],
-            [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]] * 4,
-        )
-        self.assertAllEqual(
-            x_out["decoder_token_ids"], [[2, 0, 7, 8, 9, 10, 11, 2, 1]] * 4
-        )
-        self.assertAllEqual(
-            x_out["decoder_padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 1, 0]] * 4
-        )
-        self.assertAllEqual(y_out, [[0, 7, 8, 9, 10, 11, 2, 1, 1]] * 4)
-        self.assertAllEqual(sw_out, [[1, 1, 1, 1, 1, 1, 1, 0, 0]] * 4)
-
-    def test_error_multi_segment_input(self):
-        input_data = {
-            "encoder_text": (
-                tf.constant([" airplane at airport"] * 2),
-                tf.constant([" airplane"] * 2),
-            ),
-            "decoder_text": (
-                tf.constant([" kohli is the best"] * 2),
-                tf.constant([" kohli"] * 2),
-            ),
-        }
 
+    def test_errors_for_2d_list_input(self):
+        ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
-            self.preprocessor(input_data)
+            self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        new_preprocessor = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.preprocessor)
-        )
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
-            new_preprocessor.get_config(), self.preprocessor.get_config()
+            new_preprocessor.get_config(),
+            self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
-        input_data = {
-            "encoder_text": tf.constant(" airplane at airport"),
-            "decoder_text": tf.constant(" kohli is the best"),
-        }
-
-        inputs = {
-            "encoder_text": keras.Input(dtype="string", shape=()),
-            "decoder_text": keras.Input(dtype="string", shape=()),
-        }
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant(["THE QUICK BROWN FOX."])
+        inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
-        model = keras.Model(inputs=inputs, outputs=outputs)
-
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
-
+        model = keras.Model(inputs, outputs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
-
-        model_output = model(input_data)
-        restored_model_output = restored_model(input_data)
-
-        self.assertAllClose(
-            model_output,
-            restored_model_output,
+        self.assertAllEqual(
+            model(input_data)["token_ids"],
+            restored_model(input_data)["token_ids"],
         )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_tokenizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,22 +12,21 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """BART tokenizer."""
 
 import copy
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bart.bart_presets import backbone_presets
 from keras_nlp.src.tokenizers.byte_pair_tokenizer import BytePairTokenizer
 from keras_nlp.src.utils.python_utils import classproperty
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class BartTokenizer(BytePairTokenizer):
     """A BART tokenizer using Byte-Pair Encoding subword segmentation.
 
     This tokenizer class will tokenize raw strings into integer sequences and
     is based on `keras_nlp.tokenizers.BytePairTokenizer`. Unlike the
     underlying tokenizer, it will check for all special tokens needed by BART
     models and provides a `from_preset()` method to automatically download
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bart/bart_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_tokenizer_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,25 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for BART tokenizer."""
-
 import os
 
+import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bart.bart_tokenizer import BartTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BartTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class BartTokenizerTest(TestCase):
     def setUp(self):
         vocab = {
             "<s>": 0,
             "<pad>": 1,
             "</s>": 2,
             "air": 3,
             "plane": 4,
@@ -54,15 +54,15 @@
 
     def test_tokenize_special_tokens(self):
         input_data = "<s> airplane at airport</s><pad>"
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [0, 3, 4, 5, 3, 6, 0, 1])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant([" airplane at airport", " kohli is the best"])
+        input_data = [" airplane at airport", " kohli is the best"]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[3, 4, 5, 3, 6], [7, 8, 9, 10, 11]])
 
     def test_detokenize(self):
         input_tokens = [[3, 4, 5, 3, 6]]
         output = self.tokenizer.detokenize(input_tokens)
         self.assertAllEqual(output, [" airplane at airport"])
@@ -70,29 +70,33 @@
     def test_vocabulary_size(self):
         self.assertEqual(self.tokenizer.vocabulary_size(), 12)
 
     def test_errors_missing_special_tokens(self):
         with self.assertRaises(ValueError):
             BartTokenizer(vocabulary=["a", "b", "c"], merges=[])
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_serialization(self):
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(
+            new_tokenizer.get_config(),
+            self.tokenizer.get_config(),
+        )
+
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/layers/preprocessing/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_backbone.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """BERT backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.bert.bert_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def bert_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
@@ -63,21 +61,17 @@
             embeddings.
         num_segments: int. The number of types that the 'segment_ids' input can
             take.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]]),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained BERT encoder.
     model = keras_nlp.models.BertBackbone.from_preset("bert_base_en_uncased")
     model(input_data)
 
     # Randomly initialized BERT encoder with a custom config.
@@ -142,15 +136,15 @@
         x = keras.layers.Add()(
             (token_embedding, position_embedding, segment_embedding)
         )
         x = keras.layers.LayerNormalization(
             name="embeddings_layer_norm",
             axis=-1,
             epsilon=1e-12,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="embeddings_dropout",
         )(x)
 
         # Apply successive transformer encoder blocks.
@@ -166,20 +160,21 @@
                 kernel_initializer=bert_kernel_initializer(),
                 name=f"transformer_layer_{i}",
             )(x, padding_mask=padding_mask)
 
         # Construct the two BERT outputs. The pooled output is a dense layer on
         # top of the [CLS] token.
         sequence_output = x
-        pooled_output = keras.layers.Dense(
+        x = keras.layers.Dense(
             hidden_dim,
             kernel_initializer=bert_kernel_initializer(),
             activation="tanh",
             name="pooled_dense",
-        )(x[:, cls_token_index, :])
+        )(x)
+        pooled_output = x[:, cls_token_index, :]
 
         # Instantiate using Functional API Model constructor
         super().__init__(
             inputs={
                 "token_ids": token_id_input,
                 "segment_ids": segment_id_input,
                 "padding_mask": padding_mask,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,115 +7,109 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Test for BERT backbone models."""
 
+"""Tests for XLM-RoBERTa backbone models."""
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.bert.bert_backbone import BertBackbone
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertBackboneTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaBackboneTest(TestCase):
     def setUp(self):
-        self.backbone = BertBackbone(
+        self.backbone = XLMRobertaBackbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
         )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "segment_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
-    def test_valid_call_bert(self):
+    def test_valid_call_xlm_roberta(self):
         self.backbone(self.input_batch)
 
     def test_token_embedding(self):
         output = self.backbone.token_embedding(self.input_batch["token_ids"])
         self.assertEqual(output.shape, (2, 5, 2))
 
     def test_name(self):
         # Check default name passed through
-        self.assertRegexpMatches(self.backbone.name, "bert_backbone")
+        self.assertRegexpMatches(self.backbone.name, "xlm_roberta_backbone")
 
-    def test_variable_sequence_length_call_bert(self):
+    def test_variable_sequence_length_call_xlm_roberta(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "segment_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
-            self.backbone(input_data)
+            output = self.backbone(input_data)
+            self.assertAllEqual(
+                ops.shape(output),
+                (2, seq_length, self.backbone.hidden_dim),
+            )
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, BertBackbone)
+        self.assertIsInstance(restored_model, XLMRobertaBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(
-            model_output["pooled_output"], restored_output["pooled_output"]
-        )
+        self.assertAllClose(model_output, restored_output)
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class BertBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
-            self.backbone = BertBackbone(
+            self.backbone = XLMRobertaBackbone(
                 vocabulary_size=1000,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=64,
                 intermediate_dim=128,
                 max_sequence_length=128,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "segment_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((8, 128), dtype="int32"),
+            "padding_mask": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_classifier.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,24 +11,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """BERT classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bert.bert_backbone import BertBackbone
 from keras_nlp.src.models.bert.bert_backbone import bert_kernel_initializer
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
 from keras_nlp.src.models.bert.bert_presets import backbone_presets
 from keras_nlp.src.models.bert.bert_presets import classifier_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.BertClassifier")
 class BertClassifier(Task):
     """An end-to-end BERT model for classification tasks.
 
@@ -47,17 +45,18 @@
 
     Args:
         backbone: A `keras_nlp.models.BertBackbone` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.BertPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
+        activation: Optional `str` or callable. The
             activation function to use on the model outputs. Set
             `activation="softmax"` to return output probabilities.
+            Defaults to `None`.
         dropout: float. The dropout probability value, applied after the dense
             layer.
 
     Examples:
 
     Raw string data.
     ```python
@@ -83,21 +82,17 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.BertClassifier.from_preset(
         "bert_base_en_uncased",
         num_classes=4,
@@ -166,21 +161,22 @@
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.dropout = dropout
 
         # Default compilation
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_classifier_test.py`

 * *Files 13% similar despite different names*

```diff
@@ -13,24 +13,25 @@
 # limitations under the License.
 """Tests for BERT classification model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.bert.bert_backbone import BertBackbone
 from keras_nlp.src.models.bert.bert_classifier import BertClassifier
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
 from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertClassifierTest(tf.test.TestCase, parameterized.TestCase):
+class BertClassifierTest(TestCase):
     def setUp(self):
         # Setup model.
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["the", "quick", "brown", "fox", "."]
         self.preprocessor = BertPreprocessor(
             BertTokenizer(vocabulary=self.vocab),
             sequence_length=5,
@@ -48,72 +49,79 @@
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
         )
 
         # Setup data.
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
     def test_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
+            optimizer="adam",
             loss="sparse_categorical_crossentropy",
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = BertClassifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = BertClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            name="test",
+            trainable=False,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large
+    def test_saving_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
-        # Check we got the real object back.
+        # Check we got the real object back
         self.assertIsInstance(restored_model, BertClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,26 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """BERT masked LM model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.bert.bert_backbone import BertBackbone
 from keras_nlp.src.models.bert.bert_backbone import bert_kernel_initializer
 from keras_nlp.src.models.bert.bert_masked_lm_preprocessor import (
     BertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.bert.bert_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.BertMaskedLM")
 class BertMaskedLM(Task):
     """An end-to-end BERT model for the masked language modeling task.
 
@@ -78,22 +76,18 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create preprocessed batch where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2)),
-        "segment_ids": tf.constant([[0, 0, 0, 0, 0, 0, 0, 0]] * 2, shape=(2, 8))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 0, 0, 0]] * 2)
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.BertMaskedLM.from_preset(
         "bert_base_en_uncased",
         preprocessor=None,
@@ -132,16 +126,16 @@
         )
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return BertBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_preprocessor.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 """BERT masked language model preprocessor layer."""
 
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
 @keras_nlp_export("keras_nlp.models.BertMaskedLMPreprocessor")
 class BertMaskedLMPreprocessor(BertPreprocessor):
     """BERT preprocessing for the masked language modeling task.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_preprocessor_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,24 +13,24 @@
 # limitations under the License.
 """Tests for BERT masked language model preprocessor layer."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bert.bert_masked_lm_preprocessor import (
     BertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertMaskedLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class BertMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
         self.vocab += ["the", "quick", "brown", "fox"]
 
         tokenizer = BertTokenizer(vocabulary=self.vocab)
 
@@ -124,36 +124,31 @@
             [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
         )
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -7,78 +7,89 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for BERT masked language model."""
-
+import io
 import os
 
 import pytest
+import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.bert.bert_backbone import BertBackbone
-from keras_nlp.src.models.bert.bert_masked_lm import BertMaskedLM
-from keras_nlp.src.models.bert.bert_masked_lm_preprocessor import (
-    BertMaskedLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
+from keras_nlp.src.models.f_net.f_net_masked_lm import FNetMaskedLM
+from keras_nlp.src.models.f_net.f_net_masked_lm_preprocessor import (
+    FNetMaskedLMPreprocessor,
 )
-from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class FNetMaskedLMTest(TestCase):
     def setUp(self):
-        # Setup model.
-        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
-        self.vocab += ["the", "quick", "brown", "fox", "."]
-        self.preprocessor = BertMaskedLMPreprocessor(
-            BertTokenizer(vocabulary=self.vocab),
-            # Simplify out testing by masking every available token.
-            mask_selection_rate=1.0,
-            mask_token_rate=1.0,
-            random_token_rate=0.0,
-            mask_selection_length=2,
+        # Setup Model.
+        bytes_io = io.BytesIO()
+        vocab_data = tf.data.Dataset.from_tensor_slices(
+            ["the quick brown fox", "the slow brown fox"]
+        )
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=vocab_data.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=5,
+            model_type="WORD",
+            pad_id=0,
+            bos_id=1,
+            eos_id=2,
+            unk_id=3,
+            pad_piece="<pad>",
+            unk_piece="<unk>",
+            bos_piece="[CLS]",
+            eos_piece="[SEP]",
+            user_defined_symbols="[MASK]",
+        )
+        self.proto = bytes_io.getvalue()
+        self.preprocessor = FNetMaskedLMPreprocessor(
+            FNetTokenizer(proto=self.proto),
             sequence_length=5,
+            mask_selection_length=2,
         )
-        self.backbone = BertBackbone(
+        self.backbone = FNetBackbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
-            num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.masked_lm = BertMaskedLM(
+        self.masked_lm = FNetMaskedLM(
             self.backbone,
             preprocessor=self.preprocessor,
         )
 
-        # Setup data.
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
-        self.preprocessed_batch = self.preprocessor(self.raw_batch)
+        self.raw_batch = [
+            "the quick brown fox",
+            "the slow brown fox",
+        ]
+        self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
-        self.masked_lm(self.preprocessed_batch[0])
+        self.masked_lm(self.preprocessed_batch)
 
     def test_classifier_predict(self):
-        self.masked_lm.predict(self.raw_batch)
+        # self.masked_lm.predict(self.raw_batch)
         self.masked_lm.preprocessor = None
-        self.masked_lm.predict(self.preprocessed_batch[0])
+        self.masked_lm.predict(self.preprocessed_batch)
 
     def test_classifier_fit(self):
         self.masked_lm.fit(self.raw_dataset)
         self.masked_lm.preprocessor = None
         self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
@@ -86,34 +97,28 @@
         self.masked_lm.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.masked_lm)
-        new_classifier = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.masked_lm)
+        new_classifier = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_classifier.get_config(),
             self.masked_lm.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large
+    def test_saved_model(self):
         model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, BertMaskedLM)
+        self.assertIsInstance(restored_model, FNetMaskedLM)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output, atol=0.01, rtol=0.01)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_preprocessor.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """BERT preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
 from keras_nlp.src.models.bert.bert_presets import backbone_presets
 from keras_nlp.src.models.bert.bert_presets import classifier_presets
 from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_preprocessor_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,22 +13,22 @@
 # limitations under the License.
 """Tests for BERT preprocessor layer."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
 from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class BertPreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
         self.vocab += ["the", "quick", "brown", "fox"]
         self.preprocessor = BertPreprocessor(
             BertTokenizer(vocabulary=self.vocab),
             sequence_length=8,
@@ -109,34 +109,29 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["THE QUICK BROWN FOX."])
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_presets_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,25 +10,26 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.bert.bert_backbone import BertBackbone
 from keras_nlp.src.models.bert.bert_classifier import BertClassifier
 from keras_nlp.src.models.bert.bert_preprocessor import BertPreprocessor
 from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class BertPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class BertPresetSmokeTest(TestCase):
     """
     A smoke test for BERT presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/bert/bert_presets_test.py --run_large`
     """
 
@@ -50,17 +51,17 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "segment_ids": tf.constant([[0, 0, 0, 0]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "segment_ids": ops.array([[0, 0, 0, 0]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = BertBackbone.from_preset(
             "bert_tiny_en_uncased", load_weights=load_weights
         )
         outputs = model(input_data)["sequence_output"]
         if load_weights:
             # The forward pass from a preset should be stable!
@@ -73,31 +74,31 @@
             # Keep a high tolerance, so we are robust to different hardware.
             self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = BertClassifier.from_preset(
             "bert_tiny_en_uncased",
             num_classes=2,
             load_weights=load_weights,
         )
         # We don't assert output values, as the head weights are random.
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "segment_ids": tf.constant([[0, 0, 0, 0]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "segment_ids": ops.array([[0, 0, 0, 0]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = BertClassifier.from_preset(
             "bert_tiny_en_uncased",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -166,15 +167,15 @@
             BertPreprocessor.from_preset(
                 "bert_base_en_uncased",
                 sequence_length=1024,
             )
 
 
 @pytest.mark.extra_large
-class BertPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class BertPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This every presets for BERT and is only run manually.
     Run with:
     `pytest keras_nlp/models/bert/bert_presets_test.py --run_extra_large`
     """
@@ -182,58 +183,54 @@
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_bert(self, load_weights):
         for preset in BertBackbone.presets:
             model = BertBackbone.from_preset(preset, load_weights=load_weights)
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
-                ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_bert_classifier(self, load_weights):
         for preset in BertClassifier.presets:
             classifier = BertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["This quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_bert_classifier_without_preprocessing(self, load_weights):
         for preset in BertClassifier.presets:
             classifier = BertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 preprocessor=None,
                 load_weights=load_weights,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
-                ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in BertTokenizer.presets:
             tokenizer = BertTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_tokenizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     `tf.Tensor` with static shape `[None]`.
 
     Args:
         vocabulary: A list of strings or a string filename path. If
             passing a list, each element of the list should be a single word
             piece token string. If passing a filename, the file should be a
             plain text file containing a single word piece token per line.
-        lowercase: If true, the input text will be first lowered before
+        lowercase: If `True`, the input text will be first lowered before
             tokenization.
 
     Examples:
     ```python
     # Unbatched input.
     tokenizer = keras_nlp.models.BertTokenizer.from_preset(
         "bert_base_en_uncased",
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/bert/bert_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_tokenizer_test.py`

 * *Files 19% similar despite different names*

```diff
@@ -7,83 +7,105 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for BERT tokenizer."""
 
+"""Tests for FNet tokenizer."""
+import io
 import os
 
 import pytest
+import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BertTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class FNetTokenizerTest(TestCase):
     def setUp(self):
-        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
-        self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
-        self.vocab += ["the", "quick", "brown", "fox"]
-        self.tokenizer = BertTokenizer(vocabulary=self.vocab)
+        bytes_io = io.BytesIO()
+        vocab_data = tf.data.Dataset.from_tensor_slices(
+            ["the quick brown fox", "the earth is round"]
+        )
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=vocab_data.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=12,
+            model_type="WORD",
+            pad_id=3,
+            unk_id=0,
+            bos_id=4,
+            eos_id=5,
+            pad_piece="<pad>",
+            unk_piece="<unk>",
+            bos_piece="[CLS]",
+            eos_piece="[SEP]",
+            user_defined_symbols="[MASK]",
+        )
+        self.proto = bytes_io.getvalue()
+
+        self.tokenizer = FNetTokenizer(proto=self.proto)
 
     def test_tokenize(self):
-        input_data = "THE QUICK BROWN FOX."
+        input_data = "the quick brown fox"
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [5, 6, 7, 8, 1])
+        self.assertAllEqual(output, [2, 10, 6, 8])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant(["THE QUICK BROWN FOX.", "THE FOX."])
+        input_data = ["the quick brown fox", "the earth is round"]
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [[5, 6, 7, 8, 1], [5, 8, 1]])
-
-    def test_lowercase(self):
-        input_data = "THE QUICK BROWN FOX."
-        tokenizer = BertTokenizer(vocabulary=self.vocab, lowercase=True)
-        output = tokenizer(input_data)
-        self.assertAllEqual(output, [9, 10, 11, 12, 1])
+        self.assertAllEqual(output, [[2, 10, 6, 8], [2, 7, 9, 11]])
 
     def test_detokenize(self):
-        input_tokens = [[5, 6, 7, 8]]
-        output = self.tokenizer.detokenize(input_tokens)
-        self.assertAllEqual(output, ["THE QUICK BROWN FOX"])
+        input_data = [[2, 10, 6, 8]]
+        output = self.tokenizer.detokenize(input_data)
+        self.assertEqual(output, ["the quick brown fox"])
 
     def test_vocabulary_size(self):
-        self.assertEqual(self.tokenizer.vocabulary_size(), 13)
+        tokenizer = FNetTokenizer(proto=self.proto)
+        self.assertEqual(tokenizer.vocabulary_size(), 12)
 
     def test_errors_missing_special_tokens(self):
+        bytes_io = io.BytesIO()
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=iter(["abc"]),
+            model_writer=bytes_io,
+            vocab_size=5,
+            pad_id=-1,
+            eos_id=-1,
+            bos_id=-1,
+        )
         with self.assertRaises(ValueError):
-            BertTokenizer(vocabulary=["a", "b", "c"])
+            FNetTokenizer(proto=bytes_io.getvalue())
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant(["THE QUICK BROWN FOX."])
-        tokenizer = BertTokenizer(vocabulary=self.vocab)
+    @pytest.mark.large
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant(["the quick brown fox"])
+
         inputs = keras.Input(dtype="string", shape=())
-        outputs = tokenizer(inputs)
+        outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
+
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_backbone.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,25 +12,24 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """DeBERTa backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.deberta_v3.deberta_v3_presets import backbone_presets
 from keras_nlp.src.models.deberta_v3.disentangled_attention_encoder import (
     DisentangledAttentionEncoder,
 )
 from keras_nlp.src.models.deberta_v3.relative_embedding import RelativeEmbedding
 from keras_nlp.src.utils.python_utils import classproperty
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
 def deberta_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
 
 
 @keras_nlp_export("keras_nlp.models.DebertaV3Backbone")
@@ -70,17 +69,16 @@
             `max_sequence_length`.
         bucket_size: int. The size of the relative position buckets. Generally
             equal to `max_sequence_length // 2`.
 
     Example usage:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained DeBERTa encoder.
     model = keras_nlp.models.DebertaV3Backbone.from_preset(
         "deberta_v3_base_en",
     )
     model(input_data)
@@ -108,14 +106,16 @@
         hidden_dim,
         intermediate_dim,
         dropout=0.1,
         max_sequence_length=512,
         bucket_size=256,
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         # Inputs
         token_id_input = keras.Input(
             shape=(None,), dtype="int32", name="token_ids"
         )
         padding_mask = keras.Input(
             shape=(None,), dtype="int32", name="padding_mask"
         )
@@ -127,15 +127,15 @@
             embeddings_initializer=deberta_kernel_initializer(),
             name="token_embedding",
         )(token_id_input)
 
         # Normalize and apply dropout to embeddings.
         x = keras.layers.LayerNormalization(
             epsilon=1e-7,
-            dtype=tf.float32,
+            dtype="float32",
             name="embeddings_layer_norm",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="embeddings_dropout",
         )(x)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_backbone_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,35 +13,37 @@
 # limitations under the License.
 """Test for DeBERTa backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3BackboneTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3BackboneTest(TestCase):
     def setUp(self):
         self.backbone = DebertaV3Backbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
             bucket_size=2,
         )
         self.batch_size = 8
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_valid_call_deberta(self):
@@ -53,71 +55,66 @@
     def test_token_embedding(self):
         output = self.backbone.token_embedding(self.input_batch["token_ids"])
         self.assertEqual(output.shape, (2, 5, 2))
 
     def test_variable_sequence_length_call_deberta(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
             output = self.backbone(input_data)
             self.assertAllEqual(
                 tf.shape(output),
                 [2, seq_length, self.backbone.hidden_dim],
             )
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, DebertaV3Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class DebertaV3BackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3BackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = DebertaV3Backbone(
                 vocabulary_size=10,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=2,
                 intermediate_dim=4,
                 max_sequence_length=5,
                 bucket_size=2,
             )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_classifier.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,27 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DeBERTa classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import (
     deberta_kernel_initializer,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
     DebertaV3Preprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.DebertaV3Classifier")
 class DebertaV3Classifier(Task):
     """An end-to-end DeBERTa model for classification tasks.
 
@@ -55,17 +53,18 @@
 
     Args:
         backbone: A `keras_nlp.models.DebertaV3` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.DebertaV3Preprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
+        activation: Optional `str` or callable. The
             activation function to use on the model outputs. Set
             `activation="softmax"` to return output probabilities.
+            Defaults to `None`.
         hidden_dim: int. The size of the pooler layer.
         dropout: float. Dropout probability applied to the pooled output. For
             the second dropout layer, `backbone.dropout` is used.
 
     Examples:
 
     Raw string data.
@@ -92,18 +91,16 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.DebertaV3Classifier.from_preset(
         "deberta_v3_base_en",
         num_classes=4,
@@ -197,21 +194,22 @@
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.hidden_dim = hidden_dim
         self.dropout = dropout
 
         # Default compilation
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_classifier_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -7,136 +7,143 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for DeBERTa classification model."""
+"""Tests for FNet classification model."""
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
-from keras_nlp.src.models.deberta_v3.deberta_v3_classifier import (
-    DebertaV3Classifier,
-)
-from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
-    DebertaV3Preprocessor,
-)
-from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
+from keras_nlp.src.models.f_net.f_net_classifier import FNetClassifier
+from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
+from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3ClassifierTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class FNetClassifierTest(TestCase):
     def setUp(self):
+        # Setup Model
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
+
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=10,
+            vocab_size=12,
             model_type="WORD",
-            pad_id=0,
-            bos_id=1,
-            eos_id=2,
-            unk_id=3,
-            pad_piece="[PAD]",
+            pad_id=3,
+            unk_id=0,
+            bos_id=4,
+            eos_id=5,
+            pad_piece="<pad>",
+            unk_piece="<unk>",
             bos_piece="[CLS]",
             eos_piece="[SEP]",
-            unk_piece="[UNK]",
             user_defined_symbols="[MASK]",
         )
-        self.preprocessor = DebertaV3Preprocessor(
-            DebertaV3Tokenizer(proto=bytes_io.getvalue()),
-            sequence_length=5,
+
+        self.proto = bytes_io.getvalue()
+
+        self.preprocessor = FNetPreprocessor(
+            tokenizer=FNetTokenizer(proto=self.proto),
+            sequence_length=8,
         )
-        self.backbone = DebertaV3Backbone(
+        self.backbone = FNetBackbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
-            num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
-            bucket_size=2,
         )
-        self.classifier = DebertaV3Classifier(
+        self.classifier = FNetClassifier(
             self.backbone,
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
-            hidden_dim=4,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        # Setup data.
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
-    def test_classifier_fit(self):
+    def test_fnet_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = FNetClassifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = FNetClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            name="test",
+            trainable=False,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saving_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, DebertaV3Classifier)
+        self.assertIsInstance(restored_model, FNetClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,28 +11,26 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DeBERTaV3 masked lm model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import (
     deberta_kernel_initializer,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_masked_lm_preprocessor import (
     DebertaV3MaskedLMPreprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.DebertaV3MaskedLM")
 class DebertaV3MaskedLM(Task):
     """An end-to-end DeBERTaV3 model for the masked language modeling task.
 
@@ -82,21 +80,17 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create preprocessed batch where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2)),
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2),
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.DebertaV3MaskedLM.from_preset(
         "deberta_v3_base_en",
         preprocessor=None,
@@ -138,16 +132,16 @@
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
 
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return DebertaV3Backbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 """DeBERTa masked language model preprocessor layer."""
 
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
 from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
     DebertaV3Preprocessor,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
 @keras_nlp_export("keras_nlp.models.DebertaV3MaskedLMPreprocessor")
@@ -46,23 +48,24 @@
     Args:
         tokenizer: A `keras_nlp.models.DebertaV3Tokenizer` instance.
         sequence_length: The length of the packed inputs.
         mask_selection_rate: The probability an input token will be dynamically
             masked.
         mask_selection_length: The maximum number of masked tokens supported
             by the layer.
-        mask_token_rate: float, defaults to 0.8. `mask_token_rate` must be
+        mask_token_rate: float. `mask_token_rate` must be
             between 0 and 1 which indicates how often the mask_token is
             substituted for tokens selected for masking.
-        random_token_rate: float, defaults to 0.1. `random_token_rate` must be
+            Defaults to `0.8`.
+        random_token_rate: float. `random_token_rate` must be
             between 0 and 1 which indicates how often a random token is
-            substituted for tokens selected for masking. Default is 0.1.
+            substituted for tokens selected for masking.
             Note: mask_token_rate + random_token_rate <= 1,  and for
             (1 - mask_token_rate - random_token_rate), the token will not be
-            changed.
+            changed. Defaults to `0.1`.
         truncate: string. The algorithm to truncate a list of batched segments
             to fit within `sequence_length`. The value can be either
             `round_robin` or `waterfall`:
                 - `"round_robin"`: Available space is assigned one token at a
                     time in a round-robin fashion to the inputs that still need
                     some, until the limit is reached.
                 - `"waterfall"`: The allocation of the budget is done using a
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_preprocessor_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,31 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for DeBERTa preprocessor layer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.deberta_v3_masked_lm_preprocessor import (
     DebertaV3MaskedLMPreprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class DebertaV3PreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -137,36 +136,31 @@
             x["padding_mask"], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
         )
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_masked_lm_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,26 +15,27 @@
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
 from keras_nlp.src.models.deberta_v3.deberta_v3_masked_lm import DebertaV3MaskedLM
 from keras_nlp.src.models.deberta_v3.deberta_v3_masked_lm_preprocessor import (
     DebertaV3MaskedLMPreprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3MaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3MaskedLMTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round", "an eagle flew"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -66,20 +67,18 @@
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
         self.masked_lm = DebertaV3MaskedLM(
             self.backbone,
             preprocessor=self.preprocessor,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the eagle flew over fox.",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the eagle flew over fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
@@ -100,32 +99,26 @@
         self.masked_lm.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.masked_lm)
-        new_classifier = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.masked_lm)
+        new_classifier = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_classifier.get_config(),
             self.masked_lm.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, DebertaV3MaskedLM)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DeBERTa preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
 from keras_nlp.src.models.deberta_v3.deberta_v3_presets import backbone_presets
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_preprocessor_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,31 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for DeBERTa preprocessor layer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
     DebertaV3Preprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class DebertaV3PreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -134,34 +133,29 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_presets_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,29 +10,31 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
 from keras_nlp.src.models.deberta_v3.deberta_v3_classifier import (
     DebertaV3Classifier,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
     DebertaV3Preprocessor,
 )
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class DebertaV3PresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3PresetSmokeTest(TestCase):
     """
     A smoke test for DeBERTa presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/deberta/deberta_presets_test.py --run_large`
     """
 
@@ -62,46 +64,46 @@
         self.assertEqual(preprocessor.tokenizer.token_to_id("[MASK]"), 128000)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[0, 581, 63773, 2]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[0, 581, 63773, 2]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = DebertaV3Backbone.from_preset(
             "deberta_v3_extra_small_en", load_weights=load_weights
         )
         outputs = model(input_data)
         if load_weights:
             outputs = outputs[0, 0, :5]
             expected = [0.418, -0.116, -0.122, -1.847, -0.035]
             self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = DebertaV3Classifier.from_preset(
             "deberta_v3_extra_small_en",
             num_classes=2,
             load_weights=load_weights,
         )
         # Never assert output values, as the head weights are random.
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[0, 581, 63773, 2]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[0, 581, 63773, 2]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = DebertaV3Classifier.from_preset(
             "deberta_v3_extra_small_en",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -128,15 +130,16 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("deberta_v3_extra_small_en_clowntown", **kwargs)
 
 
 @pytest.mark.extra_large
-class DebertaV3PresetFullTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3PresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every DeBERTa preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/deberta/deberta_presets_test.py --run_extra_large`
     """
@@ -146,52 +149,52 @@
     )
     def test_load_deberta(self, load_weights):
         for preset in DebertaV3Backbone.presets:
             model = DebertaV3Backbone.from_preset(
                 preset, load_weights=load_weights
             )
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_deberta_classifier(self, load_weights):
         for preset in DebertaV3Classifier.presets:
             classifier = DebertaV3Classifier.from_preset(
                 preset,
                 num_classes=4,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["The quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_deberta_classifier_without_preprocessing(self, load_weights):
         for preset in DebertaV3Classifier.presets:
             classifier = DebertaV3Classifier.from_preset(
                 preset,
                 num_classes=4,
                 load_weights=load_weights,
                 preprocessor=None,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in DebertaV3Tokenizer.presets:
             tokenizer = DebertaV3Tokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,28 +9,27 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for DeBERTa tokenizer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DebertaV3TokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class DebertaV3TokenizerTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -52,27 +51,27 @@
 
     def test_tokenize(self):
         input_data = "the quick brown fox"
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [4, 9, 5, 7])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant(["the quick brown fox", "the earth is round"])
+        input_data = ["the quick brown fox", "the earth is round"]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[4, 9, 5, 7], [4, 6, 8, 3]])
 
     def test_detokenize(self):
-        input_data = tf.constant([[4, 9, 5, 7]])
+        input_data = [[4, 9, 5, 7]]
         output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["the quick brown fox"]))
+        self.assertEqual(output, ["the quick brown fox"])
 
     def test_detokenize_mask_token(self):
-        input_data = tf.constant([[4, 9, 5, 7, self.tokenizer.mask_token_id]])
+        input_data = [[4, 9, 5, 7, self.tokenizer.mask_token_id]]
         output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["the quick brown fox"]))
+        self.assertEqual(output, ["the quick brown fox"])
 
     def test_vocabulary_size(self):
         self.assertEqual(self.tokenizer.vocabulary_size(), 11)
 
     def test_get_vocabulary_mask_token(self):
         self.assertEqual(self.tokenizer.get_vocabulary()[10], "[MASK]")
 
@@ -92,37 +91,32 @@
             eos_id=-1,
             bos_id=-1,
         )
         with self.assertRaises(ValueError):
             DebertaV3Tokenizer(proto=bytes_io.getvalue())
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/disentangled_attention_encoder.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/disentangled_attention_encoder.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,22 +10,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Disentangled attention encoder block implementation based on `keras.layers.Layer`."""
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.deberta_v3.disentangled_self_attention import (
     DisentangledSelfAttention,
 )
 from keras_nlp.src.utils.keras_utils import clone_initializer
 
-from keras_nlp.src.layers.transformer_layer_utils import (  # isort:skip
+from keras_nlp.src.layers.modeling.transformer_layer_utils import (  # isort:skip
     merge_padding_and_attention_mask,
 )
 
 
 class DisentangledAttentionEncoder(keras.layers.Layer):
     """Disentangled attention encoder.
 
@@ -38,30 +37,33 @@
     `DisentangledAttentionEncoder` is similar to
     `keras_nlp.layers.TransformerEncoder`, except for the attention layer - it
     uses disentangled self-attention instead of multi-head attention.
 
     Args:
         intermediate_dim: int, the hidden size of feedforward network.
         num_heads: int, the number of heads in the attention layer.
-        max_position_embeddings: int, defaults to 512. The maximum input
-            sequence length.
-        bucket_size: int, defaults to 256. The size of the relative position
+        max_position_embeddings: int. The maximum input
+            sequence length. Defaults to `512`.
+        bucket_size: int. The size of the relative position
             buckets. Generally equal to `max_sequence_length // 2`.
-        dropout: float, defaults to 0.0. The dropout value, shared by
+            Defaults to `256`.
+        dropout: float. The dropout value, shared by
             the attention layer and feedforward network.
-        activation: string or `keras.activations`, defaults to "relu". the
+            Defaults to `0.0`.
+        activation: string or `keras.activations`. the
             activation function of feedforward network.
-        layer_norm_epsilon: float, defaults to 1e-5. The epsilon value in layer
-            normalization components.
-        kernel_initializer: string or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for
-            the dense and disentangled self-attention layers.
-        bias_initializer: string or `keras.initializers` initializer,
-            defaults to "zeros". The bias initializer for
-            the dense and disentangled self-attention layers.
+            Defaults to `"relu"`.
+        layer_norm_epsilon: float. The epsilon value in layer
+            normalization components. Defaults to `1e-5`.
+        kernel_initializer: string or `keras.initializers` initializer.
+            The kernel initializer for the dense and disentangled
+            self-attention layers. Defaults to `"glorot_uniform"`.
+        bias_initializer: string or `keras.initializers` initializer.
+            The bias initializer for the dense and disentangled
+            self-attention layers. Defaults to `"zeros"`.
     """
 
     def __init__(
         self,
         intermediate_dim,
         num_heads,
         max_position_embeddings=512,
@@ -69,76 +71,74 @@
         dropout=0,
         activation="relu",
         layer_norm_epsilon=1e-05,
         kernel_initializer="glorot_uniform",
         bias_initializer="zeros",
         **kwargs
     ):
-        # Work around for model saving
-        self._input_shape = kwargs.pop("build_input_shape", None)
-
         super().__init__(**kwargs)
-
         self.intermediate_dim = intermediate_dim
         self.num_heads = num_heads
         self.max_position_embeddings = max_position_embeddings
         self.bucket_size = bucket_size
         self.dropout = dropout
         self.activation = keras.activations.get(activation)
         self.layer_norm_epsilon = layer_norm_epsilon
         self.kernel_initializer = keras.initializers.get(kernel_initializer)
         self.bias_initializer = keras.initializers.get(bias_initializer)
         self._built = False
         self.supports_masking = True
 
-        if self._input_shape is not None:
-            self._build(self._input_shape)
-
-    def _build(self, input_shape):
-        # Create layers based on input shape.
-        self._built = True
-        self._input_shape = input_shape
+    def build(self, inputs_shape):
         # Infer the dimension of our hidden feature size from the build shape.
-        hidden_dim = input_shape[-1]
+        hidden_dim = inputs_shape[-1]
 
         # Self attention layers.
         self._self_attention_layer = DisentangledSelfAttention(
             num_heads=self.num_heads,
             hidden_dim=hidden_dim,
             max_position_embeddings=self.max_position_embeddings,
             bucket_size=self.bucket_size,
             dropout=self.dropout,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        self._self_attention_layer.build(inputs_shape)
         self._self_attention_layernorm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon,
         )
+        self._self_attention_layernorm.build(inputs_shape)
         self._self_attention_dropout = keras.layers.Dropout(
             rate=self.dropout,
         )
 
         # Feedforward layers.
         self._feedforward_layernorm = keras.layers.LayerNormalization(
             epsilon=self.layer_norm_epsilon,
         )
+        self._feedforward_layernorm.build(inputs_shape)
         self._feedforward_intermediate_dense = keras.layers.Dense(
             self.intermediate_dim,
             activation=self.activation,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        self._feedforward_intermediate_dense.build(inputs_shape)
         self._feedforward_output_dense = keras.layers.Dense(
             hidden_dim,
             kernel_initializer=clone_initializer(self.kernel_initializer),
             bias_initializer=clone_initializer(self.bias_initializer),
         )
+        intermediate_shape = list(inputs_shape)
+        intermediate_shape[-1] = self.intermediate_dim
+        self._feedforward_output_dense.build(tuple(intermediate_shape))
         self._feedforward_dropout = keras.layers.Dropout(
             rate=self.dropout,
         )
+        self.built = True
 
     def call(
         self,
         inputs,
         rel_embeddings,
         padding_mask=None,
         attention_mask=None,
@@ -157,29 +157,25 @@
             attention_mask: a boolean Tensor. Customized mask used to mask out
                 certain tokens. `attention_mask` should have shape
                 [batch_size, sequence_length, sequence_length].
 
         Returns:
             A Tensor of the same shape as the `inputs`.
         """
-
-        if not self._built:
-            self._build(inputs.shape)
-
         x = inputs
 
         # Compute self attention mask.
         self_attention_mask = merge_padding_and_attention_mask(
             inputs, padding_mask, attention_mask
         )
 
         # Self attention block.
         residual = x
         x = self._self_attention_layer(
-            hidden_states=x,
+            x,
             rel_embeddings=rel_embeddings,
             attention_mask=self_attention_mask,
         )
         x = self._self_attention_dropout(x)
         x = x + residual
         x = self._self_attention_layernorm(x)
 
@@ -206,12 +202,14 @@
                 "layer_norm_epsilon": self.layer_norm_epsilon,
                 "kernel_initializer": keras.initializers.serialize(
                     self.kernel_initializer
                 ),
                 "bias_initializer": keras.initializers.serialize(
                     self.bias_initializer
                 ),
-                "build_input_shape": self._input_shape,
             }
         )
         return config
 
+    def compute_output_shape(self, inputs_shape):
+        return inputs_shape
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/disentangled_self_attention.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/disentangled_self_attention.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 
 """Disentangled self-attention layer."""
 
 import math
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.utils.keras_utils import clone_initializer
 
 
 class DisentangledSelfAttention(keras.layers.Layer):
     """DisentangledSelfAttention layer.
 
     This is an implementation of disentangled self-attention as described in the
@@ -31,24 +31,26 @@
     attention, i.e., to get the final attention score, we compute the
     content-to-position and position-to-content attention scores, and add these
     scores to the vanilla multi-head self-attention scores.
 
     Args:
         num_heads: int. Number of attention heads.
         hidden_dim: int. Hidden dimension of the input, i.e., `hidden_states`.
-        max_position_embeddings: int, defaults to 512. The maximum input
-            sequence length.
-        bucket_size: int, defaults to 256. The size of the relative position
+        max_position_embeddings: int. The maximum input
+            sequence length. Defaults to `512`.
+        bucket_size: int. The size of the relative position
             buckets. Generally equal to `max_sequence_length // 2`.
-        dropout: float, defaults to 0.1. Dropout probability.
-        kernel_initializer: string or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for
-            the dense layers.
-        bias_initializer: string or `keras.initializers` initializer,
-            defaults to "zeros". The bias initializer for the dense layers.
+            Defaults to `256`.
+        dropout: float. Dropout probability. Defaults to `0.1`.
+        kernel_initializer: string or `keras.initializers` initializer.
+            The kernel initializer for the dense layers.
+            Defaults to `"glorot_uniform"`.
+        bias_initializer: string or `keras.initializers` initializer.
+            The bias initializer for the dense layers.
+            Defaults to `"zeros"`.
     """
 
     def __init__(
         self,
         num_heads,
         hidden_dim,
         max_position_embeddings=512,
@@ -76,38 +78,40 @@
 
         # We have three types of attention - MHA, p2c and c2p.
         num_type_attn = 3
         self.scale_factor = 1.0 / math.sqrt(
             float(num_type_attn * self.attn_head_size)
         )
 
-        # Layers.
-
+    def build(self, inputs_shape, rel_embeddings_shape=None):
         # Q, K, V linear layers.
         self._query_dense = keras.layers.EinsumDense(
             equation="abc,cde->abde",
             output_shape=(None, self.num_heads, self.attn_head_size),
             bias_axes="de",
             **self._get_common_kwargs_for_sublayer(use_bias=True),
             name="query",
         )
+        self._query_dense.build(inputs_shape)
         self._key_dense = keras.layers.EinsumDense(
             equation="abc,cde->abde",
             output_shape=(None, self.num_heads, self.attn_head_size),
             bias_axes="de",
             **self._get_common_kwargs_for_sublayer(use_bias=True),
             name="key",
         )
+        self._key_dense.build(inputs_shape)
         self._value_dense = keras.layers.EinsumDense(
             equation="abc,cde->abde",
             output_shape=(None, self.num_heads, self.attn_head_size),
             bias_axes="de",
             **self._get_common_kwargs_for_sublayer(use_bias=True),
             name="value",
         )
+        self._value_dense.build(inputs_shape)
 
         # Relative attention.
         self._position_dropout_layer = keras.layers.Dropout(self.dropout)
 
         self._attn_dropout_layer = keras.layers.Dropout(
             self.dropout, name="attention_dropout"
         )
@@ -117,14 +121,16 @@
         self._output_dense = keras.layers.EinsumDense(
             equation="abc,cd->abd",
             output_shape=(None, self.hidden_dim),
             bias_axes="d",
             **self._get_common_kwargs_for_sublayer(use_bias=True),
             name="attention_output",
         )
+        self._output_dense.build(inputs_shape)
+        self.built = True
 
     def _get_common_kwargs_for_sublayer(self, use_bias=True):
         common_kwargs = {}
 
         kernel_initializer = clone_initializer(self._kernel_initializer)
         bias_initializer = clone_initializer(self._bias_initializer)
 
@@ -223,20 +229,20 @@
         log_pos = _get_log_pos(abs_pos, mid)
 
         bucket_pos = tf.where(
             condition=abs_pos <= mid,
             x=rel_pos,
             y=log_pos * sign,
         )
-        bucket_pos = tf.cast(bucket_pos, dtype=tf.int64)
+        bucket_pos = tf.cast(bucket_pos, dtype="int64")
 
         return bucket_pos
 
     def _get_rel_pos(self, num_positions):
-        ids = tf.range(num_positions, dtype=tf.int64)
+        ids = tf.range(num_positions, dtype="int64")
         query_ids = ids[:, tf.newaxis]
         key_ids = ids[tf.newaxis, :]
         key_ids = tf.repeat(key_ids, repeats=num_positions, axis=0)
 
         rel_pos = query_ids - key_ids
         rel_pos = self._make_log_bucket_position(rel_pos)
 
@@ -316,25 +322,25 @@
         p2c_attn_scores = tf.multiply(p2c_attn_scores, self.scale_factor)
         score += p2c_attn_scores
 
         return score
 
     def call(
         self,
-        hidden_states,
+        inputs,
         rel_embeddings,
         attention_mask=None,
         return_attention_scores=False,
         training=None,
     ):
         # `query`, `key`, `value` shape:
         # `(batch_size, sequence_length, num_heads, attn_head_size)`.
-        query = self._query_dense(hidden_states)
-        key = self._key_dense(hidden_states)
-        value = self._value_dense(hidden_states)
+        query = self._query_dense(inputs)
+        key = self._key_dense(inputs)
+        value = self._value_dense(inputs)
 
         attention_output, attention_scores = self._compute_attention(
             query=query,
             key=key,
             value=value,
             rel_embeddings=rel_embeddings,
             attention_mask=attention_mask,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/deberta_v3/relative_embedding.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/relative_embedding.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,15 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Relative embedding layer."""
 
 import tensorflow as tf
-from tensorflow import keras
+
+from keras_nlp.src.backend import keras
 
 
 class RelativeEmbedding(keras.layers.Layer):
     """Relative embedding layer.
 
     This is an implementation of relative embedding as described in the
     paper ["DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"](https://arxiv.org/abs/2111.09543).
@@ -29,17 +30,17 @@
     embedding matrix.
 
     Args:
         hidden_dim: int. The size of the dense embedding.
         bucket_size: int. The size of the relative position buckets.
         layer_norm_epsilon: float. Epsilon value to initialize the layer
             normalization layer.
-        kernel_initializer: string or `keras.initializers` initializer,
-            defaults to "glorot_uniform". The kernel initializer for
-            the dense embedding.
+        kernel_initializer: string or `keras.initializers` initializer.
+            The kernel initializer for the dense embedding.
+            Defaults to `"glorot_uniform"`.
     """
 
     def __init__(
         self,
         hidden_dim,
         bucket_size,
         layer_norm_epsilon=1e-05,
@@ -84,7 +85,10 @@
                 "kernel_initializer": keras.initializers.serialize(
                     self.kernel_initializer
                 ),
             }
         )
         return config
 
+    def compute_output_shape(self, input_shape):
+        return (input_shape[0],) + (self.bucket_size * 2, self.hidden_dim)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_backbone.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,22 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """DistilBERT backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.token_and_position_embedding import (
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.token_and_position_embedding import (
     TokenAndPositionEmbedding,
 )
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.distil_bert.distil_bert_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def distilbert_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
@@ -66,18 +64,16 @@
             can consume. If None, `max_sequence_length` uses the value from
             sequence length. This determines the variable shape for positional
             embeddings.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained DistilBERT encoder.
     model = keras_nlp.models.DistilBertBackbone.from_preset(
         "distil_bert_base_en_uncased"
     )
     model(input_data)
@@ -123,15 +119,15 @@
             name="token_and_position_embedding",
         )(token_id_input)
 
         # Normalize and apply dropout to embeddings.
         x = keras.layers.LayerNormalization(
             axis=-1,
             epsilon=1e-12,
-            dtype=tf.float32,
+            dtype="float32",
             name="embeddings_layer_norm",
         )(x)
         x = keras.layers.Dropout(
             dropout,
             name="embeddings_dropout",
         )(x)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bart/bart_backbone_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,116 +1,125 @@
-# Copyright 2023 The KerasNLP Authors
+# Copyright 2022 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Test for DistilBERT backbone models."""
+"""Test for BART backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.bart.bart_backbone import BartBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertTest(tf.test.TestCase, parameterized.TestCase):
+class BartBackboneTest(TestCase):
     def setUp(self):
-        self.backbone = DistilBertBackbone(
+        self.backbone = BartBackbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
-            hidden_dim=2,
+            hidden_dim=3,
             intermediate_dim=4,
             max_sequence_length=5,
-            name="encoder",
         )
-
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "encoder_token_ids": ops.ones((2, 5), dtype="int32"),
+            "encoder_padding_mask": ops.ones((2, 5), dtype="int32"),
+            "decoder_token_ids": ops.ones((2, 5), dtype="int32"),
+            "decoder_padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
-    def test_valid_call_distilbert(self):
+    def test_valid_call(self):
         self.backbone(self.input_batch)
 
-    def test_token_embedding(self):
-        output = self.backbone.token_embedding(self.input_batch["token_ids"])
-        self.assertEqual(output.shape, (2, 5, 2))
+    def test_name(self):
+        # Check default name passed through
+        self.assertRegexpMatches(self.backbone.name, "bart_backbone")
 
-    def test_variable_sequence_length_call_distilbert(self):
+    def test_variable_sequence_length_call(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "mask_positions": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "encoder_token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "encoder_padding_mask": ops.ones(
+                    (2, seq_length), dtype="int32"
+                ),
+                "decoder_token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "decoder_padding_mask": ops.ones(
+                    (2, seq_length), dtype="int32"
+                ),
             }
             self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, DistilBertBackbone)
+        self.assertIsInstance(restored_model, BartBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
-        self.assertAllClose(model_output, restored_output)
+        self.assertAllClose(
+            model_output["encoder_sequence_output"],
+            restored_output["encoder_sequence_output"],
+        )
+        self.assertAllClose(
+            model_output["decoder_sequence_output"],
+            restored_output["decoder_sequence_output"],
+        )
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class DistilBertTPUTest(tf.test.TestCase, parameterized.TestCase):
+class BartBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
-            self.backbone = DistilBertBackbone(
+            self.backbone = BartBackbone(
                 vocabulary_size=1000,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=64,
                 intermediate_dim=128,
                 max_sequence_length=128,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "encoder_token_ids": ops.ones((8, 128), dtype="int32"),
+            "encoder_padding_mask": ops.ones((8, 128), dtype="int32"),
+            "decoder_token_ids": ops.ones((8, 128), dtype="int32"),
+            "decoder_padding_mask": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_classifier.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,27 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DistilBERT classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import (
     distilbert_kernel_initializer,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
     DistilBertPreprocessor,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.DistilBertClassifier")
 class DistilBertClassifier(Task):
     """An end-to-end DistilBERT model for classification tasks.
 
@@ -52,17 +50,18 @@
 
     Args:
         backbone: A `keras_nlp.models.DistilBert` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.DistilBertPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
+        activation: Optional `str` or callable. The
             activation function to use on the model outputs. Set
             `activation="softmax"` to return output probabilities.
+            Defaults to `None`.
         hidden_dim: int. The size of the pooler layer.
         dropout: float. The dropout probability value, applied after the first
             dense layer.
 
     Examples:
 
     Raw string data.
@@ -94,18 +93,16 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2)
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.DistilBertClassifier.from_preset(
         "distil_bert_base_en_uncased",
         num_classes=4,
@@ -182,21 +179,22 @@
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.hidden_dim = hidden_dim
         self.dropout = dropout
 
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/deberta_v3_classifier_test.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,120 +7,145 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for DistilBERT classification model."""
+"""Tests for DeBERTa classification model."""
 
+import io
 import os
 
 import pytest
+import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
-from keras_nlp.src.models.distil_bert.distil_bert_classifier import (
-    DistilBertClassifier,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.deberta_v3.deberta_v3_backbone import DebertaV3Backbone
+from keras_nlp.src.models.deberta_v3.deberta_v3_classifier import (
+    DebertaV3Classifier,
 )
-from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
-    DistilBertPreprocessor,
-)
-from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
-    DistilBertTokenizer,
+from keras_nlp.src.models.deberta_v3.deberta_v3_preprocessor import (
+    DebertaV3Preprocessor,
 )
+from keras_nlp.src.models.deberta_v3.deberta_v3_tokenizer import DebertaV3Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertClassifierTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class DebertaV3ClassifierTest(TestCase):
     def setUp(self):
-        # Setup model
-
-        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
-        self.vocab += ["the", "quick", "brown", "fox", "."]
-        self.preprocessor = DistilBertPreprocessor(
-            DistilBertTokenizer(vocabulary=self.vocab),
-            sequence_length=8,
+        bytes_io = io.BytesIO()
+        vocab_data = tf.data.Dataset.from_tensor_slices(
+            ["the quick brown fox", "the earth is round"]
+        )
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=vocab_data.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=10,
+            model_type="WORD",
+            pad_id=0,
+            bos_id=1,
+            eos_id=2,
+            unk_id=3,
+            pad_piece="[PAD]",
+            bos_piece="[CLS]",
+            eos_piece="[SEP]",
+            unk_piece="[UNK]",
+            user_defined_symbols="[MASK]",
+        )
+        self.preprocessor = DebertaV3Preprocessor(
+            DebertaV3Tokenizer(proto=bytes_io.getvalue()),
+            sequence_length=5,
         )
-        self.backbone = DistilBertBackbone(
+        self.backbone = DebertaV3Backbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
+            bucket_size=2,
         )
-        self.classifier = DistilBertClassifier(
+        self.classifier = DebertaV3Classifier(
             self.backbone,
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
             hidden_dim=4,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
     def test_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
-            loss="sparse_categorical_crossentropy",
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = DebertaV3Classifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = DebertaV3Classifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            hidden_dim=4,
+            name="test",
+            trainable=False,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saving_model(self, save_format, filename):
+    def test_saving_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, DistilBertClassifier)
+        self.assertIsInstance(restored_model, DebertaV3Classifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,28 +11,26 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DistilBERT masked lm model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import (
     distilbert_kernel_initializer,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_masked_lm_preprocessor import (
     DistilBertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.DistilBertMaskedLM")
 class DistilBertMaskedLM(Task):
     """An end-to-end DistilBERT model for the masked language modeling task.
 
@@ -82,21 +80,17 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create preprocessed batch where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2)
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.DistilBertMaskedLM.from_preset(
         "distil_bert_base_en_uncased",
         preprocessor=None,
@@ -136,16 +130,16 @@
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
 
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return DistilBertBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 """DistilBERT masked language model preprocessor layer."""
 
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
 from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
     DistilBertPreprocessor,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
 @keras_nlp_export("keras_nlp.models.DistilBertMaskedLMPreprocessor")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_preprocessor_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,33 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for DistilBERT masked language model preprocessor layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.distil_bert.distil_bert_masked_lm_preprocessor import (
     DistilBertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
     DistilBertTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertMaskedLMPreprocessorTest(
-    tf.test.TestCase, parameterized.TestCase
-):
+class DistilBertMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
         self.vocab += ["the", "quick", "brown", "fox"]
 
         self.preprocessor = DistilBertMaskedLMPreprocessor(
             tokenizer=DistilBertTokenizer(
@@ -104,36 +101,31 @@
         self.assertAllEqual(x["token_ids"], [2, 5, 6, 7, 8, 1, 3, 0])
         self.assertAllEqual(x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" THE QUICK BROWN FOX."])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_masked_lm_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,30 +13,30 @@
 # limitations under the License.
 """Tests for DistilBERT masked language model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
 from keras_nlp.src.models.distil_bert.distil_bert_masked_lm import (
     DistilBertMaskedLM,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_masked_lm_preprocessor import (
     DistilBertMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
     DistilBertTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+class DistilBertMaskedLMTest(TestCase):
     def setUp(self):
         # Setup model.
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["the", "quick", "brown", "fox", "."]
         self.preprocessor = DistilBertMaskedLMPreprocessor(
             DistilBertTokenizer(vocabulary=self.vocab),
             sequence_length=5,
@@ -51,20 +51,18 @@
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
         self.masked_lm = DistilBertMaskedLM(
             self.backbone,
             preprocessor=self.preprocessor,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
@@ -87,25 +85,19 @@
         self.masked_lm.preprocessor = None
         self.masked_lm.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.masked_lm.fit(self.preprocessed_dataset)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, DistilBertMaskedLM)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_preprocessor.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """DistilBERT preprocessor layers."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
 from keras_nlp.src.models.distil_bert.distil_bert_presets import backbone_presets
 from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
     DistilBertTokenizer,
 )
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_preprocessor_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,126 +7,125 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for DistilBERT preprocessor layer."""
 
+"""Tests for OPT preprocessor layer."""
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
-    DistilBertPreprocessor,
-)
-from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
-    DistilBertTokenizer,
-)
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
+from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class OPTPreprocessorTest(TestCase):
     def setUp(self):
-        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
-        self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
-        self.vocab += ["the", "quick", "brown", "fox"]
-        self.preprocessor = DistilBertPreprocessor(
-            DistilBertTokenizer(vocabulary=self.vocab),
+        self.vocab = {
+            "<pad>": 0,
+            "</s>": 1,
+            "air": 2,
+            "air": 3,
+            "plane": 4,
+            "at": 5,
+            "port": 6,
+        }
+
+        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
+        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
+        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
+        merges += ["pla ne"]
+        self.merges = merges
+
+        self.preprocessor = OPTPreprocessor(
+            tokenizer=OPTTokenizer(
+                vocabulary=self.vocab,
+                merges=self.merges,
+            ),
             sequence_length=8,
         )
 
     def test_tokenize_strings(self):
-        input_data = "THE QUICK BROWN FOX."
-        output = self.preprocessor(input_data)
-        self.assertAllEqual(output["token_ids"], [2, 5, 6, 7, 8, 1, 3, 0])
-        self.assertAllEqual(output["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
+        input_data = " airplane at airport"
+
+        x = self.preprocessor(input_data)
+        self.assertAllEqual(x["token_ids"], [1, 3, 4, 5, 3, 6, 1, 0])
+        self.assertAllEqual(x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
 
     def test_tokenize_list_of_strings(self):
-        # We should handle a list of strings as as batch.
-        input_data = ["THE QUICK BROWN FOX."] * 4
-        output = self.preprocessor(input_data)
-        self.assertAllEqual(output["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
-        self.assertAllEqual(
-            output["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
+        input_data = [" airplane at airport"] * 4
+
+        x = self.preprocessor(input_data)
+        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+
+    def test_no_start_end_token(self):
+        input_data = [" airplane at airport"] * 4
+
+        preprocessor = OPTPreprocessor(
+            tokenizer=OPTTokenizer(
+                vocabulary=self.vocab,
+                merges=self.merges,
+            ),
+            sequence_length=8,
+            add_start_token=False,
+            add_end_token=False,
         )
+        x = preprocessor(input_data)
+        self.assertAllEqual(x["token_ids"], [[3, 4, 5, 3, 6, 0, 0, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0]] * 4)
 
     def test_tokenize_labeled_batch(self):
-        x = tf.constant(["THE QUICK BROWN FOX."] * 4)
-        y = tf.constant([1] * 4)
-        sw = tf.constant([1.0] * 4)
-        x_out, y_out, sw_out = self.preprocessor(x, y, sw)
-        self.assertAllEqual(x_out["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
-        self.assertAllEqual(
-            x_out["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
-        )
-        self.assertAllEqual(y_out, y)
-        self.assertAllEqual(sw_out, sw)
+        x = tf.constant([" airplane at airport"] * 4)
+        y_in = tf.constant([1] * 4)
+        sw_in = tf.constant([1.0] * 4)
+        x, y, sw = self.preprocessor(x, y_in, sw_in)
+        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        self.assertAllEqual(y, y_in)
+        self.assertAllEqual(sw, sw_in)
 
     def test_tokenize_labeled_dataset(self):
-        x = tf.constant(["THE QUICK BROWN FOX."] * 4)
-        y = tf.constant([1] * 4)
-        sw = tf.constant([1.0] * 4)
-        ds = tf.data.Dataset.from_tensor_slices((x, y, sw))
+        x = tf.constant([" airplane at airport"] * 4)
+        ds = tf.data.Dataset.from_tensor_slices(x)
         ds = ds.map(self.preprocessor)
-        x_out, y_out, sw_out = ds.batch(4).take(1).get_single_element()
-        self.assertAllEqual(x_out["token_ids"], [[2, 5, 6, 7, 8, 1, 3, 0]] * 4)
-        self.assertAllEqual(
-            x_out["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4
-        )
-        self.assertAllEqual(y_out, y)
-        self.assertAllEqual(sw_out, sw)
-
-    def test_tokenize_multiple_sentences(self):
-        sentence_one = tf.constant("THE QUICK")
-        sentence_two = tf.constant("BROWN FOX.")
-        output = self.preprocessor((sentence_one, sentence_two))
-        self.assertAllEqual(output["token_ids"], [2, 5, 6, 3, 7, 8, 1, 3])
-        self.assertAllEqual(output["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1])
-
-    def test_tokenize_multiple_batched_sentences(self):
-        sentence_one = tf.constant(["THE QUICK"] * 4)
-        sentence_two = tf.constant(["BROWN FOX."] * 4)
-        # The first tuple or list is always interpreted as an enumeration of
-        # separate sequences to concatenate.
-        output = self.preprocessor((sentence_one, sentence_two))
-        self.assertAllEqual(output["token_ids"], [[2, 5, 6, 3, 7, 8, 1, 3]] * 4)
-        self.assertAllEqual(
-            output["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 1]] * 4
-        )
-
-    def test_errors_for_2d_list_input(self):
-        ambiguous_input = [["one", "two"], ["three", "four"]]
-        with self.assertRaises(ValueError):
-            self.preprocessor(ambiguous_input)
+        x = ds.batch(4).take(1).get_single_element()
+        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+
+    def test_sequence_length_override(self):
+        input_data = " airplane at airport"
+        x = self.preprocessor(input_data, sequence_length=4)
+        self.assertAllEqual(x["token_ids"], [1, 3, 4, 1])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant(["THE QUICK BROWN FOX."])
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant([" airplane at airport"])
+
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
+
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_presets_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,31 +10,32 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
 from keras_nlp.src.models.distil_bert.distil_bert_classifier import (
     DistilBertClassifier,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
     DistilBertPreprocessor,
 )
 from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
     DistilBertTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class DistilBertPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class DistilBertPresetSmokeTest(TestCase):
     """
     A smoke test for DistilBERT presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/distilbert/distilbert_presets_test.py --run_large`
     """
 
@@ -56,44 +57,44 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = DistilBertBackbone.from_preset(
             "distil_bert_base_en_uncased", load_weights=load_weights
         )
         outputs = model(input_data)[0, 0, :5]
         if load_weights:
             expected_outputs = [-0.2381, -0.1965, 0.1053, -0.0847, -0.145]
             self.assertAllClose(outputs, expected_outputs, atol=0.01, rtol=0.01)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = DistilBertClassifier.from_preset(
             "distil_bert_base_en_uncased",
             num_classes=2,
             load_weights=load_weights,
         )
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = DistilBertClassifier.from_preset(
             "distil_bert_base_en_uncased",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -119,15 +120,15 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("distilbert_base_uncased", **kwargs)
 
 
 @pytest.mark.extra_large
-class DistilBertPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class DistilBertPresetFullTest(TestCase):
     """
     Tests the full enumeration of our preset.
 
     This tests every DistilBERT preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/distilbert/distilbert_presets_test.py --run_extra_large`
     """
@@ -137,52 +138,52 @@
     )
     def test_load_distilbert(self, load_weights):
         for preset in DistilBertBackbone.presets:
             model = DistilBertBackbone.from_preset(
                 preset, load_weights=load_weights
             )
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_distilbert_classifier(self, load_weights):
         for preset in DistilBertClassifier.presets:
             classifier = DistilBertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["This quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_distilbert_classifier_no_preprocessing(self, load_weights):
         for preset in DistilBertClassifier.presets:
             classifier = DistilBertClassifier.from_preset(
                 preset,
                 num_classes=2,
                 load_weights=load_weights,
                 preprocessor=None,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in DistilBertTokenizer.presets:
             tokenizer = DistilBertTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_tokenizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -41,15 +41,15 @@
     `tf.Tensor` with static shape `[None]`.
 
     Args:
         vocabulary: A list of strings or a string filename path. If
             passing a list, each element of the list should be a single word
             piece token string. If passing a filename, the file should be a
             plain text file containing a single word piece token per line.
-        lowercase: If true, the input text will be first lowered before
+        lowercase: If `True`, the input text will be first lowered before
             tokenization.
 
     Examples:
 
     ```python
     # Unbatched input.
     tokenizer = keras_nlp.models.DistilBertTokenizer.from_preset(
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/distil_bert/distil_bert_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_tokenizer_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,85 +7,78 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for DistilBERT tokenizer."""
+"""Tests for BERT tokenizer."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
-    DistilBertTokenizer,
-)
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class DistilBertTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class BertTokenizerTest(TestCase):
     def setUp(self):
         self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
         self.vocab += ["THE", "QUICK", "BROWN", "FOX"]
         self.vocab += ["the", "quick", "brown", "fox"]
-        self.tokenizer = DistilBertTokenizer(vocabulary=self.vocab)
+        self.tokenizer = BertTokenizer(vocabulary=self.vocab)
 
     def test_tokenize(self):
         input_data = "THE QUICK BROWN FOX."
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [5, 6, 7, 8, 1])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant(["THE QUICK BROWN FOX.", "THE FOX."])
+        input_data = ["THE QUICK BROWN FOX.", "THE FOX."]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[5, 6, 7, 8, 1], [5, 8, 1]])
 
     def test_lowercase(self):
         input_data = "THE QUICK BROWN FOX."
-        tokenizer = DistilBertTokenizer(vocabulary=self.vocab, lowercase=True)
+        tokenizer = BertTokenizer(vocabulary=self.vocab, lowercase=True)
         output = tokenizer(input_data)
         self.assertAllEqual(output, [9, 10, 11, 12, 1])
 
     def test_detokenize(self):
         input_tokens = [[5, 6, 7, 8]]
         output = self.tokenizer.detokenize(input_tokens)
         self.assertAllEqual(output, ["THE QUICK BROWN FOX"])
 
     def test_vocabulary_size(self):
         self.assertEqual(self.tokenizer.vocabulary_size(), 13)
 
     def test_errors_missing_special_tokens(self):
         with self.assertRaises(ValueError):
-            DistilBertTokenizer(vocabulary=["a", "b", "c"])
+            BertTokenizer(vocabulary=["a", "b", "c"])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["THE QUICK BROWN FOX."])
-        tokenizer = DistilBertTokenizer(vocabulary=self.vocab)
+        tokenizer = BertTokenizer(vocabulary=self.vocab)
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/deberta_v3/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_backbone.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,20 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """FNet backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.f_net_encoder import FNetEncoder
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.f_net_encoder import FNetEncoder
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.f_net.f_net_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def f_net_kernel_initializer(stddev=0.02):
     return keras.initializers.RandomNormal(stddev=stddev)
@@ -68,18 +66,16 @@
             embeddings.
         num_segments: int. The number of types that the 'segment_ids' input can
             take.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained BERT encoder.
     model = keras_nlp.models.FNetBackbone.from_preset("f_net_base_en")
     model(input_data)
 
     # Randomly initialized FNet encoder with a custom config.
@@ -139,15 +135,15 @@
         x = keras.layers.Add()(
             (token_embedding, position_embedding, segment_embedding)
         )
         x = keras.layers.LayerNormalization(
             name="embeddings_layer_norm",
             axis=-1,
             epsilon=1e-12,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
 
         x = keras.layers.Dense(
             hidden_dim,
             kernel_initializer=f_net_kernel_initializer(),
             bias_initializer=f_net_bias_initializer(),
             name="embedding_projection",
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_backbone_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,33 +13,35 @@
 # limitations under the License.
 """Test for FNet backbone model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetBackboneTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class FNetBackboneTest(TestCase):
     def setUp(self):
         self.backbone = FNetBackbone(
             vocabulary_size=10,
             num_layers=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
             num_segments=4,
         )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "segment_ids": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "segment_ids": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_valid_call_f_net(self):
@@ -47,68 +49,62 @@
 
         # Check default name passed through
         self.assertRegexpMatches(self.backbone.name, "f_net_backbone")
 
     def test_variable_sequence_length_call_f_net(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "segment_ids": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "segment_ids": ops.ones((2, seq_length), dtype="int32"),
             }
             self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, FNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(
             model_output["pooled_output"], restored_output["pooled_output"]
         )
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class FNetBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class FNetBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = FNetBackbone(
                 vocabulary_size=100,
                 num_layers=2,
                 hidden_dim=16,
                 intermediate_dim=32,
                 max_sequence_length=128,
                 num_segments=4,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "segment_ids": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((8, 128), dtype="int32"),
+            "segment_ids": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_classifier.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,23 +12,21 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """FNet classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
 from keras_nlp.src.models.f_net.f_net_backbone import f_net_kernel_initializer
 from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
 from keras_nlp.src.models.f_net.f_net_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.FNetClassifier")
 class FNetClassifier(Task):
     """An end-to-end f_net model for classification tasks.
 
@@ -47,17 +45,18 @@
 
     Args:
         backbone: A `keras_nlp.models.FNetBackbone` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.FNetPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
+        activation: Optional `str` or callable. The
             activation function to use on the model outputs. Set
             `activation="softmax"` to return output probabilities.
+            Defaults to `None`.
         hidden_dim: int. The size of the pooler layer.
         dropout: float. The dropout probability value, applied after the dense
             layer.
 
     Examples:
 
     Raw string data.
@@ -84,18 +83,16 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "segment_ids": tf.constant(
-            [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "segment_ids": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.FNetClassifier.from_preset(
         "f_net_base_en",
         num_classes=4,
@@ -133,21 +130,22 @@
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.dropout = dropout
 
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_classifier_test.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,141 +1,151 @@
-# Copyright 2023 The KerasNLP Authors
+# Copyright 2022 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for FNet classification model."""
+"""Tests for ALBERT classification model."""
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
-from keras_nlp.src.models.f_net.f_net_classifier import FNetClassifier
-from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
-from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.albert.albert_backbone import AlbertBackbone
+from keras_nlp.src.models.albert.albert_classifier import AlbertClassifier
+from keras_nlp.src.models.albert.albert_preprocessor import AlbertPreprocessor
+from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetClassifierTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertClassifierTest(TestCase):
     def setUp(self):
-        # Setup Model
+        # Setup model
+
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
-
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=12,
+            vocab_size=10,
             model_type="WORD",
-            pad_id=3,
-            unk_id=0,
-            bos_id=4,
-            eos_id=5,
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
             pad_piece="<pad>",
             unk_piece="<unk>",
             bos_piece="[CLS]",
             eos_piece="[SEP]",
             user_defined_symbols="[MASK]",
         )
-
         self.proto = bytes_io.getvalue()
 
-        self.preprocessor = FNetPreprocessor(
-            tokenizer=FNetTokenizer(proto=self.proto),
-            sequence_length=8,
+        tokenizer = AlbertTokenizer(proto=self.proto)
+
+        self.preprocessor = AlbertPreprocessor(
+            tokenizer=tokenizer,
+            sequence_length=5,
         )
-        self.backbone = FNetBackbone(
+        self.backbone = AlbertBackbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
+            num_heads=2,
+            embedding_dim=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.classifier = FNetClassifier(
+
+        self.classifier = AlbertClassifier(
             self.backbone,
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
         )
 
-        # Setup data.
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
-    def test_fnet_classifier_fit(self):
+    def test_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
-            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
+            loss="sparse_categorical_crossentropy",
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = AlbertClassifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = AlbertClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            name="test",
+            trainable=False,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saving_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
-        # Check we got the real object back.
-        self.assertIsInstance(restored_model, FNetClassifier)
+        # Check we got the real object back
+        self.assertIsInstance(restored_model, AlbertClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,26 +9,24 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
 from keras_nlp.src.models.f_net.f_net_backbone import f_net_kernel_initializer
 from keras_nlp.src.models.f_net.f_net_masked_lm_preprocessor import (
     FNetMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.f_net.f_net_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.FNetMaskedLM")
 class FNetMaskedLM(Task):
     """An end-to-end FNet model for the masked language modeling task.
 
@@ -77,21 +75,17 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create a preprocessed dataset where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "segment_ids": tf.constant(
-            [[0, 0, 0, 1, 1, 1, 0, 0]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "segment_ids": np.array([[0, 0, 0, 1, 1, 1, 0, 0]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2)
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.FNetMaskedLM.from_preset(
         "f_net_base_en",
         preprocessor=None,
@@ -130,16 +124,16 @@
         )
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return FNetBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor.py`

 * *Files 7% similar despite different names*

```diff
@@ -7,117 +7,123 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
+"""XLM-RoBERTa masked language model preprocessor layer."""
+
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
-from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
+    XLMRobertaPreprocessor,
+)
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
-@keras_nlp_export("keras_nlp.models.FNetMaskedLMPreprocessor")
-class FNetMaskedLMPreprocessor(FNetPreprocessor):
-    """FNet preprocessing for the masked language modeling task.
+@keras_nlp_export("keras_nlp.models.XLMRobertaMaskedLMPreprocessor")
+class XLMRobertaMaskedLMPreprocessor(XLMRobertaPreprocessor):
+    """XLM-RoBERTa preprocessing for the masked language modeling task.
 
     This preprocessing layer will prepare inputs for a masked language modeling
     task. It is primarily intended for use with the
-    `keras_nlp.models.FNetMaskedLM` task model. Preprocessing will occur in
+    `keras_nlp.models.XLMRobertaMaskedLM` task model. Preprocessing will occur in
     multiple steps.
 
     1. Tokenize any number of input segments using the `tokenizer`.
     2. Pack the inputs together with the appropriate `"<s>"`, `"</s>"` and
       `"<pad>"` tokens, i.e., adding a single `"<s>"` at the start of the
       entire sequence, `"</s></s>"` between each segment,
       and a `"</s>"` at the end of the entire sequence.
     3. Randomly select non-special tokens to mask, controlled by
       `mask_selection_rate`.
     4. Construct a `(x, y, sample_weight)` tuple suitable for training with a
-      `keras_nlp.models.FNetMaskedLM` task model.
+      `keras_nlp.models.XLMRobertaMaskedLM` task model.
 
     Args:
-        tokenizer: A `keras_nlp.models.FNetTokenizer` instance.
-        sequence_length: The length of the packed inputs.
-        mask_selection_rate: The probability an input token will be dynamically
-            masked.
-        mask_selection_length: The maximum number of masked tokens supported
-            by the layer.
-        mask_token_rate: float, defaults to 0.8. `mask_token_rate` must be
-            between 0 and 1 which indicates how often the mask_token is
-            substituted for tokens selected for masking.
-        random_token_rate: float, defaults to 0.1. `random_token_rate` must be
-            between 0 and 1 which indicates how often a random token is
-            substituted for tokens selected for masking. Default is 0.1.
-            Note: mask_token_rate + random_token_rate <= 1,  and for
-            (1 - mask_token_rate - random_token_rate), the token will not be
-            changed.
+        tokenizer: A `keras_nlp.models.XLMRobertaTokenizer` instance.
+        sequence_length: int. The length of the packed inputs.
         truncate: string. The algorithm to truncate a list of batched segments
             to fit within `sequence_length`. The value can be either
             `round_robin` or `waterfall`:
                 - `"round_robin"`: Available space is assigned one token at a
                     time in a round-robin fashion to the inputs that still need
                     some, until the limit is reached.
                 - `"waterfall"`: The allocation of the budget is done using a
                     "waterfall" algorithm that allocates quota in a
                     left-to-right manner and fills up the buckets until we run
                     out of budget. It supports an arbitrary number of segments.
+        mask_selection_rate: float. The probability an input token will be
+            dynamically masked.
+        mask_selection_length: int. The maximum number of masked tokens
+            in a given sample.
+        mask_token_rate: float. The probability the a selected token will be
+            replaced with the mask token.
+        random_token_rate: float. The probability the a selected token will be
+            replaced with a random token from the vocabulary. A selected token
+            will be left as is with probability
+            `1 - mask_token_rate - random_token_rate`.
+
+    Call arguments:
+        x: A tensor of single string sequences, or a tuple of multiple
+            tensor sequences to be packed together. Inputs may be batched or
+            unbatched. For single sequences, raw python inputs will be converted
+            to tensors. For multiple sequences, pass tensors directly.
+        y: Label data. Should always be `None` as the layer generates labels.
+        sample_weight: Label weights. Should always be `None` as the layer
+            generates label weights.
 
     Examples:
 
     Directly calling the layer on data.
     ```python
     # Load the preprocessor from a preset.
-    preprocessor = keras_nlp.models.FNetMaskedLMPreprocessor.from_preset(
-        "f_net_base_en"
+    preprocessor = keras_nlp.models.XLMRobertaMaskedLMPreprocessor.from_preset(
+        "xlm_roberta_base_multi"
     )
 
     # Tokenize and mask a single sentence.
     preprocessor("The quick brown fox jumped.")
-
     # Tokenize and mask a batch of single sentences.
     preprocessor(["The quick brown fox jumped.", "Call me Ishmael."])
-
     # Tokenize and mask sentence pairs.
     # In this case, always convert input to tensors before calling the layer.
     first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
     second = tf.constant(["The fox tripped.", "Oh look, a whale."])
     preprocessor((first, second))
     ```
 
     Mapping with `tf.data.Dataset`.
     ```python
-    preprocessor = keras_nlp.models.FNetMaskedLMPreprocessor.from_preset(
-        "f_net_base_en"
+    preprocessor = keras_nlp.models.XLMRobertaMaskedLMPreprocessor.from_preset(
+        "xlm_roberta_base_multi"
     )
-
     first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
     second = tf.constant(["The fox tripped.", "Oh look, a whale."])
 
     # Map single sentences.
     ds = tf.data.Dataset.from_tensor_slices(first)
     ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
 
-    # Alternatively, you can create a preprocessor from your own vocabulary.
-    vocab_data = tf.data.Dataset.from_tensor_slices(
-        ["the quick brown fox", "the earth is round"]
-    )
-
     # Map sentence pairs.
     ds = tf.data.Dataset.from_tensor_slices((first, second))
     # Watch out for tf.data's default unpacking of tuples here!
     # Best to invoke the `preprocessor` directly in this case.
     ds = ds.map(
         lambda first, second: preprocessor(x=(first, second)),
         num_parallel_calls=tf.data.AUTOTUNE,
     )
     ```
+    ```
     """
 
     def __init__(
         self,
         tokenizer,
         sequence_length=512,
         truncate="round_robin",
@@ -138,16 +144,16 @@
             mask_selection_rate=mask_selection_rate,
             mask_selection_length=mask_selection_length,
             mask_token_rate=mask_token_rate,
             random_token_rate=random_token_rate,
             vocabulary_size=tokenizer.vocabulary_size(),
             mask_token_id=tokenizer.mask_token_id,
             unselectable_token_ids=[
-                tokenizer.cls_token_id,
-                tokenizer.sep_token_id,
+                tokenizer.start_token_id,
+                tokenizer.end_token_id,
                 tokenizer.pad_token_id,
             ],
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
@@ -164,22 +170,20 @@
         if y is not None or sample_weight is not None:
             logging.warning(
                 f"{self.__class__.__name__} generates `y` and `sample_weight` "
                 "based on your input data, but your data already contains `y` "
                 "or `sample_weight`. Your `y` and `sample_weight` will be "
                 "ignored."
             )
+
         x = super().call(x)
-        token_ids, segment_ids = (
-            x["token_ids"],
-            x["segment_ids"],
-        )
+        token_ids, padding_mask = x["token_ids"], x["padding_mask"]
         masker_outputs = self.masker(token_ids)
         x = {
             "token_ids": masker_outputs["token_ids"],
-            "segment_ids": segment_ids,
+            "padding_mask": padding_mask,
             "mask_positions": masker_outputs["mask_positions"],
         }
         y = masker_outputs["mask_ids"]
         sample_weight = masker_outputs["mask_weights"]
         return pack_x_y_sample_weight(x, y, sample_weight)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_masked_lm_preprocessor_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,24 +13,24 @@
 # limitations under the License.
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.f_net.f_net_masked_lm_preprocessor import (
     FNetMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetMaskedLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class FNetMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -118,36 +118,31 @@
             x["token_ids"], [1, 5, 10, 6, 8, 2, 0, 0, 0, 0, 0, 0]
         )
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,125 +7,139 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""Tests for XLM-RoBERTa classification model."""
+
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
-from keras_nlp.src.models.f_net.f_net_masked_lm import FNetMaskedLM
-from keras_nlp.src.models.f_net.f_net_masked_lm_preprocessor import (
-    FNetMaskedLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_classifier import (
+    XLMRobertaClassifier,
+)
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
+    XLMRobertaPreprocessor,
+)
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
+    XLMRobertaTokenizer,
 )
-from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaClassifierTest(TestCase):
     def setUp(self):
-        # Setup Model.
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the slow brown fox"]
+            ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=5,
+            vocab_size=10,
             model_type="WORD",
-            pad_id=0,
+            unk_id=0,
             bos_id=1,
             eos_id=2,
-            unk_id=3,
-            pad_piece="<pad>",
-            unk_piece="<unk>",
-            bos_piece="[CLS]",
-            eos_piece="[SEP]",
-            user_defined_symbols="[MASK]",
-        )
-        self.proto = bytes_io.getvalue()
-        self.preprocessor = FNetMaskedLMPreprocessor(
-            FNetTokenizer(proto=self.proto),
+        )
+        self.preprocessor = XLMRobertaPreprocessor(
+            tokenizer=XLMRobertaTokenizer(proto=bytes_io.getvalue()),
             sequence_length=5,
-            mask_selection_length=2,
         )
-        self.backbone = FNetBackbone(
-            vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
+        self.backbone = XLMRobertaBackbone(
+            vocabulary_size=10,
             num_layers=2,
+            num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.masked_lm = FNetMaskedLM(
+        self.classifier = XLMRobertaClassifier(
             self.backbone,
+            num_classes=4,
             preprocessor=self.preprocessor,
+            # Check we handle serialization correctly.
+            activation=keras.activations.softmax,
+            hidden_dim=4,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox",
-                "the slow brown fox",
-            ]
-        )
-        self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
+        self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            self.raw_batch
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
-        self.masked_lm(self.preprocessed_batch)
+        self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
-        # self.masked_lm.predict(self.raw_batch)
-        self.masked_lm.preprocessor = None
-        self.masked_lm.predict(self.preprocessed_batch)
+        preds1 = self.classifier.predict(self.raw_batch)
+        self.classifier.preprocessor = None
+        preds2 = self.classifier.predict(self.preprocessed_batch)
+        # Assert predictions match.
+        self.assertAllClose(preds1, preds2)
+        # Assert valid softmax output.
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
     def test_classifier_fit(self):
-        self.masked_lm.fit(self.raw_dataset)
-        self.masked_lm.preprocessor = None
-        self.masked_lm.fit(self.preprocessed_dataset)
+        self.classifier.fit(self.raw_dataset)
+        self.classifier.preprocessor = None
+        self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
-        self.masked_lm.preprocessor = None
-        self.masked_lm.compile(
-            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
+        self.classifier.preprocessor = None
+        self.classifier.compile(
+            loss="sparse_categorical_crossentropy",
             jit_compile=False,
         )
-        self.masked_lm.fit(self.preprocessed_dataset)
+        self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.masked_lm)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.masked_lm.get_config(),
-        )
-
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
-        model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        # Defaults.
+        original = XLMRobertaClassifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = XLMRobertaClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            hidden_dim=4,
+            name="test",
+            trainable=False,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saving_model(self):
+        model_output = self.classifier.predict(self.raw_batch)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, FNetMaskedLM)
+        self.assertIsInstance(restored_model, XLMRobertaClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
-        self.assertAllClose(model_output, restored_output, atol=0.01, rtol=0.01)
+        self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_preprocessor.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """FNet preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.multi_segment_packer import MultiSegmentPacker
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
 from keras_nlp.src.models.f_net.f_net_presets import backbone_presets
 from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_preprocessor_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,29 +9,28 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for FNet preprocessor layer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
 from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class FNetPreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -137,34 +136,29 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_presets_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,25 +10,27 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.f_net.f_net_backbone import FNetBackbone
 from keras_nlp.src.models.f_net.f_net_classifier import FNetClassifier
 from keras_nlp.src.models.f_net.f_net_preprocessor import FNetPreprocessor
 from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class FNetPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class FNetPresetSmokeTest(TestCase):
     """
     A smoke test for FNet presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/f_net/f_net_presets_test.py --run_large`
     """
 
@@ -50,17 +52,17 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "segment_ids": tf.constant([[0, 0, 0, 0]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "segment_ids": ops.array([[0, 0, 0, 0]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = FNetBackbone.from_preset(
             "f_net_base_en", load_weights=load_weights
         )
         outputs = model(input_data)["sequence_output"]
         if load_weights:
             # The forward pass from a preset should be stable!
@@ -73,15 +75,15 @@
             # Keep a high tolerance, so we are robust to different hardware.
             self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = FNetClassifier.from_preset(
             "f_net_base_en",
             num_classes=2,
             load_weights=load_weights,
         )
         # We don't assert output values, as the head weights are random.
         model.predict(input_data)
@@ -106,15 +108,15 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("f_net_base_en_clowntown", **kwargs)
 
 
 @pytest.mark.extra_large
-class FNetPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class FNetPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every FNet preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/f_net/f_net_presets_test.py --run_extra_large`
     """
@@ -122,57 +124,53 @@
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_f_net(self, load_weights):
         for preset in FNetBackbone.presets:
             model = FNetBackbone.from_preset(preset, load_weights=load_weights)
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
-                ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_fnet_classifier(self, load_weights):
         for preset in FNetClassifier.presets:
             classifier = FNetClassifier.from_preset(
                 preset,
                 num_classes=2,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["The quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_fnet_classifier_without_preprocessing(self, load_weights):
         for preset in FNetClassifier.presets:
             classifier = FNetClassifier.from_preset(
                 preset,
                 num_classes=2,
                 preprocessor=None,
                 load_weights=load_weights,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "segment_ids": tf.constant(
-                    [0] * 200 + [1] * 312, shape=(1, 512)
-                ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "segment_ids": ops.array([0] * 200 + [1] * 312, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in FNetTokenizer.presets:
             tokenizer = FNetTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/f_net_tokenizer.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/f_net/f_net_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/albert/albert_tokenizer_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,109 +8,103 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for FNet tokenizer."""
-
+"""Tests for ALBERT tokenizer."""
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.f_net.f_net_tokenizer import FNetTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.albert.albert_tokenizer import AlbertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class FNetTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class AlbertTokenizerTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
             vocab_size=12,
             model_type="WORD",
-            pad_id=3,
-            unk_id=0,
-            bos_id=4,
-            eos_id=5,
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
             pad_piece="<pad>",
             unk_piece="<unk>",
             bos_piece="[CLS]",
             eos_piece="[SEP]",
             user_defined_symbols="[MASK]",
         )
         self.proto = bytes_io.getvalue()
 
-        self.tokenizer = FNetTokenizer(proto=self.proto)
+        self.tokenizer = AlbertTokenizer(proto=self.proto)
 
     def test_tokenize(self):
         input_data = "the quick brown fox"
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [2, 10, 6, 8])
+        self.assertAllEqual(output, [5, 10, 6, 8])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant(["the quick brown fox", "the earth is round"])
+        input_data = ["the quick brown fox", "the earth is round"]
         output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [[2, 10, 6, 8], [2, 7, 9, 11]])
+        self.assertAllEqual(output, [[5, 10, 6, 8], [5, 7, 9, 11]])
 
     def test_detokenize(self):
-        input_data = tf.constant([[2, 10, 6, 8]])
+        input_data = [[5, 10, 6, 8]]
         output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["the quick brown fox"]))
+        self.assertEqual(output, ["the quick brown fox"])
 
     def test_vocabulary_size(self):
-        tokenizer = FNetTokenizer(proto=self.proto)
+        tokenizer = AlbertTokenizer(proto=self.proto)
         self.assertEqual(tokenizer.vocabulary_size(), 12)
 
     def test_errors_missing_special_tokens(self):
         bytes_io = io.BytesIO()
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=iter(["abc"]),
             model_writer=bytes_io,
             vocab_size=5,
             pad_id=-1,
             eos_id=-1,
             bos_id=-1,
         )
         with self.assertRaises(ValueError):
-            FNetTokenizer(proto=bytes_io.getvalue())
+            AlbertTokenizer(proto=bytes_io.getvalue())
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_backbone.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,22 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """GPT-2 backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
 from tensorflow.experimental import dtensor
 from tensorflow.experimental.dtensor import Layout
+from tensorflow.keras.dtensor.experimental import LayoutMap
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.transformer_decoder import TransformerDecoder
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.transformer_decoder import TransformerDecoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.gpt2.gpt2_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def _gpt_2_kernel_initializer(stddev=0.02):
     return keras.initializers.RandomNormal(stddev=stddev)
@@ -65,18 +65,16 @@
             can consume. If `None`, `max_sequence_length` uses the value from
             sequence length. This determines the variable shape for positional
             embeddings.
 
     Example usage:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained GPT-2 decoder.
     model = keras_nlp.models.GPT2Backbone.from_preset("gpt2_base_en")
     model(input_data)
 
     # Randomly initialized GPT-2 decoder with custom config.
@@ -149,15 +147,15 @@
                 name=f"transformer_layer_{i}",
             )(x, decoder_padding_mask=padding_mask)
 
         sequence_output = keras.layers.LayerNormalization(
             name="layer_norm",
             axis=-1,
             epsilon=1e-05,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
 
         # Instantiate using Functional API Model constructor
         super().__init__(
             inputs={
                 "token_ids": token_ids,
                 "padding_mask": padding_mask,
@@ -236,15 +234,15 @@
         if len(mesh_shape) != 2:
             raise ValueError(
                 f"Expect to create layout based on 2D mesh, received {mesh}"
             )
         _, model_dim = mesh.dim_names
         unshard_dim = dtensor.UNSHARDED
 
-        layout_map = keras.dtensor.experimental.LayoutMap(mesh=mesh)
+        layout_map = LayoutMap(mesh=mesh)
         # Embedding sharding
         layout_map[r".*embeddings"] = Layout([unshard_dim, model_dim], mesh)
 
         # Transformer block sharding
         layout_map[r".*_(query|key|value)_dense.kernel"] = Layout(
             [unshard_dim, unshard_dim, model_dim], mesh
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_backbone_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,38 +13,35 @@
 # limitations under the License.
 """Test for GPT-2 backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GPT2Test(tf.test.TestCase, parameterized.TestCase):
+class GPT2Test(TestCase):
     def setUp(self):
-        # For DTensor.
-        keras.backend.experimental.enable_tf_random_generator()
-        keras.utils.set_random_seed(1337)
-
         self.backbone = GPT2Backbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
         )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "segment_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "segment_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_call(self):
         self.backbone(self.input_batch)
@@ -56,40 +53,34 @@
     def test_name(self):
         # Check default name passed through
         self.assertRegexpMatches(self.backbone.name, "gpt2_backbone")
 
     def test_variable_sequence_length(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
             self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, GPT2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
@@ -111,28 +102,28 @@
         # bridge elsewhere and must disable. See
         # https://github.com/keras-team/keras-nlp/issues/1001
         tf.config.experimental.disable_mlir_bridge()
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class GPT2BackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2BackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.model = GPT2Backbone(
                 vocabulary_size=10,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=2,
                 intermediate_dim=4,
                 max_sequence_length=5,
             )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.model.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,539 +1,517 @@
-# Copyright 2022 The KerasNLP Authors
+# Copyright 2023 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""GPT2 Causal LM (Language Model)."""
 
-import copy
+import os
+from typing import Iterable
+from typing import List
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
-from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
-    GPT2CausalLMPreprocessor,
-)
-from keras_nlp.src.models.gpt2.gpt2_presets import backbone_presets
-from keras_nlp.src.models.task import Task
-from keras_nlp.src.samplers.serialization import get as get_sampler
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tokenizers import tokenizer
 from keras_nlp.src.utils.python_utils import classproperty
-from keras_nlp.src.utils.tf_utils import tensor_to_string_list
+from keras_nlp.src.utils.python_utils import format_docstring
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
+from keras_nlp.src.utils.tensor_utils import is_string_dtype
+
+try:
+    import tensorflow_text as tf_text
+except ImportError:
+    tf_text = None
+
+# Matches whitespace and control characters.
+WHITESPACE_REGEX = r"|".join(
+    [
+        r"\s",
+        # Invisible control characters
+        r"\p{Cc}",
+        r"\p{Cf}",
+    ]
+)
 
+# Matches punctuation compatible with the original bert implementation.
+PUNCTUATION_REGEX = r"|".join(
+    [
+        # Treat all non-letter/number ASCII as punctuation.
+        # Characters such as "^", "$", and "`" are not in the Unicode
+        # Punctuation class but we treat them as punctuation anyways.
+        r"[!-/]",
+        r"[:-@]",
+        r"[\[-`]",
+        r"[{-~]",
+        # Unicode punctuation class.
+        r"[\p{P}]",
+    ]
+)
 
-@keras_nlp_export("keras_nlp.models.GPT2CausalLM")
-class GPT2CausalLM(Task):
-    """An end-to-end GPT2 model for causal langauge modeling.
-
-    A causal language model (LM) predicts the next token based on previous
-    tokens. This task setup can be used to train the model unsupervised on
-    plain text input, or to autoregressively generate plain text similar to
-    the data used for training. This task can be used for pre-training or
-    fine-tuning a GPT-2 model, simply by calling `fit()`.
-
-    This model has a `generate()` method, which generates text based on a
-    prompt. The generation strategy used is controlled by an additional
-    `sampler` argument on `compile()`. You can recompile the model with
-    different `keras_nlp.samplers` objects to control the generation. By
-    default, `"top_k"` sampling will be used.
-
-    This model can optionally be configured with a `preprocessor` layer, in
-    which case it will automatically apply preprocessing to string inputs during
-    `fit()`, `predict()`, `evaluate()` and `generate()`. This is done by default
-    when creating the model with `from_preset()`.
-
-    Disclaimer: Pre-trained models are provided on an "as is" basis, without
-    warranties or conditions of any kind. The underlying model is provided by a
-    third party and subject to a separate license, available
-    [here](https://github.com/openai/gpt-2).
+# Matches CJK characters. Obtained from
+# https://github.com/google-research/bert/blob/master/tokenization.py#L251.
+CJK_REGEX = r"|".join(
+    [
+        r"[\x{4E00}-\x{9FFF}]",
+        r"[\x{3400}-\x{4DBF}]",
+        r"[\x{20000}-\x{2A6DF}]",
+        r"[\x{2A700}-\x{2B73F}]",
+        r"[\x{2B740}-\x{2B81F}]",
+        r"[\x{2B820}-\x{2CEAF}]",
+        r"[\x{F900}-\x{FAFF}]",
+        r"[\x{2F800}-\x{2FA1F}]",
+    ]
+)
 
-    Args:
-        backbone: A `keras_nlp.models.GPT2Backbone` instance.
-        preprocessor: A `keras_nlp.models.GPT2CausalLMPreprocessor` or `None`.
-            If `None`, this model will not apply preprocessing, and inputs
-            should be preprocessed before calling the model.
+# Matches both whitespace and punctuation.
+WHITESPACE_AND_PUNCTUATION_REGEX = r"|".join(
+    [
+        WHITESPACE_REGEX,
+        PUNCTUATION_REGEX,
+    ]
+)
 
-    Examples:
+# Matches punctuation and CJK characters.
+PUNCTUATION_AND_CJK_REGEX = r"|".join(
+    [
+        PUNCTUATION_REGEX,
+        CJK_REGEX,
+    ]
+)
 
-    Use `generate()` to do text generation.
-    ```python
-    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
-    gpt2_lm.generate("I want to say", max_length=30)
-
-    # Generate with batched prompts.
-    gpt2_lm.generate(["This is a", "Where are you"], max_length=30)
-    ```
-
-    Compile the `generate()` function with a custom sampler.
-    ```python
-    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
-    gpt2_lm.compile(sampler="greedy")
-    gpt2_lm.generate("I want to say", max_length=30)
-
-    gpt2_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=2))
-    gpt2_lm.generate("I want to say", max_length=30)
-    ```
-
-    Use `generate()` without preprocessing.
-    ```python
-    # Prompt the model with `5338, 318` (the token ids for `"Who is"`).
-    # Use `"padding_mask"` to indicate values that should not be overridden.
-    prompt = {
-        "token_ids": tf.constant([[5338, 318, 0, 0, 0]] * 2),
-        "padding_mask": tf.constant([[1, 1, 0, 0, 0]] * 2),
-    }
-
-    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
-        "gpt2_base_en",
-        preprocessor=None,
-    )
-    gpt2_lm.generate(prompt)
-    ```
-
-    Call `fit()` on a single batch.
-    ```python
-    features = ["The quick brown fox jumped.", "I forgot my homework."]
-    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
-    gpt2_lm.fit(x=features, batch_size=2)
-    ```
-
-    Call `fit()` without preprocessing.
-    ```python
-    x = {
-        "token_ids": tf.constant([[50256, 1, 2, 3, 4]] * 2),
-        "padding_mask": tf.constant([[1, 1, 1, 1, 1]] * 2),
-    }
-    y = tf.constant([[1, 2, 3, 4, 50256]] * 2)
-    sw = tf.constant([[1, 1, 1, 1, 1]] * 2)
-
-    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
-        "gpt2_base_en",
-        preprocessor=None,
-    )
-    gpt2_lm.fit(x=x, y=y, sample_weight=sw, batch_size=2)
-    ```
-
-    Custom backbone and vocabulary.
-    ```python
-    features = ["a quick fox.", "a fox quick."]
-    vocab = {"<|endoftext|>": 0, "a": 4, "quick": 5, "fox": 6}
-    merges = [" q", "u i", "c k", "ui ck", "q uick"]
-    merges += [" f", "o x", "f ox"]
-
-    tokenizer = keras_nlp.models.GPT2Tokenizer(
-        vocabulary=vocab,
-        merges=merges,
-    )
-    preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor(
-        tokenizer=tokenizer,
-        sequence_length=128,
-    )
-    backbone = keras_nlp.models.GPT2Backbone(
-        vocabulary_size=30552,
-        num_layers=4,
-        num_heads=4,
-        hidden_dim=256,
-        intermediate_dim=512,
-        max_sequence_length=128,
-    )
-    gpt2_lm = keras_nlp.models.GPT2CausalLM(
-        backbone=backbone,
-        preprocessor=preprocessor,
-    )
-    gpt2_lm.fit(x=features, batch_size=2)
-    ```
-    """
+# Matches whitespace, punctuation, and CJK characters.
+WHITESPACE_PUNCTUATION_AND_CJK_REGEX = r"|".join(
+    [
+        WHITESPACE_AND_PUNCTUATION_REGEX,
+        CJK_REGEX,
+    ]
+)
 
-    def __init__(
-        self,
-        backbone,
-        preprocessor=None,
-        **kwargs,
-    ):
-        inputs = backbone.input
-        x = backbone(inputs)
-        # Use token embedding weights to project from the token representation
-        # to vocabulary logits.
-        outputs = tf.matmul(
-            x,
-            backbone.token_embedding.embeddings,
-            transpose_b=True,
-        )
 
-        # Instantiate using Functional API Model constructor.
-        super().__init__(
-            inputs=inputs,
-            outputs=outputs,
-            include_preprocessing=preprocessor is not None,
-            **kwargs,
+def pretokenize(
+    text,
+    lowercase=False,
+    strip_accents=True,
+    split=True,
+    split_on_cjk=True,
+):
+    """Helper function that takes in a dataset element and pretokenizes it.
+
+    Args:
+        text: `tf.Tensor` or `tf.RaggedTensor`. Input to be pretokenized.
+        lowercase: bool. If True, the input text will be
+            lowercased before tokenization. Defaults to `True`.
+        strip_accents: bool. If `True`, all accent marks will
+            be removed from text before tokenization. Defaults to `True`.
+        split: bool. If `True`, input will be split on
+            whitespace and punctuation marks, and all punctuation marks will be
+            kept as tokens. If `False`, input should be split ("pre-tokenized")
+            before calling the tokenizer, and passed as a dense or ragged tensor
+            of whole words. Defaults to `True`.
+        split_on_cjk: bool. If `True`, input will be split
+            on CJK characters, i.e., Chinese, Japanese, Korean and Vietnamese
+            characters (https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)).
+            Note that this is applicable only when `split` is `True`. Defaults to `True`.
+
+    Returns:
+        A tensor containing the pre-processed and pre-tokenized `text`.
+    """
+    # Check for correct types.
+    if not is_string_dtype(text.dtype):
+        raise ValueError(
+            "The dataset elements in `data` must have string dtype. "
+            f"Received: {text.dtype}."
         )
-        self.backbone = backbone
-        self.preprocessor = preprocessor
-        self.generate_function = None
-        self._sampler = None
-
-        # Default compilation
-        self.compile(
-            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
-            optimizer=keras.optimizers.Adam(2e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+    # Preprocess, lowercase, strip and split input data.
+    if text.shape.rank == 0:
+        text = tf.expand_dims(text, 0)
+    if split_on_cjk and split:
+        text = tf.strings.regex_replace(text, CJK_REGEX, r" \0 ")
+    if lowercase:
+        text = tf_text.case_fold_utf8(text)
+    if strip_accents:
+        # Normalize unicode to NFD, which splits out accent mark characters.
+        text = tf_text.normalize_utf8(text, "NFD")
+        # Remove the accent marks.
+        text = tf.strings.regex_replace(text, r"\p{Mn}", "")
+    if split:
+        if split_on_cjk:
+            split_pattern = WHITESPACE_PUNCTUATION_AND_CJK_REGEX
+            keep_split_pattern = PUNCTUATION_AND_CJK_REGEX
+        else:
+            split_pattern = WHITESPACE_AND_PUNCTUATION_REGEX
+            keep_split_pattern = PUNCTUATION_REGEX
+        text = tf_text.regex_split(
+            text,
+            delim_regex_pattern=split_pattern,
+            keep_delim_regex_pattern=keep_split_pattern,
         )
+    return text
 
-    @classproperty
-    def presets(cls):
-        return copy.deepcopy(backbone_presets)
-
-    @classproperty
-    def backbone_cls(cls):
-        return GPT2Backbone
 
-    @classproperty
-    def preprocessor_cls(cls):
-        return GPT2CausalLMPreprocessor
+@keras_nlp_export("keras_nlp.tokenizers.WordPieceTokenizer")
+class WordPieceTokenizer(tokenizer.Tokenizer):
+    """A WordPiece tokenizer layer.
+
+    This layer provides an efficient, in graph, implementation of the WordPiece
+    algorithm used by BERT and other models.
+
+    To make this layer more useful out of the box, the layer will pre-tokenize
+    the input, which will optionally lower-case, strip accents, and split the
+    input on whitespace and punctuation. Each of these pre-tokenization steps is
+    not reversible. The `detokenize` method will join words with a space, and
+    will not invert `tokenize` exactly.
+
+    If a more custom pre-tokenization step is desired, the layer can be
+    configured to apply only the strict WordPiece algorithm by passing
+    `lowercase=False`, `strip_accents=False` and `split=False`. In
+    this case, inputs should be pre-split string tensors or ragged tensors.
+
+    Tokenizer outputs can either be padded and truncated with a
+    `sequence_length` argument, or left un-truncated. The exact output will
+    depend on the rank of the input tensors.
+
+    If input is a batch of strings (rank > 0):
+    By default, the layer will output a `tf.RaggedTensor` where the last
+    dimension of the output is ragged. If `sequence_length` is set, the layer
+    will output a dense `tf.Tensor` where all inputs have been padded or
+    truncated to `sequence_length`.
+
+    If input is a scalar string (rank == 0):
+    By default, the layer will output a dense `tf.Tensor` with static shape
+    `[None]`. If `sequence_length` is set, the output will be
+    a dense `tf.Tensor` of shape `[sequence_length]`.
 
-    def call_with_cache(
-        self,
-        token_ids,
-        cache,
-        cache_index,
-    ):
-        """Forward pass of `GPT2CausalLM` with cache.
+    The output dtype can be controlled via the `dtype` argument, which should
+    be either an integer or string type.
 
-        `call_with_cache` adds an additional forward pass for the model for
-        autoregressive inference. Unlike calling the model directly, this method
-        allows caching previous key/value Tensors in multi-head attention layer,
-        and avoids recomputing the outputs of seen tokens.
+    Args:
+        vocabulary: A list of strings or a string filename path. If
+            passing a list, each element of the list should be a single
+            WordPiece token string. If passing a filename, the file should be a
+            plain text file containing a single WordPiece token per line.
+        sequence_length: int. If set, the output will be converted to a dense
+            tensor and padded/trimmed so all outputs are of sequence_length.
+        lowercase: bool. If `True`, the input text will be
+            lowercased before tokenization. Defaults to `False`.
+        strip_accents: bool. If `True`, all accent marks will
+            be removed from text before tokenization. Defaults to `False`.
+        split: bool. If `True`, input will be split on
+            whitespace and punctuation marks, and all punctuation marks will be
+            kept as tokens. If `False`, input should be split ("pre-tokenized")
+            before calling the tokenizer, and passed as a dense or ragged tensor
+            of whole words. Defaults to `True`.
+        split_on_cjk: bool. If True, input will be split
+            on CJK characters, i.e., Chinese, Japanese, Korean and Vietnamese
+            characters (https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)).
+            Note that this is applicable only when `split` is True.
+            Defaults to `True`.
+        suffix_indicator: str. The characters prepended to a
+            WordPiece to indicate that it is a suffix to another subword.
+            E.g. "##ing". Defaults to `"##"`.
+        oov_token: str. The string value to substitute for
+            an unknown token. It must be included in the vocab.
+            Defaults to `"[UNK]"`.
+
+    References:
+     - [Schuster and Nakajima, 2012](https://research.google/pubs/pub37842/)
+     - [Song et al., 2020](https://arxiv.org/abs/2012.15524)
 
-        Args:
-            token_ids: a dense int Tensor with shape `(batch_size, max_length)`.
-            cache: a dense float Tensor, the cache of key and value.
-            cache_index: int, or int Tensor. The index of current inputs in the
-                whole sequence.
-
-        Returns:
-            A (logits, hidden_states, cache) tuple. Where `logits` is the
-            language model logits for the input token_ids, `hidden_states` is
-            the final hidden representation of the input tokens, and `cache` is
-            the decoding cache.
-        """
-        token_embedding = self.backbone.get_layer("token_embedding")(token_ids)
-        position_embedding = self.backbone.get_layer("position_embedding")(
-            token_embedding, start_index=cache_index
-        )
-        x = self.backbone.get_layer("embeddings_add")(
-            (token_embedding, position_embedding)
-        )
-        x = self.backbone.get_layer("embeddings_dropout")(x)
-        # Each decoder layer has a cache; we update them separately.
-        caches = tf.unstack(cache, axis=1)
-        for i in range(self.backbone.num_layers):
-            current_cache = caches[i]
-            x, next_cache = self.backbone.get_layer(f"transformer_layer_{i}")(
-                x,
-                cache=current_cache,
-                cache_index=cache_index,
-            )
-            caches[i] = next_cache
-        cache = tf.stack(caches, axis=1)
-        x = self.backbone.get_layer("layer_norm")(x)
-        hidden_states = x
-        logits = tf.matmul(
-            hidden_states,
-            self.backbone.get_layer("token_embedding").embeddings,
-            transpose_b=True,
-        )
-        return logits, hidden_states, cache
+    Examples:
 
-    def _build_cache(self, token_ids):
-        """Build an empty cache for use with `call_with_cache()`."""
-        batch_size, max_length = tf.shape(token_ids)[0], tf.shape(token_ids)[1]
-        num_layers = self.backbone.num_layers
-        num_heads = self.backbone.num_heads
-        head_dim = self.backbone.hidden_dim // self.backbone.num_heads
-        shape = [batch_size, num_layers, 2, max_length, num_heads, head_dim]
-        cache = tf.zeros(shape, dtype=self.compute_dtype)
-        # Seed the cache.
-        _, hidden_states, cache = self.call_with_cache(token_ids, cache, 0)
-        return hidden_states, cache
+    Ragged outputs.
+    >>> vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+    >>> inputs = "The quick brown fox."
+    >>> tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
+    ...     vocabulary=vocab,
+    ...     lowercase=True,
+    ... )
+    >>> outputs = tokenizer(inputs)
+    >>> np.array(outputs)
+    array([1, 2, 3, 4, 5, 6, 7], dtype=int32)
+
+    Dense outputs.
+    >>> vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+    >>> inputs = ["The quick brown fox."]
+    >>> tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
+    ...     vocabulary=vocab,
+    ...     sequence_length=10,
+    ...     lowercase=True,
+    ... )
+    >>> outputs = tokenizer(inputs)
+    >>> np.array(outputs)
+    array([[1, 2, 3, 4, 5, 6, 7, 0, 0, 0]], dtype=int32)
+
+    String output.
+    >>> vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+    >>> inputs = "The quick brown fox."
+    >>> tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
+    ...     vocabulary=vocab,
+    ...     lowercase=True,
+    ...     dtype="string",
+    ... )
+    >>> outputs = tokenizer(inputs)
+    >>> np.array(outputs).astype("U")
+    array(['the', 'qu', '##ick', 'br', '##own', 'fox', '.'], dtype='<U5')
+
+    Detokenization.
+    >>> vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+    >>> inputs = "The quick brown fox."
+    >>> tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
+    ...     vocabulary=vocab,
+    ...     lowercase=True,
+    ... )
+    >>> outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
+    >>> np.array(outputs).astype("U")
+    array('the quick brown fox .', dtype='<U21')
+
+    Custom splitting.
+    >>> vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+    >>> inputs = "The$quick$brown$fox"
+    >>> tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
+    ...     vocabulary=vocab,
+    ...     split=False,
+    ...     lowercase=True,
+    ...     dtype='string',
+    ... )
+    >>> split_inputs = tf.strings.split(inputs, sep="$")
+    >>> outputs = tokenizer(split_inputs)
+    >>> np.array(outputs).astype("U")
+    array(['the', 'qu', '##ick', 'br', '##own', 'fox'], dtype='<U5')
+    """
 
-    def compile(
+    def __init__(
         self,
-        *args,
-        run_eagerly=False,
-        jit_compile=True,
-        sampler="top_k",
+        vocabulary=None,
+        sequence_length: int = None,
+        lowercase: bool = False,
+        strip_accents: bool = False,
+        split: bool = True,
+        split_on_cjk: bool = True,
+        suffix_indicator: str = "##",
+        oov_token: str = "[UNK]",
+        dtype="int32",
         **kwargs,
-    ):
-        xla_compatible = is_xla_compatible(self)
-        super().compile(
-            *args,
-            run_eagerly=run_eagerly,
-            # Only `jit_compile` if not eager and in a compatible environment.
-            jit_compile=jit_compile and xla_compatible and not run_eagerly,
-            **kwargs,
-        )
-        self._sampler = get_sampler(sampler)
-        # Clear the compiled generate function.
-        self.generate_function = None
-
-    def make_generate_function(self):
-        """Create or return the compiled generation function."""
-        if self.generate_function is not None:
-            return self.generate_function
+    ) -> None:
+        assert_tf_text_installed(self.__class__.__name__)
 
-        if self.run_eagerly:
-            self.generate_function = self.generate_step
-        else:
-            # `jit_compile` is a property of keras.Model after TF 2.12.
-            # Use `getattr()` for backwards compatibility.
-            jit_compile = getattr(self, "jit_compile", True)
-            self.generate_function = tf.function(
-                self.generate_step, jit_compile=jit_compile
+        if not is_integer_dtype(dtype) and not is_string_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type or a string. "
+                f"Received: dtype={dtype}"
             )
-        return self.generate_function
-
-    def generate_step(
-        self,
-        inputs,
-        end_token_id=None,
-    ):
-        """A compilable generation function for a single batch of inputs.
-
-        This function represents the inner, XLA-compilable, generation function
-        for a single batch of inputs. Inputs should have the same structure as
-        model inputs, a dictionary with keys `"token_ids"` and `"padding_mask"`.
 
-        Args:
-            inputs: A dictionary with two keys `"token_ids"` and
-                `"padding_mask"` and batched tensor values.
-            end_token_id: The id of the end token to stop on. If all
-                sequences have produced a new `end_token_id`, generation
-                will stop.
-        """
-        token_ids, padding_mask = inputs["token_ids"], inputs["padding_mask"]
-        # Create and seed cache with a single forward pass.
-        hidden_states, cache = self._build_cache(token_ids)
-        # Compute the lengths of all user inputted tokens ids.
-        row_lengths = tf.math.reduce_sum(
-            tf.cast(padding_mask, tf.int32), axis=-1
-        )
-        # Start at the first index that has no user inputted id.
-        index = tf.math.reduce_min(row_lengths)
+        super().__init__(dtype=dtype, **kwargs)
 
-        def next(prompt, cache, index):
-            # The cache index is the index of our previous token.
-            cache_index = index - 1
-            prompt = tf.slice(prompt, [0, cache_index], [-1, 1])
-            logits, hidden_states, cache = self.call_with_cache(
-                prompt,
-                cache,
-                cache_index,
+        if isinstance(vocabulary, str):
+            self.vocabulary = [
+                line.rstrip() for line in tf.io.gfile.GFile(vocabulary)
+            ]
+        elif isinstance(vocabulary, Iterable):
+            # Make a copy.
+            self.vocabulary = list(vocabulary)
+        else:
+            raise ValueError(
+                "Vocabulary must be an file path or list of terms. "
+                f"Received: vocabulary={vocabulary}"
             )
-            return (
-                tf.squeeze(logits, axis=1),
-                tf.squeeze(hidden_states, axis=1),
-                cache,
+        if oov_token is None:
+            raise ValueError("`oov_token` cannot be None.")
+
+        self.sequence_length = sequence_length
+        self.lowercase = lowercase
+        self.strip_accents = strip_accents
+        self.split = split
+        self.split_on_cjk = split_on_cjk
+        self.suffix_indicator = suffix_indicator
+        self.oov_token = oov_token
+
+        if oov_token not in self.vocabulary:
+            raise ValueError(
+                f'Cannot find `oov_token="{self.oov_token}"` in the '
+                "vocabulary.\n"
+                "You can either update the vocabulary to include "
+                f'`"{self.oov_token}"`, or pass a different value for '
+                "the `oov_token` argument when creating the tokenizer."
             )
 
-        token_ids = self._sampler(
-            next=next,
-            prompt=token_ids,
-            cache=cache,
-            index=index,
-            mask=padding_mask,
-            end_token_id=end_token_id,
-            hidden_states=hidden_states,
+        self._fast_word_piece = tf_text.FastWordpieceTokenizer(
+            vocab=self.vocabulary,
+            token_out_type=self.compute_dtype,
+            suffix_indicator=suffix_indicator,
+            unknown_token=oov_token,
+            no_pretokenization=True,
+            support_detokenization=True,
         )
 
-        # Compute an output padding mask with the token ids we updated.
-        if end_token_id is not None:
-            # Build a mask of `end_token_id` locations not in the original
-            # prompt (not in locations where `padding_mask` is True).
-            end_locations = (token_ids == end_token_id) & (~padding_mask)
-            end_locations = tf.cast(end_locations, tf.int32)
-            # Use cumsum to get ones in all locations after end_locations.
-            overflow = tf.math.cumsum(end_locations, exclusive=True, axis=-1)
-            # Our padding mask is the inverse of these overflow locations.
-            padding_mask = ~tf.cast(overflow, tf.bool)
-        else:
-            # Without early stopping, all locations will have been updated.
-            padding_mask = tf.ones_like(token_ids, dtype=tf.bool)
-        return {
-            "token_ids": token_ids,
-            "padding_mask": padding_mask,
-        }
-
-    def _normalize_generate_inputs(
-        self,
-        inputs,
-    ):
-        """Normalize user input to the generate function.
-
-        This function coverts all inputs to tensors, adds a batch dimension if
-        necessary, and returns a iterable "dataset like" object (either an
-        actual `tf.data.Dataset` or a list with a single batch element).
-        """
-        input_is_scalar = False
+    def get_vocabulary(self) -> List[str]:
+        """Get the tokenizer vocabulary as a list of strings tokens."""
+        return self.vocabulary
+
+    def vocabulary_size(self) -> int:
+        """Get the size of the tokenizer vocabulary."""
+        return len(self.vocabulary)
+
+    def id_to_token(self, id: int) -> str:
+        """Convert an integer id to a string token."""
+        if id >= self.vocabulary_size() or id < 0:
+            raise ValueError(
+                f"`id` must be in range [0, {self.vocabulary_size() - 1}]. "
+                f"Received: {id}"
+            )
+        return self.vocabulary[id]
 
-        if isinstance(inputs, tf.data.Dataset):
-            return inputs, input_is_scalar
+    def token_to_id(self, token: str) -> int:
+        """Convert a string token to an integer id."""
+        # This will be slow, but keep memory usage down compared to building a
+        # . Assuming the main use case is looking up a few special tokens
+        # early in the vocab, this should be fine.
+        return self.vocabulary.index(token)
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                # Ideally a vocabulary would be saved as a plain text asset in
+                # the saved model. We have no good way to support this
+                # currently, so we save the vocabulary in the config.
+                "vocabulary": self.vocabulary,
+                "sequence_length": self.sequence_length,
+                "lowercase": self.lowercase,
+                "strip_accents": self.strip_accents,
+                "split": self.split,
+                "suffix_indicator": self.suffix_indicator,
+                "oov_token": self.oov_token,
+            }
+        )
+        return config
 
-        if isinstance(inputs, str) or isinstance(inputs, list):
+    def tokenize(self, inputs):
+        if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):
             inputs = tf.convert_to_tensor(inputs)
 
-        if isinstance(inputs, tf.Tensor) and inputs.shape.rank == 0:
-            input_is_scalar = True
-            inputs = inputs[tf.newaxis]
-
-        # We avoid coverting to a dataset purely for speed, for a single batch
-        # of input, creating a dataset would add significant overhead.
-        return [inputs], input_is_scalar
-
-    def _normalize_generate_outputs(
-        self,
-        outputs,
-        input_is_scalar,
-    ):
-        """Normalize user output from the generate function.
+        scalar_input = inputs.shape.rank == 0
+        inputs = pretokenize(
+            inputs,
+            self.lowercase,
+            self.strip_accents,
+            self.split,
+            self.split_on_cjk,
+        )
 
-        This function converts all output to numpy (for integer output), or
-        python strings (for string output). If a batch dimension was added to
-        the input, it is removed from the output (so generate can be string in,
-        string out).
-        """
+        # Apply WordPiece and coerce shape for outputs.
+        tokens = self._fast_word_piece.tokenize(inputs)
+        # By default tf.text tokenizes text with two ragged dimensions (one for
+        # split words and one for split subwords). We will collapse to a single
+        # ragged dimension which is a better out of box default.
+        tokens = tokens.merge_dims(-2, -1)
+
+        # Convert to a dense output if `sequence_length` is set.
+        if self.sequence_length:
+            output_shape = tokens.shape.as_list()
+            output_shape[-1] = self.sequence_length
+            tokens = tokens.to_tensor(shape=output_shape)
+        # Convert to a dense output if input in scalar
+        if scalar_input:
+            tokens = tf.squeeze(tokens, 0)
+            tf.ensure_shape(tokens, shape=[self.sequence_length])
+
+        return tokens
+
+    def detokenize(self, inputs):
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
+        outputs = self._fast_word_piece.detokenize(inputs)
+        if unbatched:
+            outputs = tf.squeeze(outputs, 0)
+        return outputs
 
-        def normalize(x):
-            x = tf.concat(x, axis=0)
-            x = tf.squeeze(x, 0) if input_is_scalar else x
-            is_string = x.dtype == tf.string
-            # Convert outputs to a friendly pythonic type. For numerical outputs
-            # that is numpy, for string outputs that is `list` and `str`.
-            return tensor_to_string_list(x) if is_string else x.numpy()
-
-        if isinstance(outputs[0], dict):
-            return {
-                "token_ids": normalize([x["token_ids"] for x in outputs]),
-                "padding_mask": normalize([x["padding_mask"] for x in outputs]),
-            }
-        return normalize([x for x in outputs])
+    @classproperty
+    def presets(cls):
+        return {}
 
-    def generate(
-        self,
-        inputs,
-        max_length=None,
+    @classmethod
+    def from_preset(
+        cls,
+        preset,
+        **kwargs,
     ):
-        """Generate text given prompt `inputs`.
-
-        This method generates text based on given `inputs`. The sampling method
-        used for generation can be set in the `compile` method.
-
-        If `inputs` are a `tf.data.Dataset`, outputs will be generated
-        "batch-by-batch" and concatenated. Otherwise, all inputs will be handled
-        as a single batch.
-
-        If a `preprocessor` is attached to the model, `inputs` should be
-        strings and returned sequences will be strings. Otherwise, inputs should
-        be preprocessed before calling `generate()`, and returned sequences will
-        be token ids.
+        """Instantiate {{model_name}} tokenizer from preset vocabulary.
 
         Args:
-            inputs: a string `tf.Tensor`, a `tf.data.Dataset` of strings, a
-                python string or a list of python strings. If no `preprocessor`
-                is attached to the model, inputs should instead be a nested
-                `tf.Tensor` or `tf.data.Dataset` with keys `"token_ids"` and
-                `"padding_mask"`.
-            max_length: Optional. int. The max length of the generated sequence.
-                Will default to the max configured `sequence_length` of the
-                `preprocessor`. If `preprocessor` is `None`, `inputs` should be
-                should be padded to the desired maximum length and this argument
-                will be ignored.
-
-        Returns:
-            A string or string list if `preprocessor` is set, and a integer
-            tensor of token IDs if `preprocessor is None`.
-        """
-        # Setup our three main passes.
-        # 1. Optionally preprocessing strings to dense integer tensors.
-        # 2. Generate new tokens via a compiled function on dense tensors.
-        # 3. Optionally postprocess dense integer tensors back to string.
-        generate_function = self.make_generate_function()
-        end_token_id = None
-        if self.preprocessor is not None:
-            end_token_id = self.preprocessor.tokenizer.end_token_id
-
-        def preprocess(x):
-            return self.preprocessor.generate_preprocess(
-                x, sequence_length=max_length
-            )
+            preset: string. Must be one of "{{preset_names}}".
 
-        def generate(x):
-            return generate_function(x, end_token_id=end_token_id)
+        Examples:
+        ```python
+        # Load a preset tokenizer.
+        tokenizer = {{model_name}}.from_preset("{{example_preset_name}}")
 
-        def postprocess(x):
-            return self.preprocessor.generate_postprocess(x)
+        # Tokenize some input.
+        tokenizer("The quick brown fox tripped.")
 
-        # Normalize inputs, apply our three passes, and normalize outputs.
-        inputs, input_is_scalar = self._normalize_generate_inputs(inputs)
+        # Detokenize some input.
+        tokenizer.detokenize([5, 6, 7, 8, 9])
+        ```
+        """
 
-        if self.preprocessor is not None:
-            if isinstance(inputs, tf.data.Dataset):
-                inputs = inputs.map(preprocess, tf.data.AUTOTUNE)
-                inputs = inputs.prefetch(tf.data.AUTOTUNE)
-            else:
-                # Fast path for non-dataset, single-batch input.
-                inputs = [preprocess(x) for x in inputs]
+        if not cls.presets:
+            raise NotImplementedError(
+                "No presets have been created for this class"
+            )
 
-        outputs = [generate(x) for x in inputs]
+        if preset not in cls.presets:
+            raise ValueError(
+                "`preset` must be one of "
+                f"""{", ".join(cls.presets)}. Received: {preset}."""
+            )
+        metadata = cls.presets[preset]
 
-        if self.preprocessor is not None:
-            outputs = [postprocess(x) for x in outputs]
+        vocabulary = keras.utils.get_file(
+            "vocab.txt",
+            metadata["vocabulary_url"],
+            cache_subdir=os.path.join("models", preset),
+            file_hash=metadata["vocabulary_hash"],
+        )
 
-        return self._normalize_generate_outputs(outputs, input_is_scalar)
+        config = metadata["preprocessor_config"]
+        config.update(
+            {
+                "vocabulary": vocabulary,
+            },
+        )
 
-    @classmethod
-    def create_layout_map(cls, mesh):
-        """Create a DTensor layout map for an GPT2CausalLM.
+        return cls.from_config({**config, **kwargs})
 
-        Given a DTensor mesh describing a list of devices, this method returns a
-        DTensor layout map for creating a `keras_nlp.models.GPT2CausalLM`
-        instance. This mapping describes how to distribute all model weights
-        across multiple devices. For an overview of DTensor concepts, see
-        [this guide](https://www.tensorflow.org/guide/dtensor_overview).
+    def __init_subclass__(cls, **kwargs):
+        # Use __init_subclass__ to setup a correct docstring for from_preset.
+        super().__init_subclass__(**kwargs)
 
-        Args:
-            mesh: A 2D `tf.experimental.dtensor.Mesh` describing the arrangement
-                of devices for running distributed computation. The
-                first dimension in the mesh is expected to be for data parallel
-                distribution, and the second for model parallel distribution.
-
-        Returns:
-            A `tf.keras.dtensor.experimental.LayoutMap` which contains the
-            proper layout to weights mapping for the model parallel setting.
+        # If the subclass does not define from_preset, assign a wrapper so that
+        # each class can have a distinct docstring.
+        if "from_preset" not in cls.__dict__:
 
-        Examples:
-        ```python
-        keras.backend.experimental.enable_tf_random_generator()
-        keras.utils.set_random_seed(1337)
+            def from_preset(calling_cls, *args, **kwargs):
+                return super(cls, calling_cls).from_preset(*args, **kwargs)
 
-        # Update both dimensions below for a multi-device setting.
-        mesh = tf.experimental.dtensor.create_mesh([("batch", 1), ("model", 1)])
-        layout_map = keras_nlp.models.GPT2CausalLM.create_layout_map(mesh)
+            cls.from_preset = classmethod(from_preset)
 
-        with layout_map.scope():
-            gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
-        ```
-        """
-        # As this task has no new variables, we just re-use the backbone method.
-        return cls.backbone_cls.create_layout_map(mesh)
+        # Format and assign the docstring unless the subclass has overridden it.
+        if cls.from_preset.__doc__ is None:
+            cls.from_preset.__func__.__doc__ = (
+                WordPieceTokenizer.from_preset.__doc__
+            )
+            format_docstring(
+                model_name=cls.__name__,
+                example_preset_name=next(iter(cls.presets), ""),
+                preset_names='", "'.join(cls.presets),
+            )(cls.from_preset.__func__)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_preprocessor.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,64 +8,66 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""GPT2 Causal LM preprocessor layer."""
+"""OPT Causal LM preprocessor layer."""
 
 import tensorflow as tf
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.gpt2.gpt2_preprocessor import GPT2Preprocessor
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
-@keras_nlp_export("keras_nlp.models.GPT2CausalLMPreprocessor")
-class GPT2CausalLMPreprocessor(GPT2Preprocessor):
-    """GPT2 Causal LM preprocessor.
+@keras_nlp_export("keras_nlp.models.OPTCausalLMPreprocessor")
+class OPTCausalLMPreprocessor(OPTPreprocessor):
+    """OPT Causal LM preprocessor.
 
-    This preprocessing layer is meant for use with
-    `keras_nlp.models.GPT2CausalLM`. By default, it will take in batches of
+    This preprocessing layer is primarily meant to be used with
+    `keras_nlp.models.OPTCausalLM`. By default, it will take in batches of
     strings, and return outputs in a `(x, y, sample_weight)` format, where the
-    `y` label is the next token id in the `x` sequence.
-
-    For use with generation, the layer also exposes two methods
-    `generate_preprocess()` and `generate_postprocess()`. When this preprocessor
-    is attached to a `keras_nlp.models.GPT2CausalLM` instance, these methods
-    will be called implicitly in `generate()`. They can also be called
-    standalone (e.g. to precompute preprocessing inputs for generation in a
-    separate process).
+    `y` label is the next token id in the `x` sequence. For use with generation,
+    pass `return_labels=False` in which case the output will simply be the
+    encoded string features.
 
     Args:
-        tokenizer: A `keras_nlp.models.GPT2Tokenizer` instance.
+        tokenizer: A `keras_nlp.models.OPTTokenizer` instance.
         sequence_length: The length of the packed inputs.
-        add_start_token: If true, the preprocessor will prepend the tokenizer
+        add_start_token: If `True`, the preprocessor will prepend the tokenizer
             start token to each input sequence.
-        add_end_token: If true, the preprocessor will append the tokenizer
+        add_end_token: If `True`, the preprocessor will append the tokenizer
             end token to each input sequence.
 
     Call arguments:
         x: A string, `tf.Tensor` or list of python strings.
         y: Label data. Should always be `None` as the layer generates labels.
         sample_weight: Label weights. Should always be `None` as the layer
             generates label weights.
         sequence_length: Pass to override the configured `sequence_length` of
             the layer.
+        add_start_token: Pass to override the configured value of
+            `add_start_token` on the layer.
+        add_end_token: Pass to override the configured value of
+            `add_end_token` on the layer.
+        return_labels: If `True`, the output `"token_ids"` will be offset by one
+            and returned as labels. If `False` only features will be returned.
 
     Examples:
     ```python
     # Load the preprocessor from a preset.
-    preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
-        "gpt2_base_en"
+    preprocessor = keras_nlp.models.OPTCausalLMPreprocessor.from_preset(
+        "opt_125m_en"
     )
 
     # Tokenize and pack a single sentence.
     sentence = tf.constant("League of legends")
     preprocessor(sentence)
     # Same output.
     preprocessor("League of legends")
@@ -160,13 +162,18 @@
         """Covert integer token output to strings for generation.
 
         This method reverses `generate_preprocess()`, by first removing all
         padding and start/end tokens, and then converting the interger sequence
         back to a string.
         """
         token_ids, padding_mask = x["token_ids"], x["padding_mask"]
+        if not isinstance(token_ids, tf.Tensor):
+            token_ids = ops.convert_to_numpy(token_ids)
+        if not isinstance(padding_mask, tf.Tensor):
+            padding_mask = ops.convert_to_numpy(padding_mask)
         # Strip any special tokens during detokenization (e.g. the start and
         # end markers). In the future we could make this configurable.
         padding_mask = padding_mask & (token_ids != self.tokenizer.end_token_id)
+        padding_mask = padding_mask & (token_ids != self.tokenizer.pad_token_id)
         token_ids = tf.ragged.boolean_mask(token_ids, padding_mask)
         return self.tokenizer.detokenize(token_ids)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,30 +8,29 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for GPT2 causal LM preprocessor layer."""
-
+"""Tests for GPTNeoX causal LM preprocessor layer."""
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
-    GPT2CausalLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.gpt_neo_x.gpt_neo_x_causal_lm_preprocessor import (
+    GPTNeoXCausalLMPreprocessor,
 )
-from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.models.gpt_neo_x.gpt_neo_x_tokenizer import GPTNeoXTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GPT2CausalLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class GPTNeoXCausalLMPreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = {
             "!": 0,
             "air": 1,
             "air": 2,
             "plane": 3,
             "at": 4,
@@ -39,16 +38,16 @@
             "<|endoftext|>": 6,
         }
 
         self.merges = [" a", " t", " i", " b", "a i", "p l", "n e"]
         self.merges += ["a t", "p o", "r t", "t h", "ai r", "pl a", "po rt"]
         self.merges += ["ai r", "a i", "pla ne"]
 
-        self.preprocessor = GPT2CausalLMPreprocessor(
-            tokenizer=GPT2Tokenizer(
+        self.preprocessor = GPTNeoXCausalLMPreprocessor(
+            tokenizer=GPTNeoXTokenizer(
                 vocabulary=self.vocab,
                 merges=self.merges,
             ),
             sequence_length=8,
         )
 
     def test_strings(self):
@@ -68,16 +67,16 @@
         self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
         self.assertAllEqual(y, [[1, 3, 4, 2, 5, 6, 0, 0]] * 4)
         self.assertAllEqual(sw, [[1, 1, 1, 1, 1, 1, 0, 0]] * 4)
 
     def test_no_start_end_token(self):
         input_data = ["airplane at airport"] * 4
 
-        preprocessor = GPT2CausalLMPreprocessor(
-            tokenizer=GPT2Tokenizer(
+        preprocessor = GPTNeoXCausalLMPreprocessor(
+            tokenizer=GPTNeoXTokenizer(
                 vocabulary=self.vocab,
                 merges=self.merges,
             ),
             sequence_length=8,
             add_start_token=False,
             add_end_token=False,
         )
@@ -118,37 +117,32 @@
             "token_ids": tf.constant([6, 1, 3, 4, 2, 5, 0, 0]),
             "padding_mask": tf.cast([1, 1, 1, 1, 1, 1, 0, 0], dtype="bool"),
         }
         x = self.preprocessor.generate_postprocess(input_data)
         self.assertAllEqual(x, "airplane at airport")
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_causal_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,34 +12,30 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for GPT2 causal LM model."""
 
 import os
 from unittest.mock import patch
 
-import numpy as np
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
 from keras_nlp.src.models.gpt2.gpt2_causal_lm import GPT2CausalLM
 from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
     GPT2CausalLMPreprocessor,
 )
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GPT2CausalLMTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2CausalLMTest(TestCase):
     def setUp(self):
-        # For DTensor.
-        keras.backend.experimental.enable_tf_random_generator()
-        keras.utils.set_random_seed(1337)
-
         self.vocab = {
             "!": 0,
             "air": 1,
             "air": 2,
             "plane": 3,
             "at": 4,
             "port": 5,
@@ -61,20 +57,18 @@
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
         self.causal_lm = GPT2CausalLM(
             backbone=self.backbone,
             preprocessor=self.preprocessor,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                " airplane at airport",
-                " airplane at airport",
-            ]
-        )
+        self.raw_batch = [
+            " airplane at airport",
+            " airplane at airport",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_causal_lm(self):
@@ -122,56 +116,51 @@
 
     def test_early_stopping(self):
         call_with_cache = self.causal_lm.call_with_cache
 
         def wrapper(*args, **kwargs):
             """Modify output logits to always favor end_token_id"""
             logits, hidden_states, cache = call_with_cache(*args, **kwargs)
-            logits = np.zeros(logits.shape.as_list())
-            logits[:, :, self.preprocessor.tokenizer.end_token_id] = 1.0e9
+            index = self.preprocessor.tokenizer.end_token_id
+            update = ops.ones_like(logits)[:, :, index] * 1.0e9
+            update = ops.expand_dims(update, axis=-1)
+            logits = ops.slice_update(logits, (0, 0, index), update)
             return logits, hidden_states, cache
 
         with patch.object(self.causal_lm, "call_with_cache", wraps=wrapper):
             prompt = [" airplane at airport", " airplane"]
             output = self.causal_lm.generate(prompt)
             # We should immediately abort and output the prompt.
             self.assertEqual(prompt, output)
-            self.assertEqual(self.causal_lm.call_with_cache.call_count, 2)
 
     def test_generate_compilation(self):
         # Assert we do not recompile with successive calls.
         self.causal_lm.generate(self.raw_batch)
         first_fn = self.causal_lm.generate_function
         self.causal_lm.generate(self.raw_batch)
         second_fn = self.causal_lm.generate_function
         self.assertEqual(first_fn, second_fn)
         # Assert we do recompile after compile is called.
         self.causal_lm.compile(sampler="greedy")
         self.assertIsNone(self.causal_lm.generate_function)
 
     def test_serialization(self):
-        new_causal_lm = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.causal_lm)
+        new_causal_lm = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.causal_lm)
         )
         self.assertEqual(
             new_causal_lm.get_config(), self.causal_lm.get_config()
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         keras.utils.set_random_seed(42)
         model_output = self.causal_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.causal_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.causal_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, GPT2CausalLM)
 
         # Check that output matches.
         keras.utils.set_random_seed(42)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_preprocessor.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 """GPT2 preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.start_end_packer import StartEndPacker
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
 from keras_nlp.src.models.gpt2.gpt2_presets import backbone_presets
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
@@ -53,17 +53,17 @@
     mainly used for generation tasks. For tasks having multi-segment inputs
     like "glue/mnli", please use a model designed for classification purposes
     such as BERT or RoBERTa.
 
     Args:
         tokenizer: A `keras_nlp.models.GPT2Tokenizer` instance.
         sequence_length: The length of the packed inputs.
-        add_start_token: If true, the preprocessor will prepend the tokenizer
+        add_start_token: If `True`, the preprocessor will prepend the tokenizer
             start token to each input sequence.
-        add_end_token: If true, the preprocessor will append the tokenizer
+        add_end_token: If `True`, the preprocessor will append the tokenizer
             end token to each input sequence.
 
     Call arguments:
         x: A string, `tf.Tensor` or list of python strings.
         y: Any label data. Will be passed through unaltered.
         sample_weight: Any label weight data. Will be passed through unaltered.
         sequence_length: Pass to override the configured `sequence_length` of
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_preprocessor_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,27 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for GPT2 preprocessor layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.gpt2.gpt2_preprocessor import GPT2Preprocessor
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GPT2PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2PreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = {
             "!": 0,
             "air": 1,
             "air": 2,
             "plane": 3,
             "at": 4,
@@ -99,37 +98,32 @@
 
     def test_sequence_length_override(self):
         input_data = "airplane at airport"
         x = self.preprocessor(input_data, sequence_length=4)
         self.assertAllEqual(x["token_ids"], [6, 1, 3, 6])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_presets_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,23 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class GPT2PresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2PresetSmokeTest(TestCase):
     """
     A smoke test for GPT-2 presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/gpt2/gpt2_presets_test.py --run_large`
     """
 
@@ -37,16 +38,16 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[1169, 2068, 7586, 21831, 13]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1, 1]]),
+            "token_ids": ops.array([[1169, 2068, 7586, 21831, 13]]),
+            "padding_mask": ops.array([[1, 1, 1, 1, 1]]),
         }
         model = GPT2Backbone.from_preset(
             "gpt2_base_en", load_weights=load_weights
         )
         outputs = model(input_data)[0, 0, :5]
         if load_weights:
             # The forward pass from a preset should be stable!
@@ -74,15 +75,15 @@
     def test_unknown_preset_error(self, cls):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("gpt2_base_en_clowntown")
 
 
 @pytest.mark.extra_large
-class GPT2PresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2PresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every GPT-2 preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/gpt2/gpt2_presets_test.py --run_extra_large`
     """
@@ -90,20 +91,20 @@
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_gpt2(self, load_weights):
         for preset in GPT2Backbone.presets:
             model = GPT2Backbone.from_preset(preset, load_weights=load_weights)
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 1024),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=model.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 1024, shape=(1, 1024)),
+                "padding_mask": ops.array([1] * 1024, shape=(1, 1024)),
             }
             model(input_data)
 
     def test_load_tokenizers(self):
         for preset in GPT2Tokenizer.presets:
             tokenizer = GPT2Tokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_tokenizer.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/gpt2/gpt2_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_tokenizer_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,26 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for GPT-2 preprocessing layers."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GPT2TokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class GPT2TokenizerTest(TestCase):
     def setUp(self):
         self.vocab = {
             "<|endoftext|>": 0,
             "air": 1,
             "plane": 2,
             "at": 3,
             "port": 4,
@@ -71,15 +70,15 @@
 
     def test_tokenize_end_token(self):
         input_data = " airplane at airport<|endoftext|>"
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [1, 2, 3, 1, 4, 0])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant([" airplane at airport", " kohli is the best"])
+        input_data = [" airplane at airport", " kohli is the best"]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[1, 2, 3, 1, 4], [5, 6, 7, 8, 9]])
 
     def test_detokenize(self):
         input_tokens = [1, 2, 3, 1, 4]
         output = self.tokenizer.detokenize(input_tokens)
         self.assertEqual(output, " airplane at airport")
@@ -88,37 +87,32 @@
         self.assertEqual(self.tokenizer.vocabulary_size(), 10)
 
     def test_errors_missing_special_tokens(self):
         with self.assertRaises(ValueError):
             GPT2Tokenizer(vocabulary=["a", "b", "c"], merges=[])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/f_net/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_backbone.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,24 +12,24 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """OPT backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
 from tensorflow.experimental import dtensor
 from tensorflow.experimental.dtensor import Layout
+from tensorflow.keras.dtensor.experimental import LayoutMap
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.token_and_position_embedding import (
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.token_and_position_embedding import (
     TokenAndPositionEmbedding,
 )
-from keras_nlp.src.layers.transformer_decoder import TransformerDecoder
+from keras_nlp.src.layers.modeling.transformer_decoder import TransformerDecoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.opt.opt_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def opt_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
@@ -63,18 +63,16 @@
             can consume. If `None`, `max_sequence_length` uses the value from
             sequence length. This determines the variable shape for positional
             embeddings.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)
-        ),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained OPT decoder
     model = keras_nlp.models.OPTBackbone.from_preset("opt_125m_en")
     model(input_data)
 
     # Randomly initialized OPT decoder model with a custom config
@@ -130,15 +128,15 @@
             )(x, decoder_padding_mask=padding_mask)
 
         # Add a final layer norm.
         x = keras.layers.LayerNormalization(
             name="layer_norm",
             axis=-1,
             epsilon=1e-5,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
 
         # Instantiate using Functional API Model constructor
         super().__init__(
             inputs={
                 "token_ids": token_ids,
                 "padding_mask": padding_mask,
@@ -214,15 +212,15 @@
         if len(mesh_shape) != 2:
             raise ValueError(
                 f"Expect to create layout based on 2D mesh, received {mesh}"
             )
         _, model_dim = mesh.dim_names
         unshard_dim = dtensor.UNSHARDED
 
-        layout_map = keras.dtensor.experimental.LayoutMap(mesh=mesh)
+        layout_map = LayoutMap(mesh=mesh)
         # Embedding sharding
         layout_map[r".*embeddings"] = Layout([unshard_dim, model_dim], mesh)
 
         # Transformer block sharding
         layout_map[r".*_(query|key|value)_dense.kernel"] = Layout(
             [unshard_dim, unshard_dim, model_dim], mesh
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_backbone_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,37 +13,34 @@
 # limitations under the License.
 """Test for OPT backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.opt.opt_backbone import OPTBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class OPTTest(tf.test.TestCase, parameterized.TestCase):
+class OPTBackboneTest(TestCase):
     def setUp(self):
-        # For DTensor.
-        keras.backend.experimental.enable_tf_random_generator()
-        keras.utils.set_random_seed(1337)
-
         self.backbone = OPTBackbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
         )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_valid_call_opt(self):
@@ -56,40 +53,34 @@
     def test_name(self):
         # Check default name passed through
         self.assertRegexpMatches(self.backbone.name, "opt_backbone")
 
     def test_variable_sequence_length_call_opt(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
             self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, OPTBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
@@ -111,28 +102,28 @@
         # bridge elsewhere and must disable. See
         # https://github.com/keras-team/keras-nlp/issues/1001
         tf.config.experimental.disable_mlir_bridge()
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class OPTBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class OPTBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = OPTBackbone(
                 vocabulary_size=1000,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=32,
                 intermediate_dim=128,
                 max_sequence_length=128,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((8, 128), dtype="int32"),
+            "padding_mask": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,43 +1,53 @@
-# Copyright 2023 The KerasNLP Authors
+# Copyright 2022 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""OPT Causal LM (Language Model)."""
+"""GPT2 Causal LM (Language Model)."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.opt.opt_backbone import OPTBackbone
-from keras_nlp.src.models.opt.opt_causal_lm_preprocessor import (
-    OPTCausalLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.generative_task import GenerativeTask
+from keras_nlp.src.models.gpt2.gpt2_backbone import GPT2Backbone
+from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
+    GPT2CausalLMPreprocessor,
 )
-from keras_nlp.src.models.opt.opt_presets import backbone_presets
-from keras_nlp.src.models.task import Task
-from keras_nlp.src.samplers.serialization import get as get_sampler
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
+from keras_nlp.src.models.gpt2.gpt2_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
-from keras_nlp.src.utils.tf_utils import tensor_to_string_list
 
 
-@keras_nlp_export("keras_nlp.models.OPTCausalLM")
-class OPTCausalLM(Task):
-    """An end-to-end OPT model for causal langauge modeling.
+# TODO: Extend and factor this out into keras_nlp.layers.
+class ReverseEmbedding(keras.layers.Layer):
+    def __init__(self, embedding, **kwargs):
+        super().__init__(**kwargs)
+        self.embedding = embedding
+
+    def call(self, inputs):
+        kernel = ops.transpose(ops.convert_to_tensor(self.embedding.embeddings))
+        return ops.matmul(inputs, kernel)
+
+    def compute_output_shape(self, input_shape):
+        return (input_shape[0],) + (self.embedding.embeddings.shape[0],)
+
+
+@keras_nlp_export("keras_nlp.models.GPT2CausalLM")
+class GPT2CausalLM(GenerativeTask):
+    """An end-to-end GPT2 model for causal langauge modeling.
 
     A causal language model (LM) predicts the next token based on previous
     tokens. This task setup can be used to train the model unsupervised on
     plain text input, or to autoregressively generate plain text similar to
     the data used for training. This task can be used for pre-training or
     fine-tuning a GPT-2 model, simply by calling `fit()`.
 
@@ -51,128 +61,127 @@
     which case it will automatically apply preprocessing to string inputs during
     `fit()`, `predict()`, `evaluate()` and `generate()`. This is done by default
     when creating the model with `from_preset()`.
 
     Disclaimer: Pre-trained models are provided on an "as is" basis, without
     warranties or conditions of any kind. The underlying model is provided by a
     third party and subject to a separate license, available
-    [here](https://github.com/facebookresearch/fairseq/).
+    [here](https://github.com/openai/gpt-2).
 
     Args:
-        backbone: A `keras_nlp.models.OPTBackbone` instance.
-        preprocessor: A `keras_nlp.models.OPTCausalLMPreprocessor` or `None`.
+        backbone: A `keras_nlp.models.GPT2Backbone` instance.
+        preprocessor: A `keras_nlp.models.GPT2CausalLMPreprocessor` or `None`.
             If `None`, this model will not apply preprocessing, and inputs
             should be preprocessed before calling the model.
 
     Examples:
 
     Use `generate()` to do text generation.
     ```python
-    opt_lm = keras_nlp.models.OPTCausalLM.from_preset("opt_125m_en")
-    opt_lm.generate("I want to say", max_length=30)
+    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
+    gpt2_lm.generate("I want to say", max_length=30)
 
     # Generate with batched prompts.
-    opt_lm.generate(["This is a", "Where are you"], max_length=30)
+    gpt2_lm.generate(["This is a", "Where are you"], max_length=30)
     ```
 
     Compile the `generate()` function with a custom sampler.
     ```python
-    opt_lm = keras_nlp.models.OPTCausalLM.from_preset("opt_125m_en")
-    opt_lm.compile(sampler="greedy")
-    opt_lm.generate("I want to say", max_length=30)
+    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
+    gpt2_lm.compile(sampler="greedy")
+    gpt2_lm.generate("I want to say", max_length=30)
 
-    opt_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=2))
-    opt_lm.generate("I want to say", max_length=30)
+    gpt2_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=2))
+    gpt2_lm.generate("I want to say", max_length=30)
     ```
 
     Use `generate()` without preprocessing.
     ```python
     # Prompt the model with `5338, 318` (the token ids for `"Who is"`).
     # Use `"padding_mask"` to indicate values that should not be overridden.
     prompt = {
-        "token_ids": tf.constant([[5338, 318, 0, 0, 0]] * 2),
-        "padding_mask": tf.constant([[1, 1, 0, 0, 0]] * 2),
+        "token_ids": np.array([[5338, 318, 0, 0, 0]] * 2),
+        "padding_mask": np.array([[1, 1, 0, 0, 0]] * 2),
     }
 
-    opt_lm = keras_nlp.models.OPTCausalLM.from_preset(
-        "opt_125m_en",
+    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
+        "gpt2_base_en",
         preprocessor=None,
     )
-    opt_lm.generate(prompt)
+    gpt2_lm.generate(prompt)
     ```
 
     Call `fit()` on a single batch.
     ```python
     features = ["The quick brown fox jumped.", "I forgot my homework."]
-    opt_lm = keras_nlp.models.OPTCausalLM.from_preset("opt_125m_en")
-    opt_lm.fit(x=features, batch_size=2)
+    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
+    gpt2_lm.fit(x=features, batch_size=2)
     ```
 
     Call `fit()` without preprocessing.
     ```python
     x = {
-        "token_ids": tf.constant([[1, 2, 3, 4, 5]] * 2),
-        "padding_mask": tf.constant([[1, 1, 1, 1, 1]] * 2),
+        "token_ids": np.array([[50256, 1, 2, 3, 4]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1]] * 2),
     }
-    y = tf.constant([[2, 3, 4, 5, 0]] * 2)
-    sw = tf.constant([[1, 1, 1, 1, 1]] * 2)
+    y = np.array([[1, 2, 3, 4, 50256]] * 2)
+    sw = np.array([[1, 1, 1, 1, 1]] * 2)
 
-    opt_lm = keras_nlp.models.OPTCausalLM.from_preset(
-        "opt_base_en",
+    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
+        "gpt2_base_en",
         preprocessor=None,
     )
-    opt_lm.fit(x=x, y=y, sample_weight=sw, batch_size=2)
+    gpt2_lm.fit(x=x, y=y, sample_weight=sw, batch_size=2)
     ```
 
     Custom backbone and vocabulary.
     ```python
     features = ["a quick fox.", "a fox quick."]
     vocab = {"<|endoftext|>": 0, "a": 4, "quick": 5, "fox": 6}
     merges = [" q", "u i", "c k", "ui ck", "q uick"]
     merges += [" f", "o x", "f ox"]
 
-    tokenizer = keras_nlp.models.OPTTokenizer(
+    tokenizer = keras_nlp.models.GPT2Tokenizer(
         vocabulary=vocab,
         merges=merges,
     )
-    preprocessor = keras_nlp.models.OPTCausalLMPreprocessor(
+    preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor(
         tokenizer=tokenizer,
         sequence_length=128,
     )
-    model = keras_nlp.models.OPTBackbone(
-        vocabulary_size=50265,
+    backbone = keras_nlp.models.GPT2Backbone(
+        vocabulary_size=30552,
         num_layers=4,
         num_heads=4,
         hidden_dim=256,
         intermediate_dim=512,
         max_sequence_length=128,
     )
-    opt_lm = keras_nlp.models.OPTCausalLM(
+    gpt2_lm = keras_nlp.models.GPT2CausalLM(
         backbone=backbone,
         preprocessor=preprocessor,
     )
-    opt_lm.fit(x=features, batch_size=2)
+    gpt2_lm.fit(x=features, batch_size=2)
     ```
     """
 
     def __init__(
         self,
         backbone,
         preprocessor=None,
         **kwargs,
     ):
         inputs = backbone.input
         x = backbone(inputs)
         # Use token embedding weights to project from the token representation
         # to vocabulary logits.
-        outputs = tf.matmul(
-            x,
-            backbone.token_embedding.embeddings,
-            transpose_b=True,
-        )
+        outputs = ReverseEmbedding(
+            backbone.token_embedding,
+            name="reverse_embedding",
+        )(x)
 
         # Instantiate using Functional API Model constructor.
         super().__init__(
             inputs=inputs,
             outputs=outputs,
             include_preprocessing=preprocessor is not None,
             **kwargs,
@@ -182,126 +191,92 @@
         self.generate_function = None
         self._sampler = None
 
         # Default compilation
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(2e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def presets(cls):
         return copy.deepcopy(backbone_presets)
 
     @classproperty
     def backbone_cls(cls):
-        return OPTBackbone
+        return GPT2Backbone
 
     @classproperty
     def preprocessor_cls(cls):
-        return OPTCausalLMPreprocessor
+        return GPT2CausalLMPreprocessor
 
     def call_with_cache(
         self,
         token_ids,
         cache,
-        cache_index,
+        cache_update_index,
     ):
-        """Forward pass of `OPTCausalLM` with cache.
+        """Forward pass of `GPT2CausalLM` with cache.
 
         `call_with_cache` adds an additional forward pass for the model for
         autoregressive inference. Unlike calling the model directly, this method
         allows caching previous key/value Tensors in multi-head attention layer,
         and avoids recomputing the outputs of seen tokens.
 
         Args:
             token_ids: a dense int Tensor with shape `(batch_size, max_length)`.
             cache: a dense float Tensor, the cache of key and value.
-            cache_index: int, or int Tensor. The index of current inputs in the
+            cache_update_index: int, or int Tensor. The index of current inputs in the
                 whole sequence.
 
         Returns:
             A (logits, hidden_states, cache) tuple. Where `logits` is the
             language model logits for the input token_ids, `hidden_states` is
             the final hidden representation of the input tokens, and `cache` is
             the decoding cache.
         """
-        x = self.backbone.get_layer("embeddings")(
-            token_ids, start_index=cache_index
+        token_embedding = self.backbone.get_layer("token_embedding")(token_ids)
+        position_embedding = self.backbone.get_layer("position_embedding")(
+            token_embedding, start_index=cache_update_index
         )
+        x = self.backbone.get_layer("embeddings_add")(
+            (token_embedding, position_embedding)
+        )
+        x = self.backbone.get_layer("embeddings_dropout")(x)
         # Each decoder layer has a cache; we update them separately.
-        caches = tf.unstack(cache, axis=1)
+        caches = []
         for i in range(self.backbone.num_layers):
-            current_cache = caches[i]
+            current_cache = cache[:, i, ...]
             x, next_cache = self.backbone.get_layer(f"transformer_layer_{i}")(
                 x,
-                cache=current_cache,
-                cache_index=cache_index,
+                self_attention_cache=current_cache,
+                self_attention_cache_update_index=cache_update_index,
             )
-            caches[i] = next_cache
-        cache = tf.stack(caches, axis=1)
+            caches.append(next_cache)
+        cache = ops.stack(caches, axis=1)
         x = self.backbone.get_layer("layer_norm")(x)
         hidden_states = x
-        logits = tf.matmul(
-            hidden_states,
-            self.backbone.token_embedding.embeddings,
-            transpose_b=True,
-        )
+        logits = self.get_layer("reverse_embedding")(x)
         return logits, hidden_states, cache
 
     def _build_cache(self, token_ids):
         """Build an empty cache for use with `call_with_cache()`."""
-        batch_size, max_length = tf.shape(token_ids)[0], tf.shape(token_ids)[1]
+        batch_size = ops.shape(token_ids)[0]
+        max_length = ops.shape(token_ids)[1]
         num_layers = self.backbone.num_layers
         num_heads = self.backbone.num_heads
         head_dim = self.backbone.hidden_dim // self.backbone.num_heads
         shape = [batch_size, num_layers, 2, max_length, num_heads, head_dim]
-        cache = tf.zeros(shape, dtype=self.compute_dtype)
+        cache = ops.zeros(shape, dtype=self.compute_dtype)
         # Seed the cache.
         _, hidden_states, cache = self.call_with_cache(token_ids, cache, 0)
         return hidden_states, cache
 
-    def compile(
-        self,
-        *args,
-        run_eagerly=False,
-        jit_compile=True,
-        sampler="top_k",
-        **kwargs,
-    ):
-        xla_compatible = is_xla_compatible(self)
-        super().compile(
-            *args,
-            run_eagerly=run_eagerly,
-            # Only `jit_compile` if not eager and in a compatible environment.
-            jit_compile=jit_compile and xla_compatible and not run_eagerly,
-            **kwargs,
-        )
-        self._sampler = get_sampler(sampler)
-        # Clear the compiled generate function.
-        self.generate_function = None
-
-    def make_generate_function(self):
-        """Create or return the compiled generation function."""
-        if self.generate_function is not None:
-            return self.generate_function
-
-        if self.run_eagerly:
-            self.generate_function = self.generate_step
-        else:
-            # `jit_compile` is a property of keras.Model after TF 2.12.
-            # Use `getattr()` for backwards compatibility.
-            jit_compile = getattr(self, "jit_compile", True)
-            self.generate_function = tf.function(
-                self.generate_step, jit_compile=jit_compile
-            )
-        return self.generate_function
-
     def generate_step(
         self,
         inputs,
         end_token_id=None,
     ):
         """A compilable generation function for a single batch of inputs.
 
@@ -316,32 +291,31 @@
                 sequences have produced a new `end_token_id`, generation
                 will stop.
         """
         token_ids, padding_mask = inputs["token_ids"], inputs["padding_mask"]
         # Create and seed cache with a single forward pass.
         hidden_states, cache = self._build_cache(token_ids)
         # Compute the lengths of all user inputted tokens ids.
-        row_lengths = tf.math.reduce_sum(
-            tf.cast(padding_mask, tf.int32), axis=-1
-        )
+        row_lengths = ops.sum(ops.cast(padding_mask, "int32"), axis=-1)
         # Start at the first index that has no user inputted id.
-        index = tf.math.reduce_min(row_lengths)
+        index = ops.min(row_lengths)
 
         def next(prompt, cache, index):
             # The cache index is the index of our previous token.
-            cache_index = index - 1
-            prompt = tf.slice(prompt, [0, cache_index], [-1, 1])
+            cache_update_index = index - 1
+            batch_size = ops.shape(prompt)[0]
+            prompt = ops.slice(prompt, [0, cache_update_index], [batch_size, 1])
             logits, hidden_states, cache = self.call_with_cache(
                 prompt,
                 cache,
-                cache_index,
+                cache_update_index,
             )
             return (
-                tf.squeeze(logits, axis=1),
-                tf.squeeze(hidden_states, axis=1),
+                ops.squeeze(logits, axis=1),
+                ops.squeeze(hidden_states, axis=1),
                 cache,
             )
 
         token_ids = self._sampler(
             next=next,
             prompt=token_ids,
             cache=cache,
@@ -352,183 +326,57 @@
         )
 
         # Compute an output padding mask with the token ids we updated.
         if end_token_id is not None:
             # Build a mask of `end_token_id` locations not in the original
             # prompt (not in locations where `padding_mask` is True).
             end_locations = (token_ids == end_token_id) & (~padding_mask)
-            end_locations = tf.cast(end_locations, tf.int32)
+            end_locations = ops.cast(end_locations, "int32")
             # Use cumsum to get ones in all locations after end_locations.
-            overflow = tf.math.cumsum(end_locations, exclusive=True, axis=-1)
+            cumsum = ops.cast(ops.cumsum(end_locations, axis=-1), "int32")
+            overflow = cumsum - end_locations
             # Our padding mask is the inverse of these overflow locations.
-            padding_mask = ~tf.cast(overflow, tf.bool)
+            padding_mask = ops.logical_not(ops.cast(overflow, "bool"))
         else:
             # Without early stopping, all locations will have been updated.
-            padding_mask = tf.ones_like(token_ids, dtype=tf.bool)
+            padding_mask = ops.ones_like(token_ids, dtype="bool")
         return {
             "token_ids": token_ids,
             "padding_mask": padding_mask,
         }
 
-    def _normalize_generate_inputs(
-        self,
-        inputs,
-    ):
-        """Normalize user input to the generate function.
-
-        This function coverts all inputs to tensors, adds a batch dimension if
-        necessary, and returns a iterable "dataset like" object (either an
-        actual `tf.data.Dataset` or a list with a single batch element).
-        """
-        input_is_scalar = False
-
-        if isinstance(inputs, tf.data.Dataset):
-            return inputs, input_is_scalar
-
-        if isinstance(inputs, str) or isinstance(inputs, list):
-            inputs = tf.convert_to_tensor(inputs)
-
-        if isinstance(inputs, tf.Tensor) and inputs.shape.rank == 0:
-            input_is_scalar = True
-            inputs = inputs[tf.newaxis]
-
-        # We avoid coverting to a dataset purely for speed, for a single batch
-        # of input, creating a dataset would add significant overhead.
-        return [inputs], input_is_scalar
-
-    def _normalize_generate_outputs(
-        self,
-        outputs,
-        input_is_scalar,
-    ):
-        """Normalize user output from the generate function.
-
-        This function converts all output to numpy (for integer output), or
-        python strings (for string output). If a batch dimension was added to
-        the input, it is removed from the output (so generate can be string in,
-        string out).
-        """
-
-        def normalize(x):
-            x = tf.concat(x, axis=0)
-            x = tf.squeeze(x, 0) if input_is_scalar else x
-            is_string = x.dtype == tf.string
-            # Convert outputs to a friendly pythonic type. For numerical outputs
-            # that is numpy, for string outputs that is `list` and `str`.
-            return tensor_to_string_list(x) if is_string else x.numpy()
-
-        if isinstance(outputs[0], dict):
-            return {
-                "token_ids": normalize([x["token_ids"] for x in outputs]),
-                "padding_mask": normalize([x["padding_mask"] for x in outputs]),
-            }
-        return normalize([x for x in outputs])
-
-    def generate(
-        self,
-        inputs,
-        max_length=None,
-    ):
-        """Generate text given prompt `inputs`.
-
-        This method generates text based on given `inputs`. The sampling method
-        used for generation can be set in the `compile` method.
-
-        If `inputs` are a `tf.data.Dataset`, outputs will be generated
-        "batch-by-batch" and concatenated. Otherwise, all inputs will be handled
-        as a single batch.
-
-        If a `preprocessor` is attached to the model, `inputs` should be
-        strings and returned sequences will be strings. Otherwise, inputs should
-        be preprocessed before calling `generate()`, and returned sequences will
-        be token ids.
-
-        Args:
-            inputs: a string `tf.Tensor`, a `tf.data.Dataset` of strings, a
-                python string or a list of python strings. If no `preprocessor`
-                is attached to the model, inputs should instead be a nested
-                `tf.Tensor` or `tf.data.Dataset` with keys `"token_ids"` and
-                `"padding_mask"`.
-            max_length: Optional. int. The max length of the generated sequence.
-                Will default to the max configured `sequence_length` of the
-                `preprocessor`. If `preprocessor` is `None`, `inputs` should be
-                should be padded to the desired maximum length and this argument
-                will be ignored.
-
-        Returns:
-            A string or string list if `preprocessor` is set, and a integer
-            tensor of token IDs if `preprocessor is None`.
-        """
-        # Setup our three main passes.
-        # 1. Optionally preprocessing strings to dense integer tensors.
-        # 2. Generate new tokens via a compiled function on dense tensors.
-        # 3. Optionally postprocess dense integer tensors back to string.
-        generate_function = self.make_generate_function()
-        end_token_id = None
-        if self.preprocessor is not None:
-            end_token_id = self.preprocessor.tokenizer.end_token_id
-
-        def preprocess(x):
-            return self.preprocessor.generate_preprocess(
-                x, sequence_length=max_length
-            )
-
-        def generate(x):
-            return generate_function(x, end_token_id=end_token_id)
-
-        def postprocess(x):
-            return self.preprocessor.generate_postprocess(x)
-
-        # Normalize inputs, apply our three passes, and normalize outputs.
-        inputs, input_is_scalar = self._normalize_generate_inputs(inputs)
-
-        if self.preprocessor is not None:
-            if isinstance(inputs, tf.data.Dataset):
-                inputs = inputs.map(preprocess, tf.data.AUTOTUNE)
-                inputs = inputs.prefetch(tf.data.AUTOTUNE)
-            else:
-                # Fast path for non-dataset, single-batch input.
-                inputs = [preprocess(x) for x in inputs]
-
-        outputs = [generate(x) for x in inputs]
-
-        if self.preprocessor is not None:
-            outputs = [postprocess(x) for x in outputs]
-
-        return self._normalize_generate_outputs(outputs, input_is_scalar)
-
     @classmethod
     def create_layout_map(cls, mesh):
-        """Create a DTensor layout map for an OPTCausalLM.
+        """Create a DTensor layout map for an GPT2CausalLM.
 
         Given a DTensor mesh describing a list of devices, this method returns a
-        DTensor layout map for creating a `keras_nlp.models.OPTCausalLM`
+        DTensor layout map for creating a `keras_nlp.models.GPT2CausalLM`
         instance. This mapping describes how to distribute all model weights
         across multiple devices. For an overview of DTensor concepts, see
         [this guide](https://www.tensorflow.org/guide/dtensor_overview).
 
         Args:
             mesh: A 2D `tf.experimental.dtensor.Mesh` describing the arrangement
                 of devices for running distributed computation. The
                 first dimension in the mesh is expected to be for data parallel
                 distribution, and the second for model parallel distribution.
 
         Returns:
-            A `tf.keras.dtensor.experimental.LayoutMap` which contains the
+            A `keras.dtensor.experimental.LayoutMap` which contains the
             proper layout to weights mapping for the model parallel setting.
 
         Examples:
         ```python
         keras.backend.experimental.enable_tf_random_generator()
         keras.utils.set_random_seed(1337)
 
         # Update both dimensions below for a multi-device setting.
         mesh = tf.experimental.dtensor.create_mesh([("batch", 1), ("model", 1)])
-        layout_map = keras_nlp.models.OPTCausalLM.create_layout_map(mesh)
+        layout_map = keras_nlp.models.GPT2CausalLM.create_layout_map(mesh)
 
         with layout_map.scope():
-            opt_lm = keras_nlp.models.OPTCausalLM.from_preset("opt_125m_en")
+            gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset("gpt2_base_en")
         ```
         """
         # As this task has no new variables, we just re-use the backbone method.
         return cls.backbone_cls.create_layout_map(mesh)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,106 +8,74 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""OPT Causal LM preprocessor layer."""
+"""GPTNeoX Causal LM preprocessor layer."""
 
 import tensorflow as tf
 from absl import logging
 
-from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.gpt_neo_x.gpt_neo_x_preprocessor import (
+    GPTNeoXPreprocessor,
+)
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
-@keras_nlp_export("keras_nlp.models.OPTCausalLMPreprocessor")
-class OPTCausalLMPreprocessor(OPTPreprocessor):
-    """OPT Causal LM preprocessor.
+@keras.saving.register_keras_serializable(package="keras_nlp")
+class GPTNeoXCausalLMPreprocessor(GPTNeoXPreprocessor):
+    """GPTNeoX Causal LM preprocessor.
 
-    This preprocessing layer is primarily meant to be used with
-    `keras_nlp.models.OPTCausalLM`. By default, it will take in batches of
+    This preprocessing layer is meant for use with
+    `keras_nlp.models.GPTNeoXCausalLM`. By default, it will take in batches of
     strings, and return outputs in a `(x, y, sample_weight)` format, where the
-    `y` label is the next token id in the `x` sequence. For use with generation,
-    pass `return_labels=False` in which case the output will simply be the
-    encoded string features.
+    `y` label is the next token id in the `x` sequence.
+
+    For use with generation, the layer also exposes two methods
+    `generate_preprocess()` and `generate_postprocess()`. When this preprocessor
+    is attached to a `keras_nlp.models.GPTNeoXCausalLM` instance, these methods
+    will be called implicitly in `generate()`. They can also be called
+    standalone (e.g. to precompute preprocessing inputs for generation in a
+    separate process).
 
     Args:
-        tokenizer: A `keras_nlp.models.OPTTokenizer` instance.
+        tokenizer: A `keras_nlp.models.GPTNeoXTokenizer` instance.
         sequence_length: The length of the packed inputs.
-        add_start_token: If true, the preprocessor will prepend the tokenizer
+        add_start_token: If `True`, the preprocessor will prepend the tokenizer
             start token to each input sequence.
-        add_end_token: If true, the preprocessor will append the tokenizer
+        add_end_token: If `True`, the preprocessor will append the tokenizer
             end token to each input sequence.
 
     Call arguments:
         x: A string, `tf.Tensor` or list of python strings.
         y: Label data. Should always be `None` as the layer generates labels.
         sample_weight: Label weights. Should always be `None` as the layer
             generates label weights.
         sequence_length: Pass to override the configured `sequence_length` of
             the layer.
-        add_start_token: Pass to override the configured value of
-            `add_start_token` on the layer.
-        add_end_token: Pass to override the configured value of
-            `add_end_token` on the layer.
-        return_labels: If `True`, the output `"token_ids"` will be offset by one
-            and returned as labels. If `False` only features will be returned.
-
-    Examples:
-    ```python
-    # Load the preprocessor from a preset.
-    preprocessor = keras_nlp.models.OPTCausalLMPreprocessor.from_preset(
-        "opt_125m_en"
-    )
-
-    # Tokenize and pack a single sentence.
-    sentence = tf.constant("League of legends")
-    preprocessor(sentence)
-    # Same output.
-    preprocessor("League of legends")
-
-    # Tokenize a batch of sentences.
-    sentences = tf.constant(["Taco tuesday", "Fish taco please!"])
-    preprocessor(sentences)
-    # Same output.
-    preprocessor(["Taco tuesday", "Fish taco please!"])
-
-    # Map a dataset to preprocess a single sentence.
-    features = tf.constant(
-        [
-            "Avatar 2 is amazing!",
-            "Well, I am not sure.",
-        ]
-    )
-    labels = tf.constant([1, 0])
-    ds = tf.data.Dataset.from_tensor_slices((features, labels))
-    ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
-
-    # Map a dataset to preprocess unlabled sentences.
-    ds = tf.data.Dataset.from_tensor_slices(features)
-    ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
-    ```
+
     """
 
     def call(
         self,
         x,
         y=None,
         sample_weight=None,
         sequence_length=None,
     ):
         if y is not None or sample_weight is not None:
             logging.warning(
-                "`GPT2CausalLMPreprocessor` generates `y` and `sample_weight` "
+                "`GPTNeoXCausalLMPreprocessor` generates `y` and `sample_weight` "
                 "based on your input data, but your data already contains `y` "
                 "or `sample_weight`. Your `y` and `sample_weight` will be "
                 "ignored."
             )
         sequence_length = sequence_length or self.sequence_length
 
         x = convert_inputs_to_list_of_tensor_segments(x)[0]
@@ -157,17 +125,21 @@
     def generate_postprocess(
         self,
         x,
     ):
         """Covert integer token output to strings for generation.
 
         This method reverses `generate_preprocess()`, by first removing all
-        padding and start/end tokens, and then converting the interger sequence
+        padding and start/end tokens, and then converting the integer sequence
         back to a string.
         """
         token_ids, padding_mask = x["token_ids"], x["padding_mask"]
+        if not isinstance(token_ids, tf.Tensor):
+            token_ids = ops.convert_to_numpy(token_ids)
+        if not isinstance(padding_mask, tf.Tensor):
+            padding_mask = ops.convert_to_numpy(padding_mask)
         # Strip any special tokens during detokenization (e.g. the start and
         # end markers). In the future we could make this configurable.
         padding_mask = padding_mask & (token_ids != self.tokenizer.end_token_id)
         token_ids = tf.ragged.boolean_mask(token_ids, padding_mask)
         return self.tokenizer.detokenize(token_ids)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_preprocessor_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,29 +9,28 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for OPT causal LM preprocessor layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.opt.opt_causal_lm_preprocessor import (
     OPTCausalLMPreprocessor,
 )
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class OPTCausalLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class OPTCausalLMPreprocessorTest(TestCase):
     def setUp(self):
         self.vocab = {
             "<pad>": 0,
             "</s>": 1,
             "air": 2,
             "air": 3,
             "plane": 4,
@@ -120,37 +119,32 @@
             "token_ids": tf.constant([1, 3, 4, 5, 3, 6, 0, 0]),
             "padding_mask": tf.cast([1, 1, 1, 1, 1, 1, 0, 0], dtype="bool"),
         }
         x = self.preprocessor.generate_postprocess(input_data)
         self.assertAllEqual(x, " airplane at airport")
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_causal_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_causal_lm_test.py`

 * *Files 16% similar despite different names*

```diff
@@ -12,34 +12,30 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for OPT causal LM model."""
 
 import os
 from unittest.mock import patch
 
-import numpy as np
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.opt.opt_backbone import OPTBackbone
 from keras_nlp.src.models.opt.opt_causal_lm import OPTCausalLM
 from keras_nlp.src.models.opt.opt_causal_lm_preprocessor import (
     OPTCausalLMPreprocessor,
 )
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class OPTCausalLMTest(tf.test.TestCase, parameterized.TestCase):
+class OPTCausalLMTest(TestCase):
     def setUp(self):
-        # For DTensor.
-        keras.backend.experimental.enable_tf_random_generator()
-        keras.utils.set_random_seed(1337)
-
         self.vocab = {
             "<pad>": 0,
             "</s>": 1,
             "air": 2,
             "plane": 3,
             "at": 4,
             "port": 5,
@@ -67,20 +63,18 @@
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
         self.causal_lm = OPTCausalLM(
             backbone=self.backbone,
             preprocessor=self.preprocessor,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                " airplane at airport",
-                " airplane at airport",
-            ]
-        )
+        self.raw_batch = [
+            " airplane at airport",
+            " airplane at airport",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_causal_lm(self):
@@ -128,56 +122,51 @@
 
     def test_early_stopping(self):
         call_with_cache = self.causal_lm.call_with_cache
 
         def wrapper(*args, **kwargs):
             """Modify output logits to always favor end_token_id"""
             logits, hidden_states, cache = call_with_cache(*args, **kwargs)
-            logits = np.zeros(logits.shape.as_list())
-            logits[:, :, self.preprocessor.tokenizer.end_token_id] = 1.0e9
+            index = self.preprocessor.tokenizer.end_token_id
+            update = ops.ones_like(logits)[:, :, index] * 1.0e9
+            update = ops.expand_dims(update, axis=-1)
+            logits = ops.slice_update(logits, (0, 0, index), update)
             return logits, hidden_states, cache
 
         with patch.object(self.causal_lm, "call_with_cache", wraps=wrapper):
             prompt = [" airplane at airport", " airplane"]
             output = self.causal_lm.generate(prompt)
             # We should immediately abort and output the prompt.
             self.assertEqual(prompt, output)
-            self.assertEqual(self.causal_lm.call_with_cache.call_count, 2)
 
     def test_generate_compilation(self):
         # Assert we do not recompile with successive calls.
         self.causal_lm.generate(self.raw_batch)
         first_fn = self.causal_lm.generate_function
         self.causal_lm.generate(self.raw_batch)
         second_fn = self.causal_lm.generate_function
         self.assertEqual(first_fn, second_fn)
         # Assert we do recompile after compile is called.
         self.causal_lm.compile(sampler="greedy")
         self.assertIsNone(self.causal_lm.generate_function)
 
     def test_serialization(self):
-        new_causal_lm = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.causal_lm)
+        new_causal_lm = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.causal_lm)
         )
         self.assertEqual(
             new_causal_lm.get_config(), self.causal_lm.get_config()
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         keras.utils.set_random_seed(42)
         model_output = self.causal_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.causal_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.causal_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, OPTCausalLM)
 
         # Check that output matches.
         keras.utils.set_random_seed(42)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_preprocessor.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 """OPT preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.start_end_packer import StartEndPacker
+from keras_nlp.src.layers.preprocessing.start_end_packer import StartEndPacker
 from keras_nlp.src.models.opt.opt_presets import backbone_presets
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
@@ -53,17 +53,17 @@
     mainly used for generation tasks. For tasks having multi-segment inputs
     like "glue/mnli", please use a model designed for classification purposes
     such as BERT or RoBERTa.
 
     Args:
         tokenizer: A `keras_nlp.models.OPTTokenizer` instance.
         sequence_length: The length of the packed inputs.
-        add_start_token: If true, the preprocessor will append the tokenizer
+        add_start_token: If `True`, the preprocessor will append the tokenizer
             start token to each input sequence.
-        add_end_token: If true, the preprocessor will append the tokenizer
+        add_end_token: If `True`, the preprocessor will append the tokenizer
             end token to each input sequence.
 
     Call arguments:
         x: A string, `tf.Tensor` or list of python strings.
         y: Any label data. Will be passed through unaltered.
         sample_weight: Any label weight data. Will be passed through unaltered.
         sequence_length: Pass to override the configured `sequence_length` of
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor_test.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,137 +1,167 @@
-# Copyright 2023 The KerasNLP Authors
+# Copyright 2022 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for OPT preprocessor layer."""
-
+"""Tests for RoBERTa masked language model preprocessor layer."""
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.opt.opt_preprocessor import OPTPreprocessor
-from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import (
+    RobertaMaskedLMPreprocessor,
+)
+from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class OPTPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
-        self.vocab = {
-            "<pad>": 0,
-            "</s>": 1,
-            "air": 2,
+        vocab = {
+            "<s>": 0,
+            "<pad>": 1,
+            "</s>": 2,
             "air": 3,
             "plane": 4,
             "at": 5,
             "port": 6,
+            "koh": 7,
+            "li": 8,
+            "is": 9,
+            "the": 10,
+            "best": 11,
+            "<mask>": 12,
         }
 
         merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
         merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
         merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
         merges += ["pla ne"]
-        self.merges = merges
 
-        self.preprocessor = OPTPreprocessor(
-            tokenizer=OPTTokenizer(
-                vocabulary=self.vocab,
-                merges=self.merges,
+        self.preprocessor = RobertaMaskedLMPreprocessor(
+            tokenizer=RobertaTokenizer(
+                vocabulary=vocab,
+                merges=merges,
             ),
-            sequence_length=8,
+            # Simplify out testing by masking every available token.
+            mask_selection_rate=1.0,
+            mask_token_rate=1.0,
+            random_token_rate=0.0,
+            mask_selection_length=5,
+            sequence_length=12,
         )
 
-    def test_tokenize_strings(self):
+    def test_preprocess_strings(self):
         input_data = " airplane at airport"
 
-        x = self.preprocessor(input_data)
-        self.assertAllEqual(x["token_ids"], [1, 3, 4, 5, 3, 6, 1, 0])
-        self.assertAllEqual(x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
-
-    def test_tokenize_list_of_strings(self):
-        input_data = [" airplane at airport"] * 4
-
-        x = self.preprocessor(input_data)
-        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
-        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        x, y, sw = self.preprocessor(input_data)
+        self.assertAllEqual(
+            x["token_ids"], [0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]
+        )
+        self.assertAllEqual(
+            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
+        )
+        self.assertAllEqual(x["mask_positions"], [1, 2, 3, 4, 5])
+        self.assertAllEqual(y, [3, 4, 5, 3, 6])
+        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 1.0, 1.0])
 
-    def test_no_start_end_token(self):
+    def test_preprocess_list_of_strings(self):
         input_data = [" airplane at airport"] * 4
 
-        preprocessor = OPTPreprocessor(
-            tokenizer=OPTTokenizer(
-                vocabulary=self.vocab,
-                merges=self.merges,
-            ),
-            sequence_length=8,
-            add_start_token=False,
-            add_end_token=False,
-        )
-        x = preprocessor(input_data)
-        self.assertAllEqual(x["token_ids"], [[3, 4, 5, 3, 6, 0, 0, 0]] * 4)
-        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0]] * 4)
-
-    def test_tokenize_labeled_batch(self):
-        x = tf.constant([" airplane at airport"] * 4)
-        y_in = tf.constant([1] * 4)
-        sw_in = tf.constant([1.0] * 4)
-        x, y, sw = self.preprocessor(x, y_in, sw_in)
-        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
-        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
-        self.assertAllEqual(y, y_in)
-        self.assertAllEqual(sw, sw_in)
-
-    def test_tokenize_labeled_dataset(self):
-        x = tf.constant([" airplane at airport"] * 4)
-        ds = tf.data.Dataset.from_tensor_slices(x)
+        x, y, sw = self.preprocessor(input_data)
+        self.assertAllEqual(
+            x["token_ids"], [[0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]] * 4
+        )
+        self.assertAllEqual(
+            x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]] * 4
+        )
+        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 4, 5]] * 4)
+        self.assertAllEqual(y, [[3, 4, 5, 3, 6]] * 4)
+        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 1.0, 1.0]] * 4)
+
+    def test_preprocess_dataset(self):
+        sentences = tf.constant([" airplane at airport"] * 4)
+        ds = tf.data.Dataset.from_tensor_slices(sentences)
         ds = ds.map(self.preprocessor)
-        x = ds.batch(4).take(1).get_single_element()
-        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 5, 3, 6, 1, 0]] * 4)
-        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        x, y, sw = ds.batch(4).take(1).get_single_element()
+        self.assertAllEqual(
+            x["token_ids"], [[0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]] * 4
+        )
+        self.assertAllEqual(
+            x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]] * 4
+        )
+        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 4, 5]] * 4)
+        self.assertAllEqual(y, [[3, 4, 5, 3, 6]] * 4)
+        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 1.0, 1.0]] * 4)
+
+    def test_mask_multiple_sentences(self):
+        sentence_one = tf.constant(" airplane")
+        sentence_two = tf.constant(" kohli")
 
-    def test_sequence_length_override(self):
+        x, y, sw = self.preprocessor((sentence_one, sentence_two))
+        self.assertAllEqual(
+            x["token_ids"], [0, 12, 12, 2, 2, 12, 12, 2, 1, 1, 1, 1]
+        )
+        self.assertAllEqual(
+            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
+        )
+        self.assertAllEqual(x["mask_positions"], [1, 2, 5, 6, 0])
+        self.assertAllEqual(y, [3, 4, 7, 8, 0])
+        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 1.0, 0.0])
+
+    def test_no_masking_zero_rate(self):
+        no_mask_preprocessor = RobertaMaskedLMPreprocessor(
+            self.preprocessor.tokenizer,
+            mask_selection_rate=0.0,
+            mask_selection_length=5,
+            sequence_length=12,
+        )
         input_data = " airplane at airport"
-        x = self.preprocessor(input_data, sequence_length=4)
-        self.assertAllEqual(x["token_ids"], [1, 3, 4, 1])
+
+        x, y, sw = no_mask_preprocessor(input_data)
+        self.assertAllEqual(
+            x["token_ids"], [0, 3, 4, 5, 3, 6, 2, 1, 1, 1, 1, 1]
+        )
+        self.assertAllEqual(
+            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
+        )
+        self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0, 0])
+        self.assertAllEqual(y, [0, 0, 0, 0, 0])
+        self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        self.assertAllEqual(
-            model(input_data)["token_ids"],
-            restored_model(input_data)["token_ids"],
-        )
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
+        self.assertAllEqual(outputs, restored_outputs)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_presets_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,23 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.opt.opt_backbone import OPTBackbone
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class OPTPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class OPTPresetSmokeTest(TestCase):
     """
     A smoke test for GPT-2 presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/opt/opt_presets_test.py --run_large`
     """
 
@@ -37,16 +38,16 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[133, 2119, 6219, 23602, 4]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1, 1]]),
+            "token_ids": ops.array([[133, 2119, 6219, 23602, 4]]),
+            "padding_mask": ops.array([[1, 1, 1, 1, 1]]),
         }
         model = OPTBackbone.from_preset(
             "opt_125m_en", load_weights=load_weights
         )
         outputs = model(input_data)[0, 0, :5]
         if load_weights:
             # The forward pass from a preset should be stable!
@@ -74,15 +75,15 @@
     def test_unknown_preset_error(self, cls):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("opt_clowntown")
 
 
 @pytest.mark.extra_large
-class OPTPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class OPTPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every GPT-2 preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/opt/opt_presets_test.py --run_extra_large`
     """
@@ -90,20 +91,20 @@
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_opt(self, load_weights):
         for preset in OPTBackbone.presets:
             model = OPTBackbone.from_preset(preset, load_weights=load_weights)
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 1024),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=model.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 1024, shape=(1, 1024)),
+                "padding_mask": ops.array([1] * 1024, shape=(1, 1024)),
             }
             model(input_data)
 
     def test_load_tokenizers(self):
         for preset in OPTTokenizer.presets:
             tokenizer = OPTTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_tokenizer.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,116 +8,111 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""OPT tokenizer."""
+"""RoBERTa tokenizer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.opt.opt_presets import backbone_presets
+from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
 from keras_nlp.src.tokenizers.byte_pair_tokenizer import BytePairTokenizer
 from keras_nlp.src.utils.python_utils import classproperty
 
 
-@keras_nlp_export("keras_nlp.models.OPTTokenizer")
-class OPTTokenizer(BytePairTokenizer):
-    """An OPT tokenizer using Byte-Pair Encoding subword segmentation.
+@keras_nlp_export("keras_nlp.models.RobertaTokenizer")
+class RobertaTokenizer(BytePairTokenizer):
+    """A RoBERTa tokenizer using Byte-Pair Encoding subword segmentation.
 
     This tokenizer class will tokenize raw strings into integer sequences and
     is based on `keras_nlp.tokenizers.BytePairTokenizer`. Unlike the
-    underlying tokenizer, it will check for all special tokens needed by OPT
+    underlying tokenizer, it will check for all special tokens needed by RoBERTa
     models and provides a `from_preset()` method to automatically download
-    a matching vocabulary for a OPT preset.
+    a matching vocabulary for a RoBERTa preset.
 
-    This tokenizer does not provide truncation or padding of inputs.
+    This tokenizer does not provide truncation or padding of inputs. It can be
+    combined with a `keras_nlp.models.RobertaPreprocessor` layer for input
+    packing.
 
     If input is a batch of strings (rank > 0), the layer will output a
     `tf.RaggedTensor` where the last dimension of the output is ragged.
+
     If input is a scalar string (rank == 0), the layer will output a dense
     `tf.Tensor` with static shape `[None]`.
 
     Args:
-        vocabulary: string or dict, maps token to integer ids. If it is a
-            string, it should be the file path to a json file.
-        merges: string or list, contains the merge rule. If it is a string,
-            it should be the file path to merge rules. The merge rule file
-            should have one merge rule per line. Every merge rule contains
-            merge entities separated by a space.
+        vocabulary: A dictionary mapping tokens to integer ids, or file path
+            to a json file containing the token to id mapping.
+        merges: A list of merge rules or a string file path, If passing a file
+            path. the file should have one merge rule per line. Every merge
+            rule contains merge entities separated by a space.
 
     Examples:
-
-    Batched inputs.
-    >>> vocab = {"<pad>": 1, "</s>": 2, "a": 3, "quick": 4, "fox": 5}
-    >>> merges = [" q", "u i", "c k", "ui ck", "q uick"]
-    >>> merges += [" f", "o x", "f ox"]
-    >>> tokenizer = keras_nlp.models.OPTTokenizer(
-    ...     vocabulary=vocab,
-    ...     merges=merges,
-    ... )
-    >>> tokenizer(["a quick fox", "a fox quick"])
-    <tf.RaggedTensor [[3, 4, 5], [3, 5, 4]]>
-
-    Unbatched input.
-    >>> vocab = {"<pad>": 1, "</s>": 2, "a": 3, "quick": 4, "fox": 5}
-    >>> merges = [" q", "u i", "c k", "ui ck", "q uick"]
-    >>> merges += [" f", "o x", "f ox"]
-    >>> tokenizer = keras_nlp.models.OPTTokenizer(
-    ...     vocabulary=vocab,
-    ...     merges=merges,
-    ... )
-    >>> tokenizer("a quick fox")
-    <tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 4, 5], dtype=int32)>
-
-    Detokenization.
-    >>> vocab = {"<pad>": 1, "</s>": 2, "quick": 4, "fox": 5}
-    >>> merges = [" q", "u i", "c k", "ui ck", "q uick"]
-    >>> merges += [" f", "o x", "f ox"]
-    >>> tokenizer = keras_nlp.models.OPTTokenizer(
-    ...     vocabulary=vocab,
-    ...     merges=merges,
-    ... )
-    >>> tokenizer.detokenize(tokenizer(" quick fox")).numpy().decode('utf-8')
-    ' quick fox'
+    ```python
+    # Unbatched input.
+    tokenizer = keras_nlp.models.RobertaTokenizer.from_preset(
+        "roberta_base_en",
+    )
+    tokenizer("The quick brown fox jumped.")
+
+    # Batched input.
+    tokenizer(["The quick brown fox jumped.", "The fox slept."])
+
+    # Detokenization.
+    tokenizer.detokenize(tokenizer("The quick brown fox jumped."))
+
+    # Custom vocabulary.
+    # Note: '' is space
+    vocab = {"<s>": 0, "<pad>": 1, "</s>": 2, "<mask>": 3}
+    vocab = {**vocab, "a": 4, "quick": 5, "fox": 6}
+    merges = [" q", "u i", "c k", "ui ck", "q uick"]
+    merges += [" f", "o x", "f ox"]
+    tokenizer = keras_nlp.models.RobertaTokenizer(
+        vocabulary=vocab,
+        merges=merges
+    )
+    tokenizer(["a quick fox", "a fox quick"])
+    ```
     """
 
     def __init__(
         self,
         vocabulary,
         merges,
         **kwargs,
     ):
-        # Special tokens. We use `"</s>"` as both a start and end token, as OPT
-        # was only pre-trained with `"</s>"` marking document boundaries.
-        start_token = "</s>"
+        # Special tokens.
+        start_token = "<s>"
         pad_token = "<pad>"
         end_token = "</s>"
+        mask_token = "<mask>"
 
         super().__init__(
             vocabulary=vocabulary,
             merges=merges,
-            unsplittable_tokens=[start_token, pad_token, end_token],
+            unsplittable_tokens=[start_token, pad_token, end_token, mask_token],
             **kwargs,
         )
 
         # Check whether special tokens are present in the vocabulary.
-        for token in [start_token, pad_token, end_token]:
+        for token in [start_token, pad_token, end_token, mask_token]:
             if token not in self.get_vocabulary():
                 raise ValueError(
                     f"Cannot find token `'{token}'` in the provided "
                     f"`vocabulary`. Please provide `'{token}'` in your "
                     "`vocabulary` or use a pretrained `vocabulary` name."
                 )
 
         self.start_token_id = self.token_to_id(start_token)
         self.pad_token_id = self.token_to_id(pad_token)
         self.end_token_id = self.token_to_id(end_token)
+        self.mask_token_id = self.token_to_id(mask_token)
 
     @classproperty
     def presets(cls):
         return copy.deepcopy(backbone_presets)
 
     def get_config(self):
         config = super().get_config()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/opt/opt_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_tokenizer_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,26 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for OPT tokenizer layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.opt.opt_tokenizer import OPTTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class OPTTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class OPTTokenizerTest(TestCase):
     def setUp(self):
         self.vocab = {
             "<pad>": 0,
             "</s>": 1,
             "air": 2,
             "plane": 3,
             "at": 4,
@@ -55,15 +54,15 @@
 
     def test_tokenize_special_tokens(self):
         input_data = "</s> airplane at airport</s><pad>"
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [1, 2, 3, 4, 2, 5, 1, 0])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant([" airplane at airport", " kohli is the best"])
+        input_data = [" airplane at airport", " kohli is the best"]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[2, 3, 4, 2, 5], [6, 7, 8, 9, 10]])
 
     def test_detokenize(self):
         input_tokens = [2, 3, 4, 2, 5]
         output = self.tokenizer.detokenize(input_tokens)
         self.assertEqual(output, " airplane at airport")
@@ -72,37 +71,32 @@
         self.assertEqual(self.tokenizer.vocabulary_size(), 11)
 
     def test_errors_missing_special_tokens(self):
         with self.assertRaises(ValueError):
             OPTTokenizer(vocabulary=["a", "b", "c"], merges=[])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/preprocessor.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,28 +8,37 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
-class Preprocessor(keras.layers.Layer):
+@keras.saving.register_keras_serializable(package="keras_nlp")
+class Preprocessor(PreprocessingLayer):
     """Base class for model preprocessors."""
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._tokenizer = None
 
+    def __setattr__(self, name, value):
+        # Work around torch setattr for properties.
+        if name in ["tokenizer"]:
+            object.__setattr__(self, name, value)
+        else:
+            super().__setattr__(name, value)
+
     @property
     def tokenizer(self):
         """The tokenizer used to tokenize strings."""
         return self._tokenizer
 
     @tokenizer.setter
     def tokenizer(self, value):
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_backbone.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,22 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """RoBERTa backbone model."""
 
 import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.token_and_position_embedding import (
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.token_and_position_embedding import (
     TokenAndPositionEmbedding,
 )
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 def roberta_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
@@ -65,16 +63,16 @@
             consume. The sequence length of the input must be less than
             `max_sequence_length` default value. This determines the variable
             shape for positional embeddings.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array(
             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)),
     }
 
     # Pretrained RoBERTa encoder
     model = keras_nlp.models.RobertaBackbone.from_preset("roberta_base_en")
     model(input_data)
 
@@ -100,18 +98,18 @@
         intermediate_dim,
         dropout=0.1,
         max_sequence_length=512,
         **kwargs,
     ):
         # Inputs
         token_id_input = keras.Input(
-            shape=(None,), dtype=tf.int32, name="token_ids"
+            shape=(None,), dtype="int32", name="token_ids"
         )
         padding_mask = keras.Input(
-            shape=(None,), dtype=tf.int32, name="padding_mask"
+            shape=(None,), dtype="int32", name="padding_mask"
         )
 
         # Embed tokens and positions.
         embedding_layer = TokenAndPositionEmbedding(
             vocabulary_size=vocabulary_size,
             sequence_length=max_sequence_length,
             embedding_dim=hidden_dim,
@@ -121,15 +119,15 @@
         embedding = embedding_layer(token_id_input)
 
         # Sum, normalize and apply dropout to embeddings.
         x = keras.layers.LayerNormalization(
             name="embeddings_layer_norm",
             axis=-1,
             epsilon=1e-5,  # Original paper uses this epsilon value
-            dtype=tf.float32,
+            dtype="float32",
         )(embedding)
         x = keras.layers.Dropout(
             dropout,
             name="embeddings_dropout",
         )(x)
 
         # Apply successive transformer encoder blocks.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_backbone_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,39 +9,39 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Test for RoBERTa backbone models."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaBackboneTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaBackboneTest(TestCase):
     def setUp(self):
         self.backbone = RobertaBackbone(
             vocabulary_size=10,
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=5,
         )
         self.batch_size = 8
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
 
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_valid_call_roberta(self):
@@ -55,68 +55,62 @@
         self.assertRegexpMatches(self.backbone.name, "roberta_backbone")
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
     def test_variable_sequence_length_call_roberta(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
             output = self.backbone(input_data)
             self.assertAllEqual(
-                tf.shape(output),
-                [2, seq_length, self.backbone.hidden_dim],
+                ops.shape(output),
+                (2, seq_length, self.backbone.hidden_dim),
             )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, RobertaBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class RobertaBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = RobertaBackbone(
                 vocabulary_size=1000,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=64,
                 intermediate_dim=128,
                 max_sequence_length=128,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((8, 128), dtype="int32"),
+            "padding_mask": ops.ones((8, 128), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_classifier.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,23 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """RoBERTa classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
 from keras_nlp.src.models.roberta.roberta_backbone import roberta_kernel_initializer
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.RobertaClassifier")
 class RobertaClassifier(Task):
     """An end-to-end RoBERTa model for classification tasks.
 
@@ -48,17 +46,17 @@
 
     Args:
         backbone: A `keras_nlp.models.RobertaBackbone` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.RobertaPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
-            activation function to use on the model outputs. Set
-            `activation="softmax"` to return output probabilities.
+        activation: Optional `str` or callable. The activation function to use
+            on the model outputs. Set `activation="softmax"` to return output
+            probabilities. Defaults to `None`.
         hidden_dim: int. The size of the pooler layer.
         dropout: float. The dropout probability value, applied to the pooled
             output, and after the first dense layer.
 
     Examples:
 
     Raw string data.
@@ -85,18 +83,16 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.RobertaClassifier.from_preset(
         "roberta_base_en",
         num_classes=4,
@@ -177,21 +173,22 @@
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.hidden_dim = hidden_dim
         self.dropout = dropout
 
         # Default compilation
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(2e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "num_classes": self.num_classes,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/distil_bert/distil_bert_classifier_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -7,132 +7,128 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for RoBERTa classification model."""
+"""Tests for DistilBERT classification model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
-from keras_nlp.src.models.roberta.roberta_classifier import RobertaClassifier
-from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
-from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models.distil_bert.distil_bert_backbone import DistilBertBackbone
+from keras_nlp.src.models.distil_bert.distil_bert_classifier import (
+    DistilBertClassifier,
+)
+from keras_nlp.src.models.distil_bert.distil_bert_preprocessor import (
+    DistilBertPreprocessor,
+)
+from keras_nlp.src.models.distil_bert.distil_bert_tokenizer import (
+    DistilBertTokenizer,
+)
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaClassifierTest(tf.test.TestCase, parameterized.TestCase):
+class DistilBertClassifierTest(TestCase):
     def setUp(self):
-        self.vocab = {
-            "<s>": 0,
-            "<pad>": 1,
-            "</s>": 2,
-            "air": 3,
-            "plane": 4,
-            "at": 5,
-            "port": 6,
-            "koh": 7,
-            "li": 8,
-            "is": 9,
-            "the": 10,
-            "best": 11,
-            "<mask>": 12,
-        }
-
-        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
-        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
-        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
-        merges += ["pla ne"]
-        self.merges = merges
-        self.preprocessor = RobertaPreprocessor(
-            RobertaTokenizer(vocabulary=self.vocab, merges=self.merges),
-            sequence_length=5,
+        # Setup model
+
+        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
+        self.vocab += ["the", "quick", "brown", "fox", "."]
+        self.preprocessor = DistilBertPreprocessor(
+            DistilBertTokenizer(vocabulary=self.vocab),
+            sequence_length=8,
         )
-        self.backbone = RobertaBackbone(
+        self.backbone = DistilBertBackbone(
             vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.classifier = RobertaClassifier(
+        self.classifier = DistilBertClassifier(
             self.backbone,
             num_classes=4,
             preprocessor=self.preprocessor,
             # Check we handle serialization correctly.
             activation=keras.activations.softmax,
             hidden_dim=4,
         )
 
-        # Setup data.
-        self.raw_batch = tf.constant(
-            [
-                " airplane at airport",
-                " the airplane is the best",
-            ]
-        )
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            (self.raw_batch, ops.ones((2,)))
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
         self.classifier(self.preprocessed_batch)
 
     def test_classifier_predict(self):
         preds1 = self.classifier.predict(self.raw_batch)
         self.classifier.preprocessor = None
         preds2 = self.classifier.predict(self.preprocessed_batch)
         # Assert predictions match.
         self.assertAllClose(preds1, preds2)
         # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.assertAllClose(ops.sum(preds2, axis=-1), [1.0, 1.0])
 
     def test_classifier_fit(self):
         self.classifier.fit(self.raw_dataset)
         self.classifier.preprocessor = None
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
         self.classifier.preprocessor = None
         self.classifier.compile(
-            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
+            loss="sparse_categorical_crossentropy",
             jit_compile=False,
         )
         self.classifier.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
-        self.assertEqual(
-            new_classifier.get_config(),
-            self.classifier.get_config(),
+        # Defaults.
+        original = DistilBertClassifier(
+            self.backbone,
+            num_classes=2,
+        )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
+        # With options.
+        original = DistilBertClassifier(
+            self.backbone,
+            num_classes=4,
+            preprocessor=self.preprocessor,
+            activation=keras.activations.softmax,
+            hidden_dim=4,
+            name="test",
+            trainable=False,
         )
+        config = keras.saving.serialize_keras_object(original)
+        restored = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(restored.get_config(), original.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large
+    def test_saving_model(self):
         model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.classifier.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, RobertaClassifier)
+        self.assertIsInstance(restored_model, DistilBertClassifier)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,26 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """RoBERTa masked lm model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
 from keras_nlp.src.models.roberta.roberta_backbone import roberta_kernel_initializer
 from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import (
     RobertaMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
 from keras_nlp.src.models.task import Task
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.RobertaMaskedLM")
 class RobertaMaskedLM(Task):
     """An end-to-end RoBERTa model for the masked language modeling task.
 
@@ -80,21 +78,17 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create a preprocessed dataset where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2)
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.RobertaMaskedLM.from_preset(
         "roberta_base_en",
         preprocessor=None,
@@ -135,16 +129,16 @@
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
 
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return RobertaBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 """RoBERTa masked language model preprocessor layer."""
 
 from absl import logging
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
+from keras_nlp.src.layers.preprocessing.masked_lm_mask_generator import (
+    MaskedLMMaskGenerator,
+)
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
 @keras_nlp_export("keras_nlp.models.RobertaMaskedLMPreprocessor")
 class RobertaMaskedLMPreprocessor(RobertaPreprocessor):
     """RoBERTa preprocessing for the masked language modeling task.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,166 +8,162 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for RoBERTa masked language model preprocessor layer."""
-
+"""Tests for XLM-RoBERTa masked language model preprocessor layer."""
+import io
 import os
 
 import pytest
+import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import (
-    RobertaMaskedLMPreprocessor,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import (
+    XLMRobertaMaskedLMPreprocessor,
+)
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
+    XLMRobertaTokenizer,
 )
-from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaMaskedLMPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaMaskedLMPreprocessorTest(TestCase):
     def setUp(self):
-        vocab = {
-            "<s>": 0,
-            "<pad>": 1,
-            "</s>": 2,
-            "air": 3,
-            "plane": 4,
-            "at": 5,
-            "port": 6,
-            "koh": 7,
-            "li": 8,
-            "is": 9,
-            "the": 10,
-            "best": 11,
-            "<mask>": 12,
-        }
-
-        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
-        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
-        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
-        merges += ["pla ne"]
-
-        self.preprocessor = RobertaMaskedLMPreprocessor(
-            tokenizer=RobertaTokenizer(
-                vocabulary=vocab,
-                merges=merges,
-            ),
+        bytes_io = io.BytesIO()
+        vocab_data = tf.data.Dataset.from_tensor_slices(
+            ["the quick brown fox", "the earth is round"]
+        )
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=vocab_data.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=12,
+            model_type="WORD",
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
+            pad_piece="<pad>",
+            unk_piece="<unk>",
+            bos_piece="<s>",
+            eos_piece="</s>",
+            user_defined_symbols="[MASK]",
+        )
+        self.proto = bytes_io.getvalue()
+
+        self.tokenizer = XLMRobertaTokenizer(proto=self.proto)
+        self.preprocessor = XLMRobertaMaskedLMPreprocessor(
+            tokenizer=self.tokenizer,
             # Simplify out testing by masking every available token.
             mask_selection_rate=1.0,
             mask_token_rate=1.0,
             random_token_rate=0.0,
             mask_selection_length=5,
             sequence_length=12,
         )
 
     def test_preprocess_strings(self):
-        input_data = " airplane at airport"
+        input_data = " brown fox quick"
 
         x, y, sw = self.preprocessor(input_data)
         self.assertAllEqual(
-            x["token_ids"], [0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]
+            x["token_ids"], [0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]
         )
         self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
+            x["padding_mask"], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
         )
-        self.assertAllEqual(x["mask_positions"], [1, 2, 3, 4, 5])
-        self.assertAllEqual(y, [3, 4, 5, 3, 6])
-        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 1.0, 1.0])
+        self.assertAllEqual(x["mask_positions"], [1, 2, 3, 0, 0])
+        self.assertAllEqual(y, [7, 9, 11, 0, 0])
+        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 0.0, 0.0])
 
     def test_preprocess_list_of_strings(self):
-        input_data = [" airplane at airport"] * 4
+        input_data = [" brown fox quick"] * 13
 
         x, y, sw = self.preprocessor(input_data)
         self.assertAllEqual(
-            x["token_ids"], [[0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]] * 4
+            x["token_ids"], [[0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]] * 13
         )
         self.assertAllEqual(
-            x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]] * 4
+            x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]] * 13
         )
-        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 4, 5]] * 4)
-        self.assertAllEqual(y, [[3, 4, 5, 3, 6]] * 4)
-        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 1.0, 1.0]] * 4)
+        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 0, 0]] * 13)
+        self.assertAllEqual(y, [[7, 9, 11, 0, 0]] * 13)
+        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 0.0, 0.0]] * 13)
 
     def test_preprocess_dataset(self):
-        sentences = tf.constant([" airplane at airport"] * 4)
+        sentences = tf.constant([" brown fox quick"] * 13)
         ds = tf.data.Dataset.from_tensor_slices(sentences)
         ds = ds.map(self.preprocessor)
-        x, y, sw = ds.batch(4).take(1).get_single_element()
+        x, y, sw = ds.batch(13).take(1).get_single_element()
         self.assertAllEqual(
-            x["token_ids"], [[0, 12, 12, 12, 12, 12, 2, 1, 1, 1, 1, 1]] * 4
+            x["token_ids"], [[0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]] * 13
         )
         self.assertAllEqual(
-            x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]] * 4
+            x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]] * 13
         )
-        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 4, 5]] * 4)
-        self.assertAllEqual(y, [[3, 4, 5, 3, 6]] * 4)
-        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 1.0, 1.0]] * 4)
+        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 0, 0]] * 13)
+        self.assertAllEqual(y, [[7, 9, 11, 0, 0]] * 13)
+        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 0.0, 0.0]] * 13)
 
     def test_mask_multiple_sentences(self):
         sentence_one = tf.constant(" airplane")
-        sentence_two = tf.constant(" kohli")
+        sentence_two = tf.constant(" round")
 
         x, y, sw = self.preprocessor((sentence_one, sentence_two))
         self.assertAllEqual(
-            x["token_ids"], [0, 12, 12, 2, 2, 12, 12, 2, 1, 1, 1, 1]
+            x["token_ids"], [0, 2, 2, 2, 13, 2, 1, 1, 1, 1, 1, 1]
         )
         self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
+            x["padding_mask"], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
         )
-        self.assertAllEqual(x["mask_positions"], [1, 2, 5, 6, 0])
-        self.assertAllEqual(y, [3, 4, 7, 8, 0])
-        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 1.0, 0.0])
+        self.assertAllEqual(x["mask_positions"], [4, 0, 0, 0, 0])
+        self.assertAllEqual(y, [12, 0, 0, 0, 0])
+        self.assertAllEqual(sw, [1.0, 0.0, 0.0, 0.0, 0.0])
 
     def test_no_masking_zero_rate(self):
-        no_mask_preprocessor = RobertaMaskedLMPreprocessor(
+        no_mask_preprocessor = XLMRobertaMaskedLMPreprocessor(
             self.preprocessor.tokenizer,
             mask_selection_rate=0.0,
             mask_selection_length=5,
             sequence_length=12,
         )
-        input_data = " airplane at airport"
+        input_data = " quick brown fox"
 
         x, y, sw = no_mask_preprocessor(input_data)
         self.assertAllEqual(
-            x["token_ids"], [0, 3, 4, 5, 3, 6, 2, 1, 1, 1, 1, 1]
+            x["token_ids"], [0, 11, 7, 9, 2, 1, 1, 1, 1, 1, 1, 1]
         )
         self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
+            x["padding_mask"], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
         )
         self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0, 0])
         self.assertAllEqual(y, [0, 0, 0, 0, 0])
         self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0, 0.0])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant([" airplane at airport"])
+    @pytest.mark.large
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant([" quick brown fox"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
+        outputs = model(input_data)["token_ids"]
+        restored_outputs = restored_model(input_data)["token_ids"]
         self.assertAllEqual(outputs, restored_outputs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_masked_lm_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,26 +13,26 @@
 # limitations under the License.
 """Tests for RoBERTa masked language model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
 from keras_nlp.src.models.roberta.roberta_masked_lm import RobertaMaskedLM
 from keras_nlp.src.models.roberta.roberta_masked_lm_preprocessor import (
     RobertaMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaMaskedLMTest(TestCase):
     def setUp(self):
         self.vocab = {
             "<s>": 0,
             "<pad>": 1,
             "</s>": 2,
             "air": 3,
             "plane": 4,
@@ -69,20 +69,18 @@
             preprocessor=self.preprocessor,
         )
         self.masked_lm_no_preprocessing = RobertaMaskedLM(
             self.backbone,
             preprocessor=None,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                " airplane at airport",
-                " the airplane is the best",
-            ]
-        )
+        self.raw_batch = [
+            " airplane at airport",
+            " the airplane is the best",
+        ]
         self.preprocessed_batch = self.preprocessor(self.raw_batch)
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
             self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
     def test_valid_call_classifier(self):
@@ -95,40 +93,34 @@
 
     def test_classifier_fit(self):
         self.masked_lm.fit(self.raw_dataset)
         self.masked_lm.preprocessor = None
         self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.masked_lm)
-        new_classifier = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.masked_lm)
+        new_classifier = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_classifier.get_config(),
             self.masked_lm.get_config(),
         )
 
     def test_classifier_fit_no_xla(self):
         self.masked_lm.preprocessor = None
         self.masked_lm.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
         self.masked_lm.fit(self.preprocessed_dataset)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model_output = self.masked_lm.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.masked_lm.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, RobertaMaskedLM)
 
         # Check that output matches.
         restored_output = restored_model.predict(self.raw_batch)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_multi_segment_packer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,147 +1,168 @@
-# Copyright 2022 The KerasNLP Authors
+# Copyright 2023 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
+"""XLM-RoBERTa tokenizer."""
+
+import copy
+
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_presets import backbone_presets
+from keras_nlp.src.tokenizers.sentence_piece_tokenizer import SentencePieceTokenizer
+from keras_nlp.src.utils.python_utils import classproperty
+from keras_nlp.src.utils.tensor_utils import tensor_to_list
+
+
+@keras_nlp_export("keras_nlp.models.XLMRobertaTokenizer")
+class XLMRobertaTokenizer(SentencePieceTokenizer):
+    """An XLM-RoBERTa tokenizer using SentencePiece subword segmentation.
+
+    This tokenizer class will tokenize raw strings into integer sequences and
+    is based on `keras_nlp.tokenizers.SentencePieceTokenizer`. Unlike the
+    underlying tokenizer, it will check for all special tokens needed by
+    XLM-RoBERTa models and provides a `from_preset()` method to automatically
+    download a matching vocabulary for an XLM-RoBERTa preset.
+
+    Note: If you are providing your own custom SentencePiece model, the original
+    fairseq implementation of XLM-RoBERTa re-maps some token indices from the
+    underlying sentencepiece output. To preserve compatibility, we do the same
+    re-mapping here.
+
+    If input is a batch of strings (rank > 0), the layer will output a
+    `tf.RaggedTensor` where the last dimension of the output is ragged.
+
+    If input is a scalar string (rank == 0), the layer will output a dense
+    `tf.Tensor` with static shape `[None]`.
+
+    Args:
+        proto: Either a `string` path to a SentencePiece proto file or a
+            `bytes` object with a serialized SentencePiece proto. See the
+            [SentencePiece repository](https://github.com/google/sentencepiece)
+            for more details on the format.
+
+    Examples:
+    ```python
+    tokenizer = keras_nlp.models.XLMRobertaTokenizer.from_preset(
+        "xlm_roberta_base_multi",
+    )
+
+    # Unbatched inputs.
+    tokenizer("the quick brown fox")
+
+    # Batched inputs.
+    tokenizer(["the quick brown fox", " "])
+
+    # Detokenization.
+    tokenizer.detokenize(tokenizer("the quick brown fox"))
+
+    # Custom vocabulary
+    def train_sentencepiece(ds, vocab_size):
+        bytes_io = io.BytesIO()
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=ds.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=vocab_size,
+            model_type="WORD",
+            unk_id=0,
+            bos_id=1,
+            eos_id=2,
+        )
+        return bytes_io.getvalue()
 
-try:
-    import tensorflow_text as tf_text
-except ImportError:
-    tf_text = None
-
-
-# TODO: This is a temporary, unexported layer until we find a way to make the
-# `MultiSegmentPacker` layer more generic.
-@keras_nlp_export("keras_nlp.models.RobertaMultiSegmentPacker")
-class RobertaMultiSegmentPacker(keras.layers.Layer):
-    """Packs multiple sequences into a single fixed width model input.
-
-    This layer packs multiple input sequences into a single fixed width sequence
-    containing start and end delimiters, forming a dense input suitable for a
-    classification task for RoBERTa.
-
-    Takes as input a list or tuple of token segments. The layer will process
-    inputs as follows:
-     - Truncate all input segments to fit within `sequence_length` according to
-       the `truncate` strategy.
-     - Concatenate all input segments, adding a single `start_value` at the
-       start of the entire sequence, `[end_value, end_value]` at the end of
-       each segment save the last, and a single `end_value` at the end of the
-       entire sequence.
-     - Pad the resulting sequence to `sequence_length` using `pad_tokens`.
+    ds = tf.data.Dataset.from_tensor_slices(
+        ["the quick brown fox", "the earth is round"]
+    )
+    proto = train_sentencepiece(ds, vocab_size=10)
+    tokenizer = keras_nlp.models.XLMRobertaTokenizer(proto=proto)
+    ```
+    """
 
-    Input should be either a `tf.RaggedTensor` or a dense `tf.Tensor`, and
-    either rank-1 or rank-2.
+    def __init__(self, proto, **kwargs):
+        super().__init__(proto=proto, **kwargs)
 
-    Please refer to the arguments of `keras_nlp.layers.MultiSegmentPacker` for
-    more details.
-    """
+        # List of special tokens.
+        self._vocabulary_prefix = ["<s>", "<pad>", "</s>", "<unk>"]
 
-    def __init__(
-        self,
-        sequence_length,
-        start_value,
-        end_value,
-        pad_value=None,
-        truncate="round_robin",
-        **kwargs,
-    ):
-        assert_tf_text_installed(self.__class__.__name__)
-
-        super().__init__(**kwargs)
-        self.sequence_length = sequence_length
-        if truncate not in ("round_robin", "waterfall"):
-            raise ValueError(
-                "Only 'round_robin' and 'waterfall' algorithms are "
-                "supported. Received %s" % truncate
+        # IDs of special tokens.
+        self.start_token_id = 0  # <s>
+        self.pad_token_id = 1  # <pad>
+        self.end_token_id = 2  # </s>
+        self.unk_token_id = 3  # <unk>
+        self.mask_token_id = self.vocabulary_size() - 1  # <mask>
+
+    def vocabulary_size(self):
+        """Get the size of the tokenizer vocabulary."""
+        return super().vocabulary_size() + 2
+
+    def get_vocabulary(self):
+        """Get the size of the tokenizer vocabulary."""
+        vocabulary = tensor_to_list(
+            self._sentence_piece.id_to_string(
+                tf.range(super().vocabulary_size())
             )
-        self.truncate = truncate
-        self.start_value = start_value
-        self.end_value = end_value
-        self.pad_value = pad_value
-
-    def get_config(self):
-        config = super().get_config()
-        config.update(
-            {
-                "sequence_length": self.sequence_length,
-                "start_value": self.start_value,
-                "end_value": self.end_value,
-                "pad_value": self.pad_value,
-                "truncate": self.truncate,
-            }
         )
-        return config
+        return self._vocabulary_prefix + vocabulary[3:] + ["<mask>"]
 
-    def _trim_inputs(self, inputs):
-        """Trim inputs to desired length."""
-        # Special tokens include the start token at the beginning of the
-        # sequence, two `end_value` at the end of every segment save the last,
-        # and the `end_value` at the end of the sequence.
-        num_special_tokens = 2 * len(inputs)
-        if self.truncate == "round_robin":
-            return tf_text.RoundRobinTrimmer(
-                self.sequence_length - num_special_tokens
-            ).trim(inputs)
-        elif self.truncate == "waterfall":
-            return tf_text.WaterfallTrimmer(
-                self.sequence_length - num_special_tokens
-            ).trim(inputs)
-        else:
-            raise ValueError("Unsupported truncate: %s" % self.truncate)
-
-    def _combine_inputs(self, segments):
-        """Combine inputs with start and end values added."""
-        dtype = segments[0].dtype
-        batch_size = segments[0].nrows()
-
-        start_value = tf.convert_to_tensor(self.start_value, dtype=dtype)
-        end_value = tf.convert_to_tensor(self.end_value, dtype=dtype)
-
-        start_column = tf.fill((batch_size, 1), start_value)
-        end_column = tf.fill((batch_size, 1), end_value)
-
-        segments_to_combine = []
-        for i, seg in enumerate(segments):
-            segments_to_combine.append(start_column if i == 0 else end_column)
-            segments_to_combine.append(seg)
-            segments_to_combine.append(end_column)
-
-        token_ids = tf.concat(segments_to_combine, 1)
-        return token_ids
-
-    def call(self, inputs):
-        def to_ragged(x):
-            return tf.RaggedTensor.from_tensor(x[tf.newaxis, :])
-
-        # If rank 1, add a batch dim.
-        rank_1 = inputs[0].shape.rank == 1
-        if rank_1:
-            inputs = [to_ragged(x) for x in inputs]
-
-        segments = self._trim_inputs(inputs)
-        token_ids = self._combine_inputs(segments)
-        # Pad to dense tensor output.
-        shape = tf.cast([-1, self.sequence_length], tf.int64)
-        token_ids = token_ids.to_tensor(
-            shape=shape, default_value=self.pad_value
-        )
-        # Remove the batch dim if added.
-        if rank_1:
-            token_ids = tf.squeeze(token_ids, 0)
+    def id_to_token(self, id):
+        """Convert an integer id to a string token."""
+
+        if id == self.mask_token_id:
+            return "<mask>"
+
+        if id < len(self._vocabulary_prefix) and id >= 0:
+            return self._vocabulary_prefix[id]
+
+        if id - 1 >= super().vocabulary_size() or id - 1 < 0:
+            raise ValueError(
+                f"`id` must be in range [0, {self.vocabulary_size() - 1}]. "
+                f"Received: {id}"
+            )
+
+        return tensor_to_list(self._sentence_piece.id_to_string(id - 1))
+
+    def token_to_id(self, token):
+        """Convert a string token to an integer id."""
+
+        if token in self._vocabulary_prefix:
+            return self._vocabulary_prefix.index(token)
+
+        spm_token_id = self._sentence_piece.string_to_id(token)
+
+        # OOV token
+        spm_unk_token_id = self._sentence_piece.string_to_id("<unk>")
+        if spm_token_id == spm_unk_token_id:
+            return self.unk_token_id
+
+        return int(spm_token_id.numpy()) + 1
+
+    def tokenize(self, inputs):
+        tokens = super().tokenize(inputs)
+
+        # Correct `unk_token_id` (0 -> 3). Note that we do not correct
+        # `start_token_id` and `end_token_id`; they are dealt with in
+        # `XLMRobertaPreprocessor`.
+        tokens = tf.where(tf.equal(tokens, 0), self.unk_token_id - 1, tokens)
+
+        # Shift the tokens IDs right by one.
+        return tf.add(tokens, 1)
+
+    def detokenize(self, ids):
+        ids = tf.ragged.boolean_mask(ids, tf.not_equal(ids, self.mask_token_id))
+        return super().detokenize(ids)
 
-        return token_ids
+    @classproperty
+    def presets(cls):
+        return copy.deepcopy(backbone_presets)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_preprocessor.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,18 +13,18 @@
 # limitations under the License.
 
 """RoBERTa preprocessor layer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.preprocessor import Preprocessor
-from keras_nlp.src.models.roberta.roberta_multi_segment_packer import (
-    RobertaMultiSegmentPacker,
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
 )
+from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
 from keras_nlp.src.utils.keras_utils import (
     convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 from keras_nlp.src.utils.python_utils import classproperty
@@ -141,17 +141,18 @@
         sequence_length=512,
         truncate="round_robin",
         **kwargs,
     ):
         super().__init__(**kwargs)
 
         self.tokenizer = tokenizer
-        self.packer = RobertaMultiSegmentPacker(
+        self.packer = MultiSegmentPacker(
             start_value=self.tokenizer.start_token_id,
             end_value=self.tokenizer.end_token_id,
+            sep_value=[self.tokenizer.end_token_id] * 2,
             pad_value=self.tokenizer.pad_token_id,
             truncate=truncate,
             sequence_length=sequence_length,
         )
 
     def get_config(self):
         config = super().get_config()
@@ -162,15 +163,15 @@
             }
         )
         return config
 
     def call(self, x, y=None, sample_weight=None):
         x = convert_inputs_to_list_of_tensor_segments(x)
         x = [self.tokenizer(segment) for segment in x]
-        token_ids = self.packer(x)
+        token_ids, _ = self.packer(x)
         x = {
             "token_ids": token_ids,
             "padding_mask": token_ids != self.tokenizer.pad_token_id,
         }
         return pack_x_y_sample_weight(x, y, sample_weight)
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_preprocessor_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,27 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for RoBERTa preprocessor layer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaPreprocessorTest(TestCase):
     def setUp(self):
         vocab = {
             "<s>": 0,
             "<pad>": 1,
             "</s>": 2,
             "air": 3,
             "plane": 4,
@@ -137,37 +136,32 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_presets_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,26 +10,27 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.roberta.roberta_backbone import RobertaBackbone
 from keras_nlp.src.models.roberta.roberta_classifier import RobertaClassifier
 from keras_nlp.src.models.roberta.roberta_masked_lm import RobertaMaskedLM
 from keras_nlp.src.models.roberta.roberta_preprocessor import RobertaPreprocessor
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class RobertaPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaPresetSmokeTest(TestCase):
     """
     A smoke test for RoBERTa presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/roberta/roberta_presets_test.py --run_large`
     """
 
@@ -51,16 +52,16 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[0, 133, 2119, 2]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[0, 133, 2119, 2]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = RobertaBackbone.from_preset(
             "roberta_base_en", load_weights=load_weights
         )
         outputs = model(input_data)
         if load_weights:
             outputs = outputs[0, 0, :5]
@@ -79,16 +80,16 @@
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = RobertaClassifier.from_preset(
             "roberta_base_en",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -107,17 +108,17 @@
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_masked_lm_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[101, 1996, 4248, 102]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
-            "mask_positions": tf.constant([[0, 0]]),
+            "token_ids": ops.array([[101, 1996, 4248, 102]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
+            "mask_positions": ops.array([[0, 0]]),
         }
         model = RobertaMaskedLM.from_preset(
             "roberta_base_en",
             load_weights=load_weights,
             preprocessor=None,
         )
         # Never assert output values, as the head weights are random.
@@ -145,15 +146,15 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("roberta_base_en_clowntown", **kwargs)
 
 
 @pytest.mark.extra_large
-class RobertaPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every RoBERTa preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/roberta/roberta_presets_test.py --run_extra_large`
     """
@@ -163,82 +164,82 @@
     )
     def test_load_roberta(self, load_weights):
         for preset in RobertaBackbone.presets:
             model = RobertaBackbone.from_preset(
                 preset, load_weights=load_weights
             )
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_roberta_classifier(self, load_weights):
         for preset in RobertaClassifier.presets:
             classifier = RobertaClassifier.from_preset(
                 preset, num_classes=4, load_weights=load_weights
             )
-            input_data = tf.constant(["The quick brown fox."])
+            input_data = ["The quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_roberta_classifier_without_preprocessing(self, load_weights):
         for preset in RobertaClassifier.presets:
             classifier = RobertaClassifier.from_preset(
                 preset,
                 num_classes=2,
                 preprocessor=None,
                 load_weights=load_weights,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_roberta_masked_lm(self, load_weights):
         for preset in RobertaMaskedLM.presets:
             classifier = RobertaMaskedLM.from_preset(
                 preset, load_weights=load_weights
             )
-            input_data = tf.constant(["The quick brown fox."])
+            input_data = ["The quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("load_weights", True), ("no_load_weights", False)
     )
     def test_load_roberta_masked_lm_without_preprocessing(self, load_weights):
         for preset in RobertaMaskedLM.presets:
             classifier = RobertaMaskedLM.from_preset(
                 preset,
                 preprocessor=None,
                 load_weights=load_weights,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
-                "mask_positions": tf.constant([1] * 128, shape=(1, 128)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
+                "mask_positions": ops.array([1] * 128, shape=(1, 128)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in RobertaTokenizer.presets:
             tokenizer = RobertaTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/opt_tokenizer.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,111 +8,103 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""RoBERTa tokenizer."""
+"""OPT tokenizer."""
 
 import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.roberta.roberta_presets import backbone_presets
+from keras_nlp.src.models.opt.opt_presets import backbone_presets
 from keras_nlp.src.tokenizers.byte_pair_tokenizer import BytePairTokenizer
 from keras_nlp.src.utils.python_utils import classproperty
 
 
-@keras_nlp_export("keras_nlp.models.RobertaTokenizer")
-class RobertaTokenizer(BytePairTokenizer):
-    """A RoBERTa tokenizer using Byte-Pair Encoding subword segmentation.
+@keras_nlp_export("keras_nlp.models.OPTTokenizer")
+class OPTTokenizer(BytePairTokenizer):
+    """An OPT tokenizer using Byte-Pair Encoding subword segmentation.
 
     This tokenizer class will tokenize raw strings into integer sequences and
     is based on `keras_nlp.tokenizers.BytePairTokenizer`. Unlike the
-    underlying tokenizer, it will check for all special tokens needed by RoBERTa
+    underlying tokenizer, it will check for all special tokens needed by OPT
     models and provides a `from_preset()` method to automatically download
-    a matching vocabulary for a RoBERTa preset.
+    a matching vocabulary for a OPT preset.
 
-    This tokenizer does not provide truncation or padding of inputs. It can be
-    combined with a `keras_nlp.models.RobertaPreprocessor` layer for input
-    packing.
+    This tokenizer does not provide truncation or padding of inputs.
 
     If input is a batch of strings (rank > 0), the layer will output a
     `tf.RaggedTensor` where the last dimension of the output is ragged.
-
     If input is a scalar string (rank == 0), the layer will output a dense
     `tf.Tensor` with static shape `[None]`.
 
     Args:
-        vocabulary: A dictionary mapping tokens to integer ids, or file path
-            to a json file containing the token to id mapping.
-        merges: A list of merge rules or a string file path, If passing a file
-            path. the file should have one merge rule per line. Every merge
-            rule contains merge entities separated by a space.
+        vocabulary: string or dict, maps token to integer ids. If it is a
+            string, it should be the file path to a json file.
+        merges: string or list, contains the merge rule. If it is a string,
+            it should be the file path to merge rules. The merge rule file
+            should have one merge rule per line. Every merge rule contains
+            merge entities separated by a space.
 
     Examples:
     ```python
     # Unbatched input.
-    tokenizer = keras_nlp.models.RobertaTokenizer.from_preset(
-        "roberta_base_en",
+    tokenizer = keras_nlp.models.OPTTokenizer.from_preset(
+        "opt_125m_en",
     )
     tokenizer("The quick brown fox jumped.")
 
     # Batched input.
     tokenizer(["The quick brown fox jumped.", "The fox slept."])
 
     # Detokenization.
     tokenizer.detokenize(tokenizer("The quick brown fox jumped."))
 
     # Custom vocabulary.
-    # Note: '' is space
-    vocab = {"<s>": 0, "<pad>": 1, "</s>": 2, "<mask>": 3}
-    vocab = {**vocab, "a": 4, "quick": 5, "fox": 6}
+    vocab = {"<pad>": 1, "</s>": 2, "quick": 4, "fox": 5}
     merges = [" q", "u i", "c k", "ui ck", "q uick"]
     merges += [" f", "o x", "f ox"]
-    tokenizer = keras_nlp.models.RobertaTokenizer(
-        vocabulary=vocab,
-        merges=merges
-    )
-    tokenizer(["a quick fox", "a fox quick"])
+    tokenizer = keras_nlp.models.OPTTokenizer(vocabulary=vocab, merges=merges)
+    tokenizer("The quick brown fox jumped.")
     ```
     """
 
     def __init__(
         self,
         vocabulary,
         merges,
         **kwargs,
     ):
-        # Special tokens.
-        start_token = "<s>"
+        # Special tokens. We use `"</s>"` as both a start and end token, as OPT
+        # was only pre-trained with `"</s>"` marking document boundaries.
+        start_token = "</s>"
         pad_token = "<pad>"
         end_token = "</s>"
-        mask_token = "<mask>"
 
         super().__init__(
             vocabulary=vocabulary,
             merges=merges,
-            unsplittable_tokens=[start_token, pad_token, end_token, mask_token],
+            unsplittable_tokens=[start_token, pad_token, end_token],
             **kwargs,
         )
 
         # Check whether special tokens are present in the vocabulary.
-        for token in [start_token, pad_token, end_token, mask_token]:
+        for token in [start_token, pad_token, end_token]:
             if token not in self.get_vocabulary():
                 raise ValueError(
                     f"Cannot find token `'{token}'` in the provided "
                     f"`vocabulary`. Please provide `'{token}'` in your "
                     "`vocabulary` or use a pretrained `vocabulary` name."
                 )
 
         self.start_token_id = self.token_to_id(start_token)
         self.pad_token_id = self.token_to_id(pad_token)
         self.end_token_id = self.token_to_id(end_token)
-        self.mask_token_id = self.token_to_id(mask_token)
 
     @classproperty
     def presets(cls):
         return copy.deepcopy(backbone_presets)
 
     def get_config(self):
         config = super().get_config()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/roberta/roberta_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/roberta_tokenizer_test.py`

 * *Files 21% similar despite different names*

```diff
@@ -9,26 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for RoBERTa tokenizer."""
-
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.roberta.roberta_tokenizer import RobertaTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RobertaTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class RobertaTokenizerTest(TestCase):
     def setUp(self):
         vocab = {
             "<s>": 0,
             "<pad>": 1,
             "</s>": 2,
             "air": 3,
             "plane": 4,
@@ -56,15 +55,15 @@
 
     def test_tokenize_special_tokens(self):
         input_data = "<s> airplane at airport</s><pad>"
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [0, 3, 4, 5, 3, 6, 0, 1])
 
     def test_tokenize_batch(self):
-        input_data = tf.constant([" airplane at airport", " kohli is the best"])
+        input_data = [" airplane at airport", " kohli is the best"]
         output = self.tokenizer(input_data)
         self.assertAllEqual(output, [[3, 4, 5, 3, 6], [7, 8, 9, 10, 11]])
 
     def test_detokenize(self):
         input_tokens = [[3, 4, 5, 3, 6]]
         output = self.tokenizer.detokenize(input_tokens)
         self.assertAllEqual(output, [" airplane at airport"])
@@ -73,37 +72,32 @@
         self.assertEqual(self.tokenizer.vocabulary_size(), 13)
 
     def test_errors_missing_special_tokens(self):
         with self.assertRaises(ValueError):
             RobertaTokenizer(vocabulary=["a", "b", "c"], merges=[])
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_tokenizer.get_config(),
             self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant([" airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.tokenizer(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_backbone.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,25 +10,23 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """T5 backbone model."""
 
-import tensorflow as tf
-from tensorflow import keras
-
-from keras_nlp.src.layers.transformer_layer_utils import compute_causal_mask
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.t5.t5_layer_norm import T5LayerNorm
 from keras_nlp.src.models.t5.t5_transformer_layer import T5TransformerLayer
 from keras_nlp.src.utils.python_utils import classproperty
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class T5Backbone(Backbone):
     """T5 encoder-decoder backbone model.
 
     T5 is a LLM pretrained on a mix of unsupervised and supervised tasks,
     where each task is converted to a sequence-to-sequence format.
     T5 works well on a variety of tasks out-of-the-box by prepending
     various prefixex to the input sequence, e.g., for translation:
@@ -52,21 +50,20 @@
             The hidden size must be divisible by the number of attention heads.
         hidden_dim: int. The hidden size of the Transformer layers.
         intermediate_dim: int. The output dimension of the first Dense layer in
             a two-layer feedforward network for each Transformer layer.
         dropout: float. Dropout probability for the Transformer layers.
         activation: activation function (or activation string name). The
             activation to be used in the inner dense blocks of the
-            Transformer layers. Defaults to `"gelu"`. The original
-            T5 architecture used `"relu"`,
-            but more recent versions use `"gelu"`.
+            Transformer layers. The original T5 architecture used `"relu"`,
+            but more recent versions use `"gelu"`. Defaults to `"gelu"`.
         use_gated_activation: boolean. Whether to use activation gating in
             the inner dense blocks of the Transformer layers.
-            Defaults to True. The original T5 architecture didn't use
-            gating, but more recent versions do.
+            The original T5 architecture didn't use gating, but more
+            recent versions do. Defaults to `True`.
         layer_norm_epsilon: float. Epsilon factor to be used in the
             layer normalization layers in the Transformer layers.
     """
 
     def __init__(
         self,
         vocabulary_size,
@@ -76,14 +73,16 @@
         intermediate_dim,
         dropout=0.1,
         activation="gelu",
         use_gated_activation=True,
         layer_norm_epsilon=1e-06,
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         # Encoder inputs
         encoder_token_ids = keras.Input(
             shape=(None,), dtype="int32", name="encoder_token_ids"
         )
         encoder_padding_mask = keras.Input(
             shape=(None,), dtype="int32", name="encoder_padding_mask"
         )
@@ -109,16 +108,15 @@
         # Embed tokens.
         token_embedding = token_embedding_layer(encoder_token_ids)
         x = keras.layers.Dropout(
             dropout,
             name="encoder_embedding_dropout",
         )(token_embedding)
 
-        # Encoder attention mask is just our padding mask.
-        encoder_attention_mask = encoder_padding_mask[:, tf.newaxis, :]
+        encoder_attention_mask = encoder_padding_mask[:, None, :]
 
         position_bias = None
         for i in range(num_layers):
             x, position_bias = T5TransformerLayer(
                 is_decoder=False,
                 hidden_dim=hidden_dim,
                 intermediate_dim=intermediate_dim,
@@ -129,14 +127,15 @@
                 use_gated_activation=use_gated_activation,
                 use_relative_attention_bias=bool(i == 0),
                 name=f"transformer_encoder_layer_{i}",
             )(
                 x,
                 attention_mask=encoder_attention_mask,
                 position_bias=position_bias,
+                use_causal_mask=False,
             )
 
         x = T5LayerNorm(
             epsilon=layer_norm_epsilon,
             name="encoder_output_layer_norm",
         )(x)
         x = keras.layers.Dropout(
@@ -150,19 +149,15 @@
         # Embed tokens.
         token_embedding = token_embedding_layer(decoder_token_ids)
         x = keras.layers.Dropout(
             dropout,
             name="decoder_embedding_dropout",
         )(token_embedding)
 
-        # Decoder attention mask is padding mask plus a causal mask.
-        decoder_attention_mask = decoder_padding_mask[:, tf.newaxis, :]
-        batch_size, length = tf.shape(x)[0], tf.shape(x)[1]
-        causal_mask = compute_causal_mask(batch_size, length, length)
-        decoder_attention_mask = causal_mask & decoder_attention_mask
+        decoder_attention_mask = decoder_padding_mask[:, None, :]
 
         position_bias = None
         for i in range(num_layers):
             x, position_bias = T5TransformerLayer(
                 is_decoder=True,
                 hidden_dim=hidden_dim,
                 intermediate_dim=intermediate_dim,
@@ -175,14 +170,15 @@
                 name=f"transformer_decoder_layer_{i}",
             )(
                 x,
                 attention_mask=decoder_attention_mask,
                 position_bias=position_bias,
                 encoder_hidden_states=encoder_output,
                 encoder_attention_mask=encoder_attention_mask,
+                use_causal_mask=True,
             )
 
         x = T5LayerNorm(
             epsilon=layer_norm_epsilon,
             name="decoder_output_layer_norm",
         )(x)
         x = keras.layers.Dropout(
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_backbone_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,42 +13,44 @@
 # limitations under the License.
 """Test for T5 backbone model."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.t5.t5_backbone import T5Backbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class T5Test(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class T5Test(TestCase):
     def setUp(self):
         self.backbone = T5Backbone(
             vocabulary_size=4,
             num_layers=2,
             num_heads=2,
             hidden_dim=4,
             intermediate_dim=4,
         )
         self.batch_size = 2
         seq_length = 3
         self.input_batch = {
-            "encoder_token_ids": tf.ones(
+            "encoder_token_ids": ops.ones(
                 (self.batch_size, seq_length), dtype="int32"
             ),
-            "encoder_padding_mask": tf.ones(
+            "encoder_padding_mask": ops.ones(
                 (self.batch_size, seq_length), dtype="int32"
             ),
-            "decoder_token_ids": tf.ones(
+            "decoder_token_ids": ops.ones(
                 (self.batch_size, seq_length), dtype="int32"
             ),
-            "decoder_padding_mask": tf.ones(
+            "decoder_padding_mask": ops.ones(
                 (self.batch_size, seq_length), dtype="int32"
             ),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
@@ -64,78 +66,72 @@
     def test_name(self):
         # Check default name passed through
         self.assertRegexpMatches(self.backbone.name, "t5_backbone")
 
     def test_variable_sequence_length_call_t5(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "encoder_token_ids": tf.ones(
+                "encoder_token_ids": ops.ones(
                     (self.batch_size, seq_length), dtype="int32"
                 ),
-                "encoder_padding_mask": tf.ones(
+                "encoder_padding_mask": ops.ones(
                     (self.batch_size, seq_length), dtype="int32"
                 ),
-                "decoder_token_ids": tf.ones(
+                "decoder_token_ids": ops.ones(
                     (self.batch_size, seq_length), dtype="int32"
                 ),
-                "decoder_padding_mask": tf.ones(
+                "decoder_padding_mask": ops.ones(
                     (self.batch_size, seq_length), dtype="int32"
                 ),
             }
             outputs = self.backbone(input_data)
             self.assertIn("encoder_sequence_output", outputs)
             self.assertIn("decoder_sequence_output", outputs)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         outputs = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, T5Backbone)
 
         # Check that output matches.
         restored_outputs = restored_model(self.input_batch)
         for key in ["encoder_sequence_output", "decoder_sequence_output"]:
             self.assertAllClose(outputs[key], restored_outputs[key])
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class T5BackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class T5BackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
             self.backbone = T5Backbone(
                 vocabulary_size=4,
                 num_layers=2,
                 num_heads=2,
                 hidden_dim=4,
                 intermediate_dim=4,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 4), dtype="int32"),
-            "padding_mask": tf.ones((8, 4), dtype="int32"),
+            "token_ids": ops.ones((8, 4), dtype="int32"),
+            "padding_mask": ops.ones((8, 4), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
         self.backbone.compile()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_layer_norm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_layer_norm.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,26 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
-from tensorflow import keras
+
+from keras_nlp.src.backend import keras
 
 
 class T5LayerNorm(keras.layers.Layer):
     def __init__(self, epsilon=1e-6, **kwargs):
         super().__init__(**kwargs)
         self.epsilon = epsilon
 
     def build(self, input_shape):
         self.weight = self.add_weight(
-            "weight", shape=(input_shape[-1],), initializer="ones"
+            name="weight",
+            shape=(input_shape[-1],),
+            initializer="ones",
         )
+        self.built = True
 
     def call(self, hidden_states):
         variance = tf.math.reduce_mean(
             tf.math.square(hidden_states), axis=-1, keepdims=True
         )
         hidden_states = hidden_states * tf.math.rsqrt(variance + self.epsilon)
         return self.weight * hidden_states
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_multi_head_attention.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_multi_head_attention.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,17 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import tensorflow as tf
-from tensorflow import keras
 from tensorflow.compiler.tf2xla.python.xla import dynamic_slice
 
+from keras_nlp.src.backend import keras
+
 
 def shape_list(tensor):
     dynamic = tf.shape(tensor)
     if tensor.shape == tf.TensorShape(None):
         return dynamic
     static = tensor.shape.as_list()
     return [dynamic[i] if s is None else s for i, s in enumerate(static)]
@@ -77,15 +78,14 @@
             name="output_projector",
             kernel_initializer=keras.initializers.RandomNormal(
                 mean=0, stddev=self.inner_dim**-0.5
             ),
         )
         self.dropout_layer = keras.layers.Dropout(dropout)
 
-    def build(self, input_shape):
         if self.use_relative_attention_bias:
             self.relative_attention_bias = self.add_weight(
                 name="embeddings",
                 shape=[self.relative_attention_buckets, self.num_heads],
                 initializer=keras.initializers.RandomNormal(
                     mean=0, stddev=self.inner_dim**-0.5
                 ),
@@ -132,16 +132,16 @@
         else:
             relative_position = -tf.math.minimum(relative_position, 0)
         # now n is in the range [0, inf)
         max_exact = num_buckets // 2
         is_small = tf.math.less(relative_position, max_exact)
         relative_position_if_large = max_exact + tf.cast(
             tf.math.log(
-                tf.cast(relative_position, tf.float32)
-                / tf.cast(max_exact, tf.float32)
+                tf.cast(relative_position, "float32")
+                / tf.cast(max_exact, "float32")
             )
             / tf.math.log(max_distance / max_exact)
             * (num_buckets - max_exact),
             dtype=relative_position.dtype,
         )
         relative_position_if_large = tf.math.minimum(
             relative_position_if_large, num_buckets - 1
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_tokenizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,20 +10,19 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """T5 tokenizer."""
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.tokenizers.sentence_piece_tokenizer import SentencePieceTokenizer
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class T5Tokenizer(SentencePieceTokenizer):
     """T5 tokenizer layer based on SentencePiece.
 
     This tokenizer class will tokenize raw strings into integer sequences and
     is based on `keras_nlp.tokenizers.SentencePieceTokenizer`. Unlike the
     underlying tokenizer, it will check for all special tokens needed by
     T5 models and provides a `from_preset()` method to automatically
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_audio_feature_extractor_test.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,109 +7,92 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""Tests for Whisper audio feature extractor."""
 
-"""Tests for T5 tokenizer."""
-
-import io
 import os
 
 import pytest
-import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.t5.t5_tokenizer import T5Tokenizer
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.whisper.whisper_audio_feature_extractor import (
+    WhisperAudioFeatureExtractor,
+)
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class T5TokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class WhisperAudioFeatureExtractorTest(TestCase):
     def setUp(self):
-        bytes_io = io.BytesIO()
-        vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the earth is round"]
-        )
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=vocab_data.as_numpy_iterator(),
-            model_writer=bytes_io,
-            vocab_size=11,
-            model_type="WORD",
-            bos_id=-1,
-            pad_id=0,
-            eos_id=1,
-            unk_id=2,
-            pad_piece="<pad>",
-            eos_piece="</s>",
-            unk_piece="<unk>",
-            user_defined_symbols="[MASK]",
+        self.num_mels = 80
+        self.num_fft_bins = 400
+        self.stride = 100
+        self.sampling_rate = 100
+        self.max_audio_length = 5
+        self.audio_feature_extractor = WhisperAudioFeatureExtractor(
+            num_mels=self.num_mels,
+            num_fft_bins=self.num_fft_bins,
+            stride=self.stride,
+            sampling_rate=self.sampling_rate,
+            max_audio_length=self.max_audio_length,
         )
-        self.proto = bytes_io.getvalue()
 
-        self.tokenizer = T5Tokenizer(proto=self.proto)
+    def test_unbatched_inputs(self):
+        audio_tensor = tf.ones((2,), dtype="float32")
 
-    def test_tokenize(self):
-        input_data = "the quick brown fox"
-        output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [4, 9, 5, 7])
-
-    def test_tokenize_batch(self):
-        input_data = tf.constant(["the quick brown fox", "the earth is round"])
-        output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [[4, 9, 5, 7], [4, 6, 8, 10]])
-
-    def test_detokenize(self):
-        input_data = tf.constant([[4, 9, 5, 7]])
-        output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["the quick brown fox"]))
-
-    def test_vocabulary_size(self):
-        tokenizer = T5Tokenizer(proto=self.proto)
-        self.assertEqual(tokenizer.vocabulary_size(), 11)
-
-    def test_errors_missing_special_tokens(self):
-        bytes_io = io.BytesIO()
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=iter(["abc"]),
-            model_writer=bytes_io,
-            vocab_size=5,
-            pad_id=-1,
-            eos_id=-1,
-            bos_id=-1,
-        )
-        with self.assertRaises(ValueError):
-            T5Tokenizer(proto=bytes_io.getvalue())
+        outputs = self.audio_feature_extractor(audio_tensor)
+
+        # Verify shape.
+        self.assertEqual(outputs.shape, (1, 5, self.num_mels))
+        # Verify output.
+        expected = [1.1656, 1.0151, -0.8343, -0.8343, -0.8343]
+        self.assertAllClose(outputs[0, :, 0], expected, atol=0.01, rtol=0.01)
+
+    def test_batched_inputs(self):
+        audio_tensor_1 = tf.ones((2,), dtype="float32")
+        audio_tensor_2 = tf.ones((25,), dtype="float32")
+        audio_tensor = tf.ragged.stack([audio_tensor_1, audio_tensor_2], axis=0)
+
+        outputs = self.audio_feature_extractor(audio_tensor)
+
+        # Verify shape.
+        self.assertEqual(outputs.shape, (2, 5, self.num_mels))
+        # Verify output.
+        expected_1 = [1.1656, 1.0151, -0.8343, -0.8343, -0.8343]
+        self.assertAllClose(outputs[0, :, 0], expected_1, atol=0.01, rtol=0.01)
+        expected_2 = [1.2299, 1.0970, 0.3997, -0.7700, -0.7700]
+        self.assertAllClose(outputs[1, :, 0], expected_2, atol=0.01, rtol=0.01)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(
+            self.audio_feature_extractor
+        )
+        new_audio_feature_extractor = keras.saving.deserialize_keras_object(
+            config
+        )
         self.assertEqual(
-            new_tokenizer.get_config(),
-            self.tokenizer.get_config(),
+            new_audio_feature_extractor.get_config(),
+            self.audio_feature_extractor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant(["the quick brown fox"])
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        audio_tensor = tf.ones((2, 200), dtype="float32")
 
-        inputs = keras.Input(dtype="string", shape=())
-        outputs = self.tokenizer(inputs)
+        inputs = keras.Input(dtype="float32", shape=(None,))
+        outputs = self.audio_feature_extractor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
-            model(input_data),
-            restored_model(input_data),
+            model(audio_tensor),
+            restored_model(audio_tensor),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/t5/t5_transformer_layer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/t5_transformer_layer.py`

 * *Files 11% similar despite different names*

```diff
@@ -7,16 +7,20 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from tensorflow import keras
+import tensorflow as tf
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.transformer_layer_utils import (
+    compute_causal_mask,
+)
 from keras_nlp.src.models.t5.t5_layer_norm import T5LayerNorm
 from keras_nlp.src.models.t5.t5_multi_head_attention import T5MultiHeadAttention
 
 
 class T5TransformerLayer(keras.layers.Layer):
     def __init__(
         self,
@@ -88,16 +92,24 @@
     def call(
         self,
         hidden_states,
         attention_mask=None,
         position_bias=None,
         encoder_hidden_states=None,
         encoder_attention_mask=None,
+        use_causal_mask=False,
         training=False,
     ):
+        if use_causal_mask:
+            shape = tf.shape(hidden_states)
+            batch_size, length = shape[0], shape[1]
+            causal_mask = compute_causal_mask(batch_size, length, length)
+            attention_mask = tf.cast(attention_mask, "int32")
+            attention_mask = causal_mask & attention_mask
+
         x = hidden_states  # Intermediate result.
 
         residual = x
         x = self.self_attention_layernorm(x)
         x, position_bias = self.self_attention(
             x,
             mask=attention_mask,
@@ -119,15 +131,15 @@
             x = self.cross_attention_dropout(x, training=training)
             x = x + residual
 
         residual = x
         x = self.layer_norm(x)
         if self.use_gated_activation:
             hidden_activation = self.input_projector(x)
-            hidden_linear = self.gate_projector(hidden_states)
+            hidden_linear = self.gate_projector(x)
             x = hidden_activation * hidden_linear
         else:
             x = self.input_projector(x)
         x = self.dropout_layer(x, training=training)
         x = self.output_projector(x)
         x = self.dropout_layer(x, training=training)
         x = x + residual
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/task.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/task.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,83 +11,91 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Base class for Task models."""
 
 import os
 
+import keras_core
+import rich
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.utils.keras_utils import print_msg
-from keras_nlp.src.utils.keras_utils import print_row
 from keras_nlp.src.utils.pipeline_model import PipelineModel
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class Task(PipelineModel):
     """Base class for Task models."""
 
     def __init__(self, *args, **kwargs):
         self._backbone = None
         self._preprocessor = None
         super().__init__(*args, **kwargs)
 
-    def _check_for_loss_mismatch(self):
+    def _check_for_loss_mismatch(self, loss):
         """Check for a softmax/from_logits mismatch after compile.
 
         We cannot handle this in the general case, but we can handle this for
         the extremely common case of a single `SparseCategoricalCrossentropy`
         loss, and a `None` or `"softmax"` activation.
         """
         # Only handle a single loss.
-        if tf.nest.is_nested(self.loss):
+        if tf.nest.is_nested(loss):
             return
         # Only handle tasks with activation.
         if not hasattr(self, "activation"):
             return
 
-        loss = keras.losses.get(self.loss)
+        loss = keras.losses.get(loss)
         activation = keras.activations.get(self.activation)
         if isinstance(loss, keras.losses.SparseCategoricalCrossentropy):
             from_logits = loss.get_config()["from_logits"]
         elif loss == keras.losses.sparse_categorical_crossentropy:
             from_logits = False
         else:
             # Only handle sparse categorical crossentropy.
             return
 
-        is_softmax = activation == keras.activations.softmax
-        is_linear = activation == keras.activations.linear
-        if is_softmax and from_logits:
+        softmax_output = activation == keras.activations.softmax
+        logit_output = activation == keras.activations.linear
+        if softmax_output and from_logits:
             raise ValueError(
                 "The `loss` passed to `compile()` expects logit output, but "
                 "the model is configured to output softmax probabilities "
                 "(`activation='softmax'`). This will not converge! Pass "
                 "`from_logits=False` to your loss, e.g. "
                 "`loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)`. "
             )
-        if is_linear and not from_logits:
+        if logit_output and not from_logits:
             raise ValueError(
                 "The `loss` passed to `compile()` expects softmax probability "
                 "output, but the model is configured to output logits "
                 "(`activation=None`). This will not converge! Pass "
                 "`from_logits=True` to your loss, e.g. "
                 "`loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)`. "
             )
 
-    def compile(self, *args, **kwargs):
-        super().compile(*args, **kwargs)
-        self._check_for_loss_mismatch()
+    def compile(self, optimizer="rmsprop", loss=None, **kwargs):
+        self._check_for_loss_mismatch(loss)
+        super().compile(optimizer=optimizer, loss=loss, **kwargs)
 
     def preprocess_samples(self, x, y=None, sample_weight=None):
         return self.preprocessor(x, y=y, sample_weight=sample_weight)
 
+    def __setattr__(self, name, value):
+        # Work around torch setattr for properties.
+        if name in ["backbone", "preprocessor"]:
+            object.__setattr__(self, name, value)
+        else:
+            super().__setattr__(name, value)
+
     @property
     def backbone(self):
         """A `keras.Model` instance providing the backbone submodel."""
         return self._backbone
 
     @backbone.setter
     def backbone(self, value):
@@ -106,15 +114,14 @@
     def get_config(self):
         # Don't chain to super here. The default `get_config()` for functional
         # models is nested and cannot be passed to our Task constructors.
         return {
             "backbone": keras.layers.serialize(self.backbone),
             "preprocessor": keras.layers.serialize(self.preprocessor),
             "name": self.name,
-            "trainable": self.trainable,
         }
 
     @classmethod
     def from_config(cls, config):
         # The default `from_config()` for functional models will return a
         # vanilla `keras.Model`. We override it to get a subclass instance back.
         if "backbone" in config and isinstance(config["backbone"], dict):
@@ -236,38 +243,74 @@
         self,
         line_length=None,
         positions=None,
         print_fn=None,
         **kwargs,
     ):
         """Override `model.summary()` to show a preprocessor if set."""
-        # Defaults are copied from core Keras; we should try to stay in sync.
-        line_length = line_length or 98
-        positions = positions or [0.33, 0.55, 0.67, 1.0]
-        if positions[-1] <= 1:
-            positions = [int(line_length * p) for p in positions]
-        if print_fn is None:
+        # Below is copied from keras-core for now.
+        # We should consider an API contract.
+        line_length = line_length or 108
+
+        if not print_fn and not keras.utils.is_interactive_logging_enabled():
             print_fn = print_msg
 
+        def highlight_number(x):
+            return f"[color(45)]{x}[/]" if x is None else f"[color(34)]{x}[/]"
+
+        def highlight_symbol(x):
+            return f"[color(33)]{x}[/]"
+
+        def bold_text(x):
+            return f"[bold]{x}[/]"
+
         if self.preprocessor:
-            column_names = ["Tokenizer (type)", "Vocab #"]
+            # Create a rich console for printing. Capture for non-interactive logging.
+            if print_fn:
+                console = rich.console.Console(
+                    highlight=False, force_terminal=False, color_system=None
+                )
+                console.begin_capture()
+            else:
+                console = rich.console.Console(highlight=False)
+
+            column_1 = rich.table.Column(
+                "Tokenizer (type)",
+                justify="left",
+                width=int(0.5 * line_length),
+            )
+            column_2 = rich.table.Column(
+                "Vocab #",
+                justify="right",
+                width=int(0.5 * line_length),
+            )
+            table = rich.table.Table(
+                column_1, column_2, width=line_length, show_lines=True
+            )
             tokenizer = self.preprocessor.tokenizer
-            column_values = [
-                f"{tokenizer.name} ({tokenizer.__class__.__name__})",
-                f"{tokenizer.vocabulary_size()}",
-            ]
-
-            print_fn(f'Preprocessor: "{self.preprocessor.name}"')
-            print_fn("_" * line_length)
-            print_row(column_names, positions[1:3], print_fn)
-            print_fn("=" * line_length)
-            print_row(column_values, positions[1:3], print_fn)
-            print_fn("_" * line_length)
-            print_fn(" " * line_length)
+            tokenizer_name = rich.markup.escape(tokenizer.name)
+            tokenizer_class = highlight_symbol(
+                rich.markup.escape(tokenizer.__class__.__name__)
+            )
+            table.add_row(
+                f"{tokenizer_name} ({tokenizer_class})",
+                highlight_number(f"{tokenizer.vocabulary_size():,}"),
+            )
 
-        super().summary(
+            # Print the to the console.
+            preprocessor_name = rich.markup.escape(self.preprocessor.name)
+            console.print(bold_text(f'Preprocessor: "{preprocessor_name}"'))
+            console.print(table)
+
+            # Output captured summary for non-interactive logging.
+            if print_fn:
+                print_fn(console.end_capture(), line_break=False)
+
+        # Hardcode summary from keras_core for now.
+        keras_core.Model.summary(
+            self,
             line_length=line_length,
             positions=positions,
             print_fn=print_fn,
             **kwargs,
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/task_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/task_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -7,20 +7,18 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras.losses import SparseCategoricalCrossentropy
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.preprocessor import Preprocessor
 from keras_nlp.src.models.task import Task
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.tokenizer import Tokenizer
 
 
 class SimpleTokenizer(Tokenizer):
     def vocabulary_size(self):
         return 10
 
@@ -36,46 +34,62 @@
         inputs = keras.Input(shape=(5,))
         outputs = keras.layers.Dense(5)(inputs)
         super().__init__(inputs, outputs, **kwargs)
         self.preprocessor = preprocessor
         self.activation = keras.activations.get(activation)
 
 
-class TestTask(tf.test.TestCase):
+class TestTask(TestCase):
     def test_summary_with_preprocessor(self):
         preprocessor = SimplePreprocessor()
         model = SimpleTask(preprocessor)
         summary = []
-        model.summary(print_fn=lambda x: summary.append(x))
+        model.summary(print_fn=lambda x, line_break: summary.append(x))
         self.assertRegex("\n".join(summary), "Preprocessor:")
 
     def test_summary_without_preprocessor(self):
         model = SimpleTask()
         summary = []
-        model.summary(print_fn=lambda x: summary.append(x))
+        model.summary(print_fn=lambda x, line_break: summary.append(x))
         self.assertNotRegex("\n".join(summary), "Preprocessor:")
 
     def test_mismatched_loss(self):
         # Logit output.
         model = SimpleTask(activation=None)
-        model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))
+        model.compile(
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)
+        )
         # Non-standard losses should not throw.
         model.compile(loss="mean_squared_error")
         with self.assertRaises(ValueError):
             model.compile(loss="sparse_categorical_crossentropy")
         with self.assertRaises(ValueError):
-            model.compile(loss=SparseCategoricalCrossentropy(from_logits=False))
+            model.compile(
+                loss=keras.losses.SparseCategoricalCrossentropy(
+                    from_logits=False
+                )
+            )
 
         # Probability output.
         model = SimpleTask(activation="softmax")
-        model.compile(loss=SparseCategoricalCrossentropy(from_logits=False))
+        model.compile(
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)
+        )
         model.compile(loss="sparse_categorical_crossentropy")
         # Non-standard losses should not throw.
         model.compile(loss="mean_squared_error")
         with self.assertRaises(ValueError):
-            model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))
+            model.compile(
+                loss=keras.losses.SparseCategoricalCrossentropy(
+                    from_logits=True
+                )
+            )
 
         # Non-standard activations should not throw.
         model = SimpleTask(activation="tanh")
-        model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))
-        model.compile(loss=SparseCategoricalCrossentropy(from_logits=False))
+        model.compile(
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)
+        )
+        model.compile(
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)
+        )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/whisper/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/opt/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_backbone.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,37 +9,40 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Whisper backbone model."""
 
+import copy
 
-import tensorflow as tf
-from tensorflow import keras
-
-from keras_nlp.src.layers.position_embedding import PositionEmbedding
-from keras_nlp.src.layers.token_and_position_embedding import (
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.layers.modeling.position_embedding import PositionEmbedding
+from keras_nlp.src.layers.modeling.token_and_position_embedding import (
     TokenAndPositionEmbedding,
 )
 from keras_nlp.src.models.backbone import Backbone
 from keras_nlp.src.models.whisper.whisper_decoder import WhisperDecoder
 from keras_nlp.src.models.whisper.whisper_encoder import WhisperEncoder
-
-# We hardcode the number of mel-frequency filters:
-# https://github.com/openai/whisper/blob/v20230124/whisper/audio.py#L101-L102.
-# TODO: If needed, we can make it configurable.
-NUM_MELS = 80
+from keras_nlp.src.models.whisper.whisper_presets import backbone_presets
+from keras_nlp.src.utils.python_utils import classproperty
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
 def whisper_kernel_initializer(stddev=0.02):
     return keras.initializers.TruncatedNormal(stddev=stddev)
 
 
-@keras.utils.register_keras_serializable(package="keras_nlp")
+class Padder(keras.layers.Layer):
+    def call(self, x):
+        return ops.pad(x, [[0, 0], [1, 1], [0, 0]])
+
+
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class WhisperBackbone(Backbone):
     """A Whisper encoder-decoder network for speech.
 
     This class implements a Transformer-based encoder-decoder model as
     described in
     ["Robust Speech Recognition via Large-Scale Weak Supervision"](https://arxiv.org/abs/2212.04356).
     It includes the embedding lookups and transformer layers, but not the head
@@ -59,30 +62,32 @@
         num_layers: int. The number of transformer encoder layers and
             transformer decoder layers.
         num_heads: int. The number of attention heads for each transformer.
             The hidden size must be divisible by the number of attention heads.
         hidden_dim: int. The size of the transformer encoding and pooler layers.
         intermediate_dim: int. The output dimension of the first Dense layer in
             a two-layer feedforward network for each transformer.
+        num_mels: int. The number of mel-frequency filters. Defaults to `80`.
         dropout: float. Dropout probability for the Transformer encoder.
         max_encoder_sequence_length: int. The maximum sequence length that the
             audio encoder can consume. Since the second convolutional layer in
             the encoder reduces the sequence length by half (stride of 2), we
             use `max_encoder_sequence_length // 2` as the sequence length for the
             positional embedding layer.
         max_decoder_sequence_length: int. The maximum sequence length that the
             text decoder can consume.
 
     Examples:
+
     ```python
     input_data = {
-        "encoder_features": tf.ones(shape=(1, 12, 80), dtype=tf.int64),
-        "decoder_token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "decoder_padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], shape=(1, 12)
+        "encoder_features": np.ones(shape=(1, 12, 80), dtype="int32"),
+        "decoder_token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "decoder_padding_mask": np.array(
+            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]
         ),
     }
 
     # Randomly initialized Whisper encoder-decoder model with a custom config.
     model = keras_nlp.models.WhisperBackbone(
         vocabulary_size=51864,
         num_layers=4,
@@ -99,23 +104,26 @@
     def __init__(
         self,
         vocabulary_size,
         num_layers,
         num_heads,
         hidden_dim,
         intermediate_dim,
+        num_mels=80,
         dropout=0.0,
         max_encoder_sequence_length=3000,
         max_decoder_sequence_length=448,
         **kwargs,
     ):
+        assert_tf_backend(self.__class__.__name__)
+
         # Encoder inputs. Note that the encoder does not have a padding mask:
         # https://github.com/openai/whisper/blob/v20230124/whisper/model.py#L132.
         encoder_feature_input = keras.Input(
-            shape=(None, NUM_MELS), dtype="float32", name="encoder_features"
+            shape=(None, num_mels), dtype="float32", name="encoder_features"
         )
 
         # Decoder inputs.
         decoder_token_id_input = keras.Input(
             shape=(None,), dtype="int32", name="decoder_token_ids"
         )
         decoder_padding_mask = keras.Input(
@@ -139,17 +147,15 @@
             encoder_conv_layer_1(encoder_feature_input),
             approximate=False,
         )
 
         # For the second conv. layer, we cannot use `padding="same"` since
         # that corresponds to a padding size of 1.5 (since stride is 2). Hence,
         # we will manually pad the input.
-        embedded_features = tf.pad(
-            embedded_features, paddings=[[0, 0], [1, 1], [0, 0]]
-        )
+        embedded_features = Padder()(embedded_features)
         encoder_conv_layer_2 = keras.layers.Conv1D(
             filters=hidden_dim,
             kernel_size=3,
             strides=2,
             padding="valid",
             name="encoder_token_embedding_conv_layer_2",
         )
@@ -191,15 +197,15 @@
                 name=f"transformer_encoder_layer_{i}",
             )(x)
 
         x = keras.layers.LayerNormalization(
             name="encoder_layer_norm",
             axis=-1,
             epsilon=1e-5,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         encoder_output = x
 
         # ====== Decoder ======
 
         # Embed tokens and positions.
         x = TokenAndPositionEmbedding(
@@ -225,27 +231,26 @@
                 activation=lambda x: keras.activations.gelu(
                     x, approximate=False
                 ),
                 layer_norm_epsilon=1e-5,
                 kernel_initializer=whisper_kernel_initializer(),
                 normalize_first=True,
                 name=f"transformer_decoder_layer_{i}",
-                has_cross_attention=True,
             )
             x = transformer_decoder_layer(
                 decoder_sequence=x,
                 encoder_sequence=encoder_output,
                 decoder_padding_mask=decoder_padding_mask,
             )
 
         x = keras.layers.LayerNormalization(
             name="decoder_layer_norm",
             axis=-1,
             epsilon=1e-5,
-            dtype=tf.float32,
+            dtype="float32",
         )(x)
         decoder_output = x
 
         # Instantiate using Functional API Model constructor
         super().__init__(
             inputs={
                 "encoder_features": encoder_feature_input,
@@ -261,33 +266,39 @@
 
         # All references to `self` below this line
         self.vocabulary_size = vocabulary_size
         self.num_layers = num_layers
         self.num_heads = num_heads
         self.hidden_dim = hidden_dim
         self.intermediate_dim = intermediate_dim
+        self.num_mels = num_mels
         self.dropout = dropout
         self.max_encoder_sequence_length = max_encoder_sequence_length
         self.max_decoder_sequence_length = max_decoder_sequence_length
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "vocabulary_size": self.vocabulary_size,
                 "num_layers": self.num_layers,
                 "num_heads": self.num_heads,
                 "hidden_dim": self.hidden_dim,
                 "intermediate_dim": self.intermediate_dim,
+                "num_mels": self.num_mels,
                 "dropout": self.dropout,
                 "max_encoder_sequence_length": self.max_encoder_sequence_length,
                 "max_decoder_sequence_length": self.max_decoder_sequence_length,
             }
         )
         return config
 
     @property
     def token_embedding(self):
         return self.get_layer(
             "decoder_token_and_position_embedding"
         ).token_embedding
 
+    @classproperty
+    def presets(cls):
+        return copy.deepcopy(backbone_presets)
+
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/whisper/whisper_encoder.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_encoder.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,27 +9,27 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Whisper encoder block."""
 
-from tensorflow import keras
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.transformer_encoder import TransformerEncoder
 
-from keras_nlp.src.layers.transformer_encoder import TransformerEncoder
 
-
-@keras.utils.register_keras_serializable(package="keras_nlp")
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class WhisperEncoder(TransformerEncoder):
     """A Whisper encoder.
 
     Inherits from `keras_nlp.layers.TransformerEncoder`, and overrides the
-    `_build` method so as to remove the bias term from the key projection layer.
+    `build` method so as to remove the bias term from the key projection layer.
     """
 
-    def _build(self, input_shape):
-        super()._build(input_shape)
+    def build(self, inputs_shape):
+        super().build(inputs_shape)
 
         # Since there is no exposed option for this in MHA, we will reach into
         # the internals of the layer for now.
         self._self_attention_layer._key_dense.bias_axes = None
+        self._self_attention_layer._key_dense.bias = None
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/roberta/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,17 +54,16 @@
             consume. The sequence length of the input must be less than
             `max_sequence_length` default value. This determines the variable
             shape for positional embeddings.
 
     Examples:
     ```python
     input_data = {
-        "token_ids": tf.ones(shape=(1, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], shape=(1, 12)),
+        "token_ids": np.ones(shape=(1, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),
     }
 
     # Pretrained XLM-R encoder.
     model = keras_nlp.models.XLMRobertaBackbone.from_preset(
         "xlm_roberta_base_multi",
     )
     model(input_data)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt_neo_x/gpt_neo_x_backbone_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,116 +7,108 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-"""Tests for XLM-RoBERTa backbone models."""
+"""Test for GPTNeoX backbone models."""
 
 import os
 
 import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.models import GPTNeoXBackbone
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaBackboneTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class GPTNeoXTest(TestCase):
     def setUp(self):
-        self.backbone = XLMRobertaBackbone(
+        self.backbone = GPTNeoXBackbone(
             vocabulary_size=10,
-            num_layers=2,
-            num_heads=2,
-            hidden_dim=2,
-            intermediate_dim=4,
-            max_sequence_length=5,
+            num_layers=4,
+            num_heads=4,
+            hidden_dim=64,
+            intermediate_dim=64,
+            max_sequence_length=10,
         )
         self.input_batch = {
-            "token_ids": tf.ones((2, 5), dtype="int32"),
-            "padding_mask": tf.ones((2, 5), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
-    def test_valid_call_xlm_roberta(self):
+    def test_call(self):
         self.backbone(self.input_batch)
 
     def test_token_embedding(self):
         output = self.backbone.token_embedding(self.input_batch["token_ids"])
-        self.assertEqual(output.shape, (2, 5, 2))
+        self.assertEqual(output.shape, (2, 5, 64))
 
     def test_name(self):
         # Check default name passed through
-        self.assertRegexpMatches(self.backbone.name, "xlm_roberta_backbone")
+        self.assertRegexpMatches(self.backbone.name, "gpt_neo_x_backbone")
 
-    def test_variable_sequence_length_call_xlm_roberta(self):
+    def test_variable_sequence_length(self):
         for seq_length in (2, 3, 4):
             input_data = {
-                "token_ids": tf.ones((2, seq_length), dtype="int32"),
-                "padding_mask": tf.ones((2, seq_length), dtype="int32"),
+                "token_ids": ops.ones((2, seq_length), dtype="int32"),
+                "padding_mask": ops.ones((2, seq_length), dtype="int32"),
             }
-            output = self.backbone(input_data)
-            self.assertAllEqual(
-                tf.shape(output), [2, seq_length, self.backbone.hidden_dim]
-            )
+            self.backbone(input_data)
 
     def test_predict(self):
         self.backbone.predict(self.input_batch)
         self.backbone.predict(self.input_dataset)
 
     def test_serialization(self):
-        new_backbone = keras.utils.deserialize_keras_object(
-            keras.utils.serialize_keras_object(self.backbone)
+        new_backbone = keras.saving.deserialize_keras_object(
+            keras.saving.serialize_keras_object(self.backbone)
         )
         self.assertEqual(new_backbone.get_config(), self.backbone.get_config())
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.large
+    def test_saved_model(self):
         model_output = self.backbone(self.input_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.backbone.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.backbone.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, XLMRobertaBackbone)
+        self.assertIsInstance(restored_model, GPTNeoXBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
 
 @pytest.mark.tpu
 @pytest.mark.usefixtures("tpu_test_class")
-class XLMRobertaBackboneTPUTest(tf.test.TestCase, parameterized.TestCase):
+class GPTNeoXBackboneTPUTest(TestCase):
     def setUp(self):
         with self.tpu_strategy.scope():
-            self.backbone = XLMRobertaBackbone(
-                vocabulary_size=1000,
-                num_layers=2,
-                num_heads=2,
+            GPTNeoXBackbone(
+                vocabulary_size=10,
+                num_layers=4,
+                num_heads=4,
                 hidden_dim=64,
-                intermediate_dim=128,
-                max_sequence_length=128,
+                intermediate_dim=64,
+                max_sequence_length=10,
             )
         self.input_batch = {
-            "token_ids": tf.ones((8, 128), dtype="int32"),
-            "padding_mask": tf.ones((8, 128), dtype="int32"),
+            "token_ids": ops.ones((2, 5), dtype="int32"),
+            "padding_mask": ops.ones((2, 5), dtype="int32"),
         }
         self.input_dataset = tf.data.Dataset.from_tensor_slices(
             self.input_batch
         ).batch(2)
 
     def test_predict(self):
-        self.backbone.compile()
-        self.backbone.predict(self.input_dataset)
+        self.model.compile()
+        self.model.predict(self.input_dataset)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,25 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """XLM-RoBERTa classification model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.roberta.roberta_backbone import roberta_kernel_initializer
 from keras_nlp.src.models.task import Task
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
     XLMRobertaPreprocessor,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_presets import backbone_presets
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.XLMRobertaClassifier")
 class XLMRobertaClassifier(Task):
     """An end-to-end XLM-RoBERTa model for classification tasks.
 
@@ -50,17 +48,17 @@
 
     Args:
         backbone: A `keras_nlp.models.XLMRobertaBackbone` instance.
         num_classes: int. Number of classes to predict.
         preprocessor: A `keras_nlp.models.XLMRobertaPreprocessor` or `None`. If
             `None`, this model will not apply preprocessing, and inputs should
             be preprocessed before calling the model.
-        activation: Optional `str` or callable, defaults to `None`. The
-            activation function to use on the model outputs. Set
-            `activation="softmax"` to return output probabilities.
+        activation: Optional `str` or callable. The activation function to use
+            on the model outputs. Set `activation="softmax"` to return output
+            probabilities. Defaults to `None`.
         hidden_dim: int. The size of the pooler layer.
         dropout: float. The dropout probability value, applied to the pooled
             output, and after the first dense layer.
 
     Examples:
 
     Raw string data.
@@ -87,18 +85,16 @@
     # Fit again.
     classifier.fit(x=features, y=labels, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     features = {
-        "token_ids": tf.ones(shape=(2, 12), dtype=tf.int64),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2, shape=(2, 12)
-        ),
+        "token_ids": np.ones(shape=(2, 12), dtype="int32"),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),
     }
     labels = [0, 3]
 
     # Pretrained classifier without preprocessing.
     classifier = keras_nlp.models.XLMRobertaClassifier.from_preset(
         "xlm_roberta_base_multi",
         num_classes=4,
@@ -189,21 +185,22 @@
         self.backbone = backbone
         self.preprocessor = preprocessor
         self.num_classes = num_classes
         self.activation = keras.activations.get(activation)
         self.hidden_dim = hidden_dim
         self.dropout = dropout
 
+        logit_output = self.activation == keras.activations.linear
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(
-                from_logits=activation is None
+                from_logits=logit_output
             ),
             optimizer=keras.optimizers.Adam(5e-5),
-            metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     def preprocess_samples(self, x, y=None, sample_weight=None):
         return self.preprocessor(x, y=y, sample_weight=sample_weight)
 
     def get_config(self):
         config = super().get_config()
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_test.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,137 +1,132 @@
-# Copyright 2023 The KerasNLP Authors
+# Copyright 2022 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for XLM-RoBERTa classification model."""
+"""Tests for XLM-RoBERTa masked language model."""
 
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_classifier import (
-    XLMRobertaClassifier,
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm import (
+    XLMRobertaMaskedLM,
 )
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
-    XLMRobertaPreprocessor,
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import (
+    XLMRobertaMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
     XLMRobertaTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaClassifierTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaMaskedLMTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the earth is round"]
+            ["the quick brown fox", "the slow brown fox"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=10,
+            vocab_size=5,
             model_type="WORD",
-            unk_id=0,
-            bos_id=1,
-            eos_id=2,
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
+            pad_piece="<pad>",
+            unk_piece="<unk>",
+            bos_piece="<s>",
+            eos_piece="</s>",
+            user_defined_symbols="[MASK]",
         )
-        self.preprocessor = XLMRobertaPreprocessor(
-            tokenizer=XLMRobertaTokenizer(proto=bytes_io.getvalue()),
+        self.proto = bytes_io.getvalue()
+
+        self.preprocessor = XLMRobertaMaskedLMPreprocessor(
+            XLMRobertaTokenizer(proto=self.proto),
             sequence_length=5,
+            mask_selection_length=2,
         )
+
         self.backbone = XLMRobertaBackbone(
-            vocabulary_size=10,
+            vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
             num_layers=2,
             num_heads=2,
             hidden_dim=2,
             intermediate_dim=4,
             max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.classifier = XLMRobertaClassifier(
+
+        self.masked_lm = XLMRobertaMaskedLM(
             self.backbone,
-            num_classes=4,
             preprocessor=self.preprocessor,
-            # Check we handle serialization correctly.
-            activation=keras.activations.softmax,
-            hidden_dim=4,
         )
 
-        self.raw_batch = tf.constant(
-            [
-                "the quick brown fox.",
-                "the slow brown fox.",
-            ]
-        )
-        self.preprocessed_batch = self.preprocessor(self.raw_batch)
+        self.raw_batch = [
+            "the quick brown fox",
+            "the slow brown fox",
+        ]
+        self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
         self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            (self.raw_batch, tf.ones((2,)))
+            self.raw_batch
         ).batch(2)
         self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
 
-    def test_valid_call_classifier(self):
-        self.classifier(self.preprocessed_batch)
+    def test_valid_call_masked_lm(self):
+        self.masked_lm(self.preprocessed_batch)
 
     def test_classifier_predict(self):
-        preds1 = self.classifier.predict(self.raw_batch)
-        self.classifier.preprocessor = None
-        preds2 = self.classifier.predict(self.preprocessed_batch)
-        # Assert predictions match.
-        self.assertAllClose(preds1, preds2)
-        # Assert valid softmax output.
-        self.assertAllClose(tf.reduce_sum(preds2, axis=-1), [1.0, 1.0])
+        self.masked_lm.predict(self.raw_batch)
+        self.masked_lm.preprocessor = None
+        self.masked_lm.predict(self.preprocessed_batch)
 
     def test_classifier_fit(self):
-        self.classifier.fit(self.raw_dataset)
-        self.classifier.preprocessor = None
-        self.classifier.fit(self.preprocessed_dataset)
+        self.masked_lm.fit(self.raw_dataset)
+        self.masked_lm.preprocessor = None
+        self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_classifier_fit_no_xla(self):
-        self.classifier.preprocessor = None
-        self.classifier.compile(
-            loss="sparse_categorical_crossentropy",
+        self.masked_lm.preprocessor = None
+        self.masked_lm.compile(
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
             jit_compile=False,
         )
-        self.classifier.fit(self.preprocessed_dataset)
+        self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.classifier)
-        new_classifier = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.masked_lm)
+        new_classifier = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_classifier.get_config(),
-            self.classifier.get_config(),
+            self.masked_lm.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saving_model(self, save_format, filename):
-        model_output = self.classifier.predict(self.raw_batch)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        self.classifier.save(path, save_format=save_format, **kwargs)
-        restored_model = keras.models.load_model(path)
+    @pytest.mark.large
+    def test_saved_model(self):
+        save_path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(save_path, save_format="keras_v3")
+        restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
-        self.assertIsInstance(restored_model, XLMRobertaClassifier)
+        self.assertIsInstance(restored_model, XLMRobertaMaskedLM)
+
+        model_output = self.masked_lm(self.preprocessed_batch)
+        restored_output = restored_model(self.preprocessed_batch)
 
-        # Check that output matches.
-        restored_output = restored_model.predict(self.raw_batch)
         self.assertAllClose(model_output, restored_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,26 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """XLM-RoBERTa masked lm model."""
 
 import copy
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_head import MaskedLMHead
+from keras_nlp.src.backend import keras
+from keras_nlp.src.layers.modeling.masked_lm_head import MaskedLMHead
 from keras_nlp.src.models.roberta.roberta_backbone import roberta_kernel_initializer
 from keras_nlp.src.models.task import Task
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import (
     XLMRobertaMaskedLMPreprocessor,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_presets import backbone_presets
-from keras_nlp.src.utils.keras_utils import is_xla_compatible
 from keras_nlp.src.utils.python_utils import classproperty
 
 
 @keras_nlp_export("keras_nlp.models.XLMRobertaMaskedLM")
 class XLMRobertaMaskedLM(Task):
     """An end-to-end XLM-RoBERTa model for the masked language modeling task.
 
@@ -83,21 +81,17 @@
     masked_lm.fit(x=features, batch_size=2)
     ```
 
     Preprocessed integer data.
     ```python
     # Create a preprocessed dataset where 0 is the mask token.
     features = {
-        "token_ids": tf.constant(
-            [[1, 2, 0, 4, 0, 6, 7, 8]] * 2, shape=(2, 8)
-        ),
-        "padding_mask": tf.constant(
-            [[1, 1, 1, 1, 1, 1, 1, 1]] * 2, shape=(2, 8)
-        ),
-        "mask_positions": tf.constant([[2, 4]] * 2, shape=(2, 2))
+        "token_ids": np.array([[1, 2, 0, 4, 0, 6, 7, 8]] * 2),
+        "padding_mask": np.array([[1, 1, 1, 1, 1, 1, 1, 1]] * 2),
+        "mask_positions": np.array([[2, 4]] * 2)
     }
     # Labels are the original masked values.
     labels = [[3, 5]] * 2
 
     masked_lm = keras_nlp.models.XLMRobertaMaskedLM.from_preset(
         "xlm_roberta_base_multi",
         preprocessor=None,
@@ -138,16 +132,16 @@
         # All references to `self` below this line
         self.backbone = backbone
         self.preprocessor = preprocessor
 
         self.compile(
             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             optimizer=keras.optimizers.Adam(5e-5),
-            weighted_metrics=keras.metrics.SparseCategoricalAccuracy(),
-            jit_compile=is_xla_compatible(self),
+            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
+            jit_compile=True,
         )
 
     @classproperty
     def backbone_cls(cls):
         return XLMRobertaBackbone
 
     @classproperty
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,180 +8,191 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""XLM-RoBERTa masked language model preprocessor layer."""
+"""XLM-RoBERTa preprocessor layer."""
 
-from absl import logging
+import copy
 
 from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.layers.masked_lm_mask_generator import MaskedLMMaskGenerator
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
-    XLMRobertaPreprocessor,
+from keras_nlp.src.layers.preprocessing.multi_segment_packer import (
+    MultiSegmentPacker,
+)
+from keras_nlp.src.models.preprocessor import Preprocessor
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_presets import backbone_presets
+from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
+    XLMRobertaTokenizer,
+)
+from keras_nlp.src.utils.keras_utils import (
+    convert_inputs_to_list_of_tensor_segments,
 )
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
+from keras_nlp.src.utils.python_utils import classproperty
+
 
+@keras_nlp_export("keras_nlp.models.XLMRobertaPreprocessor")
+class XLMRobertaPreprocessor(Preprocessor):
+    """An XLM-RoBERTa preprocessing layer which tokenizes and packs inputs.
 
-@keras_nlp_export("keras_nlp.models.XLMRobertaMaskedLMPreprocessor")
-class XLMRobertaMaskedLMPreprocessor(XLMRobertaPreprocessor):
-    """XLM-RoBERTa preprocessing for the masked language modeling task.
-
-    This preprocessing layer will prepare inputs for a masked language modeling
-    task. It is primarily intended for use with the
-    `keras_nlp.models.XLMRobertaMaskedLM` task model. Preprocessing will occur in
-    multiple steps.
+    This preprocessing layer will do three things:
 
     1. Tokenize any number of input segments using the `tokenizer`.
-    2. Pack the inputs together with the appropriate `"<s>"`, `"</s>"` and
-      `"<pad>"` tokens, i.e., adding a single `"<s>"` at the start of the
-      entire sequence, `"</s></s>"` between each segment,
-      and a `"</s>"` at the end of the entire sequence.
-    3. Randomly select non-special tokens to mask, controlled by
-      `mask_selection_rate`.
-    4. Construct a `(x, y, sample_weight)` tuple suitable for training with a
-      `keras_nlp.models.XLMRobertaMaskedLM` task model.
+    2. Pack the inputs together using a `keras_nlp.layers.MultiSegmentPacker`.
+      with the appropriate `"<s>"`, `"</s>"` and `"<pad>"` tokens, i.e., adding
+      a single `"<s>"` at the start of the entire sequence, `"</s></s>"` at the
+      end of each segment, save the last and a `"</s>"` at the end of the
+      entire sequence.
+    3. Construct a dictionary with keys `"token_ids"` and `"padding_mask"`,
+      that can be passed directly to an XLM-RoBERTa model.
+
+    This layer can be used directly with `tf.data.Dataset.map` to preprocess
+    string data in the `(x, y, sample_weight)` format used by
+    `keras.Model.fit`.
 
     Args:
-        tokenizer: A `keras_nlp.models.XLMRobertaTokenizer` instance.
-        sequence_length: int. The length of the packed inputs.
-        truncate: string. The algorithm to truncate a list of batched segments
-            to fit within `sequence_length`. The value can be either
-            `round_robin` or `waterfall`:
+        tokenizer: A `keras_nlp.tokenizers.XLMRobertaTokenizer` instance.
+        sequence_length: The length of the packed inputs.
+        truncate: The algorithm to truncate a list of batched segments to fit
+            within `sequence_length`. The value can be either `round_robin` or
+            `waterfall`:
                 - `"round_robin"`: Available space is assigned one token at a
                     time in a round-robin fashion to the inputs that still need
                     some, until the limit is reached.
                 - `"waterfall"`: The allocation of the budget is done using a
                     "waterfall" algorithm that allocates quota in a
                     left-to-right manner and fills up the buckets until we run
                     out of budget. It supports an arbitrary number of segments.
-        mask_selection_rate: float. The probability an input token will be
-            dynamically masked.
-        mask_selection_length: int. The maximum number of masked tokens
-            in a given sample.
-        mask_token_rate: float. The probability the a selected token will be
-            replaced with the mask token.
-        random_token_rate: float. The probability the a selected token will be
-            replaced with a random token from the vocabulary. A selected token
-            will be left as is with probability
-            `1 - mask_token_rate - random_token_rate`.
 
     Call arguments:
         x: A tensor of single string sequences, or a tuple of multiple
             tensor sequences to be packed together. Inputs may be batched or
             unbatched. For single sequences, raw python inputs will be converted
             to tensors. For multiple sequences, pass tensors directly.
-        y: Label data. Should always be `None` as the layer generates labels.
-        sample_weight: Label weights. Should always be `None` as the layer
-            generates label weights.
+        y: Any label data. Will be passed through unaltered.
+        sample_weight: Any label weight data. Will be passed through unaltered.
 
     Examples:
 
     Directly calling the layer on data.
     ```python
-    # Load the preprocessor from a preset.
-    preprocessor = keras_nlp.models.XLMRobertaMaskedLMPreprocessor.from_preset(
+    preprocessor = keras_nlp.models.XLMRobertaPreprocessor.from_preset(
         "xlm_roberta_base_multi"
     )
 
-    # Tokenize and mask a single sentence.
+    # Tokenize and pack a single sentence.
     preprocessor("The quick brown fox jumped.")
-    # Tokenize and mask a batch of single sentences.
-    preprocessor(["The quick brown fox jumped.", "Call me Ishmael."])
-    # Tokenize and mask sentence pairs.
-    # In this case, always convert input to tensors before calling the layer.
-    first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
-    second = tf.constant(["The fox tripped.", "Oh look, a whale."])
+
+    # Tokenize a batch of single sentences.
+    preprocessor(["The quick brown fox jumped.", " "])
+
+    # Preprocess a batch of sentence pairs.
+    # When handling multiple sequences, always convert to tensors first!
+    first = tf.constant(["The quick brown fox jumped.", " "])
+    second = tf.constant(["The fox tripped.", "  "])
     preprocessor((first, second))
+
+    # Custom vocabulary.
+    def train_sentencepiece(ds, vocab_size):
+        bytes_io = io.BytesIO()
+        sentencepiece.SentencePieceTrainer.train(
+            sentence_iterator=ds.as_numpy_iterator(),
+            model_writer=bytes_io,
+            vocab_size=vocab_size,
+            model_type="WORD",
+            unk_id=0,
+            bos_id=1,
+            eos_id=2,
+        )
+        return bytes_io.getvalue()
+    ds = tf.data.Dataset.from_tensor_slices(
+        ["the quick brown fox", "the earth is round"]
+    )
+    proto = train_sentencepiece(ds, vocab_size=10)
+    tokenizer = keras_nlp.models.XLMRobertaTokenizer(proto=proto)
+    preprocessor = keras_nlp.models.XLMRobertaPreprocessor(tokenizer)
+    preprocessor("The quick brown fox jumped.")
     ```
 
     Mapping with `tf.data.Dataset`.
     ```python
-    preprocessor = keras_nlp.models.XLMRobertaMaskedLMPreprocessor.from_preset(
+    preprocessor = keras_nlp.models.XLMRobertaPreprocessor.from_preset(
         "xlm_roberta_base_multi"
     )
+
     first = tf.constant(["The quick brown fox jumped.", "Call me Ishmael."])
     second = tf.constant(["The fox tripped.", "Oh look, a whale."])
+    label = tf.constant([1, 1])
 
-    # Map single sentences.
+    # Map labeled single sentences.
+    ds = tf.data.Dataset.from_tensor_slices((first, label))
+    ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
+
+    # Map unlabeled single sentences.
     ds = tf.data.Dataset.from_tensor_slices(first)
     ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
 
-    # Map sentence pairs.
+    # Map labeled sentence pairs.
+    ds = tf.data.Dataset.from_tensor_slices(((first, second), label))
+    ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)
+
+    # Map unlabeled sentence pairs.
     ds = tf.data.Dataset.from_tensor_slices((first, second))
     # Watch out for tf.data's default unpacking of tuples here!
     # Best to invoke the `preprocessor` directly in this case.
     ds = ds.map(
         lambda first, second: preprocessor(x=(first, second)),
         num_parallel_calls=tf.data.AUTOTUNE,
     )
     ```
-    ```
     """
 
     def __init__(
         self,
         tokenizer,
         sequence_length=512,
         truncate="round_robin",
-        mask_selection_rate=0.15,
-        mask_selection_length=96,
-        mask_token_rate=0.8,
-        random_token_rate=0.1,
         **kwargs,
     ):
-        super().__init__(
-            tokenizer,
-            sequence_length=sequence_length,
-            truncate=truncate,
-            **kwargs,
-        )
+        super().__init__(**kwargs)
+
+        self.tokenizer = tokenizer
 
-        self.masker = MaskedLMMaskGenerator(
-            mask_selection_rate=mask_selection_rate,
-            mask_selection_length=mask_selection_length,
-            mask_token_rate=mask_token_rate,
-            random_token_rate=random_token_rate,
-            vocabulary_size=tokenizer.vocabulary_size(),
-            mask_token_id=tokenizer.mask_token_id,
-            unselectable_token_ids=[
-                tokenizer.start_token_id,
-                tokenizer.end_token_id,
-                tokenizer.pad_token_id,
-            ],
+        self.packer = MultiSegmentPacker(
+            start_value=self.tokenizer.start_token_id,
+            end_value=self.tokenizer.end_token_id,
+            sep_value=[self.tokenizer.end_token_id] * 2,
+            pad_value=self.tokenizer.pad_token_id,
+            truncate=truncate,
+            sequence_length=sequence_length,
         )
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
-                "mask_selection_rate": self.masker.mask_selection_rate,
-                "mask_selection_length": self.masker.mask_selection_length,
-                "mask_token_rate": self.masker.mask_token_rate,
-                "random_token_rate": self.masker.random_token_rate,
+                "sequence_length": self.packer.sequence_length,
+                "truncate": self.packer.truncate,
             }
         )
         return config
 
     def call(self, x, y=None, sample_weight=None):
-        if y is not None or sample_weight is not None:
-            logging.warning(
-                f"{self.__class__.__name__} generates `y` and `sample_weight` "
-                "based on your input data, but your data already contains `y` "
-                "or `sample_weight`. Your `y` and `sample_weight` will be "
-                "ignored."
-            )
-
-        x = super().call(x)
-        token_ids, padding_mask = x["token_ids"], x["padding_mask"]
-        masker_outputs = self.masker(token_ids)
+        x = convert_inputs_to_list_of_tensor_segments(x)
+        x = [self.tokenizer(segment) for segment in x]
+        token_ids, _ = self.packer(x)
         x = {
-            "token_ids": masker_outputs["token_ids"],
-            "padding_mask": padding_mask,
-            "mask_positions": masker_outputs["mask_positions"],
+            "token_ids": token_ids,
+            "padding_mask": token_ids != self.tokenizer.pad_token_id,
         }
-        y = masker_outputs["mask_ids"]
-        sample_weight = masker_outputs["mask_weights"]
         return pack_x_y_sample_weight(x, y, sample_weight)
 
+    @classproperty
+    def tokenizer_cls(cls):
+        return XLMRobertaTokenizer
+
+    @classproperty
+    def presets(cls):
+        return copy.deepcopy(backbone_presets)
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/gpt2/gpt2_causal_lm_preprocessor_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,175 +1,148 @@
-# Copyright 2022 The KerasNLP Authors
+# Copyright 2023 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Tests for XLM-RoBERTa masked language model preprocessor layer."""
-
-import io
+"""Tests for GPT2 causal LM preprocessor layer."""
 import os
 
 import pytest
-import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import (
-    XLMRobertaMaskedLMPreprocessor,
-)
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
-    XLMRobertaTokenizer,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor import (
+    GPT2CausalLMPreprocessor,
 )
+from keras_nlp.src.models.gpt2.gpt2_tokenizer import GPT2Tokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaMaskedLMPreprocessorTest(
-    tf.test.TestCase, parameterized.TestCase
-):
+class GPT2CausalLMPreprocessorTest(TestCase):
     def setUp(self):
-        bytes_io = io.BytesIO()
-        vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the earth is round"]
-        )
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=vocab_data.as_numpy_iterator(),
-            model_writer=bytes_io,
-            vocab_size=12,
-            model_type="WORD",
-            pad_id=0,
-            unk_id=1,
-            bos_id=2,
-            eos_id=3,
-            pad_piece="<pad>",
-            unk_piece="<unk>",
-            bos_piece="<s>",
-            eos_piece="</s>",
-            user_defined_symbols="[MASK]",
-        )
-        self.proto = bytes_io.getvalue()
-
-        self.tokenizer = XLMRobertaTokenizer(proto=self.proto)
-        self.preprocessor = XLMRobertaMaskedLMPreprocessor(
-            tokenizer=self.tokenizer,
-            # Simplify out testing by masking every available token.
-            mask_selection_rate=1.0,
-            mask_token_rate=1.0,
-            random_token_rate=0.0,
-            mask_selection_length=5,
-            sequence_length=12,
+        self.vocab = {
+            "!": 0,
+            "air": 1,
+            "air": 2,
+            "plane": 3,
+            "at": 4,
+            "port": 5,
+            "<|endoftext|>": 6,
+        }
+
+        self.merges = [" a", " t", " i", " b", "a i", "p l", "n e"]
+        self.merges += ["a t", "p o", "r t", "t h", "ai r", "pl a", "po rt"]
+        self.merges += ["ai r", "a i", "pla ne"]
+
+        self.preprocessor = GPT2CausalLMPreprocessor(
+            tokenizer=GPT2Tokenizer(
+                vocabulary=self.vocab,
+                merges=self.merges,
+            ),
+            sequence_length=8,
         )
 
-    def test_preprocess_strings(self):
-        input_data = " brown fox quick"
+    def test_strings(self):
+        input_data = "airplane at airport"
 
         x, y, sw = self.preprocessor(input_data)
-        self.assertAllEqual(
-            x["token_ids"], [0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]
-        )
-        self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
-        )
-        self.assertAllEqual(x["mask_positions"], [1, 2, 3, 0, 0])
-        self.assertAllEqual(y, [7, 9, 11, 0, 0])
-        self.assertAllEqual(sw, [1.0, 1.0, 1.0, 0.0, 0.0])
+        self.assertAllEqual(x["token_ids"], [6, 1, 3, 4, 2, 5, 6, 0])
+        self.assertAllEqual(x["padding_mask"], [1, 1, 1, 1, 1, 1, 1, 0])
+        self.assertAllEqual(y, [1, 3, 4, 2, 5, 6, 0, 0])
+        self.assertAllEqual(sw, [1, 1, 1, 1, 1, 1, 0, 0])
 
-    def test_preprocess_list_of_strings(self):
-        input_data = [" brown fox quick"] * 13
+    def test_list_of_strings(self):
+        input_data = ["airplane at airport"] * 4
 
         x, y, sw = self.preprocessor(input_data)
-        self.assertAllEqual(
-            x["token_ids"], [[0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]] * 13
-        )
-        self.assertAllEqual(
-            x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]] * 13
-        )
-        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 0, 0]] * 13)
-        self.assertAllEqual(y, [[7, 9, 11, 0, 0]] * 13)
-        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 0.0, 0.0]] * 13)
-
-    def test_preprocess_dataset(self):
-        sentences = tf.constant([" brown fox quick"] * 13)
-        ds = tf.data.Dataset.from_tensor_slices(sentences)
+        self.assertAllEqual(x["token_ids"], [[6, 1, 3, 4, 2, 5, 6, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        self.assertAllEqual(y, [[1, 3, 4, 2, 5, 6, 0, 0]] * 4)
+        self.assertAllEqual(sw, [[1, 1, 1, 1, 1, 1, 0, 0]] * 4)
+
+    def test_no_start_end_token(self):
+        input_data = ["airplane at airport"] * 4
+
+        preprocessor = GPT2CausalLMPreprocessor(
+            tokenizer=GPT2Tokenizer(
+                vocabulary=self.vocab,
+                merges=self.merges,
+            ),
+            sequence_length=8,
+            add_start_token=False,
+            add_end_token=False,
+        )
+        x, y, sw = preprocessor(input_data)
+        self.assertAllEqual(x["token_ids"], [[1, 3, 4, 2, 5, 0, 0, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0]] * 4)
+        self.assertAllEqual(y, [[3, 4, 2, 5, 0, 0, 0, 0]] * 4)
+        self.assertAllEqual(sw, [[1, 1, 1, 1, 0, 0, 0, 0]] * 4)
+
+    def test_labeled_batch(self):
+        x = tf.constant(["airplane at airport"] * 4)
+        y = tf.constant([1] * 4)  # Ignored.
+        sw = tf.constant([1.0] * 4)  # Ignored.
+        x, y, sw = self.preprocessor(x, y, sw)
+        self.assertAllEqual(x["token_ids"], [[6, 1, 3, 4, 2, 5, 6, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        self.assertAllEqual(y, [[1, 3, 4, 2, 5, 6, 0, 0]] * 4)
+        self.assertAllEqual(sw, [[1, 1, 1, 1, 1, 1, 0, 0]] * 4)
+
+    def test_dataset(self):
+        x = tf.constant(["airplane at airport"] * 4)
+        ds = tf.data.Dataset.from_tensor_slices(x)
         ds = ds.map(self.preprocessor)
-        x, y, sw = ds.batch(13).take(1).get_single_element()
-        self.assertAllEqual(
-            x["token_ids"], [[0, 13, 13, 13, 2, 1, 1, 1, 1, 1, 1, 1]] * 13
-        )
-        self.assertAllEqual(
-            x["padding_mask"], [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]] * 13
-        )
-        self.assertAllEqual(x["mask_positions"], [[1, 2, 3, 0, 0]] * 13)
-        self.assertAllEqual(y, [[7, 9, 11, 0, 0]] * 13)
-        self.assertAllEqual(sw, [[1.0, 1.0, 1.0, 0.0, 0.0]] * 13)
-
-    def test_mask_multiple_sentences(self):
-        sentence_one = tf.constant(" airplane")
-        sentence_two = tf.constant(" round")
-
-        x, y, sw = self.preprocessor((sentence_one, sentence_two))
-        self.assertAllEqual(
-            x["token_ids"], [0, 2, 2, 2, 13, 2, 1, 1, 1, 1, 1, 1]
-        )
-        self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
-        )
-        self.assertAllEqual(x["mask_positions"], [4, 0, 0, 0, 0])
-        self.assertAllEqual(y, [12, 0, 0, 0, 0])
-        self.assertAllEqual(sw, [1.0, 0.0, 0.0, 0.0, 0.0])
-
-    def test_no_masking_zero_rate(self):
-        no_mask_preprocessor = XLMRobertaMaskedLMPreprocessor(
-            self.preprocessor.tokenizer,
-            mask_selection_rate=0.0,
-            mask_selection_length=5,
-            sequence_length=12,
-        )
-        input_data = " quick brown fox"
-
-        x, y, sw = no_mask_preprocessor(input_data)
-        self.assertAllEqual(
-            x["token_ids"], [0, 11, 7, 9, 2, 1, 1, 1, 1, 1, 1, 1]
-        )
-        self.assertAllEqual(
-            x["padding_mask"], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
-        )
-        self.assertAllEqual(x["mask_positions"], [0, 0, 0, 0, 0])
-        self.assertAllEqual(y, [0, 0, 0, 0, 0])
-        self.assertAllEqual(sw, [0.0, 0.0, 0.0, 0.0, 0.0])
+        x, y, sw = ds.batch(4).take(1).get_single_element()
+        self.assertAllEqual(x["token_ids"], [[6, 1, 3, 4, 2, 5, 6, 0]] * 4)
+        self.assertAllEqual(x["padding_mask"], [[1, 1, 1, 1, 1, 1, 1, 0]] * 4)
+        self.assertAllEqual(y, [[1, 3, 4, 2, 5, 6, 0, 0]] * 4)
+        self.assertAllEqual(sw, [[1, 1, 1, 1, 1, 1, 0, 0]] * 4)
+
+    def test_generate_preprocess(self):
+        input_data = "airplane at airport"
+        x = self.preprocessor.generate_preprocess(input_data)
+        self.assertAllEqual(x["token_ids"], [6, 1, 3, 4, 2, 5, 0, 0])
+        self.assertAllEqual(x["padding_mask"], [1, 1, 1, 1, 1, 1, 0, 0])
+
+    def test_generate_postprocess(self):
+        input_data = {
+            "token_ids": tf.constant([6, 1, 3, 4, 2, 5, 0, 0]),
+            "padding_mask": tf.cast([1, 1, 1, 1, 1, 1, 0, 0], dtype="bool"),
+        }
+        x = self.preprocessor.generate_postprocess(input_data)
+        self.assertAllEqual(x, "airplane at airport")
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant([" quick brown fox"])
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant(["airplane at airport"])
 
         inputs = keras.Input(dtype="string", shape=())
-        outputs = self.preprocessor(inputs)
+        outputs, y, sw = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
 
-        path = os.path.join(self.get_temp_dir(), filename)
-        model.save(path, save_format=save_format)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
 
         restored_model = keras.models.load_model(path)
-        outputs = model(input_data)[0]["token_ids"]
-        restored_outputs = restored_model(input_data)[0]["token_ids"]
-        self.assertAllEqual(outputs, restored_outputs)
+        self.assertAllEqual(
+            model(input_data)["token_ids"],
+            restored_model(input_data)["token_ids"],
+        )
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer_test.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,135 +1,135 @@
-# Copyright 2022 The KerasNLP Authors
+# Copyright 2023 The KerasNLP Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tests for XLM-RoBERTa masked language model."""
 
+"""Tests for XLM-RoBERTa tokenizer."""
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm import (
-    XLMRobertaMaskedLM,
-)
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_masked_lm_preprocessor import (
-    XLMRobertaMaskedLMPreprocessor,
-)
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
     XLMRobertaTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaMaskedLMTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaTokenizerTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the slow brown fox"]
+            ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
             model_writer=bytes_io,
-            vocab_size=5,
+            vocab_size=10,
             model_type="WORD",
-            pad_id=0,
-            unk_id=1,
-            bos_id=2,
-            eos_id=3,
-            pad_piece="<pad>",
-            unk_piece="<unk>",
-            bos_piece="<s>",
-            eos_piece="</s>",
-            user_defined_symbols="[MASK]",
+            unk_id=0,
+            bos_id=1,
+            eos_id=2,
         )
         self.proto = bytes_io.getvalue()
 
-        self.preprocessor = XLMRobertaMaskedLMPreprocessor(
-            XLMRobertaTokenizer(proto=self.proto),
-            sequence_length=5,
-            mask_selection_length=2,
-        )
+        self.tokenizer = XLMRobertaTokenizer(proto=self.proto)
 
-        self.backbone = XLMRobertaBackbone(
-            vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
-            num_layers=2,
-            num_heads=2,
-            hidden_dim=2,
-            intermediate_dim=4,
-            max_sequence_length=self.preprocessor.packer.sequence_length,
-        )
-
-        self.masked_lm = XLMRobertaMaskedLM(
-            self.backbone,
-            preprocessor=self.preprocessor,
-        )
-
-        self.raw_batch = tf.constant(
-            ["the quick brown fox", "the slow brown fox"]
-        )
-        self.preprocessed_batch = self.preprocessor(self.raw_batch)[0]
-        self.raw_dataset = tf.data.Dataset.from_tensor_slices(
-            self.raw_batch
-        ).batch(2)
-        self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
-
-    def test_valid_call_masked_lm(self):
-        self.masked_lm(self.preprocessed_batch)
-
-    def test_classifier_predict(self):
-        self.masked_lm.predict(self.raw_batch)
-        self.masked_lm.preprocessor = None
-        self.masked_lm.predict(self.preprocessed_batch)
-
-    def test_classifier_fit(self):
-        self.masked_lm.fit(self.raw_dataset)
-        self.masked_lm.preprocessor = None
-        self.masked_lm.fit(self.preprocessed_dataset)
-
-    def test_classifier_fit_no_xla(self):
-        self.masked_lm.preprocessor = None
-        self.masked_lm.compile(
-            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
-            jit_compile=False,
-        )
-        self.masked_lm.fit(self.preprocessed_dataset)
+    def test_tokenize(self):
+        input_data = "the quick brown fox"
+        output = self.tokenizer(input_data)
+        self.assertAllEqual(output, [4, 9, 5, 7])
+
+    def test_tokenize_batch(self):
+        input_data = ["the quick brown fox", "the earth is round"]
+        output = self.tokenizer(input_data)
+        self.assertAllEqual(output, [[4, 9, 5, 7], [4, 6, 8, 10]])
+
+    def test_unk_token(self):
+        input_data = "the quick brown fox running"
+
+        output = self.tokenizer(input_data)
+        self.assertAllEqual(output, [4, 9, 5, 7, 3])
+
+    def test_detokenize(self):
+        input_data = [[4, 9, 5, 7]]
+        output = self.tokenizer.detokenize(input_data)
+        self.assertEqual(output, ["brown round earth is"])
+
+    def test_vocabulary(self):
+        vocabulary = self.tokenizer.get_vocabulary()
+        self.assertAllEqual(
+            vocabulary,
+            [
+                "<s>",
+                "<pad>",
+                "</s>",
+                "<unk>",
+                "the",
+                "brown",
+                "earth",
+                "fox",
+                "is",
+                "quick",
+                "round",
+                "<mask>",
+            ],
+        )
+        self.assertEqual(self.tokenizer.vocabulary_size(), 12)
+
+    def test_id_to_token(self):
+        print(self.tokenizer.id_to_token(9))
+        self.assertEqual(self.tokenizer.id_to_token(9), "quick")
+        self.assertEqual(self.tokenizer.id_to_token(5), "brown")
+
+    def test_error_id_out_of_vocabulary(self):
+        with self.assertRaises(ValueError):
+            self.tokenizer.id_to_token(self.tokenizer.vocabulary_size())
+        with self.assertRaises(ValueError):
+            self.tokenizer.id_to_token(-1)
+
+    def test_token_to_id(self):
+        self.assertEqual(self.tokenizer.token_to_id("the"), 4)
+        self.assertEqual(self.tokenizer.token_to_id("round"), 10)
+        # Test any random OOV token.
+        self.assertEqual(self.tokenizer.token_to_id("<oov-token>"), 3)
+        # Test a special token.
+        self.assertEqual(self.tokenizer.token_to_id("<pad>"), 1)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.masked_lm)
-        new_classifier = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.tokenizer)
+        new_tokenizer = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
-            new_classifier.get_config(),
-            self.masked_lm.get_config(),
+            new_tokenizer.get_config(),
+            self.tokenizer.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large
-    def test_saved_model(self, save_format, filename):
-        save_path = os.path.join(self.get_temp_dir(), filename)
-        self.masked_lm.save(save_path, save_format=save_format)
-        restored_model = keras.models.load_model(save_path)
-
-        # Check we got the real object back.
-        self.assertIsInstance(restored_model, XLMRobertaMaskedLM)
-
-        model_output = self.masked_lm(self.preprocessed_batch)
-        restored_output = restored_model(self.preprocessed_batch)
-
-        self.assertAllClose(model_output, restored_output)
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    @pytest.mark.tf_only
+    def test_saved_model(self):
+        input_data = tf.constant(["the quick brown fox"])
+
+        inputs = keras.Input(dtype="string", shape=())
+        outputs = self.tokenizer(inputs)
+        model = keras.Model(inputs, outputs)
+
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
+
+        restored_model = keras.models.load_model(path)
+        self.assertAllEqual(
+            model(input_data),
+            restored_model(input_data),
+        )
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_preprocessor_test.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,33 +9,32 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for XLM-RoBERTa preprocessor layer."""
-
 import io
 import os
 
 import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
     XLMRobertaPreprocessor,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
     XLMRobertaTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaPreprocessorTest(TestCase):
     def setUp(self):
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox", "the earth is round"]
         )
         sentencepiece.SentencePieceTrainer.train(
             sentence_iterator=vocab_data.as_numpy_iterator(),
@@ -130,34 +129,29 @@
 
     def test_errors_for_2d_list_input(self):
         ambiguous_input = [["one", "two"], ["three", "four"]]
         with self.assertRaises(ValueError):
             self.preprocessor(ambiguous_input)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.preprocessor)
-        new_preprocessor = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
             new_preprocessor.get_config(),
             self.preprocessor.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
     @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["the quick brown fox"])
         inputs = keras.Input(dtype="string", shape=())
         outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data)["token_ids"],
             restored_model(input_data)["token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/xlm_roberta_presets_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,31 +10,32 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
-import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_backbone import XLMRobertaBackbone
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_classifier import (
     XLMRobertaClassifier,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_preprocessor import (
     XLMRobertaPreprocessor,
 )
 from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
     XLMRobertaTokenizer,
 )
+from keras_nlp.src.tests.test_case import TestCase
 
 
 @pytest.mark.large
-class XLMRobertaPresetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaPresetSmokeTest(TestCase):
     """
     A smoke test for XLM-RoBERTa presets we run continuously.
 
     This only tests the smallest weights we have available. Run with:
     `pytest keras_nlp/models/xlm_roberta/xlm_roberta_presets_test.py --run_large`
     """
 
@@ -56,44 +57,44 @@
         self.assertAllEqual(outputs, expected_outputs)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_backbone_output(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[0, 581, 63773, 2]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[0, 581, 63773, 2]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = XLMRobertaBackbone.from_preset(
             "xlm_roberta_base_multi", load_weights=load_weights
         )
         outputs = model(input_data)
         if load_weights:
             outputs = outputs[0, 0, :5]
             expected = [0.084763, 0.097018, 0.051329, -0.000805, 0.028415]
             self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output(self, load_weights):
-        input_data = tf.constant(["The quick brown fox."])
+        input_data = ["The quick brown fox."]
         model = XLMRobertaClassifier.from_preset(
             "xlm_roberta_base_multi", num_classes=2, load_weights=load_weights
         )
         # Never assert output values, as the head weights are random.
         model.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_classifier_output_without_preprocessing(self, load_weights):
         input_data = {
-            "token_ids": tf.constant([[0, 581, 63773, 2]]),
-            "padding_mask": tf.constant([[1, 1, 1, 1]]),
+            "token_ids": ops.array([[0, 581, 63773, 2]]),
+            "padding_mask": ops.array([[1, 1, 1, 1]]),
         }
         model = XLMRobertaClassifier.from_preset(
             "xlm_roberta_base_multi",
             num_classes=2,
             load_weights=load_weights,
             preprocessor=None,
         )
@@ -120,15 +121,15 @@
     def test_unknown_preset_error(self, cls, kwargs):
         # Not a preset name
         with self.assertRaises(ValueError):
             cls.from_preset("xlm_roberta_base_clowntown", **kwargs)
 
 
 @pytest.mark.extra_large
-class XLMRobertaPresetFullTest(tf.test.TestCase, parameterized.TestCase):
+class XLMRobertaPresetFullTest(TestCase):
     """
     Test the full enumeration of our preset.
 
     This tests every XLM-RoBERTa preset and is only run manually.
     Run with:
     `pytest keras_nlp/models/xlm_roberta/xlm_roberta_presets_test.py --run_extra_large`
     """
@@ -138,32 +139,32 @@
     )
     def test_load_xlm_roberta(self, load_weights):
         for preset in XLMRobertaBackbone.presets:
             model = XLMRobertaBackbone.from_preset(
                 preset, load_weights=load_weights
             )
             input_data = {
-                "token_ids": tf.random.uniform(
-                    shape=(1, 512), dtype=tf.int64, maxval=model.vocabulary_size
+                "token_ids": ops.random.uniform(
+                    shape=(1, 512), dtype="int64", maxval=model.vocabulary_size
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             model(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_xlm_roberta_classifier(self, load_weights):
         for preset in XLMRobertaClassifier.presets:
             classifier = XLMRobertaClassifier.from_preset(
                 preset,
                 num_classes=4,
                 load_weights=load_weights,
             )
-            input_data = tf.constant(["This quick brown fox"])
+            input_data = ["The quick brown fox."]
             classifier.predict(input_data)
 
     @parameterized.named_parameters(
         ("preset_weights", True), ("random_weights", False)
     )
     def test_load_xlm_roberta_classifier_without_preprocessing(
         self, load_weights
@@ -172,20 +173,20 @@
             classifier = XLMRobertaClassifier.from_preset(
                 preset,
                 num_classes=4,
                 load_weights=load_weights,
                 preprocessor=None,
             )
             input_data = {
-                "token_ids": tf.random.uniform(
+                "token_ids": ops.random.uniform(
                     shape=(1, 512),
-                    dtype=tf.int64,
+                    dtype="int64",
                     maxval=classifier.backbone.vocabulary_size,
                 ),
-                "padding_mask": tf.constant([1] * 512, shape=(1, 512)),
+                "padding_mask": ops.array([1] * 512, shape=(1, 512)),
             }
             classifier.predict(input_data)
 
     def test_load_tokenizers(self):
         for preset in XLMRobertaTokenizer.presets:
             tokenizer = XLMRobertaTokenizer.from_preset(preset)
             tokenizer("The quick brown fox.")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,156 +7,139 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-"""XLM-RoBERTa tokenizer."""
-
-import copy
+"""Trainer for SentencePiece Tokenizer."""
+import io
 
 import tensorflow as tf
 
-from keras_nlp.src.api_export import keras_nlp_export
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_presets import backbone_presets
-from keras_nlp.src.tokenizers.sentence_piece_tokenizer import SentencePieceTokenizer
-from keras_nlp.src.utils.python_utils import classproperty
-from keras_nlp.src.utils.tf_utils import tensor_to_string_list
-
-
-@keras_nlp_export("keras_nlp.models.XLMRobertaTokenizer")
-class XLMRobertaTokenizer(SentencePieceTokenizer):
-    """An XLM-RoBERTa tokenizer using SentencePiece subword segmentation.
-
-    This tokenizer class will tokenize raw strings into integer sequences and
-    is based on `keras_nlp.tokenizers.SentencePieceTokenizer`. Unlike the
-    underlying tokenizer, it will check for all special tokens needed by
-    XLM-RoBERTa models and provides a `from_preset()` method to automatically
-    download a matching vocabulary for an XLM-RoBERTa preset.
-
-    Note: If you are providing your own custom SentencePiece model, the original
-    fairseq implementation of XLM-RoBERTa re-maps some token indices from the
-    underlying sentencepiece output. To preserve compatibility, we do the same
-    re-mapping here.
-
-    If input is a batch of strings (rank > 0), the layer will output a
-    `tf.RaggedTensor` where the last dimension of the output is ragged.
+try:
+    import sentencepiece as spm
+except ImportError:
+    spm = None
 
-    If input is a scalar string (rank == 0), the layer will output a dense
-    `tf.Tensor` with static shape `[None]`.
+from keras_nlp.src.api_export import keras_nlp_export
 
-    Args:
-        proto: Either a `string` path to a SentencePiece proto file or a
-            `bytes` object with a serialized SentencePiece proto. See the
-            [SentencePiece repository](https://github.com/google/sentencepiece)
-            for more details on the format.
 
-    Examples:
-    ```python
-    tokenizer = keras_nlp.models.XLMRobertaTokenizer.from_preset(
-        "xlm_roberta_base_multi",
-    )
+@keras_nlp_export("keras_nlp.tokenizers.compute_sentence_piece_proto")
+def compute_sentence_piece_proto(
+    data,
+    vocabulary_size,
+    model_type="unigram",
+    proto_output_file=None,
+    lowercase=False,
+):
+    r"""A utility to train a SentencePiece vocabulary.
 
-    # Unbatched inputs.
-    tokenizer("the quick brown fox")
+    Trains a SentencePiece vocabulary from an input dataset or a list of
+    filenames.
 
-    # Batched inputs.
-    tokenizer(["the quick brown fox", " "])
+    If `data` is a list of filenames, the file format is required to be plain
+    text files, and the text will be read in line by line during training.
 
-    # Detokenization.
-    tokenizer.detokenize(tokenizer("the quick brown fox"))
+    Args:
+        data: A `tf.data.Dataset`, or a list of filenames.
+        vocabulary_size: int. The maximum size of a vocabulary to be trained.
+        model_type: str. The model algorithm must be one of
+            `"unigram"`, `"bpe"`, `"word"` or `"char"`. Defaults to `"unigram"`.
+        proto_output_file: str. If provided it will be used
+            as model_file which is passed to model_writer.
+            If `None`, the model_file will be `io.BytesIO` object.
+            Defaults to `None`.
+        lowercase: bool. If True, the input text will be
+            lowercased before tokenization. Defaults to `False`.
+
+    Returns:
+        A `bytes` object with a serialized SentencePiece proto or
+        `None` if proto_output_file if provided.
 
-    # Custom vocabulary
-    def train_sentencepiece(ds, vocab_size):
-        bytes_io = io.BytesIO()
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=ds.as_numpy_iterator(),
-            model_writer=bytes_io,
-            vocab_size=vocab_size,
-            model_type="WORD",
-            unk_id=0,
-            bos_id=1,
-            eos_id=2,
-        )
-        return bytes_io.getvalue()
+    Examples:
 
-    ds = tf.data.Dataset.from_tensor_slices(
-        ["the quick brown fox", "the earth is round"]
-    )
-    proto = train_sentencepiece(ds, vocab_size=10)
-    tokenizer = keras_nlp.models.XLMRobertaTokenizer(proto=proto)
+    Basic Usage (from Dataset).
+    >>> inputs = tf.data.Dataset.from_tensor_slices(["Drifting Along"])
+    >>> proto = keras_nlp.tokenizers.compute_sentence_piece_proto(inputs, vocabulary_size=15)
+    >>> tokenizer = keras_nlp.tokenizers.SentencePieceTokenizer(proto=proto)
+    >>> outputs = inputs.map(tokenizer)
+    >>> for output in outputs:
+    ...     print(output)
+    tf.Tensor([ 4  8 12  5  9 14  5  6 13  4  7 10 11  6 13],
+    shape=(15,), dtype=int32)
+
+    Basic Usage (with files).
+    ``` python
+    with open("test.txt", "w+") as f: f.write("Drifting Along\n")
+    inputs = ["test.txt"]
+    proto = keras_nlp.tokenizers.compute_sentence_piece_proto(
+         inputs, vocabulary_size=15, proto_output_file="model.spm")
+    tokenizer = keras_nlp.tokenizers.SentencePieceTokenizer(proto="model.spm")
+    ds = tf.data.Dataset.from_tensor_slices(["the quick brown fox."])
+    ds = ds.map(tokenizer)
     ```
-    """
-
-    def __init__(self, proto, **kwargs):
-        super().__init__(proto=proto, **kwargs)
 
-        # List of special tokens.
-        self._vocabulary_prefix = ["<s>", "<pad>", "</s>", "<unk>"]
+    Usage with lowercase
+    >>> inputs = tf.data.Dataset.from_tensor_slices(["Drifting Along"])
+    >>> proto = keras_nlp.tokenizers.compute_sentence_piece_proto(
+    ...     inputs, vocabulary_size=15, lowercase=True)
+    >>> tokenizer = keras_nlp.tokenizers.SentencePieceTokenizer(proto=proto)
+    >>> outputs = inputs.map(tokenizer)
+    >>> for output in outputs:
+    ...     print(output)
+    tf.Tensor([ 4  8 12  5  9 14  5  6 13  4  7 10 11  6 13],
+    shape=(15,), dtype=int32)
+    """
 
-        # IDs of special tokens.
-        self.start_token_id = 0  # <s>
-        self.pad_token_id = 1  # <pad>
-        self.end_token_id = 2  # </s>
-        self.unk_token_id = 3  # <unk>
-        self.mask_token_id = self.vocabulary_size() - 1  # <mask>
-
-    def vocabulary_size(self):
-        """Get the size of the tokenizer vocabulary."""
-        return super().vocabulary_size() + 2
-
-    def get_vocabulary(self):
-        """Get the size of the tokenizer vocabulary."""
-        vocabulary = tensor_to_string_list(
-            self._sentence_piece.id_to_string(
-                tf.range(super().vocabulary_size())
-            )
+    if spm is None:
+        raise ImportError(
+            f"{compute_sentence_piece_proto.__name__} requires the "
+            "`sentencepiece` package. Please install it with "
+            "`pip install sentencepiece`."
         )
-        return self._vocabulary_prefix + vocabulary[3:] + ["<mask>"]
-
-    def id_to_token(self, id):
-        """Convert an integer id to a string token."""
 
-        if id == self.mask_token_id:
-            return "<mask>"
-
-        if id < len(self._vocabulary_prefix):
-            return self._vocabulary_prefix[id]
-
-        return tensor_to_string_list(self._sentence_piece.id_to_string(id - 1))
-
-    def token_to_id(self, token):
-        """Convert a string token to an integer id."""
-
-        if token in self._vocabulary_prefix:
-            return self._vocabulary_prefix.index(token)
-
-        spm_token_id = self._sentence_piece.string_to_id(token)
-
-        # OOV token
-        spm_unk_token_id = self._sentence_piece.string_to_id("<unk>")
-        if spm_token_id == spm_unk_token_id:
-            return self.unk_token_id
-
-        return int(spm_token_id.numpy()) + 1
-
-    def tokenize(self, inputs):
-        tokens = super().tokenize(inputs)
-
-        # Correct `unk_token_id` (0 -> 3). Note that we do not correct
-        # `start_token_id` and `end_token_id`; they are dealt with in
-        # `XLMRobertaPreprocessor`.
-        tokens = tf.where(tf.equal(tokens, 0), self.unk_token_id - 1, tokens)
-
-        # Shift the tokens IDs right by one.
-        return tf.add(tokens, 1)
+    if not isinstance(data, (list, tuple, tf.data.Dataset)):
+        raise ValueError(
+            "The `data` argument must be either `tf.data.Dataset` or `tuple` or `list`. "
+            f"Received: type(data)={type(data)}."
+        )
 
-    def detokenize(self, ids):
-        ids = tf.ragged.boolean_mask(ids, tf.not_equal(ids, self.mask_token_id))
-        return super().detokenize(ids)
+    if model_type not in ["unigram", "bpe", "word", "char"]:
+        raise ValueError(
+            "The `model_type` argument must be one of `unigram`, `bpe`, `word`"
+            f"or `char`. Received: model_type={model_type}."
+        )
 
-    @classproperty
-    def presets(cls):
-        return copy.deepcopy(backbone_presets)
+    model_writer = (
+        open(proto_output_file, "wb") if proto_output_file else io.BytesIO()
+    )
+    is_dataset = isinstance(data, tf.data.Dataset)
+    if is_dataset:
+        spm.SentencePieceTrainer.train(
+            sentence_iterator=data.as_numpy_iterator(),
+            model_writer=model_writer,
+            vocab_size=vocabulary_size,
+            model_type=model_type,
+            normalization_rule_name="nmt_nfkc_cf" if lowercase else "nmt_nfkc",
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
+        )
+    else:
+        spm.SentencePieceTrainer.train(
+            input=data,
+            model_writer=model_writer,
+            vocab_size=vocabulary_size,
+            model_type=model_type,
+            normalization_rule_name="nmt_nfkc_cf" if lowercase else "nmt_nfkc",
+            pad_id=0,
+            unk_id=1,
+            bos_id=2,
+            eos_id=3,
+        )
+    if proto_output_file:
+        model_writer.close()
+    else:
+        return model_writer.getvalue()
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/models/xlm_roberta/xlm_roberta_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/bert/bert_masked_lm_test.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,129 +7,105 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""Tests for BERT masked language model."""
 
-"""Tests for XLM-RoBERTa tokenizer."""
-
-import io
 import os
 
 import pytest
-import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
-from keras_nlp.src.models.xlm_roberta.xlm_roberta_tokenizer import (
-    XLMRobertaTokenizer,
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.bert.bert_backbone import BertBackbone
+from keras_nlp.src.models.bert.bert_masked_lm import BertMaskedLM
+from keras_nlp.src.models.bert.bert_masked_lm_preprocessor import (
+    BertMaskedLMPreprocessor,
 )
+from keras_nlp.src.models.bert.bert_tokenizer import BertTokenizer
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class XLMRobertaTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class BertMaskedLMTest(TestCase):
     def setUp(self):
-        bytes_io = io.BytesIO()
-        vocab_data = tf.data.Dataset.from_tensor_slices(
-            ["the quick brown fox", "the earth is round"]
+        # Setup model.
+        self.vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
+        self.vocab += ["the", "quick", "brown", "fox", "."]
+        self.preprocessor = BertMaskedLMPreprocessor(
+            BertTokenizer(vocabulary=self.vocab),
+            # Simplify out testing by masking every available token.
+            mask_selection_rate=1.0,
+            mask_token_rate=1.0,
+            random_token_rate=0.0,
+            mask_selection_length=2,
+            sequence_length=5,
         )
-        sentencepiece.SentencePieceTrainer.train(
-            sentence_iterator=vocab_data.as_numpy_iterator(),
-            model_writer=bytes_io,
-            vocab_size=10,
-            model_type="WORD",
-            unk_id=0,
-            bos_id=1,
-            eos_id=2,
+        self.backbone = BertBackbone(
+            vocabulary_size=self.preprocessor.tokenizer.vocabulary_size(),
+            num_layers=2,
+            num_heads=2,
+            hidden_dim=2,
+            intermediate_dim=4,
+            max_sequence_length=self.preprocessor.packer.sequence_length,
         )
-        self.proto = bytes_io.getvalue()
-
-        self.tokenizer = XLMRobertaTokenizer(proto=self.proto)
-
-    def test_tokenize(self):
-        input_data = "the quick brown fox"
-        output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [4, 9, 5, 7])
-
-    def test_tokenize_batch(self):
-        input_data = tf.constant(["the quick brown fox", "the earth is round"])
-        output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [[4, 9, 5, 7], [4, 6, 8, 10]])
-
-    def test_unk_token(self):
-        input_data = "the quick brown fox running"
-
-        output = self.tokenizer(input_data)
-        self.assertAllEqual(output, [4, 9, 5, 7, 3])
-
-    def test_detokenize(self):
-        input_data = tf.constant([[4, 9, 5, 7]])
-        output = self.tokenizer.detokenize(input_data)
-        self.assertEqual(output, tf.constant(["brown round earth is"]))
-
-    def test_vocabulary(self):
-        vocabulary = self.tokenizer.get_vocabulary()
-        self.assertAllEqual(
-            vocabulary,
-            [
-                "<s>",
-                "<pad>",
-                "</s>",
-                "<unk>",
-                "the",
-                "brown",
-                "earth",
-                "fox",
-                "is",
-                "quick",
-                "round",
-                "<mask>",
-            ],
+        self.masked_lm = BertMaskedLM(
+            self.backbone,
+            preprocessor=self.preprocessor,
         )
-        self.assertEqual(self.tokenizer.vocabulary_size(), 12)
 
-    def test_id_to_token(self):
-        print(self.tokenizer.id_to_token(9))
-        self.assertEqual(self.tokenizer.id_to_token(9), "quick")
-        self.assertEqual(self.tokenizer.id_to_token(5), "brown")
-
-    def test_token_to_id(self):
-        self.assertEqual(self.tokenizer.token_to_id("the"), 4)
-        self.assertEqual(self.tokenizer.token_to_id("round"), 10)
-        # Test any random OOV token.
-        self.assertEqual(self.tokenizer.token_to_id("<oov-token>"), 3)
-        # Test a special token.
-        self.assertEqual(self.tokenizer.token_to_id("<pad>"), 1)
+        # Setup data.
+        self.raw_batch = [
+            "the quick brown fox.",
+            "the slow brown fox.",
+        ]
+        self.preprocessed_batch = self.preprocessor(self.raw_batch)
+        self.raw_dataset = tf.data.Dataset.from_tensor_slices(
+            self.raw_batch
+        ).batch(2)
+        self.preprocessed_dataset = self.raw_dataset.map(self.preprocessor)
+
+    def test_valid_call(self):
+        self.masked_lm(self.preprocessed_batch[0])
+
+    def test_predict(self):
+        self.masked_lm.predict(self.raw_batch)
+        self.masked_lm.preprocessor = None
+        self.masked_lm.predict(self.preprocessed_batch[0])
+
+    def test_fit(self):
+        self.masked_lm.fit(self.raw_dataset)
+        self.masked_lm.preprocessor = None
+        self.masked_lm.fit(self.preprocessed_dataset)
+
+    def test_fit_no_xla(self):
+        self.masked_lm.preprocessor = None
+        self.masked_lm.compile(
+            optimizer="adam",
+            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
+            jit_compile=False,
+        )
+        self.masked_lm.fit(self.preprocessed_dataset)
 
     def test_serialization(self):
-        config = keras.utils.serialize_keras_object(self.tokenizer)
-        new_tokenizer = keras.utils.deserialize_keras_object(config)
+        config = keras.saving.serialize_keras_object(self.masked_lm)
+        new_classifier = keras.saving.deserialize_keras_object(config)
         self.assertEqual(
-            new_tokenizer.get_config(),
-            self.tokenizer.get_config(),
+            new_classifier.get_config(),
+            self.masked_lm.get_config(),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    @pytest.mark.large  # Saving is slow, so mark these large.
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant(["the quick brown fox"])
-
-        inputs = keras.Input(dtype="string", shape=())
-        outputs = self.tokenizer(inputs)
-        model = keras.Model(inputs, outputs)
-
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
-
+    @pytest.mark.large
+    def test_saved_model(self):
+        model_output = self.masked_lm.predict(self.raw_batch)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        self.masked_lm.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
-        self.assertAllEqual(
-            model(input_data),
-            restored_model(input_data),
-        )
+
+        # Check we got the real object back.
+        self.assertIsInstance(restored_model, BertMaskedLM)
+        # Check that output matches.
+        restored_output = restored_model.predict(self.raw_batch)
+        self.assertAllClose(model_output, restored_output, atol=0.01, rtol=0.01)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
+from keras_nlp.src.backend import keras
 from keras_nlp.src.samplers.beam_sampler import BeamSampler
 from keras_nlp.src.samplers.contrastive_sampler import ContrastiveSampler
 from keras_nlp.src.samplers.greedy_sampler import GreedySampler
 from keras_nlp.src.samplers.random_sampler import RandomSampler
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.serialization import deserialize
 from keras_nlp.src.samplers.serialization import get
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/beam_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/beam_sampler.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,21 +10,22 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Beam Sampler."""
 
 import tensorflow as tf
-from tensorflow import keras
 from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.BeamSampler")
 class BeamSampler(Sampler):
     """Beam Sampler class.
 
@@ -48,22 +49,22 @@
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, cache, index):
         prompt_batch_size = tf.shape(prompt)[0]
-        hidden_states = tf.ones((prompt_batch_size, 10))
+        hidden_states = np.ones((prompt_batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((prompt_batch_size, vocab_size))
+        logits = np.ones((prompt_batch_size, vocab_size))
         return logits, hidden_states, cache
 
     output = keras_nlp.samplers.BeamSampler()(
         next=next,
-        prompt=tf.fill((batch_size, length), char_lookup["z"]),
+        prompt=np.full((batch_size, length), char_lookup["z"], dtype="int32"),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzeeeeeee']
     ```
 
     Return all beams and their probabilities.
@@ -71,22 +72,22 @@
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 8, len(int_lookup)
 
     def next(prompt, cache, index):
         prompt_batch_size = tf.shape(prompt)[0]
-        hidden_states = tf.ones((prompt_batch_size, 10))
+        hidden_states = np.ones((prompt_batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = np.ones((batch_size, vocab_size))
         return logits, hidden_states, cache
 
     beams, probs = keras_nlp.samplers.BeamSampler(return_all_beams=True)(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=np.full((batch_size, length,), char_lookup['z'], dtype="int32"),
         index=5,
     )
 
     print(beams.shape)
     # >>> (1, 5, 8)
     print(probs.shape)
     # >>> (1, 5)
@@ -97,14 +98,18 @@
 
     def __init__(
         self,
         num_beams=5,
         return_all_beams=False,
         **kwargs,
     ):
+        # Temporarily turn off beam search in other backends.
+        # No technical blockers here, just need tf -> ops rewrite.
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(**kwargs)
         self.num_beams = num_beams
         self.return_all_beams = return_all_beams
 
     def __call__(
         self,
         next,
@@ -130,17 +135,17 @@
 
         def unflatten_beams(x):
             """Separate the beam dim and batch dim."""
             unflat_shape = [batch_size, self.num_beams] + x.shape.as_list()[1:]
             return tf.reshape(x, shape=unflat_shape)
 
         if mask is None:
-            mask = tf.zeros_like(prompt, dtype=tf.bool)
+            mask = tf.zeros_like(prompt, dtype="bool")
         else:
-            mask = tf.cast(mask, dtype=tf.bool)
+            mask = tf.cast(mask, dtype="bool")
         # `tf.while_loop` will not accept `None` as a value for `loop_vars`.
         cache = () if cache is None else cache
         # Add extra sequences for each beam.
         prompt, mask = create_beams(prompt), create_beams(mask)
         cache = tf.nest.map_structure(create_beams, cache)
         # Setup the initial beam log-likelihoods.
         # On the first loop, make sure only the original beam is considered.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/beam_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/beam_sampler_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,119 +9,125 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Beam sampler."""
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.beam_sampler import BeamSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class BeamSamplerTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class BeamSamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 26).
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.vocab_size = len(self.int_lookup)
 
         def next(prompt, cache, index):
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             # Dummy hidden states.
-            hidden_states = tf.ones([batch_size, 5])
+            hidden_states = ops.ones([batch_size, 5])
             # Return a distribution favoring the next char in cache.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = BeamSampler(num_beams=5, temperature=1.0)
         self.sampler_all_beams = BeamSampler(num_beams=5, return_all_beams=True)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             # Dummy hidden states.
-            hidden_states = tf.ones([batch_size, 5])
+            hidden_states = ops.ones([batch_size, 5])
             # Return a distribution favoring the first token in the vocab.
             logits = (
-                tf.one_hot(
-                    tf.zeros(self.batch_size, dtype=tf.int32),
+                ops.one_hot(
+                    ops.zeros(self.batch_size, dtype="int32"),
                     self.vocab_size,
                 )
                 * 1e9
             )
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_return_all_beams(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         sorted_prompts, sorted_log_probs = self.sampler_all_beams(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
 
         self.assertEqual(
             sorted_prompts.shape, (self.batch_size, 5, self.length)
         )
         self.assertEqual(sorted_log_probs.shape, (self.batch_size, 5))
         self.assertTrue(
-            tf.reduce_all(sorted_log_probs[:, 1:] <= sorted_log_probs[:, :-1])
+            ops.all(sorted_log_probs[:, 1:] <= sorted_log_probs[:, :-1])
         )
         self.assertEqual(
             self.join_as_string(sorted_prompts[:, 0, :]), ["sequentially"]
         )
 
     def test_early_stopping(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
         )
         self.assertEqual(self.join_as_string(output), ["sequentzzzzz"])
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(self.next, prompt=prompt, cache=cache)
 
         output = generate(prompt, cache)
         self.assertEqual(self.join_as_string(output), ["sequentially"])
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/contrastive_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/contrastive_sampler.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,21 +10,22 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Contrastive Sampler."""
 
 import tensorflow as tf
-from tensorflow import keras
 from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
+from keras_nlp.src.utils.tensor_utils import assert_tf_backend
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.ContrastiveSampler")
 class ContrastiveSampler(Sampler):
     """Contrastive Sampler class.
 
@@ -35,15 +36,15 @@
     behavior of duplicating seen tokens.
 
     Args:
         k: int, the `k` value of top-k. Next token will be chosen from k tokens.
         alpha: float, the weight of minus max similarity in joint score
             computation. The larger the value of `alpha`, the score relies more
             on the similarity than the token probability.
-        seed: int, defaults to None. The random seed.
+        seed: int. The random seed. Defaults to `None`.
 
     Call arguments:
         {{call_args}}
 
     Examples:
     ```python
     # Use a simple alphabet of lowercase characters to [0, 26).
@@ -51,37 +52,41 @@
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
     hidden_size = 5
     index = 5
 
     def next(prompt, cache, index):
         prompt_batch_size = tf.shape(prompt)[0]
-        hidden_states = tf.ones((prompt_batch_size, hidden_size))
+        hidden_states = np.ones((prompt_batch_size, hidden_size))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((prompt_batch_size, vocab_size))
+        logits = np.ones((prompt_batch_size, vocab_size))
         return logits, hidden_states, cache
 
     output = keras_nlp.samplers.ContrastiveSampler()(
         next=next,
-        prompt=tf.fill((batch_size, length), char_lookup["z"]),
+        prompt=np.full((batch_size, length), char_lookup["z"], dtype="int32"),
         index=index,
-        hidden_states=tf.ones([batch_size, index, hidden_size]),
+        hidden_states=np.ones([batch_size, index, hidden_size]),
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> "zzzzzeeeeeee"
     ```
     """
 
     def __init__(
         self,
         k=5,
         alpha=0.6,
         seed=None,
         **kwargs,
     ):
+        # Temporarily turn off beam search in other backends.
+        # No technical blockers here, just need tf -> ops rewrite.
+        assert_tf_backend(self.__class__.__name__)
+
         super().__init__(**kwargs)
         self.k = k
         self.alpha = alpha
         self.seed = seed
 
     def __call__(
         self,
@@ -114,15 +119,15 @@
             return tf.reshape(x, shape=flat_shape)
 
         def unflatten_beams(x):
             """Separate the beam dim and batch dim."""
             unflat_shape = [batch_size, self.k] + x.shape.as_list()[1:]
             return tf.reshape(x, shape=unflat_shape)
 
-        mask = tf.zeros_like(prompt, dtype=tf.bool) if mask is None else mask
+        mask = tf.zeros_like(prompt, dtype="bool") if mask is None else mask
         # Compute initial logits.
         logits, _, cache = next(prompt, cache, index)
         # `tf.while_loop` will not accept `None` as a value for `loop_vars`.
         cache = () if cache is None else cache
 
         def cond(prompt, cache, index, logits, hidden_states):
             if end_token_id is None:
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/contrastive_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/contrastive_sampler_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,149 +9,154 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Contrastive Sampler."""
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.contrastive_sampler import ContrastiveSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class ContrastiveSamplerTest(tf.test.TestCase, parameterized.TestCase):
+@pytest.mark.tf_only
+class ContrastiveSamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 26).
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.hidden_dim = 3
         self.vocab_size = len(self.int_lookup)
-        self.hidden_states = tf.ones(
+        self.hidden_states = ops.ones(
             [
                 self.batch_size,
                 self.length,
                 self.hidden_dim,
             ]
         )
 
         def next(prompt, cache, index):
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             # Return a distribution favoring the next char in cache.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
-            hidden_states = tf.ones([batch_size, self.hidden_dim])
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
+            hidden_states = ops.ones([batch_size, self.hidden_dim])
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = ContrastiveSampler(k=5, alpha=0.2, temperature=1.0)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
             # Return a distribution favoring the first token in the vocab.
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             logits = (
-                tf.one_hot(
-                    tf.zeros(batch_size, dtype=tf.int32),
+                ops.one_hot(
+                    ops.zeros(batch_size, dtype="int32"),
                     self.vocab_size,
                 )
                 * 1e9
             )
-            hidden_states = tf.ones([batch_size, self.hidden_dim])
+            hidden_states = ops.ones([batch_size, self.hidden_dim])
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
             hidden_states=self.hidden_states,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentiallyy")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["s"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["s"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             index=1,
             hidden_states=self.hidden_states,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_early_stopping(self):
         cache_chars = list("sequentiallyy")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["s"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["s"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
             index=0,
             hidden_states=self.hidden_states,
         )
         self.assertEqual(self.join_as_string(output), ["sequentsssss"])
 
     def test_outputs_in_top_k(self):
         def next(prompt, cache, index):
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             # Return a distribution where each id is progressively less likely.
-            logits = tf.range(self.vocab_size, 0, -1, dtype="float32")
-            logits = tf.repeat(logits[tf.newaxis, :], batch_size, axis=0)
-            hidden_states = tf.ones([batch_size, self.hidden_dim])
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.repeat(logits[None, :], batch_size, axis=0)
+            hidden_states = ops.ones([batch_size, self.hidden_dim])
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=1,
             hidden_states=self.hidden_states,
         )
-        output_ids = set(output[0, 1:].numpy())
+        output_ids = set(ops.convert_to_numpy(output[0, 1:]))
         self.assertContainsSubset(output_ids, range(5))
 
     def test_alpha_penalty(self):
         def next(prompt, cache, index):
-            batch_size = tf.shape(prompt)[0]
+            batch_size = ops.shape(prompt)[0]
             best_token_id = self.char_lookup["h"]
-            logits = tf.ones([batch_size, self.vocab_size])
+            logits = ops.ones([batch_size, self.vocab_size])
             # Favoring `best_token_id` in the logits.
             logits += (
-                tf.one_hot(
-                    tf.zeros(self.batch_size, dtype=tf.int32) + best_token_id,
+                ops.one_hot(
+                    ops.zeros(self.batch_size, dtype="int32") + best_token_id,
                     self.vocab_size,
                 )
                 * 1e9
             )
 
             # Set the hidden states for `best_token_id` as [1, 1, ..., 1], so it
             # gets the max similarity penality score.
             mask_of_best_token = prompt[:, index - 1] == best_token_id
-            random_states = tf.random.uniform([batch_size, self.hidden_dim]) * (
-                1 - tf.cast(mask_of_best_token, dtype=tf.float32)[:, tf.newaxis]
-            )
+            random_states = ops.random.uniform(
+                [batch_size, self.hidden_dim]
+            ) * (1 - ops.cast(mask_of_best_token, dtype="float32")[:, None])
             hidden_states = (
-                tf.ones([batch_size, self.hidden_dim])
-                * tf.cast(mask_of_best_token, dtype=tf.float32)[:, tf.newaxis]
+                ops.ones([batch_size, self.hidden_dim])
+                * ops.cast(mask_of_best_token, dtype="float32")[:, None]
             )
             hidden_states = hidden_states + random_states
             return logits, hidden_states, cache
 
-        prompt = tf.fill((1, self.length), self.char_lookup["z"])
-        hidden_states = tf.ones([1, self.length, self.hidden_dim]) + 1e-5
+        prompt = ops.full((1, self.length), self.char_lookup["z"])
+        hidden_states = ops.ones([1, self.length, self.hidden_dim]) + 1e-5
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
             hidden_states=hidden_states,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzhhhhhhh"])
@@ -164,18 +169,19 @@
             hidden_states=hidden_states,
         )
         self.assertTrue("h" not in self.join_as_string(output))
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentiallyy")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["s"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["s"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(
                 self.next,
                 prompt=prompt,
                 cache=cache,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/greedy_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/greedy_sampler.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,17 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Greedy Sampler."""
 
-import tensorflow as tf
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.GreedySampler")
@@ -36,31 +35,31 @@
     ```python
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, cache, index):
-        hidden_states = tf.ones((batch_size, 10))
+        hidden_states = np.ones((batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = np.ones((batch_size, vocab_size))
         return logits, hidden_states, cache
 
     output = keras_nlp.samplers.GreedySampler()(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=np.full((batch_size, length,), char_lookup['z'], dtype="int32"),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzaaaaaaa']
     ```
     """
 
     def __init__(
         self,
         **kwargs,
     ):
         super().__init__(**kwargs)
 
     def get_next_token(self, probabilities):
-        return tf.argmax(probabilities, axis=-1)
+        return ops.argmax(probabilities, axis=-1)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/greedy_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/greedy_sampler_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,112 +9,117 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Greedy sampler."""
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.greedy_sampler import GreedySampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class GreedySamplerTest(tf.test.TestCase, parameterized.TestCase):
+class GreedySamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 26).
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.vocab_size = len(self.int_lookup)
 
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the next char in cache.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = GreedySampler(temperature=1.0)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the first token in the vocab.
             logits = (
-                tf.one_hot(
-                    tf.zeros(self.batch_size, dtype=tf.int32),
+                ops.one_hot(
+                    ops.zeros(self.batch_size, dtype="int32"),
                     self.vocab_size,
                 )
                 * 1e9
             )
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_early_stopping(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
         )
         self.assertEqual(self.join_as_string(output), ["sequentzzzzz"])
 
     def test_is_greedy(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution where each id is progressively less likely.
-            logits = tf.range(self.vocab_size, 0, -1, dtype="float32")
-            logits = tf.repeat(logits[tf.newaxis, :], self.batch_size, axis=0)
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.repeat(logits[None, :], self.batch_size, axis=0)
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
         )
-        output_ids = set(output[0].numpy())
+        output_ids = set(ops.convert_to_numpy(output[0]))
         self.assertContainsSubset(output_ids, [0])
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(self.next, prompt=prompt, cache=cache)
 
         output = generate(prompt, cache)
         self.assertEqual(self.join_as_string(output), ["sequentially"])
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/random_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/random_sampler.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Random Sampler."""
 
-import tensorflow as tf
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import ops
+from keras_nlp.src.backend import random
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.RandomSampler")
@@ -27,56 +27,60 @@
     """Random Sampler class.
 
     This sampler implements random sampling. Briefly, random sampler randomly
     selects a token from the entire distribution of the tokens, with selection
     chance determined by the probability of each token.
 
     Args:
-        seed: int, defaults to None. The random seed.
+        seed: int. The random seed. Defaults to `None`.
 
     Call arguments:
         {{call_args}}
 
     Examples:
     ```python
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, state, index):
-        hidden_states = tf.ones((batch_size, 10))
+        hidden_states = np.ones((batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = np.ones((batch_size, vocab_size))
         return logits, hidden_states, state
 
     output = keras_nlp.samplers.RandomSampler()(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=np.full((batch_size, length,), char_lookup['z'], dtype="int32"),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzcpnjqij']
     ```
     """
 
     def __init__(
         self,
         seed=None,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.seed = seed
+        self.seed_generator = random.SeedGenerator(seed)
 
     def get_next_token(self, probabilities):
         # Sample the next token from the probability distribution.
-        next_token_id = tf.random.categorical(
-            tf.math.log(probabilities), 1, seed=self.seed, dtype="int32"
+        next_token_id = random.categorical(
+            ops.log(probabilities),
+            1,
+            seed=self.seed,
+            dtype="int32",
         )
-        return tf.squeeze(next_token_id, axis=-1)
+        return ops.squeeze(next_token_id, axis=-1)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "seed": self.seed,
             }
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/random_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/random_sampler_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,106 +10,111 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Random sampler."""
 
 import numpy as np
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.random_sampler import RandomSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class RandomSamplerTest(tf.test.TestCase, parameterized.TestCase):
+class RandomSamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 25].
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.vocab_size = len(self.int_lookup)
 
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the next char in state.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = RandomSampler(temperature=1.0)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the first token in the vocab.
             logits = np.zeros((self.batch_size, self.vocab_size))
             logits[:, 0] = 1e9
-            return tf.constant(logits), hidden_states, cache
+            return ops.array(logits), hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_temperature(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
-            logits = tf.range(self.vocab_size, 0, -1, dtype=tf.float32)
-            logits = tf.reshape(logits[tf.newaxis, :], (self.batch_size, -1))
-            return tf.constant(logits), hidden_states, cache
+            hidden_states = ops.ones([self.batch_size, 5])
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.reshape(logits[None, :], (self.batch_size, -1))
+            return ops.array(logits), hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         output = RandomSampler(temperature=1e-5)(
             next=next,
             prompt=prompt,
         )
-        self.assertAllEqual(output, tf.zeros_like(output))
+        self.assertAllEqual(output, ops.zeros_like(output))
 
     def test_early_stopping(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
         )
         self.assertEqual(self.join_as_string(output), ["sequentzzzzz"])
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(self.next, prompt=prompt, cache=cache)
 
         output = generate(prompt, cache)
         self.assertEqual(self.join_as_string(output), ["sequentially"])
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/sampler.py`

 * *Files 25% similar despite different names*

```diff
@@ -9,34 +9,34 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Base sampler class."""
 
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import config
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.backend import random
 from keras_nlp.src.utils.python_utils import format_docstring
 
 call_args_docstring = """next: A function which takes in the
             `prompt, cache, index` of the current generation loop, and outputs
-            a tuple `(logits, cache, hidden_states)` with `logits` being the
-            logits of next token, `cache` for next iteration, and
-            `hidden_states` being the representation of the token.
+            a tuple `(logits, hidden_states, cache)` with `logits` being the
+            logits of next token, `hidden_states` being the representation of
+            the next token, and `cache` for next iteration.
         prompt: A 2D integer tensor with shape `(batch_size, max_length)`. This
             tensor will be iteratively updated column by column with new sampled
             values, starting at `index`.
         cache: Optional. A tensor or nested structure of tensors that will be
             updated by each call to `next`. This can be used to cache
             computations from early iterations of the generative loop.
         index: Optional. The first index of `prompt` to start sampling at.
-            Usually this is set as the length of the shortest non-padding
+            Usually this is set as the length of the shortest non-padded
             sequence in `prompt`.
         mask: Optional. A 2D integer tensor with the same shape as `prompt`.
             Locations which are `True` in the mask are never updated during
             sampling. Usually used to mark all locations in the dense prompt
             tensor which were present in a user input.
         end_token_id: Optional. The token marking the end of the sequence. If
             specified, sampling will stop as soon as all sequences in the prompt
@@ -46,17 +46,17 @@
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.Sampler")
 class Sampler:
     """Base sampler class.
 
     Args:
-        temperature: float. optional. defaults to '1.0'. Used to control the
+        temperature: float. optional. Used to control the
             randomness of the sampling. The higher the temperature, the
-            more diverse the samples.
+            more diverse the samples. Defaults to `1.0`.
 
     Call arguments:
         {{call_args}}
 
     This base class can be extended to implement different auto-regressive
     sampling methods. Subclasses can either:
 
@@ -73,87 +73,137 @@
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, cache, index):
         # return a uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = ops.ones((batch_size, vocab_size))
         return logits, None, cache
 
     output = keras_nlp.samplers.GreedySampler()(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=ops.fill((batch_size, length,), char_lookup['z']),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzaaaaaaa']
     ```
     """
 
     def __init__(
         self,
         temperature=1.0,
     ):
         self.temperature = temperature
+        self._seed_generators = []
+
+    def __setattr__(self, name, value):
+        # We could update to the `Tracker` class from keras-core if our needs
+        # become more advanced (e.g. list assignment, nested trackables). For
+        # now, we only track `SeedGenerator` instances directly on the sampler.
+        if isinstance(value, random.SeedGenerator):
+            self._seed_generators.append(value)
+        return super().__setattr__(name, value)
+
+    @property
+    def variables(self):
+        variables = []
+        for sg in self._seed_generators:
+            variables.append(sg.state)
+        return variables
 
     def __call__(
         self,
         next,
         prompt,
         cache=None,
         index=0,
         mask=None,
         end_token_id=None,
         hidden_states=None,
     ):
-        max_length = tf.shape(prompt)[-1]
+        max_length = ops.shape(prompt)[-1]
         # Make sure `max_length` and `index` are the same dtype.
-        index = tf.cast(index, max_length.dtype)
+        index = ops.cast(index, "int32")
+        max_length = ops.cast(max_length, "int32")
         if mask is None:
-            mask = tf.zeros_like(prompt, dtype=tf.bool)
+            mask = ops.zeros_like(prompt, dtype="bool")
         else:
-            mask = tf.cast(mask, dtype=tf.bool)
-        # `tf.while_loop` will not accept `None` as a value for `loop_vars`.
+            mask = ops.cast(mask, dtype="bool")
+        # `ops.while_loop` will not accept `None` as a value for `loop_vars`.
         cache = () if cache is None else cache
 
         def cond(prompt, cache, index):
             if end_token_id is None:
                 return True
             # Stop if all sequences have produced a *new* end_token_id.
             end_tokens = (prompt == end_token_id) & (~mask)
-            prompt_done = tf.reduce_any(end_tokens, axis=-1)
-            return not tf.reduce_all(prompt_done)
+            prompt_done = ops.any(end_tokens, axis=-1)
+            return ops.logical_not(ops.all(prompt_done))
 
         def body(prompt, cache, index):
             # Compute the softmax distribution for the next token.
             logits, _, cache = next(prompt, cache, index)
             probabilities = keras.activations.softmax(logits / self.temperature)
             # Compute the next token.
             next_token = self.get_next_token(probabilities)
             # Don't overwrite anywhere mask is True.
-            next_token = tf.cast(next_token, prompt.dtype)
-            # Ensure shape is `[None]`, otherwise it causes issues after
-            # converting to TFLite.
-            next_token = tf.ensure_shape(next_token, [None])
-            next_token = tf.where(mask[:, index], prompt[:, index], next_token)
+            next_token = ops.cast(next_token, prompt.dtype)
+            next_token = ops.where(mask[:, index], prompt[:, index], next_token)
             # Update the prompt with the next token.
-            next_token = next_token[:, tf.newaxis]
-            prompt = dynamic_update_slice(prompt, next_token, [0, index])
+            next_token = next_token[:, None]
+            prompt = ops.slice_update(prompt, [0, index], next_token)
+
             # Return the next prompt, cache and incremented index.
             return (prompt, cache, index + 1)
 
-        prompt, _, _ = tf.while_loop(
-            cond=cond,
-            body=body,
+        prompt, _, _ = self.run_loop(
+            cond,
+            body,
             loop_vars=(prompt, cache, index),
             maximum_iterations=(max_length - index),
         )
         return prompt
 
+    def run_loop(self, cond, body, loop_vars=None, maximum_iterations=None):
+        """Run ops.while_loops with a `StatelessScope` if necessary."""
+        if config.backend() == "jax":
+
+            def stateless_cond(variables, *loop_vars):
+                return cond(*loop_vars)
+
+            def stateless_body(variables, *loop_vars):
+                mapping = zip(self.variables, variables)
+                with keras.StatelessScope(state_mapping=mapping) as scope:
+                    loop_vars = body(*loop_vars)
+
+                variables = []
+                for v in self.variables:
+                    new_v = scope.get_current_value(v)
+                    variables.append(new_v if new_v is not None else v)
+                return variables, *loop_vars
+
+            variables = [ops.convert_to_tensor(v) for v in self.variables]
+            variables, *loop_vars = ops.while_loop(
+                cond=stateless_cond,
+                body=stateless_body,
+                loop_vars=(variables, *loop_vars),
+                maximum_iterations=maximum_iterations,
+            )
+            [ref_v.assign(v) for ref_v, v in zip(self.variables, variables)]
+        else:
+            loop_vars = ops.while_loop(
+                cond=cond,
+                body=body,
+                loop_vars=(loop_vars),
+                maximum_iterations=maximum_iterations,
+            )
+        return loop_vars
+
     def get_next_token(self, probabilities):
         """Get the next token.
         Args:
             probabilities: a Tensor, the probability distribution for next
                 token over all vocab tokens.
         Get the next token based on given probability distribution over tokens.
         Subclasses must implement this method.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/serialization.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/serialization.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,42 +8,41 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.samplers.beam_sampler import BeamSampler
 from keras_nlp.src.samplers.contrastive_sampler import ContrastiveSampler
 from keras_nlp.src.samplers.greedy_sampler import GreedySampler
 from keras_nlp.src.samplers.random_sampler import RandomSampler
 from keras_nlp.src.samplers.top_k_sampler import TopKSampler
 from keras_nlp.src.samplers.top_p_sampler import TopPSampler
 
 
 @keras_nlp_export("keras_nlp.samplers.serialize")
 def serialize(sampler):
-    return keras.utils.serialize_keras_object(sampler)
+    return keras.saving.serialize_keras_object(sampler)
 
 
 @keras_nlp_export("keras_nlp.samplers.deserialize")
 def deserialize(config, custom_objects=None):
     """Return a `Sampler` object from its config."""
     all_classes = {
         "beam": BeamSampler,
         "contrastive": ContrastiveSampler,
         "greedy": GreedySampler,
         "random": RandomSampler,
         "top_k": TopKSampler,
         "top_p": TopPSampler,
     }
-    return keras.utils.deserialize_keras_object(
+    return keras.saving.deserialize_keras_object(
         config,
         module_objects=all_classes,
         custom_objects=custom_objects,
         printable_module_name="samplers",
     )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/serialization_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/serialization_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,23 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Sampler classes."""
 
-import tensorflow as tf
-
 from keras_nlp.src.samplers.serialization import deserialize
 from keras_nlp.src.samplers.serialization import get
 from keras_nlp.src.samplers.serialization import serialize
 from keras_nlp.src.samplers.top_k_sampler import TopKSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class SerializationTest(tf.test.TestCase):
+class SerializationTest(TestCase):
     def test_serialization(self):
         sampler = TopKSampler(k=5)
         restored = deserialize(serialize(sampler))
         self.assertDictEqual(sampler.get_config(), restored.get_config())
 
     def test_get(self):
         # Test get from string.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/top_k_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_k_sampler.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Top-k Sampler."""
 
-import tensorflow as tf
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import ops
+from keras_nlp.src.backend import random
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.TopKSampler")
@@ -28,35 +28,35 @@
 
     This sampler implements top-k search algorithm. Briefly, top-k algorithm
     randomly selects a token from the tokens of top K probability, with
     selection chance determined by the probability.
 
     Args:
         k: int, the `k` value of top-k.
-        seed: int, defaults to None. The random seed.
+        seed: int. The random seed. Defaults to `None`.
 
     Call arguments:
         {{call_args}}
 
     Examples:
     ```python
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, cache, index):
-        hidden_states = tf.ones((batch_size, 10))
+        hidden_states = np.ones((batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = np.ones((batch_size, vocab_size))
         return logits, hidden_states, cache
 
     output = keras_nlp.samplers.TopKSampler(k=3)(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=np.full((batch_size, length,), char_lookup['z'], dtypes="int32"),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzacbbcaa']
     ```
     """
 
@@ -65,27 +65,34 @@
         k=5,
         seed=None,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.k = k
         self.seed = seed
+        self.seed_generator = random.SeedGenerator(seed)
 
     def get_next_token(self, probabilities):
         # Filter out top-k tokens.
-        top_k_pred, top_k_indices = tf.math.top_k(
-            probabilities, k=self.k, sorted=False
+        top_k_pred, top_k_indices = ops.top_k(
+            probabilities,
+            k=self.k,
+            sorted=False,
         )
         # Sample the next token from the probability distribution.
-        next_token = tf.random.categorical(
-            tf.math.log(top_k_pred), 1, seed=self.seed
+        sample_indices = random.categorical(
+            ops.log(top_k_pred),
+            1,
+            seed=self.seed_generator,
+            dtype="int32",
         )
 
         # Rearrange to get the next token idx from the original order.
-        return tf.gather_nd(top_k_indices, next_token, batch_dims=1)
+        output = ops.take_along_axis(top_k_indices, sample_indices, axis=-1)
+        return ops.squeeze(output, axis=-1)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "k": self.k,
                 "seed": self.seed,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/top_k_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_k_sampler_test.py`

 * *Files 13% similar despite different names*

```diff
@@ -9,112 +9,117 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Top-K sampler."""
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.top_k_sampler import TopKSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class TopKSamplerTest(tf.test.TestCase, parameterized.TestCase):
+class TopKSamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 26).
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.vocab_size = len(self.int_lookup)
 
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the next char in cache.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = TopKSampler(k=5, temperature=1.0)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the first token in the vocab.
             logits = (
-                tf.one_hot(
-                    tf.zeros(self.batch_size, dtype=tf.int32),
+                ops.one_hot(
+                    ops.zeros(self.batch_size, dtype="int32"),
                     self.vocab_size,
                 )
                 * 1e9
             )
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_early_stopping(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
         )
         self.assertEqual(self.join_as_string(output), ["sequentzzzzz"])
 
     def test_outputs_in_top_k(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution where each id is progressively less likely.
-            logits = tf.range(self.vocab_size, 0, -1, dtype="float32")
-            logits = tf.repeat(logits[tf.newaxis, :], self.batch_size, axis=0)
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.repeat(logits[None, :], self.batch_size, axis=0)
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
         )
-        output_ids = set(output[0].numpy())
+        output_ids = set(ops.convert_to_numpy(output[0]))
         self.assertContainsSubset(output_ids, range(5))
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(self.next, prompt=prompt, cache=cache)
 
         output = generate(prompt, cache)
         self.assertEqual(self.join_as_string(output), ["sequentially"])
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/top_p_sampler.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_p_sampler.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Top-p Sampler."""
 
-import tensorflow as tf
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import ops
+from keras_nlp.src.backend import random
 from keras_nlp.src.samplers.sampler import Sampler
 from keras_nlp.src.samplers.sampler import call_args_docstring
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
 @format_docstring(call_args=call_args_docstring)
 @keras_nlp_export("keras_nlp.samplers.TopPSampler")
@@ -30,41 +30,41 @@
     from the smallest subset of output probabilities that sum to greater than
     `p`. Put in another way, top-p will first order token predictions by
     likelihood, and ignore all tokens after the cumulative probability of
     selected tokens exceeds `p`, then select a token from the remaining tokens.
 
     Args:
         p: float, the `p` value of top-p.
-        k: int, defaults to None. If set, this argument defines a
+        k: int. If set, this argument defines a
             heuristic "top-k" cutoff applied before the "top-p" sampling. All
             logits not in the top `k` will be discarded, and the remaining
             logits will be sorted to find a cutoff point for `p`. Setting this
             arg can significantly speed sampling up by reducing the number
-            of tokens to sort.
-        seed: int, defaults to None. The random seed.
+            of tokens to sort. Defaults to `None`.
+        seed: int. The random seed. Defaults to `None`.
 
     Call arguments:
         {{call_args}}
 
     Examples:
     ```python
     # Use a simple alphabet of lowercase characters with ids in range [0, 25].
     int_lookup = {i: chr(i + ord('a')) for i in range(26)}
     char_lookup = {v: k for k, v in int_lookup.items()}
     batch_size, length, vocab_size = 1, 12, len(int_lookup)
 
     def next(prompt, cache, index):
-        hidden_states = tf.ones((batch_size, 10))
+        hidden_states = np.ones((batch_size, 10))
         # A uniform distribution over our alphabet.
-        logits = tf.ones((batch_size, vocab_size))
+        logits = np.ones((batch_size, vocab_size))
         return logits, hidden_states, cache
 
     output = keras_nlp.samplers.TopPSampler(p=0.1)(
         next=next,
-        prompt=tf.fill((batch_size, length,), char_lookup['z']),
+        prompt=np.full((batch_size, length,), char_lookup['z'], dtype="int32"),
         index=5,
     )
     print(["".join([int_lookup[i] for i in s]) for s in output.numpy()])
     # >>> ['zzzzzbabcccb']
     ```
     """
 
@@ -75,41 +75,46 @@
         seed=None,
         **kwargs,
     ):
         super().__init__(**kwargs)
         self.p = p
         self.k = k
         self.seed = seed
+        self.seed_generator = random.SeedGenerator(seed)
 
     def get_next_token(self, probabilities):
-        cutoff = tf.shape(probabilities)[1]
+        cutoff = ops.shape(probabilities)[1]
         if self.k is not None:
             # If `k` is set, only sample from top `k` tokens.
-            cutoff = tf.math.minimum(cutoff, self.k)
-        sorted_preds, sorted_indices = tf.math.top_k(
+            cutoff = self.k
+        sorted_preds, sorted_indices = ops.top_k(
             probabilities, k=cutoff, sorted=True
         )
         # Calculate cumulative probability distribution.
-        cumulative_probabilities = tf.math.cumsum(sorted_preds, axis=-1)
+        cumulative_probabilities = ops.cumsum(sorted_preds, axis=-1)
         # Create a mask for the tokens to keep.
         keep_mask = cumulative_probabilities <= self.p
         # Shift to include the last token that exceed p.
-        shifted_keep_mask = tf.concat(
-            [tf.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1
+        shifted_keep_mask = ops.concatenate(
+            [ops.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1
         )
         # Filter out unmasked tokens and sample from filtered distribution.
-        probabilities = tf.where(
+        probabilities = ops.where(
             shifted_keep_mask,
             sorted_preds,
-            tf.zeros(tf.shape(sorted_preds), dtype=sorted_preds.dtype),
+            ops.zeros(ops.shape(sorted_preds), dtype=sorted_preds.dtype),
         )
-        sorted_next_token = tf.random.categorical(
-            tf.math.log(probabilities), 1, seed=self.seed
+        sorted_next_token = random.categorical(
+            ops.log(probabilities),
+            1,
+            seed=self.seed_generator,
+            dtype="int32",
         )
-        return tf.gather_nd(sorted_indices, sorted_next_token, batch_dims=1)
+        output = ops.take_along_axis(sorted_indices, sorted_next_token, axis=-1)
+        return ops.squeeze(output, axis=-1)
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "p": self.p,
                 "k": self.k,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/samplers/top_p_sampler_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/samplers/top_p_sampler_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,145 +10,151 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for Top-P sampler."""
 
 import numpy as np
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
+from keras_nlp.src.backend import ops
 from keras_nlp.src.samplers.top_p_sampler import TopPSampler
+from keras_nlp.src.tests.test_case import TestCase
 
 
-class TopPSamplerTest(tf.test.TestCase, parameterized.TestCase):
+class TopPSamplerTest(TestCase):
     def setUp(self):
         super().setUp()
         # Use a simple alphabet of lowercase characters to [0, 26).
         self.int_lookup = {i: chr(i + ord("a")) for i in range(26)}
         self.char_lookup = {v: k for k, v in self.int_lookup.items()}
         self.batch_size = 1
         self.length = 12
         self.vocab_size = len(self.int_lookup)
 
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the next char in cache.
-            logits = tf.one_hot(cache[:, index], self.vocab_size) * 1e9
+            logits = ops.one_hot(cache[:, index], self.vocab_size) * 1e9
             return logits, hidden_states, cache
 
         self.next = next
         self.sampler = TopPSampler(p=0.1)
 
     def join_as_string(self, x):
-        return ["".join([self.int_lookup[i] for i in s]) for s in x.numpy()]
+        x = ops.convert_to_numpy(x)
+        return ["".join([self.int_lookup[i] for i in s]) for s in x]
 
     def test_stateless_call(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution favoring the first token in the vocab.
             logits = (
-                tf.one_hot(
-                    tf.zeros(self.batch_size, dtype=tf.int32),
+                ops.one_hot(
+                    ops.zeros(self.batch_size, dtype="int32"),
                     self.vocab_size,
                 )
                 * 1e9
             )
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=next,
             prompt=prompt,
             index=5,
         )
         self.assertEqual(self.join_as_string(output), ["zzzzzaaaaaaa"])
 
     def test_stateful_call(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
         )
         self.assertEqual(self.join_as_string(output), ["sequentially"])
 
     def test_early_stopping(self):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = self.sampler(
             next=self.next,
             prompt=prompt,
             cache=cache,
             end_token_id=self.char_lookup["t"],
         )
         self.assertEqual(self.join_as_string(output), ["sequentzzzzz"])
 
     def test_only_sample_from_top_k_tokens(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             # Return a distribution where each id is progressively less likely.
-            logits = tf.range(self.vocab_size, 0, -1, dtype="float32")
-            logits = tf.repeat(logits[tf.newaxis, :], self.batch_size, axis=0)
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.repeat(logits[None, :], self.batch_size, axis=0)
             return logits, hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = TopPSampler(p=1, k=5)(
             next=next,
             prompt=prompt,
             index=5,
         )
         generated_str = self.join_as_string(output[:, 5:])[0]
         token_set = set(generated_str)
         self.assertContainsSubset(token_set, set("abcde"))
 
     def test_outputs_in_top_p(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
+            hidden_states = ops.ones([self.batch_size, 5])
             logits = np.zeros((self.batch_size, self.vocab_size))
-            return tf.constant(logits), hidden_states, cache
+            logits[:, :3] = 1.0
+            return ops.array(logits), hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
         output = TopPSampler(p=(2.0 / self.vocab_size))(
             next=next,
             prompt=prompt,
         )
-        output_ids = set(output[0].numpy())
+        output_ids = set(ops.convert_to_numpy(output[0]))
         self.assertContainsSubset(output_ids, range(3))
 
     def test_temperature(self):
         def next(prompt, cache, index):
             # Dummy hidden states.
-            hidden_states = tf.ones([self.batch_size, 5])
-            logits = tf.range(self.vocab_size, 0, -1, dtype=tf.float32)
-            logits = tf.reshape(logits[tf.newaxis, :], (self.batch_size, -1))
-            return tf.constant(logits), hidden_states, cache
+            hidden_states = ops.ones([self.batch_size, 5])
+            logits = ops.arange(self.vocab_size, 0, -1, dtype="float32")
+            logits = ops.reshape(logits[None, :], (self.batch_size, -1))
+            return ops.array(logits), hidden_states, cache
 
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         output = TopPSampler(p=0.5, temperature=1e-9)(
             next=next,
             prompt=prompt,
         )
-        self.assertAllEqual(output, tf.zeros_like(output))
+        self.assertAllEqual(output, ops.zeros_like(output))
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.tf_only
     def test_compilation(self, jit_compile):
         cache_chars = list("sequentially")
-        cache = tf.constant([[self.char_lookup[c] for c in cache_chars]])
-        prompt = tf.fill((self.batch_size, self.length), self.char_lookup["z"])
+        cache = ops.array([[self.char_lookup[c] for c in cache_chars]])
+        prompt = ops.full((self.batch_size, self.length), self.char_lookup["z"])
 
         @tf.function(jit_compile=jit_compile)
         def generate(prompt, cache):
             return self.sampler(self.next, prompt=prompt, cache=cache)
 
         output = generate(prompt, cache)
         self.assertEqual(self.join_as_string(output), ["sequentially"])
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tests/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/t5/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/docstring_lib.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/docstring_lib.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/docstring_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/docstring_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import sys
 import unittest
 
 import numpy as np
 import pytest
 import sentencepiece
 import tensorflow as tf
-from tensorflow import keras
 
 import keras_nlp.src as keras_nlp
+from keras_nlp.src.backend import keras
 from keras_nlp.src.tests.doc_tests import docstring_lib
 from keras_nlp.src.tests.doc_tests import fenced_docstring_lib
 from keras_nlp.src.tests.doc_tests.fenced_docstring_lib import (
     astor,  # For checking conditional import.
 )
 
 PACKAGE = "keras_nlp."
@@ -44,14 +44,15 @@
 
 
 @pytest.fixture(scope="session")
 def docstring_module(pytestconfig):
     return pytestconfig.getoption("docstring_module")
 
 
+@pytest.mark.tf_only
 def test_docstrings(docstring_module):
     keras_nlp_modules = find_modules()
     # As of this writing, it doesn't seem like pytest support load_tests
     # protocol for unittest:
     #     https://docs.pytest.org/en/7.1.x/how-to/unittest.html
     # So we run the unittest.TestSuite manually and report the results back.
     runner = unittest.TextTestRunner()
@@ -82,14 +83,15 @@
         )
     result = runner.run(suite)
     if not result.wasSuccessful():
         print(result)
     assert result.wasSuccessful()
 
 
+@pytest.mark.tf_only
 @pytest.mark.extra_large
 @pytest.mark.skipif(
     astor is None,
     reason="This test requires `astor`. Please `pip install astor` to run.",
 )
 def test_fenced_docstrings(docstring_module):
     """Tests fenced code blocks in docstrings.
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tests/doc_tests/fenced_docstring_lib.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tests/doc_tests/fenced_docstring_lib.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_pair_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_pair_tokenizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -22,21 +22,24 @@
 import json
 import os
 from typing import Iterable
 from typing import List
 
 import regex as re
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.tokenizers import tokenizer
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
+from keras_nlp.src.utils.tensor_utils import is_string_dtype
 
 try:
     import tensorflow_text as tf_text
 except ImportError:
     tf_text = None
 
 # As python and TF handles special spaces differently, we need to
@@ -88,15 +91,15 @@
 
 
 def remove_strings_from_inputs(tensor, string_to_remove):
     """Remove certain strings from input tensor."""
     non_empty_mask = tensor != string_to_remove
     flatten_indexes = tf.where(non_empty_mask)
     flatten_result = tf.gather_nd(tensor, flatten_indexes)
-    row_lengths = tf.reduce_sum(tf.cast(non_empty_mask, tf.int64), axis=1)
+    row_lengths = tf.reduce_sum(tf.cast(non_empty_mask, "int64"), axis=1)
     result = tf.RaggedTensor.from_row_lengths(
         values=flatten_result,
         row_lengths=row_lengths,
     )
     return result
 
 
@@ -150,27 +153,27 @@
     """
 
     def __init__(self):
         # `tf.lookup.experimental.MutableHashTable` does not support string to
         # string mapping. So we first convert to string to an integer key, and
         # use the integer key to find the value.
         self.factors = tf.pow(
-            tf.constant(256, dtype=tf.int64), tf.range(0, 8, dtype=tf.int64)
+            tf.constant(256, dtype="int64"), tf.range(0, 8, dtype="int64")
         )
         self.id2value = tf.lookup.experimental.MutableHashTable(
-            tf.int64, tf.string, ""
+            "int64", tf.string, ""
         )
 
     def _get_key(self, keys):
         """Get the hash key for given inputs."""
         # `tf.fingerprint` converts token to a array of uint8 of length 8, we
         # need to convert it to a uint64.
         return tf.squeeze(
             tf.matmul(
-                tf.cast(tf.fingerprint(keys), dtype=tf.int64),
+                tf.cast(tf.fingerprint(keys), dtype="int64"),
                 self.factors[:, tf.newaxis],
             ),
             -1,
         )
 
     def lookup(self, keys):
         """Look up the encoded outputs of given tokens."""
@@ -218,46 +221,50 @@
 
     Args:
         vocabulary: string or dict, maps token to integer ids. If it is a
             string, it should be the file path to a json file.
         merges: string or list, contains the merge rule. If it is a string,
             it should be the file path to merge rules. The merge rule file
             should have one merge rule per line.
-        sequence_length: int, defaults to None. If set, the output will be
-            padded or truncated to the `sequence_length`.
-        add_prefix_space: bool, defaults to False. Whether or not to add an
+        sequence_length: int. If set, the output will be
+            padded or truncated to the `sequence_length`. Defaults to `None`.
+        add_prefix_space: bool. Whether to add an
             initial space to the input. This tokenizer is whitespace aware,
             and will tokenize a word with a leading space differently. Adding
             a prefix space to the first word will cause it to be tokenized
             equivalently to all subsequent words in the sequence.
-        unsplittable_tokens: list, defaults to None. A list of strings that will
+            Defaults to `False`.
+        unsplittable_tokens: list. A list of strings that will
             never be split during the word-level splitting applied before the
             byte-pair encoding. This can be used to ensure special tokens map to
             unique indices in the vocabulary, even if these special tokens
             contain splittable characters such as punctuation. Special tokens
-            must still be included in `vocabulary`.
+            must still be included in `vocabulary`. Defaults to `None`.
 
     Examples:
 
     Tokenize
     >>> vocab = {"butter": 1, "fly": 2}
     >>> merge = ["b u", "t t", "e r", "bu tt", "butt er", "f l", "fl y"]
     >>> tokenizer = keras_nlp.tokenizers.BytePairTokenizer(vocab, merge)
-    >>> tokenizer("butterfly")
-    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>
-    >>> tokenizer(["butterfly"])
-    <tf.RaggedTensor [[1, 2]]>
-    >>> tokenizer(["butterfly", "butter"])
-    <tf.RaggedTensor [[1, 2], [1]]>
+    >>> outputs = tokenizer("butterfly")
+    >>> np.array(outputs)
+    array([1, 2], dtype=int32)
+    >>> seq1, seq2 = tokenizer(["butterfly", "butter"])
+    >>> np.array(seq1)
+    array([1, 2], dtype=int32)
+    >>> np.array(seq2)
+    array([1], dtype=int32)
     >>> tokenizer = keras_nlp.tokenizers.BytePairTokenizer(
     ...     vocab, merge, sequence_length=2)
-    >>> tokenizer(["butterfly", "butter"])
-    <tf.Tensor: shape=(2, 2), dtype=int32, numpy=
-    array([[1, 2],
-           [1, 0]], dtype=int32)>
+    >>> seq1, seq2 = tokenizer(["butterfly", "butter"])
+    >>> np.array(seq1)
+    array([1, 2], dtype=int32)
+    >>> np.array(seq2)
+    array([1, 0], dtype=int32)
 
     Detokenize
     >>> vocab = {"butter": 1, "fly": 2}
     >>> merge = ["b u", "t t", "e r", "bu tt", "butt er", "f l", "fl y"]
     >>> tokenizer = keras_nlp.tokenizers.BytePairTokenizer(vocab, merge)
     >>> tokenizer.detokenize([[1, 2]])
     <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'butterfly'],
@@ -267,30 +274,26 @@
     def __init__(
         self,
         vocabulary,
         merges,
         sequence_length=None,
         add_prefix_space=False,
         unsplittable_tokens=None,
+        dtype="int32",
         **kwargs,
     ) -> None:
         assert_tf_text_installed(self.__class__.__name__)
 
-        # Check dtype and provide a default.
-        if "dtype" not in kwargs or kwargs["dtype"] is None:
-            kwargs["dtype"] = tf.int32
-        else:
-            dtype = tf.dtypes.as_dtype(kwargs["dtype"])
-            if not dtype.is_integer and dtype != tf.string:
-                raise ValueError(
-                    "Output dtype must be an integer type or a string. "
-                    f"Received: `dtype={dtype}`"
-                )
+        if not is_integer_dtype(dtype) and not is_string_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type or a string. "
+                f"Received: dtype={dtype}"
+            )
 
-        super().__init__(**kwargs)
+        super().__init__(dtype=dtype, **kwargs)
 
         if isinstance(vocabulary, str):
             with open(vocabulary, "r") as f:
                 self.vocabulary = json.load(f)
         elif isinstance(vocabulary, dict):
             self.vocabulary = vocabulary.copy()
         else:
@@ -365,15 +368,15 @@
         # dict. Assuming the main use case is looking up a few special tokens
         # early in the vocab, this should be fine.
 
         keys = self.get_vocabulary()
         for token in keys:
             if self.vocabulary[token] == id:
                 return token
-        return None
+        raise ValueError(f"`id` is out of the vocabulary. Received: {id}")
 
     def token_to_id(self, token: str) -> int:
         """Convert a string token to an integer id."""
         return self.vocabulary[token]
 
     def get_config(self):
         config = super().get_config()
@@ -436,15 +439,15 @@
             unfinished_words, min_pair_rank_indices + 1, batch_dims=1
         )
 
         merged_pairs = tf.strings.join([pair_left, pair_right])
         empty_strs = tf.fill(tf.shape(merged_pairs), "")
 
         unfinished_word_indices = tf.cast(
-            tf.boolean_mask(tf.range(tf.shape(mask)[0]), mask), dtype=tf.int64
+            tf.boolean_mask(tf.range(tf.shape(mask)[0]), mask), dtype="int64"
         )
         merged_pair_indices = tf.concat(
             [
                 unfinished_word_indices[:, tf.newaxis],
                 min_pair_rank_indices[:, tf.newaxis],
             ],
             axis=1,
@@ -553,30 +556,27 @@
         if scalar_input:
             tokens = tf.squeeze(tokens, 0)
             tf.ensure_shape(tokens, shape=[self.sequence_length])
 
         return tokens
 
     def detokenize(self, inputs):
-        if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):
-            inputs = tf.convert_to_tensor(inputs)
-
-        scalar_input = inputs.shape.rank == 0
-        if scalar_input:
-            inputs = tf.expand_dims(inputs, 0)
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
 
         unicode_text = tf.strings.reduce_join(
             self.id_to_token_map.lookup(inputs), axis=-1
         )
         split_unicode_text = tf.strings.unicode_split(unicode_text, "UTF-8")
-        byte_text = tf.strings.reduce_join(
+        outputs = tf.strings.reduce_join(
             self.unicode2byte.lookup(split_unicode_text), axis=-1
         )
 
-        return byte_text
+        if unbatched:
+            outputs = tf.squeeze(outputs, 0)
+        return outputs
 
     def _transform_bytes(self, tokens):
         """Map token bytes to unicode using `byte2unicode`."""
         split_bytes = tf.strings.bytes_split(tokens)
         split_unicode = self.byte2unicode.lookup(split_bytes)
         return split_unicode
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_pair_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/whisper/whisper_preprocessor_test.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,186 +8,217 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+"""Tests for Whisper preprocessor layer."""
+
 import os
 
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
-from tensorflow import keras
-
-from keras_nlp.src.tokenizers.byte_pair_tokenizer import BytePairTokenizer
 
-VOCAB_PATH = keras.utils.get_file(
-    None,
-    "https://storage.googleapis.com/keras-nlp/models/roberta_base/vocab.json",
-)
-MERGE_PATH = keras.utils.get_file(
-    None,
-    "https://storage.googleapis.com/keras-nlp/models/roberta_base/merges.txt",
+from keras_nlp.src.backend import keras
+from keras_nlp.src.models.whisper.whisper_audio_feature_extractor import (
+    WhisperAudioFeatureExtractor,
 )
+from keras_nlp.src.models.whisper.whisper_preprocessor import WhisperPreprocessor
+from keras_nlp.src.models.whisper.whisper_tokenizer import WhisperTokenizer
 
 
-@pytest.mark.large
-class BytePairTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class WhisperPreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
-        super().setUp()
-        self.tokenizer = BytePairTokenizer(
-            vocabulary=VOCAB_PATH, merges=MERGE_PATH
-        )
-
-    def test_tokenize_list_input(self):
-        input_data = ["brown.", "black."]
-        call_output = self.tokenizer(input_data)
-        tokenize_output = self.tokenizer.tokenize(input_data)
-        expected = tf.ragged.constant([[31876, 4], [14178, 4]])
-        self.assertAllEqual(call_output, expected)
-        self.assertAllEqual(tokenize_output, expected)
-
-        input_data = tf.convert_to_tensor(["brown.", "black."])
-        encoded = self.tokenizer(input_data)
-        self.assertAllEqual(encoded, expected)
-
-    def test_tokenize_string_output(self):
-        input_data = ["quick brown fox.", "slow black bear."]
-        tokenizer = BytePairTokenizer(
-            vocabulary=VOCAB_PATH, merges=MERGE_PATH, dtype=tf.string
-        )
-        call_output = tokenizer(input_data)
-        expected = tf.ragged.constant(
-            [
-                ["quick", "brown", "fox", "."],
-                ["slow", "black", "bear", "."],
-            ]
-        )
-        self.assertAllEqual(call_output, expected)
-
-    def test_tokenize_with_special_tokens(self):
-        vocab = {"sp": 0, "s": 1, "p": 2}
-        merges = ["s p"]
-        tokenizer = BytePairTokenizer(
-            vocabulary=vocab,
-            merges=merges,
-            unsplittable_tokens=["s", "p"],
-        )
-        output = tokenizer("sp")
-        self.assertAllEqual(output, [1, 2])
-
-        # If not setting special tokens, "sp" is one token.
-        tokenizer = BytePairTokenizer(
-            vocabulary=vocab,
-            merges=merges,
-        )
-        output = tokenizer("sp")
-        self.assertEqual(output, [0])
-
-    def test_tokenize_prefix_space(self):
-        input_data = ["brown.", "black."]
-        tokenizer = BytePairTokenizer(
-            vocabulary=VOCAB_PATH,
-            merges=MERGE_PATH,
-            dtype=tf.string,
-            add_prefix_space=True,
-        )
-        call_output = tokenizer(input_data)
-
-        expected = tf.ragged.constant(
-            [
-                ["brown", "."],
-                ["black", "."],
-            ]
-        )
-        self.assertAllEqual(call_output, expected)
-
-    def test_tokenize_scalar_input(self):
-        input_data = "brown."
-        encoded = self.tokenizer.tokenize(input_data)
-        self.assertAllEqual(encoded, [31876, 4])
-
-    def test_detokenize_scalar_input(self):
-        input_data = ["quick brown fox."]
-        encoded = self.tokenizer.tokenize(input_data)
-        decoded = self.tokenizer.detokenize(encoded)
-        self.assertAllEqual(input_data, decoded)
-
-    def test_detokenize_list_input(self):
-        input_data = ["quick brown fox.", "slow black bear."]
-        encoded = self.tokenizer.tokenize(input_data)
-        decoded = self.tokenizer.detokenize(encoded)
-        self.assertAllEqual(input_data, decoded)
-
-    def test_whitespace_split(self):
-        input_data = "\n\n\n  s"
-        encoded = self.tokenizer(input_data)
-        self.assertAllEqual(encoded, [50140, 50118, 1437, 579])
-
-        input_data = "  \n\n\ns"
-        encoded = self.tokenizer(input_data)
-        self.assertAllEqual(encoded, [1437, 1437, 50140, 50118, 29])
-
-    def test_special_whitespace(self):
-        input_data = "\xa0 \xa0 \x3000 s"
-        encoded = self.tokenizer(input_data)
-        self.assertAllEqual(encoded, [50141, 50143, 12096, 579])
-
-    def test_cjk_input(self):
-        input_data = "Q"
-        # Black formats long list by one element per line, which is bad to read.
-        expected = [36714, 20024, 21402, 37127, 27, 20024, 48945, 47918]
-        expected += [47780, 43251, 4394, 10172, 36484, 27969, 12410, 37127]
-        expected += [10965, 10674, 1864, 42393, 15722, 18164, 43251, 10809]
-        expected += [17772]
-        encoded = self.tokenizer(input_data)
-        self.assertAllEqual(encoded, expected)
-
-    def test_tokenize_with_tf_data(self):
-        data = [
-            "I am just a test string",
-            "I am also a test string",
-            "I am still a test string",
-            "me too",
-            "I am not a test string (joking)",
-            "You guys should add punctuation!",
-            "Period matters!",
-        ]
-        ds = tf.data.Dataset.from_tensor_slices(data)
-        ds = ds.batch(2).map(self.tokenizer)
-        encoded = next(iter(ds))
-        expected = tf.ragged.constant(
-            [[100, 524, 95, 10, 1296, 6755], [100, 524, 67, 10, 1296, 6755]]
-        )
-        self.assertAllEqual(encoded, expected)
-
-    def test_config(self):
-        input_data = ["the quick brown whale."]
-        cloned_tokenizer = BytePairTokenizer.from_config(
-            self.tokenizer.get_config()
-        )
-        self.assertAllEqual(
-            self.tokenizer(input_data),
-            cloned_tokenizer(input_data),
-        )
-
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
-        input_data = tf.constant(["the quick brown whale."])
-        tokenizer = self.tokenizer
-        inputs = keras.Input(dtype="string", shape=())
-        outputs = tokenizer(inputs)
+        self.num_mels = 80
+        self.num_fft_bins = 400
+        self.stride = 100
+        self.sampling_rate = 100
+        self.max_audio_length = 5
+        self.output_length = (
+            self.max_audio_length * self.sampling_rate
+        ) // self.stride
+        self.audio_feature_extractor = WhisperAudioFeatureExtractor(
+            num_mels=self.num_mels,
+            num_fft_bins=self.num_fft_bins,
+            stride=self.stride,
+            sampling_rate=self.sampling_rate,
+            max_audio_length=self.max_audio_length,
+        )
+
+        self.vocab = {
+            "air": 0,
+            "plane": 1,
+            "at": 2,
+            "port": 3,
+            "koh": 4,
+            "li": 5,
+            "is": 6,
+            "the": 7,
+            "best": 8,
+        }
+
+        merges = [" a", " t", " k", " i", " b", "a i", "p l", "n e"]
+        merges += ["a t", "p o", "r t", "o h", "l i", "i s", "b e", "s t"]
+        merges += ["t h", "ai r", "pl a", "k oh", "th e", "be st", "po rt"]
+        merges += ["pla ne"]
+        self.merges = merges
+
+        self.special_tokens = {
+            "<|startoftranscript|>": 9,
+            "<|endoftext|>": 10,
+            "<|notimestamps|>": 11,
+            "<|transcribe|>": 12,
+            "<|translate|>": 13,
+        }
+
+        self.language_tokens = {
+            "<|en|>": 14,
+            "<|fr|>": 15,
+        }
+
+        self.tokenizer = WhisperTokenizer(
+            vocabulary=self.vocab,
+            merges=self.merges,
+            special_tokens=self.special_tokens,
+            language_tokens=self.language_tokens,
+        )
+
+        self.preprocessor = WhisperPreprocessor(
+            audio_feature_extractor=self.audio_feature_extractor,
+            tokenizer=self.tokenizer,
+            decoder_sequence_length=12,
+            language="<|en|>",
+            task="translate",
+        )
+
+    def test_unbatched_preprocess(self):
+        input_data = {
+            "encoder_audio": tf.ones((200,)),
+            "decoder_text": tf.constant(" airplane at airport"),
+        }
+
+        x = self.preprocessor(input_data)
+        self.assertAllEqual(
+            x["encoder_features"].shape, [self.output_length, self.num_mels]
+        )
+        self.assertAllEqual(
+            x["decoder_token_ids"], [9, 14, 13, 11, 0, 1, 2, 0, 3, 10, 10, 10]
+        )
+        self.assertAllEqual(
+            x["decoder_padding_mask"], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]
+        )
+
+    def test_preprocess_batch(self):
+        input_data = {
+            "encoder_audio": tf.ones((4, 200)),
+            "decoder_text": tf.constant([" airplane at airport"] * 4),
+        }
+
+        x = self.preprocessor(input_data)
+        self.assertAllEqual(
+            x["encoder_features"].shape, [4, self.output_length, self.num_mels]
+        )
+        self.assertAllEqual(
+            x["decoder_token_ids"],
+            [[9, 14, 13, 11, 0, 1, 2, 0, 3, 10, 10, 10]] * 4,
+        )
+        self.assertAllEqual(
+            x["decoder_padding_mask"],
+            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 4,
+        )
+
+    def test_preprocess_labeled_batch(self):
+        x = {
+            "encoder_audio": tf.ones((4, 200)),
+            "decoder_text": tf.constant([" airplane at airport"] * 4),
+        }
+        y_in = tf.constant([1] * 4)
+        sw_in = tf.constant([1.0] * 4)
+        x, y, sw = self.preprocessor(x, y_in, sw_in)
+        self.assertAllEqual(
+            x["encoder_features"].shape, [4, self.output_length, self.num_mels]
+        )
+        self.assertAllEqual(
+            x["decoder_token_ids"],
+            [[9, 14, 13, 11, 0, 1, 2, 0, 3, 10, 10, 10]] * 4,
+        )
+        self.assertAllEqual(
+            x["decoder_padding_mask"],
+            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 4,
+        )
+        self.assertAllEqual(y, y_in)
+        self.assertAllEqual(sw, sw_in)
+
+    def test_preprocess_dataset(self):
+        x = {
+            "encoder_audio": tf.ones((4, 200)),
+            "decoder_text": tf.constant([" airplane at airport"] * 4),
+        }
+        ds = tf.data.Dataset.from_tensor_slices(x)
+        ds = ds.map(self.preprocessor)
+        x = ds.batch(4).take(1).get_single_element()
+        self.assertAllEqual(
+            x["encoder_features"].shape, [4, self.output_length, self.num_mels]
+        )
+        self.assertAllEqual(
+            x["decoder_token_ids"],
+            [[9, 14, 13, 11, 0, 1, 2, 0, 3, 10, 10, 10]] * 4,
+        )
+        self.assertAllEqual(
+            x["decoder_padding_mask"],
+            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 4,
+        )
+
+    def test_sequence_length_override(self):
+        input_data = {
+            "encoder_audio": tf.ones((200,)),
+            "decoder_text": tf.constant(" airplane at airport"),
+        }
+        x = self.preprocessor(input_data, decoder_sequence_length=6)
+        self.assertAllEqual(x["decoder_token_ids"], [9, 14, 13, 11, 0, 10])
+
+    def test_serialization(self):
+        config = keras.saving.serialize_keras_object(self.preprocessor)
+        new_preprocessor = keras.saving.deserialize_keras_object(config)
+        self.assertEqual(
+            new_preprocessor.get_config(),
+            self.preprocessor.get_config(),
+        )
+
+    @pytest.mark.tf_only
+    @pytest.mark.large
+    def test_saved_model(self):
+        input_data = {
+            "encoder_audio": tf.ones((1, 200)),
+            "decoder_text": tf.constant([" airplane at airport"]),
+        }
+
+        inputs = {
+            "encoder_audio": keras.Input(
+                dtype="float32", shape=(None,), name="encoder_audio"
+            ),
+            "decoder_text": keras.Input(
+                dtype="string", shape=(), name="decoder_text"
+            ),
+        }
+        outputs = self.preprocessor(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
+
         restored_model = keras.models.load_model(path)
+
+        model_output = model(input_data)
+        restored_model_output = restored_model(input_data)
+
+        self.assertAllEqual(
+            model_output["encoder_features"],
+            restored_model_output["encoder_features"],
+        )
         self.assertAllEqual(
-            model(input_data),
-            restored_model(input_data),
+            model_output["decoder_token_ids"],
+            restored_model_output["decoder_token_ids"],
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_tokenizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,15 +15,17 @@
 """Byte Tokenizer."""
 
 import numpy as np
 import tensorflow as tf
 
 from keras_nlp.src.api_export import keras_nlp_export
 from keras_nlp.src.tokenizers import tokenizer
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
 
 try:
     import tensorflow_text as tf_text
 except ImportError:
     tf_text = None
 
 
@@ -47,15 +49,15 @@
     If input is a scalar string:
     There are two cases here. If `sequence_length` is set, the output will be
     a dense `tf.Tensor` of shape `[sequence_length]`. Otherwise, the output will
     be a dense `tf.Tensor` of shape `[None]`.
 
     The output dtype can be controlled via the
     `dtype` argument, which should be an integer type
-    (tf.int16, tf.int32, etc.).
+    ("int16", "int32", etc.).
 
     Args:
         lowercase: boolean. If True, the input text will be converted to
             lowercase before tokenization.
         sequence_length: int. If set, the output will be converted to a dense
             tensor and padded/trimmed so all outputs are of sequence_length.
         normalization_form: string. One of the following values: (None, "NFC",
@@ -66,47 +68,45 @@
             The value of `'strict'` will cause the operation to produce a
             `InvalidArgument` error on any invalid input formatting. A value of
             `'replace'` will cause the tokenizer to replace any invalid
             formatting in the input with the `replacement_char` codepoint.
             A value of `'ignore'` will cause the tokenizer to skip any invalid
             formatting in the input and produce no corresponding output
             character.
-        replacement_char: int. Defaults to 65533. The replacement character to
+        replacement_char: int. The replacement character to
             use when an invalid byte sequence is encountered and when `errors`
             is set to "replace" (same behaviour as
             https://www.tensorflow.org/api_docs/python/tf/strings/unicode_transcode).
+            (U+FFFD) is `65533`. Defaults to `65533`.
 
     Examples:
 
     Basic usage.
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer()
-    >>> tokenizer("hello")
-    <tf.Tensor: shape=(5,), dtype=int32, numpy=
-    array([104, 101, 108, 108, 111], dtype=int32)>
+    >>> outputs = tokenizer("hello")
+    >>> np.array(outputs)
+    array([104, 101, 108, 108, 111], dtype=int32)
 
     Ragged outputs.
-    >>> inputs = tf.constant(["hello", "hi"])
+    >>> inputs = ["hello", "hi"]
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer()
-    >>> tokenizer(inputs)
-    <tf.RaggedTensor [[104, 101, 108, 108, 111], [104, 105]]>
+    >>> seq1, seq2 = tokenizer(inputs)
+    >>> np.array(seq1)
+    array([104, 101, 108, 108, 111], dtype=int32)
+    >>> np.array(seq2)
+    array([104, 105], dtype=int32)
 
     Dense outputs.
-    >>> inputs = tf.constant(["hello", "hi"])
+    >>> inputs = ["hello", "hi"]
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer(sequence_length=8)
-    >>> tokenizer(inputs)
-    <tf.Tensor: shape=(2, 8), dtype=int32, numpy=
-    array([[104, 101, 108, 108, 111,   0,   0,   0],
-           [104, 105,   0,   0,   0,   0,   0,   0]], dtype=int32)>
-
-    Dense outputs.
-    >>> inputs = tf.constant(["hello"])
-    >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer(sequence_length=8)
-    >>> tokenizer(inputs)
-    <tf.Tensor: shape=(1, 8), dtype=int32, numpy=
-    array([[104, 101, 108, 108, 111,   0,   0,   0]], dtype=int32)>
+    >>> seq1, seq2 = tokenizer(inputs)
+    >>> np.array(seq1)
+    array([104, 101, 108, 108, 111,   0,   0,   0], dtype=int32)
+    >>> np.array(seq2)
+    array([104, 105,   0,   0,   0,   0,   0,   0], dtype=int32)
 
     Tokenize, then batch for ragged outputs.
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer()
     >>> ds = tf.data.Dataset.from_tensor_slices(["hello", "fun"])
     >>> ds = ds.map(tokenizer)
     >>> ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))
     >>> ds.take(1).get_single_element()
@@ -135,49 +135,47 @@
     >>> ds = ds.batch(2).map(tokenizer)
     >>> ds.take(1).get_single_element()
     <tf.Tensor: shape=(2, 5), dtype=int32, numpy=
     array([[104, 101, 108, 108, 111],
            [102, 117, 110,   0,   0]], dtype=int32)>
 
     Detokenization.
-    >>> inputs = tf.constant([104, 101, 108, 108, 111], dtype=tf.int32)
+    >>> inputs = [104, 101, 108, 108, 111]
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer()
-    >>> tokenizer.detokenize(inputs)
-    <tf.Tensor: shape=(), dtype=string, numpy=b'hello'>
+    >>> outputs = tokenizer.detokenize(inputs)
+    >>> np.array(outputs).astype("U")
+    array('hello', dtype='<U5')
 
     Detokenization with invalid bytes.
     >>> # The 255 below is invalid utf-8.
-    >>> inputs = tf.constant([104, 101, 255, 108, 108, 111], dtype=tf.int32)
+    >>> inputs = [104, 101, 255, 108, 108, 111]
     >>> tokenizer = keras_nlp.tokenizers.ByteTokenizer(
     ...     errors="replace", replacement_char=88)
-    >>> tokenizer.detokenize(inputs).numpy().decode('utf-8')
-    'heXllo'
+    >>> outputs = tokenizer.detokenize(inputs)
+    >>> np.array(outputs).astype("U")
+    array('heXllo', dtype='<U6')
     """
 
     def __init__(
         self,
         lowercase: bool = True,
         sequence_length: int = None,
         normalization_form: str = None,
         errors: str = "replace",
         replacement_char: int = 65533,
+        dtype="int32",
         **kwargs,
     ):
         assert_tf_text_installed(self.__class__.__name__)
 
-        # Check dtype and provide a default.
-        if "dtype" not in kwargs or kwargs["dtype"] is None:
-            kwargs["dtype"] = tf.int32
-        else:
-            dtype = tf.dtypes.as_dtype(kwargs["dtype"])
-            if not dtype.is_integer:
-                raise ValueError(
-                    "Output dtype must be an integer type. "
-                    f"Received: dtype={dtype}"
-                )
+        if not is_integer_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type. "
+                f"Received: dtype={dtype}"
+            )
 
         # Check normalization_form.
         if normalization_form not in (None, "NFC", "NFKC", "NFD", "NFKD"):
             raise ValueError(
                 '`normalization_form` must be one of None, "NFC", "NFKC", '
                 '"NFD", "NFKD". Received: normalization_form='
                 f"{normalization_form}"
@@ -186,15 +184,15 @@
         # Check errors.
         if errors not in ("strict", "replace", "ignore"):
             raise ValueError(
                 '`errors` must be one of "strict", "replace", "ignore" '
                 f"Received: errors={errors}"
             )
 
-        super().__init__(**kwargs)
+        super().__init__(dtype=dtype, **kwargs)
 
         self.lowercase = lowercase
         self.sequence_length = sequence_length
         self.normalization_form = normalization_form
         self.errors = errors
         self.replacement_char = replacement_char
 
@@ -236,31 +234,34 @@
             tokens = tokens.to_tensor(shape=output_shape)
 
         if scalar_input:
             tokens = tf.squeeze(tokens, 0)
         return tokens
 
     def detokenize(self, inputs):
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
         # Remove trailing padding tokens, so that trailing "\x00" bytes don't
         # show up in the detokenized output.
         inputs = tf.ragged.boolean_mask(inputs, tf.not_equal(inputs, 0))
 
-        decoded = tf.strings.reduce_join(
+        outputs = tf.strings.reduce_join(
             tf.gather(self._char_lst, inputs), axis=-1
         )
 
         # Handle errors if an invalid byte sequence is encountered.
-        decoded = tf.strings.unicode_transcode(
-            decoded,
+        outputs = tf.strings.unicode_transcode(
+            outputs,
             "UTF-8",
             "UTF-8",
             errors=self.errors,
             replacement_char=self.replacement_char,
         )
-        return decoded
+        if unbatched:
+            outputs = tf.squeeze(outputs, 0)
+        return outputs
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "lowercase": self.lowercase,
                 "sequence_length": self.sequence_length,
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/byte_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/byte_tokenizer_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,110 +10,106 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.byte_tokenizer import ByteTokenizer
 
 
-class ByteTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class ByteTokenizerTest(TestCase):
     def test_tokenize(self):
-        input_data = tf.constant(["hello", "fun", ""])
+        input_data = ["hello", "fun", ""]
         tokenizer = ByteTokenizer()
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.RaggedTensor)
         exp_outputs = [
             [104, 101, 108, 108, 111],
             [102, 117, 110],
             [226, 150, 128, 226, 150, 129, 226, 150, 130, 226, 150, 131],
         ]
-        for i in range(call_output.shape[0]):
-            self.assertAllEqual(call_output[i], exp_outputs[i])
-            self.assertAllEqual(tokenize_output[i], exp_outputs[i])
+        self.assertAllEqual(call_output, exp_outputs)
+        self.assertAllEqual(tokenize_output, exp_outputs)
 
     def test_tokenize_scalar(self):
         input_data = "hello"
         tokenizer = ByteTokenizer()
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
 
         self.assertAllEqual(call_output, [104, 101, 108, 108, 111])
         self.assertAllEqual(tokenize_output, [104, 101, 108, 108, 111])
 
     def test_dense_output(self):
-        input_data = tf.constant(["hello", "fun", ""])
+        input_data = ["hello", "fun", ""]
         tokenizer = ByteTokenizer(sequence_length=10)
         call_output = tokenizer(input_data)
-        self.assertIsInstance(call_output, tf.Tensor)
         self.assertAllEqual(
             call_output,
             [
                 [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],
                 [102, 117, 110, 0, 0, 0, 0, 0, 0, 0],
                 [226, 150, 128, 226, 150, 129, 226, 150, 130, 226],
             ],
         )
 
     def test_detokenize(self):
-        input_data = tf.ragged.constant(
-            [
-                [104, 101, 108, 108, 111],
-                [102, 117, 110],
-                [226, 150, 128, 226, 150, 129, 226, 150, 130, 226, 150, 131],
-            ]
-        )
+        input_data = [
+            [104, 101, 108, 108, 111],
+            [102, 117, 110],
+            [226, 150, 128, 226, 150, 129, 226, 150, 130, 226, 150, 131],
+        ]
 
         tokenizer = ByteTokenizer()
         detokenize_output = tokenizer.detokenize(input_data)
         self.assertAllEqual(detokenize_output, ["hello", "fun", ""])
 
     def test_detokenize_replace_error(self):
         # 226 is an invalid UTF-8 byte.
-        input_data = tf.ragged.constant([[104, 101, 226, 150, 108, 108, 111]])
+        input_data = [[104, 101, 226, 150, 108, 108, 111]]
 
         tokenizer = ByteTokenizer(errors="replace", replacement_char=341)
         detokenize_output = tokenizer.detokenize(input_data)
         self.assertAllEqual(detokenize_output, [b"he\xc5\x95llo"])
 
     def test_detokenize_ignore_error(self):
-        input_data = tf.ragged.constant([[104, 101, 226, 150, 108, 108, 111]])
+        input_data = [[104, 101, 226, 150, 108, 108, 111]]
 
         tokenizer = ByteTokenizer(errors="ignore")
         detokenize_output = tokenizer.detokenize(input_data)
         self.assertAllEqual(detokenize_output, [b"hello"])
 
     def test_detokenize_strict_error(self):
-        input_data = tf.ragged.constant([[104, 101, 226, 150, 108, 108, 111]])
+        input_data = [[104, 101, 226, 150, 108, 108, 111]]
 
         tokenizer = ByteTokenizer(errors="strict")
         with self.assertRaises(tf.errors.InvalidArgumentError):
             _ = tokenizer.detokenize(input_data)
 
     def test_vocab_size(self):
         tokenizer = ByteTokenizer()
         self.assertEqual(tokenizer.vocabulary_size(), 256)
 
     def test_lowercase(self):
-        input_data = tf.constant(["HeLlO wOrLd"])
+        input_data = ["HeLlO wOrLd"]
         tokenizer = ByteTokenizer()
         call_output = tokenizer(input_data)
         self.assertAllEqual(
             call_output,
             [[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]],
         )
 
     def test_skip_lowercase(self):
-        input_data = tf.constant(["HeLlO wOrLd"])
+        input_data = ["HeLlO wOrLd"]
         tokenizer = ByteTokenizer(lowercase=False)
         call_output = tokenizer(input_data)
         self.assertAllEqual(
             call_output, [[72, 101, 76, 108, 79, 32, 119, 79, 114, 76, 100]]
         )
 
     def test_tokenize_first_batch_second(self):
@@ -128,16 +124,15 @@
 
         exp_output = [
             [104, 101, 108, 108, 111],
             [102, 117, 110],
             [226, 150, 128, 226, 150, 129, 226, 150, 130, 226, 150, 131],
             [104, 97, 104, 97],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_tokenize_first_batch_second_with_sequence_length(self):
         tokenizer = ByteTokenizer(sequence_length=10)
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["hello", "fun", "", "haha"]
         )
@@ -147,16 +142,15 @@
 
         exp_output = [
             [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],
             [102, 117, 110, 0, 0, 0, 0, 0, 0, 0],
             [226, 150, 128, 226, 150, 129, 226, 150, 130, 226],
             [104, 97, 104, 97, 0, 0, 0, 0, 0, 0],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_batch_first_tokenize_second(self):
         tokenizer = ByteTokenizer()
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["hello", "fun", "", "haha"]
         )
@@ -165,16 +159,15 @@
 
         exp_output = [
             [104, 101, 108, 108, 111],
             [102, 117, 110],
             [226, 150, 128, 226, 150, 129, 226, 150, 130, 226, 150, 131],
             [104, 97, 104, 97],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_batch_first_tokenize_second_with_sequence_length(self):
         tokenizer = ByteTokenizer(sequence_length=10)
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["hello", "fun", "", "haha"]
         )
@@ -183,28 +176,28 @@
 
         exp_output = [
             [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],
             [102, 117, 110, 0, 0, 0, 0, 0, 0, 0],
             [226, 150, 128, 226, 150, 129, 226, 150, 130, 226],
             [104, 97, 104, 97, 0, 0, 0, 0, 0, 0],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
+    @pytest.mark.tf_only
     def test_functional_model(self):
         input_data = tf.constant(["hello", "fun", ""])
         tokenizer = ByteTokenizer()
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
         model = keras.Model(inputs, outputs)
         model_output = model(input_data)
         self.assertAllEqual(model_output, ["hello", "fun", ""])
 
     def test_load_model_with_config(self):
-        input_data = tf.constant(["hello"])
+        input_data = ["hello"]
 
         original_tokenizer = ByteTokenizer(
             lowercase=False,
             sequence_length=8,
             normalization_form="NFC",
             errors="ignore",
         )
@@ -239,34 +232,29 @@
             "normalization_form": "NFC",
             "replacement_char": 0,
             "sequence_length": 8,
             "trainable": True,
         }
         self.assertEqual(tokenizer.get_config(), exp_config)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["this is fun"])
 
         tokenizer = ByteTokenizer(
             name="byte_tokenizer_config_test",
             lowercase=False,
             sequence_length=20,
             normalization_form="NFKC",
             errors="replace",
         )
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,22 +14,25 @@
 
 import base64
 import binascii
 import os
 from typing import List
 
 import tensorflow as tf
-from tensorflow import keras
 
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.backend import keras
 from keras_nlp.src.tokenizers import tokenizer
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
-from keras_nlp.src.utils.tf_utils import tensor_to_string_list
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
+from keras_nlp.src.utils.tensor_utils import is_string_dtype
+from keras_nlp.src.utils.tensor_utils import tensor_to_list
 
 try:
     import tensorflow_text as tf_text
 except ImportError:
     tf_text = None
 
 
@@ -101,30 +104,26 @@
     ```
     """
 
     def __init__(
         self,
         proto,
         sequence_length: int = None,
+        dtype="int32",
         **kwargs,
     ) -> None:
         assert_tf_text_installed(self.__class__.__name__)
 
-        # Check dtype and provide a default.
-        if "dtype" not in kwargs or kwargs["dtype"] is None:
-            kwargs["dtype"] = tf.int32
-        else:
-            dtype = tf.dtypes.as_dtype(kwargs["dtype"])
-            if not dtype.is_integer and dtype != tf.string:
-                raise ValueError(
-                    "Output dtype must be one of `'string'`, `'int32'`, and "
-                    f"`'int64'`. Received: dtype={dtype}"
-                )
+        if not is_integer_dtype(dtype) and not is_string_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type or a string. "
+                f"Received: dtype={dtype}"
+            )
 
-        super().__init__(**kwargs)
+        super().__init__(dtype=dtype, **kwargs)
 
         if isinstance(proto, str):
             # A string could be either a filepath, or a base64 encoded byte
             # array (which we need for serialization). We will heuristically
             # try to distinguish, by checking if a string is both longer and
             # than 2048 characters and valid base64 characters.
             is_base64 = False
@@ -157,23 +156,28 @@
 
     def vocabulary_size(self) -> int:
         """Get the size of the tokenizer vocabulary."""
         return int(self._sentence_piece.vocab_size().numpy())
 
     def get_vocabulary(self) -> List[str]:
         """Get the tokenizer vocabulary."""
-        return tensor_to_string_list(
+        return tensor_to_list(
             self._sentence_piece.id_to_string(
                 tf.range(int(self._sentence_piece.vocab_size().numpy()))
             )
         )
 
     def id_to_token(self, id: int) -> str:
         """Convert an integer id to a string token."""
-        return tensor_to_string_list(self._sentence_piece.id_to_string(id))
+        if id >= self.vocabulary_size() or id < 0:
+            raise ValueError(
+                f"`id` must be in range [0, {self.vocabulary_size() - 1}]. "
+                f"Received: {id}"
+            )
+        return tensor_to_list(self._sentence_piece.id_to_string(id))
 
     def token_to_id(self, token: str) -> int:
         """Convert a string token to an integer id."""
         return int(self._sentence_piece.string_to_id(token).numpy())
 
     def get_config(self):
         config = super().get_config()
@@ -207,15 +211,19 @@
         if scalar_input:
             tokens = tf.squeeze(tokens, 0)
             tf.ensure_shape(tokens, shape=[self.sequence_length])
 
         return tokens
 
     def detokenize(self, inputs):
-        return self._sentence_piece.detokenize(inputs)
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
+        outputs = self._sentence_piece.detokenize(inputs)
+        if unbatched:
+            outputs = tf.squeeze(outputs, 0)
+        return outputs
 
     @classproperty
     def presets(cls):
         return {}
 
     @classmethod
     def from_preset(
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,23 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import io
 import os
 
+import pytest
 import sentencepiece
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.sentence_piece_tokenizer import SentencePieceTokenizer
 
 
-class SentencePieceTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class SentencePieceTokenizerTest(TestCase):
     def setUp(self):
         super().setUp()
         bytes_io = io.BytesIO()
         vocab_data = tf.data.Dataset.from_tensor_slices(
             ["the quick brown fox."]
         )
         sentencepiece.SentencePieceTrainer.train(
@@ -41,58 +42,54 @@
     def test_tokenize(self):
         input_data = ["the quick brown fox."]
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
         )
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.RaggedTensor)
         self.assertAllEqual(call_output, [[6, 5, 3, 4]])
         self.assertAllEqual(tokenize_output, [[6, 5, 3, 4]])
 
     def test_scalar_tokenize(self):
         input_data = "the quick brown fox."
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
         )
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.Tensor)
         self.assertAllEqual(call_output, [6, 5, 3, 4])
         self.assertAllEqual(tokenize_output, [6, 5, 3, 4])
 
     def test_dense_output(self):
         input_data = ["the quick brown fox."]
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
             sequence_length=10,
         )
         output_data = tokenizer(input_data)
-        self.assertIsInstance(output_data, tf.Tensor)
         self.assertAllEqual(output_data, [[6, 5, 3, 4, 0, 0, 0, 0, 0, 0]])
 
     def test_string_tokenize(self):
         input_data = ["the quick brown fox."]
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
             dtype="string",
         )
         output_data = tokenizer(input_data)
         self.assertAllEqual(
             output_data,
-            tf.ragged.constant([["the", "quick", "brown", "fox."]]),
+            [["the", "quick", "brown", "fox."]],
         )
 
     def test_detokenize(self):
-        input_data = [[6, 5, 3, 4]]
-        tokenizer = SentencePieceTokenizer(
-            proto=self.proto,
-        )
-        output_data = tokenizer.detokenize(input_data)
-        self.assertAllEqual(output_data, ["the quick brown fox."])
+        tokenizer = SentencePieceTokenizer(proto=self.proto)
+        outputs = tokenizer.detokenize([6, 5, 3, 4])
+        self.assertAllEqual(outputs, "the quick brown fox.")
+        outputs = tokenizer.detokenize([[6, 5, 3, 4], [6, 4]])
+        self.assertAllEqual(outputs, ["the quick brown fox.", "the fox."])
 
     def test_accessors(self):
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
         )
         self.assertEqual(
             tokenizer.get_vocabulary(),
@@ -104,14 +101,24 @@
         self.assertEqual(tokenizer.id_to_token(0), "<unk>")
         self.assertEqual(tokenizer.id_to_token(5), "quick")
         self.assertEqual(type(tokenizer.id_to_token(0)), str)
         self.assertEqual(tokenizer.token_to_id("<unk>"), 0)
         self.assertEqual(tokenizer.token_to_id("quick"), 5)
         self.assertEqual(type(tokenizer.token_to_id("<unk>")), int)
 
+    def test_error_id_out_of_vocabulary(self):
+        tokenizer = SentencePieceTokenizer(
+            proto=self.proto,
+        )
+        with self.assertRaises(ValueError):
+            tokenizer.id_to_token(tokenizer.vocabulary_size())
+        with self.assertRaises(ValueError):
+            tokenizer.id_to_token(-1)
+
+    @pytest.mark.tf_only
     def test_functional_model(self):
         input_data = tf.constant(["the quick brown fox."])
         tokenizer = SentencePieceTokenizer(
             proto=self.proto,
         )
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
@@ -181,32 +188,27 @@
             original_tokenizer.get_config()
         )
         self.assertAllEqual(
             original_tokenizer(input_data),
             cloned_tokenizer(input_data),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         filepath = os.path.join(self.get_temp_dir(), "model.txt")
         input_data = tf.constant(["the quick brown whale."])
         with tf.io.gfile.GFile(filepath, "wb") as file:
             file.write(self.proto)
         tokenizer = SentencePieceTokenizer(
             proto=filepath,
         )
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/sentence_piece_tokenizer_trainer_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,21 +14,22 @@
 """Tests for SentencePiece Tokenizer Trainer."""
 
 import os
 import re
 
 import tensorflow as tf
 
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.sentence_piece_tokenizer import SentencePieceTokenizer
 from keras_nlp.src.tokenizers.sentence_piece_tokenizer_trainer import (
     compute_sentence_piece_proto,
 )
 
 
-class SentencePieceTokenizerTrainerTest(tf.test.TestCase):
+class SentencePieceTokenizerTrainerTest(TestCase):
     def test_dataset_input(self):
         test_text = ["Ninjas and Samurais"]
         expected_output = [
             [5, 9, 6, 7, 11, 4, 8, 5, 4, 7, 10, 5, 12, 4, 13, 15, 14, 4, 6, 8]
         ]
         data = tf.data.Dataset.from_tensor_slices(test_text)
         proto = compute_sentence_piece_proto(data, 16)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/tokenizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,21 +10,22 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List
 
-from tensorflow import keras
-
 from keras_nlp.src.api_export import keras_nlp_export
+from keras_nlp.src.layers.preprocessing.preprocessing_layer import (
+    PreprocessingLayer,
+)
 
 
 @keras_nlp_export("keras_nlp.tokenizers.Tokenizer")
-class Tokenizer(keras.layers.Layer):
+class Tokenizer(PreprocessingLayer):
     """A base class for tokenizer layers.
 
     Tokenizers in the KerasNLP library should all subclass this layer.
     The class provides two core methods `tokenize()` and `detokenize()` for
     going from plain text to sequences and back. A tokenizer is a subclass of
     `keras.layers.Layer` and can be combined into a `keras.Model`.
 
@@ -58,14 +59,17 @@
     tokenizer("This is a test")
 
     # Detokenize some outputs.
     tokenizer.detokenize(["This", "is", "a", "test"])
     ```
     """
 
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
     def __new__(cls, *args, **kwargs):
         # Wrap the `tokenize` and `detokenize` methods so they route through
         # __call__. This is needed for functional model support.
         obj = super().__new__(cls, *args, **kwargs)
         obj._tokenize_without_call = obj.tokenize
         obj._detokenize_without_call = obj.detokenize
         obj.tokenize = obj._tokenize_with_call
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/tokenizer_test.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,45 +8,48 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.tokenizer import Tokenizer
 
 
 class SimpleTokenizer(Tokenizer):
     __test__ = False  # for pytest
 
     def tokenize(self, inputs):
         return tf.strings.split(inputs).to_tensor()
 
     def detokenize(self, inputs):
         return tf.strings.reduce_join([inputs], separator=" ", axis=-1)
 
 
-class TokenizerTest(tf.test.TestCase):
+class TokenizerTest(TestCase):
     def test_tokenize(self):
         input_data = ["the quick brown fox"]
         tokenizer = SimpleTokenizer()
         tokenize_output = tokenizer.tokenize(input_data)
         call_output = tokenizer(input_data)
         self.assertAllEqual(tokenize_output, [["the", "quick", "brown", "fox"]])
         self.assertAllEqual(call_output, [["the", "quick", "brown", "fox"]])
 
     def test_detokenize(self):
         input_data = ["the", "quick", "brown", "fox"]
         tokenizer = SimpleTokenizer()
         detokenize_output = tokenizer.detokenize(input_data)
         self.assertAllEqual(detokenize_output, ["the quick brown fox"])
 
+    @pytest.mark.tf_only
     def test_functional_model(self):
         input_data = tf.constant(["the   quick   brown   fox"])
         tokenizer = SimpleTokenizer()
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
         model = keras.Model(inputs, outputs)
         model_output = model(input_data)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,15 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
 
 from keras_nlp.src.api_export import keras_nlp_export
 from keras_nlp.src.tokenizers import tokenizer
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import convert_to_ragged_batch
+from keras_nlp.src.utils.tensor_utils import is_integer_dtype
 
 try:
     import tensorflow_text as tf_text
 except ImportError:
     tf_text = None
 
 
@@ -43,18 +45,18 @@
 
     If input is a scalar string (rank == 0):
     By default, the layer will output a dense `tf.Tensor` with static shape
     `[None]`. If `sequence_length` is set, the output will be
     a dense `tf.Tensor` of shape `[sequence_length]`.
 
     The output dtype can be controlled via the `dtype` argument, which should be
-    an integer type (tf.int16, tf.int32, etc.).
+    an integer type ("int16", "int32", etc.).
 
     Args:
-        lowercase: If true, the input text will be first lowered before
+        lowercase: If `True`, the input text will be first lowered before
             tokenization.
         sequence_length: If set, the output will be converted to a dense
             tensor and padded/trimmed so all outputs are of sequence_length.
         normalization_form: One of the following string values (None, 'NFC',
             'NFKC', 'NFD', 'NFKD'). If set will normalize unicode to the given
             form before tokenizing.
         errors: One of ('replace', 'remove', 'strict'). Specifies the
@@ -63,51 +65,52 @@
             `InvalidArgument` error on any invalid input formatting. A value of
             `'replace'` will cause the tokenizer to replace any invalid
             formatting in the input with the replacement_char codepoint.
             A value of `'ignore'` will cause the tokenizer to skip any invalid
             formatting in the input and produce no corresponding output
             character.
         replacement_char: The unicode codepoint to use in place of invalid
-            codepoints. Defaults to 65533 (U+FFFD).
+            codepoints. (U+FFFD) is `65533`. Defaults to `65533`.
         input_encoding: One of ("UTF-8", "UTF-16-BE", or "UTF-32-BE").
-            One of The encoding of the input text. Defaults to "UTF-8".
+            One of The encoding of the input text. Defaults to `"UTF-8"`.
         output_encoding: One of ("UTF-8", "UTF-16-BE", or "UTF-32-BE").
-            The encoding of the output text. Defaults to "UTF-8".
+            The encoding of the output text. Defaults to `"UTF-8"`.
         vocabulary_size: Set the vocabulary `vocabulary_size`,
             by clamping all codepoints to the range [0, vocabulary_size).
             Effectively this will make the `vocabulary_size - 1` id the
             the OOV value.
 
     Examples:
 
     Basic Usage.
     >>> inputs = "Unicode Tokenizer"
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer()
-    >>> tokenizer(inputs)
-    <tf.Tensor: shape=(17,), dtype=int32, numpy=
+    >>> outputs = tokenizer(inputs)
+    >>> np.array(outputs)
     array([117, 110, 105,  99, 111, 100, 101,  32, 116, 111, 107, 101, 110,
-        105, 122, 101, 114], dtype=int32)>
+        105, 122, 101, 114], dtype=int32)
 
     Ragged outputs.
-    >>> inputs = ["Book", "", ""]
+    >>> inputs = ["", ""]
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer()
-    >>> tokenizer(inputs)
-    <tf.RaggedTensor [[98, 111, 111, 107],
-        [2346, 2369, 2360, 2381, 2340, 2325],
-        [1705, 1578, 1575, 1576]]>
+    >>> seq1, seq2 = tokenizer(inputs)
+    >>> np.array(seq1)
+    array([2346, 2369, 2360, 2381, 2340, 2325], dtype=int32)
+    >>> np.array(seq2)
+    array([1705, 1578, 1575, 1576], dtype=int32)
 
     Dense outputs.
-    >>> inputs = ["Book", "", ""]
+    >>> inputs = ["", ""]
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer(
     ...     sequence_length=8)
-    >>> tokenizer(inputs)
-    <tf.Tensor: shape=(3, 8), dtype=int32, numpy=
-    array([[  98,  111,  111,  107,    0,    0,    0,    0],
-        [2346, 2369, 2360, 2381, 2340, 2325,    0,    0],
-        [1705, 1578, 1575, 1576,    0,    0,    0,    0]], dtype=int32)>
+    >>> seq1, seq2 = tokenizer(inputs)
+    >>> np.array(seq1)
+    array([2346, 2369, 2360, 2381, 2340, 2325,    0,    0], dtype=int32)
+    >>> np.array(seq2)
+    array([1705, 1578, 1575, 1576,    0,    0,    0,    0], dtype=int32)
 
     Tokenize, then batch for ragged outputs.
     >>> inputs = ["Book", "", ""]
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer()
     >>> ds = tf.data.Dataset.from_tensor_slices(inputs)
     >>> ds = ds.map(tokenizer)
     >>> ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(3))
@@ -151,37 +154,38 @@
         [2346, 2369, 2360, 2381, 2340],
         [1705, 1578, 1575, 1576,    0]], dtype=int32)>
 
     Tokenization with truncation.
     >>> inputs = ["I Like to Travel a Lot", "     "]
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer(
     ...     sequence_length=5)
-    >>> tokenizer(inputs)
-    <tf.Tensor: shape=(5,), dtype=int32,
-        numpy=array([[ 105,   32,  108,  105,  107],
-       [2350, 2376, 2306,   32, 2325]], dtype=int32)>
+    >>> outputs = tokenizer(inputs)
+    >>> np.array(outputs)
+    array([[ 105,   32,  108,  105,  107],
+           [2350, 2376, 2306,   32, 2325]], dtype=int32)
 
     Tokenization with vocabulary_size.
     >>> latin_ext_cutoff = 592
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer(
     ...     vocabulary_size=latin_ext_cutoff)
-    >>> tokenizer("Cmo ests?")
-    <tf.Tensor: shape=(10,), dtype=int32,
-    numpy=array([191,  99, 243, 109, 111,  32, 101, 115, 116, 225, 115,  63],
-    dtype=int32)>
-    >>> tokenizer("  ")
-    <tf.Tensor: shape=(11,), dtype=int32,
-    numpy=array([591, 591,  32, 591, 591, 591, 591,  32, 591, 591, 591],
-    dtype=int32)>
+    >>> outputs = tokenizer("Cmo ests?")
+    >>> np.array(outputs)
+    array([191,  99, 243, 109, 111,  32, 101, 115, 116, 225, 115,  63],
+          dtype=int32)
+    >>> outputs = tokenizer("  ")
+    >>> np.array(outputs)
+    array([591, 591,  32, 591, 591, 591, 591,  32, 591, 591, 591],
+          dtype=int32)
 
     Detokenization.
-    >>> inputs = tf.constant([110, 105, 110, 106,  97], dtype=tf.int32)
+    >>> inputs = tf.constant([110, 105, 110, 106,  97], dtype="int32")
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer()
-    >>> tokenizer.detokenize(inputs)
-    <tf.Tensor: shape=(), dtype=string, numpy=b'ninja'>
+    >>> outputs = tokenizer.detokenize(inputs)
+    >>> np.array(outputs).astype("U")
+    array('ninja', dtype='<U5')
 
     Detokenization with padding.
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer(
     ...     sequence_length=7)
     >>> dataset = tf.data.Dataset.from_tensor_slices(["a b c", "b c", "a"])
     >>> dataset = dataset.map(tokenizer)
     >>> dataset.take(1).get_single_element()
@@ -191,42 +195,39 @@
     >>> detokunbatched.take(1).get_single_element()
     <tf.Tensor: shape=(), dtype=string, numpy=b'a b c'>
 
     Detokenization with invalid bytes.
     >>> inputs = tf.constant([110, 105, 10000000, 110, 106,  97])
     >>> tokenizer = keras_nlp.tokenizers.UnicodeCodepointTokenizer(
     ...     errors="replace", replacement_char=88)
-    >>> tokenizer.detokenize(inputs).numpy().decode('utf-8')
-    'niXnja'
+    >>> outputs = tokenizer.detokenize(inputs)
+    >>> np.array(outputs).astype("U")
+    array('niXnja', dtype='<U6')
     """
 
     def __init__(
         self,
         sequence_length: int = None,
         lowercase: bool = True,
         normalization_form: str = None,
         errors: str = "replace",
         replacement_char: int = 65533,
         input_encoding: str = "UTF-8",
         output_encoding: str = "UTF-8",
         vocabulary_size: int = None,
+        dtype="int32",
         **kwargs,
     ) -> None:
         assert_tf_text_installed(self.__class__.__name__)
 
-        # Check dtype and provide a default.
-        if "dtype" not in kwargs or kwargs["dtype"] is None:
-            kwargs["dtype"] = tf.int32
-        else:
-            dtype = tf.dtypes.as_dtype(kwargs["dtype"])
-            if not dtype.is_integer:
-                raise ValueError(
-                    "Output dtype must be an integer type. "
-                    f"Received: dtype={dtype}"
-                )
+        if not is_integer_dtype(dtype):
+            raise ValueError(
+                "Output dtype must be an integer type. "
+                f"Received: dtype={dtype}"
+            )
 
         # Check normalization_form.
         if normalization_form not in [None, "NFC", "NFKC", "NFD", "NFKD"]:
             raise ValueError(
                 '`normalization_form` must be one of None, "NFC", "NFKC", '
                 '"NFD", "NFKD". Received: normalization_form='
                 f"{normalization_form}"
@@ -243,15 +244,15 @@
         if normalization_form:
             if input_encoding != "UTF-8":
                 raise ValueError(
                     """Normalization Forms are Only Supported for Input Encoding
                      UTF-8"""
                 )
 
-        super().__init__(**kwargs)
+        super().__init__(dtype=dtype, **kwargs)
 
         self.sequence_length = sequence_length
         self.lowercase = lowercase
         self.normalization_form = normalization_form
         self.errors = errors
         self.replacement_char = replacement_char
         self.input_encoding = input_encoding
@@ -315,16 +316,19 @@
         # range [0, vocabulary_size)
         if self._vocabulary_size:
             tokens = tf.clip_by_value(tokens, 0, self._vocabulary_size - 1)
 
         return tokens
 
     def detokenize(self, inputs):
+        inputs, unbatched, _ = convert_to_ragged_batch(inputs)
         inputs = tf.ragged.boolean_mask(inputs, tf.not_equal(inputs, 0))
-        encoded_string = tf.strings.unicode_encode(
+        outputs = tf.strings.unicode_encode(
             inputs,
             errors=self.errors,
             replacement_char=self.replacement_char,
             output_encoding=self.output_encoding,
         )
-        return encoded_string
+        if unbatched:
+            outputs = tf.squeeze(outputs, 0)
+        return outputs
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/unicode_codepoint_tokenizer_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,53 +10,51 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.unicode_codepoint_tokenizer import (
     UnicodeCodepointTokenizer,
 )
 
 
-class UnicodeCodepointTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class UnicodeCodepointTokenizerTest(TestCase):
     def test_tokenize(self):
-        input_data = tf.constant(["ninja", "samurai", ""])
+        input_data = ["ninja", "samurai", ""]
         tokenizer = UnicodeCodepointTokenizer()
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.RaggedTensor)
         exp_outputs = [
             [110, 105, 110, 106, 97],
             [115, 97, 109, 117, 114, 97, 105],
             [9600, 9601, 9602, 9603],
         ]
-        for i in range(call_output.shape[0]):
-            self.assertAllEqual(call_output[i], exp_outputs[i])
-            self.assertAllEqual(tokenize_output[i], exp_outputs[i])
+        self.assertAllEqual(call_output, exp_outputs)
+        self.assertAllEqual(tokenize_output, exp_outputs)
 
     def test_tokenize_scalar(self):
         input_data = "ninja"
         tokenizer = UnicodeCodepointTokenizer()
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
 
         self.assertAllEqual(call_output, [110, 105, 110, 106, 97])
         self.assertAllEqual(tokenize_output, [110, 105, 110, 106, 97])
 
     def test_dense_output(self):
-        input_data = tf.constant(["ninja", "samurai", ""])
+        input_data = ["ninja", "samurai", ""]
         tokenizer = UnicodeCodepointTokenizer(sequence_length=10)
         call_output = tokenizer(input_data)
-        self.assertIsInstance(call_output, tf.Tensor)
         self.assertAllEqual(
             call_output,
             [
                 [110, 105, 110, 106, 97, 0, 0, 0, 0, 0],
                 [115, 97, 109, 117, 114, 97, 105, 0, 0, 0],
                 [9600, 9601, 9602, 9603, 0, 0, 0, 0, 0, 0],
             ],
@@ -68,52 +66,47 @@
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
 
         self.assertAllEqual(call_output, [104, 104, 104, 104, 97])
         self.assertAllEqual(tokenize_output, [104, 104, 104, 104, 97])
 
     def test_tokenize_dense_with_vocabulary_size(self):
-        input_data = tf.constant(["ninja", "samurai", ""])
+        input_data = ["ninja", "samurai", ""]
         tokenizer = UnicodeCodepointTokenizer(
             sequence_length=10, vocabulary_size=105
         )
         call_output = tokenizer(input_data)
-        self.assertIsInstance(call_output, tf.Tensor)
         self.assertAllEqual(
             call_output,
             [
                 [104, 104, 104, 104, 97, 0, 0, 0, 0, 0],
                 [104, 97, 104, 104, 104, 97, 104, 0, 0, 0],
                 [104, 104, 104, 104, 0, 0, 0, 0, 0, 0],
             ],
         )
 
     def test_tokenize_ragged_with_vocabulary_size(self):
-        input_data = tf.constant(["ninja", "samurai", ""])
+        input_data = ["ninja", "samurai", ""]
         tokenizer = UnicodeCodepointTokenizer(vocabulary_size=105)
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.RaggedTensor)
         exp_outputs = [
             [104, 104, 104, 104, 97],
             [104, 97, 104, 104, 104, 97, 104],
             [104, 104, 104, 104],
         ]
-        for i in range(call_output.shape[0]):
-            self.assertAllEqual(call_output[i], exp_outputs[i])
-            self.assertAllEqual(tokenize_output[i], exp_outputs[i])
+        self.assertAllEqual(call_output, exp_outputs)
+        self.assertAllEqual(tokenize_output, exp_outputs)
 
     def test_detokenize(self):
-        input_data = tf.ragged.constant(
-            [
-                [110, 105, 110, 106, 97],
-                [115, 97, 109, 117, 114, 97, 105],
-                [9600, 9601, 9602, 9603],
-            ]
-        )
+        input_data = [
+            [110, 105, 110, 106, 97],
+            [115, 97, 109, 117, 114, 97, 105],
+            [9600, 9601, 9602, 9603],
+        ]
 
         tokenizer = UnicodeCodepointTokenizer()
         detokenize_output = tokenizer.detokenize(input_data)
         self.assertAllEqual(
             detokenize_output,
             [
                 b"ninja",
@@ -182,16 +175,15 @@
         exp_output = [
             [110, 105, 110, 106, 97],
             [115, 97, 109, 117, 114, 97, 105],
             [9600, 9601, 9602, 9603],
             [107, 101, 114, 97, 115],
             [116, 101, 110, 115, 111, 114, 102, 108, 111, 119],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_tokenize_first_batch_second_with_sequence_length(self):
         tokenizer = UnicodeCodepointTokenizer(sequence_length=10)
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["ninja", "samurai", "", "keras", "tensorflow"]
         )
@@ -202,16 +194,15 @@
         exp_output = [
             [110, 105, 110, 106, 97, 0, 0, 0, 0, 0],
             [115, 97, 109, 117, 114, 97, 105, 0, 0, 0],
             [9600, 9601, 9602, 9603, 0, 0, 0, 0, 0, 0],
             [107, 101, 114, 97, 115, 0, 0, 0, 0, 0],
             [116, 101, 110, 115, 111, 114, 102, 108, 111, 119],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_batch_first_tokenize_second(self):
         tokenizer = UnicodeCodepointTokenizer()
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["ninja", "samurai", "", "keras", "tensorflow"]
         )
@@ -221,16 +212,15 @@
         exp_output = [
             [110, 105, 110, 106, 97],
             [115, 97, 109, 117, 114, 97, 105],
             [9600, 9601, 9602, 9603],
             [107, 101, 114, 97, 115],
             [116, 101, 110, 115, 111, 114, 102, 108, 111, 119],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
     def test_batch_first_tokenize_second_with_sequence_length(self):
         tokenizer = UnicodeCodepointTokenizer(sequence_length=10)
 
         ds = tf.data.Dataset.from_tensor_slices(
             ["ninja", "samurai", "", "keras", "tensorflow"]
         )
@@ -240,17 +230,17 @@
         exp_output = [
             [110, 105, 110, 106, 97, 0, 0, 0, 0, 0],
             [115, 97, 109, 117, 114, 97, 105, 0, 0, 0],
             [9600, 9601, 9602, 9603, 0, 0, 0, 0, 0, 0],
             [107, 101, 114, 97, 115, 0, 0, 0, 0, 0],
             [116, 101, 110, 115, 111, 114, 102, 108, 111, 119],
         ]
-        for i in range(output.shape[0]):
-            self.assertAllEqual(output[i], exp_output[i])
+        self.assertAllEqual(output, exp_output)
 
+    @pytest.mark.tf_only
     def test_functional_model(self):
         input_data = tf.constant(
             ["ninja", "samurai", "", "keras", "tensorflow"]
         )
         tokenizer = UnicodeCodepointTokenizer()
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
@@ -340,35 +330,30 @@
             "vocabulary_size": None,
         }
         self.assertEqual(
             tokenize_different_encoding.get_config(),
             exp_config_different_encoding,
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["ninjas and samurais", "time travel"])
 
         tokenizer = UnicodeCodepointTokenizer(
             name="unicode_character_tokenizer_config_gen",
             lowercase=False,
             sequence_length=20,
             normalization_form="NFKC",
             errors="replace",
             vocabulary_size=None,
         )
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,95 +10,103 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.word_piece_tokenizer import WordPieceTokenizer
 
 
-class WordPieceTokenizerTest(tf.test.TestCase, parameterized.TestCase):
+class WordPieceTokenizerTest(TestCase):
     def test_tokenize(self):
         input_data = ["the quick brown fox."]
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data)
         call_output = tokenizer(input_data)
         tokenize_output = tokenizer.tokenize(input_data)
-        self.assertIsInstance(call_output, tf.RaggedTensor)
         self.assertAllEqual(call_output, [[1, 2, 3, 4, 5, 6, 7]])
         self.assertAllEqual(tokenize_output, [[1, 2, 3, 4, 5, 6, 7]])
 
     def test_dense_output(self):
         input_data = ["the quick brown fox."]
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
         tokenizer = WordPieceTokenizer(
             vocabulary=vocab_data, sequence_length=10
         )
         call_output = tokenizer(input_data)
-        self.assertIsInstance(call_output, tf.Tensor)
         self.assertAllEqual(call_output, [[1, 2, 3, 4, 5, 6, 7, 0, 0, 0]])
 
     def test_string_tokenize(self):
         input_data = ["the quick brown fox"]
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data, dtype="string")
         call_output = tokenizer(input_data)
         self.assertAllEqual(
             call_output,
-            tf.ragged.constant([["the", "qu", "##ick", "br", "##own", "fox"]]),
+            [["the", "qu", "##ick", "br", "##own", "fox"]],
         )
 
     def test_detokenize(self):
-        input_data = [[1, 2, 3, 4, 5, 6]]
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data)
-        detokenize_output = tokenizer.detokenize(input_data)
-        self.assertAllEqual(detokenize_output, ["the quick brown fox"])
+        outputs = tokenizer.detokenize([1, 2, 3, 4, 5, 6])
+        self.assertAllEqual(outputs, "the quick brown fox")
+        outputs = tokenizer.detokenize([[1, 2, 3, 4, 5, 6], [1, 6]])
+        self.assertAllEqual(outputs, ["the quick brown fox", "the fox"])
 
     def test_accessors(self):
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data)
         self.assertEqual(tokenizer.vocabulary_size(), 7)
         self.assertEqual(
             tokenizer.get_vocabulary(),
             ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"],
         )
         self.assertEqual(tokenizer.id_to_token(0), "[UNK]")
         self.assertEqual(tokenizer.id_to_token(6), "fox")
         self.assertEqual(tokenizer.token_to_id("[UNK]"), 0)
         self.assertEqual(tokenizer.token_to_id("fox"), 6)
 
+    def test_error_id_out_of_vocabulary(self):
+        vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
+        tokenizer = WordPieceTokenizer(vocabulary=vocab_data)
+        with self.assertRaises(ValueError):
+            tokenizer.id_to_token(tokenizer.vocabulary_size())
+        with self.assertRaises(ValueError):
+            tokenizer.id_to_token(-1)
+
     def test_special_tokens(self):
         input_data = ["quick brown whale"]
         vocab_data = ["@UNK@", "qu", "@@ick", "br", "@@own", "fox"]
         tokenizer = WordPieceTokenizer(
             vocabulary=vocab_data,
             oov_token="@UNK@",
             suffix_indicator="@@",
             dtype="string",
         )
         call_output = tokenizer(input_data)
         self.assertAllEqual(
             call_output,
-            tf.ragged.constant([["qu", "@@ick", "br", "@@own", "@UNK@"]]),
+            [["qu", "@@ick", "br", "@@own", "@UNK@"]],
         )
 
     def test_cjk_tokens(self):
         input_data = ["ahzz"]
         vocab_data = ["[UNK]", "", "", "", "", "", "", "ah", "zz"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data, dtype="string")
         call_output = tokenizer(input_data)
         self.assertAllEqual(
             call_output,
-            tf.ragged.constant([["ah", "", "", "zz"]]),
+            [["ah", "", "", "zz"]],
         )
 
     def test_lowercase(self):
         input_data = ["the QUicK brOWN FOX"]
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data, lowercase=True)
         call_output = tokenizer(input_data)
@@ -144,14 +152,15 @@
             lowercase=False,
             strip_accents=False,
             split=False,
         )
         call_output = tokenizer(input_data)
         self.assertAllEqual(call_output, [1, 2, 3, 4, 5, 6])
 
+    @pytest.mark.tf_only
     def test_functional_model(self):
         input_data = tf.constant(["the quick brown fox"])
         vocab_data = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox"]
         tokenizer = WordPieceTokenizer(vocabulary=vocab_data)
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer.detokenize(tokenizer.tokenize(inputs))
         model = keras.Model(inputs, outputs)
@@ -195,35 +204,30 @@
             original_tokenizer.get_config()
         )
         self.assertAllEqual(
             original_tokenizer(input_data),
             cloned_tokenizer(input_data),
         )
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    @pytest.mark.tf_only
+    def test_saved_model(self):
         input_data = tf.constant(["quick brOWN whale"])
         vocab_data = ["@UNK@", "qu", "@@ick", "br", "@@OWN", "fox"]
         tokenizer = WordPieceTokenizer(
             vocabulary=vocab_data,
             lowercase=False,
             oov_token="@UNK@",
             suffix_indicator="@@",
             dtype="string",
         )
         inputs = keras.Input(dtype="string", shape=())
         outputs = tokenizer(inputs)
         model = keras.Model(inputs, outputs)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(path)
         self.assertAllEqual(
             model(input_data),
             restored_model(input_data),
         )
 
     def test_no_oov_token_in_vocabulary(self):
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 """Trainer for WordPiece Tokenizer."""
 
 import tensorflow as tf
 
 from keras_nlp.src.api_export import keras_nlp_export
 from keras_nlp.src.tokenizers.word_piece_tokenizer import pretokenize
-from keras_nlp.src.utils.tf_utils import assert_tf_text_installed
+from keras_nlp.src.utils.tensor_utils import assert_tf_text_installed
 
 try:
     from tensorflow_text.tools.wordpiece_vocab import (
         wordpiece_tokenizer_learner_lib as learner,
     )
 except ImportError:
     learner = None
@@ -47,33 +47,34 @@
     `data` should be a `tf.data.Dataset`. If `data` is a list of filenames,
     the file format is required to be plain text files, and the text would be
     read in line by line during training.
 
     Args:
         data: A `tf.data.Dataset`, or a list of filenames.
         vocabulary_size: int. The maximum size of a vocabulary to be trained.
-        vocabulary_output_file: str, defaults to `None`. The location to write a
-            vocabulary file.
-        lowercase: bool, defaults to `False`. If true, the input text will be
-            lowercased before tokenization.
-        strip_accents: bool, defaults to `False`. If true, all accent marks will
-            be removed from text before tokenization.
-        split: bool, defaults to `True`. If true, input will be split on
+        vocabulary_output_file: str. The location to write a
+            vocabulary file. defaults to `None`.
+        lowercase: bool. If `True`, the input text will be
+            lowercased before tokenization. Defaults to `False`.
+        strip_accents: bool. If `True`, all accent marks will
+            be removed from text before tokenization. Defaults to `False`.
+        split: bool. If `True`, input will be split on
             whitespace and punctuation marks, and all punctuation marks will be
-            kept as tokens. If false, input should be split ("pre-tokenized")
+            kept as tokens. If `False`, input should be split ("pre-tokenized")
             before calling the tokenizer, and passed as a dense or ragged tensor
             of whole words. `split` is required to be `True` when `data` is a
-            list of filenames.
-        split_on_cjk: bool, defaults to `True`. If true, input will be split
+            list of filenames. Defaults to `True`.
+        split_on_cjk: bool. If `True`, input will be split
             on CJK characters, i.e., Chinese, Japanese, Korean and Vietnamese
             characters (https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)).
-            Note that this is applicable only when `split` is true.
-        suffix_indicator: str, defaults to "##". The characters prepended to a
+            Note that this is applicable only when `split` is `True`.
+            Defaults to `True`.
+        suffix_indicator: str. The characters prepended to a
             WordPiece to indicate that it is a suffix to another subword.
-            E.g. "##ing".
+            E.g. `"##ing"`. Defaults to `"##"`.
         reserved_tokens: list of strings. A list of tokens that must be included in the vocabulary.
 
     Returns:
         Returns a list of vocabulary terms.
 
     Examples:
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,20 +13,21 @@
 # limitations under the License.
 """Tests for WordPiece Tokenizer Trainer."""
 
 import os
 
 import tensorflow as tf
 
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.tokenizers.word_piece_tokenizer_trainer import (
     compute_word_piece_vocabulary,
 )
 
 
-class WordPieceTokenizerTrainerTest(tf.test.TestCase):
+class WordPieceTokenizerTrainerTest(TestCase):
     def test_dataset_input(self):
         test_text = ["baa maa caa saa aaa"]
         test_output = ["a", "b", "c", "m", "s", "##aa", "##a", "##b"]
         data = tf.data.Dataset.from_tensor_slices(test_text)
         vocab = compute_word_piece_vocabulary(data, 8, reserved_tokens=[])
         self.assertAllEqual(vocab, test_output)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/models/xlm_roberta/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/keras_utils.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/keras_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,22 +9,21 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-import platform
 import sys
 
 import tensorflow as tf
 from absl import logging
-from tensorflow import keras
 
-from keras_nlp.src.utils.tf_utils import is_tensor_type
+from keras_nlp.src.backend import keras
+from keras_nlp.src.utils.tensor_utils import is_tensor_type
 
 
 def clone_initializer(initializer):
     """Clones an initializer to ensure a new seed.
 
     As of tensorflow 2.10, we need to clone user passed initializers when
     invoking them twice to avoid creating the same randomized initialization.
@@ -98,27 +97,14 @@
             "strings, or a list of tensors. If passing multiple segments "
             "which should packed together, please convert your inputs to a "
             f"list of tensors. Received `x={x}`"
         )
     return x
 
 
-def is_xla_compatible(model):
-    """Determine if model and platform xla-compatible."""
-    return not (
-        platform.system() == "Darwin" and "arm" in platform.processor().lower()
-    ) and not isinstance(
-        model.distribute_strategy,
-        (
-            tf.compat.v1.distribute.experimental.TPUStrategy,
-            tf.distribute.TPUStrategy,
-        ),
-    )
-
-
 def print_msg(message, line_break=True):
     """Print the message to absl logging or stdout."""
     # Copied from core Keras.
     if keras.utils.is_interactive_logging_enabled():
         if line_break:
             sys.stdout.write(message + "\n")
         else:
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/keras_utils_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/keras_utils_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -9,21 +9,22 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.utils.keras_utils import clone_initializer
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
 
 
-class CloneInitializerTest(tf.test.TestCase):
+class CloneInitializerTest(TestCase):
     def test_config_equality(self):
         initializer = keras.initializers.VarianceScaling(
             scale=2.0,
             mode="fan_out",
         )
         clone = clone_initializer(initializer)
         self.assertAllEqual(initializer.get_config(), clone.get_config())
@@ -38,15 +39,15 @@
 
     def test_strings(self):
         initializer = "glorot_uniform"
         clone = clone_initializer(initializer)
         self.assertAllEqual(initializer, clone)
 
 
-class PackTest(tf.test.TestCase):
+class PackTest(TestCase):
     def test_pack_dict(self):
         tensor_dict = {"foo": tf.constant([1, 2])}
         data = pack_x_y_sample_weight(tensor_dict)
         self.assertAllEqual(data, tensor_dict)
 
     def test_pack_tuple(self):
         tensor_tuple = (tf.constant([1, 2]),)
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/pipeline_model.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/pipeline_model.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,18 +14,19 @@
 
 """A base class for models including preprocessing."""
 
 import functools
 import math
 
 import tensorflow as tf
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
 from keras_nlp.src.utils.keras_utils import pack_x_y_sample_weight
-from keras_nlp.src.utils.tf_utils import is_tensor_type
+from keras_nlp.src.utils.tensor_utils import is_tensor_type
 
 
 def _convert_inputs_to_dataset(
     x=None,
     y=None,
     sample_weight=None,
     batch_size=None,
@@ -50,15 +51,15 @@
             raise ValueError(
                 "When `x` is a `tf.data.Dataset`, please do not provide "
                 "`batch_size`. Received: "
                 f"`type(batch_size)={type(batch_size)}`."
             )
         return x
 
-    inputs = keras.utils.pack_x_y_sample_weight(x, y, sample_weight)
+    inputs = pack_x_y_sample_weight(x, y, sample_weight)
     try:
         ds = tf.data.Dataset.from_tensor_slices(inputs)
     except ValueError as e:
         # If our inputs are unbatched, re-raise with a more friendly error
         # message the default from tf.data. We expect this to come up with
         # some frequency, so it's important to have a good sign post here.
         if "only supported for rank >= 1" in str(e):
@@ -125,14 +126,15 @@
     val_arrays = tf.nest.map_structure(
         functools.partial(_split, start=split_at, end=batch_dim), arrays
     )
 
     return train_arrays, val_arrays
 
 
+@keras.saving.register_keras_serializable(package="keras_nlp")
 class PipelineModel(keras.Model):
     """A model which allows automatically applying preprocessing."""
 
     def __init__(self, *args, include_preprocessing=True, **kwargs):
         # Workaround for https://github.com/keras-team/keras/issues/17270
         # Reset any attempt to overwrite this classes base class to this class
         # can continue to be used for functional and non-functional models.
@@ -245,15 +247,20 @@
         x,
         y=None,
         sample_weight=None,
         **kwargs,
     ):
         if self.include_preprocessing:
             data = self.preprocess_samples(x, y, sample_weight)
-            x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
+            x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)
+            x = ops.convert_to_tensor(x)
+            if y is not None:
+                y = ops.convert_to_tensor(y)
+            if sample_weight is not None:
+                sample_weight = ops.convert_to_tensor(sample_weight)
         return super().train_on_batch(
             x=x,
             y=y,
             sample_weight=sample_weight,
             **kwargs,
         )
 
@@ -262,28 +269,34 @@
         x,
         y=None,
         sample_weight=None,
         **kwargs,
     ):
         if self.include_preprocessing:
             data = self.preprocess_samples(x, y, sample_weight)
-            x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
+            x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)
+            x = ops.convert_to_tensor(x)
+            if y is not None:
+                y = ops.convert_to_tensor(y)
+            if sample_weight is not None:
+                sample_weight = ops.convert_to_tensor(sample_weight)
         return super().test_on_batch(
             x=x,
             y=y,
             sample_weight=sample_weight,
             **kwargs,
         )
 
     def predict_on_batch(
         self,
         x,
         **kwargs,
     ):
         if self.include_preprocessing:
             data = self.preprocess_samples(x)
-            x, _, _ = tf.keras.utils.unpack_x_y_sample_weight(data)
+            x, _, _ = keras.utils.unpack_x_y_sample_weight(data)
+            x = ops.convert_to_tensor(x)
         return super().predict_on_batch(
             x=x,
             **kwargs,
         )
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/pipeline_model_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/pipeline_model_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,17 +10,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import os
 
 import tensorflow as tf
-from absl.testing import parameterized
-from tensorflow import keras
 
+from keras_nlp.src.backend import keras
+from keras_nlp.src.backend import ops
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.utils.pipeline_model import PipelineModel
 
 
 class NoopPipeline(PipelineModel):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.dense = keras.layers.Dense(1)
@@ -57,15 +58,17 @@
     """This model generates labels straight from the input data."""
 
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.dense = keras.layers.Dense(1)
 
     def preprocess_samples(self, x, y=None, sample_weight=None):
-        return tf.strings.to_number(x), tf.strings.to_number(x), sample_weight
+        x = tf.strings.to_number(x)
+        y = x
+        return keras.utils.pack_x_y_sample_weight(x, y, sample_weight)
 
     def call(self, inputs):
         return self.dense(inputs)
 
 
 class FunctionalPipeline(PipelineModel):
     def __init__(self, **kwargs):
@@ -80,505 +83,474 @@
         return {}
 
     @classmethod
     def from_config(cls, config):
         return cls(**config)
 
 
-class TestNoopPipelineModel(tf.test.TestCase, parameterized.TestCase):
+class TestNoopPipelineModel(TestCase):
     def test_fit(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = NoopPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = NoopPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.evaluate(x=x, y=y, sample_weight=sw, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_predict(self):
-        x = tf.random.uniform((8, 5))
+        x = ops.random.uniform((8, 5))
         model = NoopPipeline()
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_on_batch(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = NoopPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
         model.predict_on_batch(x=x)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = NoopPipeline()
-        x = tf.random.uniform((8, 5))
+        x = ops.random.uniform((8, 5))
         model_output = model.predict(x)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(
             path, custom_objects={"NoopPipeline": NoopPipeline}
         )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, NoopPipeline)
         # Check that output matches.
         restored_output = restored_model.predict(x)
         self.assertAllClose(model_output, restored_output)
 
 
-class TestFeaturePreprocessingModel(tf.test.TestCase, parameterized.TestCase):
+class TestFeaturePreprocessingModel(TestCase):
     def test_fit_with_preprocessing(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = FeaturePipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_fit_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = FeaturePipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate_with_preprocessing(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = FeaturePipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.evaluate(x=x, y=y, sample_weight=sw, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = FeaturePipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.evaluate(x=x, y=y, sample_weight=sw, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_predict_with_preprocessing(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
         model = FeaturePipeline()
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_predict_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
+        x = ops.random.uniform((100, 5))
         model = FeaturePipeline(include_preprocessing=False)
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_on_batch(self):
-        x = tf.strings.as_string(tf.random.uniform((8, 5)))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = tf.strings.as_string(ops.random.uniform((8, 5)))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = FeaturePipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
         model.predict_on_batch(x=x)
 
     def test_on_batch_no_preprocessing(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = FeaturePipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
-        model.predict_on_batch(x=x)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = FeaturePipeline()
-        x = tf.strings.as_string(tf.random.uniform((8, 5)))
+        x = tf.strings.as_string(ops.random.uniform((8, 5)))
         model_output = model.predict(x)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(
             path, custom_objects={"FeaturePipeline": FeaturePipeline}
         )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, FeaturePipeline)
         # Check that output matches.
         restored_output = restored_model.predict(x)
         self.assertAllClose(model_output, restored_output)
 
 
-class TestLabelPreprocessingModel(tf.test.TestCase, parameterized.TestCase):
+class TestLabelPreprocessingModel(TestCase):
     def test_fit_with_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.strings.as_string(tf.random.uniform((100, 1)))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = tf.strings.as_string(ops.random.uniform((100, 1)))
+        sw = ops.random.uniform((100, 1))
         model = LabelPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_fit_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = LabelPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate_with_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.strings.as_string(tf.random.uniform((100, 1)))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = tf.strings.as_string(ops.random.uniform((100, 1)))
+        sw = ops.random.uniform((100, 1))
         model = LabelPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.evaluate(x=x, y=y, sample_weight=sw, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = LabelPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.evaluate(x=x, y=y, sample_weight=sw, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_predict_with_preprocessing(self):
-        x = tf.random.uniform((100, 5))
+        x = ops.random.uniform((100, 5))
         model = LabelPipeline()
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_on_batch(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.strings.as_string(tf.random.uniform((8, 1)))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = tf.strings.as_string(ops.random.uniform((8, 1)))
+        sw = ops.random.uniform((8, 1))
         model = LabelPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
         model.predict_on_batch(x=x)
 
     def test_on_batch_no_preprocessing(self):
-        x = tf.random.uniform((8, 5))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 5))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = LabelPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
         model.predict_on_batch(x=x)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = LabelPipeline()
-        x = tf.random.uniform((8, 5))
+        x = ops.random.uniform((8, 5))
         model_output = model.predict(x)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(
             path, custom_objects={"LabelPipeline": LabelPipeline}
         )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, LabelPipeline)
         # Check that output matches.
         restored_output = restored_model.predict(x)
         self.assertAllClose(model_output, restored_output)
 
 
-class TestDataPreprocessingModel(tf.test.TestCase, parameterized.TestCase):
+class TestDataPreprocessingModel(TestCase):
     def test_fit_with_preprocessing(self):
-        data = tf.strings.as_string(tf.random.uniform((100, 1)))
+        data = tf.strings.as_string(ops.random.uniform((100, 1)))
         model = DataPipeline()
         model.compile(loss="mse")
         model.fit(x=data, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices(data).batch(8))
 
     def test_fit_no_preprocessing(self):
-        x = tf.random.uniform((100, 1))
-        y = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 1))
+        y = ops.random.uniform((100, 1))
         model = DataPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_evaluate_with_preprocessing(self):
-        data = tf.strings.as_string(tf.random.uniform((100, 1)))
+        data = tf.strings.as_string(ops.random.uniform((100, 1)))
         model = DataPipeline()
         model.compile(loss="mse")
         model.evaluate(x=data, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices(data).batch(8))
 
     def test_evaluate_no_preprocessing(self):
-        x = tf.random.uniform((100, 1))
-        y = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 1))
+        y = ops.random.uniform((100, 1))
         model = DataPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         model.evaluate(x=x, y=y, batch_size=8)
         model.evaluate(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_predict_with_preprocessing(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 1)))
+        x = tf.strings.as_string(ops.random.uniform((100, 1)))
         model = DataPipeline()
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_predict_no_preprocessing(self):
-        x = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 1))
         model = DataPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         model.predict(x=x, batch_size=8)
         model.predict(tf.data.Dataset.from_tensor_slices(x).batch(8))
 
     def test_on_batch(self):
-        data = tf.strings.as_string(tf.random.uniform((8, 1)))
+        data = tf.strings.as_string(ops.random.uniform((8, 1)))
         model = DataPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=data)
         model.test_on_batch(x=data)
         # Without sample weight.
         model.train_on_batch(x=data)
         model.test_on_batch(x=data)
         model.predict_on_batch(x=data)
 
     def test_on_batch_no_preprocessing(self):
-        x = tf.random.uniform((8, 1))
-        y = tf.random.uniform((8, 1))
-        sw = tf.random.uniform((8, 1))
+        x = ops.random.uniform((8, 1))
+        y = ops.random.uniform((8, 1))
+        sw = ops.random.uniform((8, 1))
         model = DataPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.train_on_batch(x=x, y=y, sample_weight=sw)
         model.test_on_batch(x=x, y=y, sample_weight=sw)
         # Without sample weight.
         model.train_on_batch(x=x, y=y)
         model.test_on_batch(x=x, y=y)
         model.predict_on_batch(x=x)
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = DataPipeline()
-        data = tf.strings.as_string(tf.random.uniform((8, 1)))
+        data = tf.strings.as_string(ops.random.uniform((8, 1)))
         model_output = model.predict(data)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(
             path, custom_objects={"DataPipeline": DataPipeline}
         )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, DataPipeline)
         # Check that output matches.
         restored_output = restored_model.predict(data)
         self.assertAllClose(model_output, restored_output)
 
 
-class TestFunctional(tf.test.TestCase, parameterized.TestCase):
+class TestFunctional(TestCase):
     def test_fit(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
 
         model = FunctionalPipeline()
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
     def test_fit_no_preprocessing(self):
-        x = tf.random.uniform((100, 5))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = ops.random.uniform((100, 5))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         model = FunctionalPipeline(include_preprocessing=False)
         model.compile(loss="mse")
         # With sample weight.
         model.fit(x=x, y=y, sample_weight=sw, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y, sw)).batch(8))
         # Without sample weight.
         model.fit(x=x, y=y, batch_size=8)
         model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(8))
 
-    @parameterized.named_parameters(
-        ("tf_format", "tf", "model"),
-        ("keras_format", "keras_v3", "model.keras"),
-    )
-    def test_saved_model(self, save_format, filename):
+    def test_saved_model(self):
         model = FunctionalPipeline()
-        x = tf.strings.as_string(tf.random.uniform((8, 5)))
+        x = tf.strings.as_string(ops.random.uniform((8, 5)))
         model_output = model.predict(x)
-        path = os.path.join(self.get_temp_dir(), filename)
-        # Don't save traces in the tf format, we check compilation elsewhere.
-        kwargs = {"save_traces": False} if save_format == "tf" else {}
-        model.save(path, save_format=save_format, **kwargs)
+        path = os.path.join(self.get_temp_dir(), "model.keras")
+        model.save(path, save_format="keras_v3")
         restored_model = keras.models.load_model(
             path, custom_objects={"FunctionalPipeline": FunctionalPipeline}
         )
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, FunctionalPipeline)
         # Check that output matches.
         restored_output = restored_model.predict(x)
         self.assertAllClose(model_output, restored_output)
 
 
-class TestFitArguments(tf.test.TestCase):
+class TestFitArguments(TestCase):
     def test_validation_data(self):
-        x = tf.strings.as_string(tf.random.uniform((80, 5)))
-        y = tf.random.uniform((80, 1))
-        val_x = tf.strings.as_string(tf.random.uniform((20, 5)))
-        val_y = tf.random.uniform((20, 1))
+        x = tf.strings.as_string(ops.random.uniform((80, 5)))
+        y = ops.random.uniform((80, 1))
+        val_x = tf.strings.as_string(ops.random.uniform((20, 5)))
+        val_y = ops.random.uniform((20, 1))
 
         model = FeaturePipeline()
         model.compile(loss="mse")
 
         model.fit(x=x, y=y, validation_data=(val_x, val_y), batch_size=8)
         model.fit(
             x=tf.data.Dataset.from_tensor_slices((x, y)).batch(8),
             validation_data=tf.data.Dataset.from_tensor_slices(
                 (val_x, val_y)
             ).batch(8),
         )
 
     def test_validation_split(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
-        y = tf.random.uniform((100, 1))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
+        y = ops.random.uniform((100, 1))
 
         model = FeaturePipeline()
         model.compile(loss="mse")
 
         model.fit(x=x, y=y, validation_split=0.2, batch_size=8)
 
     def test_error_dataset_and_invalid_arguments(self):
-        x = tf.strings.as_string(tf.random.uniform((100, 5)))
-        y = tf.random.uniform((100, 1))
-        sw = tf.random.uniform((100, 1))
+        x = tf.strings.as_string(ops.random.uniform((100, 5)))
+        y = ops.random.uniform((100, 1))
+        sw = ops.random.uniform((100, 1))
         ds = tf.data.Dataset.from_tensor_slices((x, y))
 
         model = FeaturePipeline()
         model.compile(loss="mse")
         with self.assertRaises(ValueError):
             model.fit(ds, validation_split=0.2)
         with self.assertRaises(ValueError):
             model.fit(ds, batch_size=0.2)
         with self.assertRaises(ValueError):
             model.fit(ds, y=y)
         with self.assertRaises(ValueError):
             model.fit(ds, sample_weight=sw)
 
 
-class TestInputErrors(tf.test.TestCase):
+class TestInputErrors(TestCase):
     def test_unbatched_input_raises(self):
         model = FeaturePipeline()
         with self.assertRaisesRegex(ValueError, "must have a batch dimension"):
             model.fit(x=tf.constant("test"))
         with self.assertRaisesRegex(ValueError, "must have a batch dimension"):
             model.fit(x=tf.constant(["test"]), y=tf.constant(0))
         with self.assertRaisesRegex(ValueError, "must have a batch dimension"):
```

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/python_utils.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/python_utils.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp/src/utils/python_utils_test.py` & `keras-nlp-0.6.0.dev0/keras_nlp/src/utils/python_utils_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,31 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tensorflow as tf
-
+from keras_nlp.src.tests.test_case import TestCase
 from keras_nlp.src.utils.python_utils import classproperty
 from keras_nlp.src.utils.python_utils import format_docstring
 
 
-class ClassPropertyTest(tf.test.TestCase):
+class ClassPropertyTest(TestCase):
     def test_class_property(self):
         class Foo:
             @classproperty
             def bar(cls):
                 return "class property"
 
         self.assertAllEqual(Foo.bar, "class property")
 
 
-class FormatDocstringTest(tf.test.TestCase):
+class FormatDocstringTest(TestCase):
     def test_function(self):
         @format_docstring(adjective="salubrious")
         def foo():
             """It was a {{adjective}} November day."""
             return "function"
 
         self.assertAllEqual(foo(), "function")
```

### Comparing `keras-nlp-0.5.2/keras_nlp/tokenizers/__init__.py` & `keras-nlp-0.6.0.dev0/keras_nlp/tokenizers/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/keras_nlp.egg-info/PKG-INFO` & `keras-nlp-0.6.0.dev0/keras_nlp.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: keras-nlp
-Version: 0.5.2
+Version: 0.6.0.dev0
 Summary: Industry-strength Natural Language Processing extensions for Keras.
 Home-page: https://github.com/keras-team/keras-nlp
 Author: Keras team
 Author-email: keras-nlp@google.com
 License: Apache License 2.0
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3
```

### Comparing `keras-nlp-0.5.2/keras_nlp.egg-info/SOURCES.txt` & `keras-nlp-0.6.0.dev0/keras_nlp.egg-info/SOURCES.txt`

 * *Files 14% similar despite different names*

```diff
@@ -10,57 +10,66 @@
 keras_nlp/layers/__init__.py
 keras_nlp/metrics/__init__.py
 keras_nlp/models/__init__.py
 keras_nlp/samplers/__init__.py
 keras_nlp/src/__init__.py
 keras_nlp/src/api_export.py
 keras_nlp/src/conftest.py
+keras_nlp/src/backend/__init__.py
+keras_nlp/src/backend/config.py
+keras_nlp/src/backend/keras.py
+keras_nlp/src/backend/ops.py
+keras_nlp/src/backend/random.py
 keras_nlp/src/layers/__init__.py
-keras_nlp/src/layers/cached_multi_head_attention.py
-keras_nlp/src/layers/cached_multi_head_attention_test.py
-keras_nlp/src/layers/f_net_encoder.py
-keras_nlp/src/layers/f_net_encoder_test.py
-keras_nlp/src/layers/masked_lm_head.py
-keras_nlp/src/layers/masked_lm_head_test.py
-keras_nlp/src/layers/masked_lm_mask_generator.py
-keras_nlp/src/layers/masked_lm_mask_generator_test.py
-keras_nlp/src/layers/multi_segment_packer.py
-keras_nlp/src/layers/multi_segment_packer_test.py
-keras_nlp/src/layers/position_embedding.py
-keras_nlp/src/layers/position_embedding_test.py
-keras_nlp/src/layers/random_deletion.py
-keras_nlp/src/layers/random_deletion_test.py
-keras_nlp/src/layers/random_swap.py
-keras_nlp/src/layers/random_swap_test.py
-keras_nlp/src/layers/sine_position_encoding.py
-keras_nlp/src/layers/sine_position_encoding_test.py
-keras_nlp/src/layers/start_end_packer.py
-keras_nlp/src/layers/start_end_packer_test.py
-keras_nlp/src/layers/token_and_position_embedding.py
-keras_nlp/src/layers/token_and_position_embedding_test.py
-keras_nlp/src/layers/transformer_decoder.py
-keras_nlp/src/layers/transformer_decoder_test.py
-keras_nlp/src/layers/transformer_encoder.py
-keras_nlp/src/layers/transformer_encoder_test.py
-keras_nlp/src/layers/transformer_layer_utils.py
-keras_nlp/src/layers/transformer_layer_utils_test.py
+keras_nlp/src/layers/modeling/__init__.py
+keras_nlp/src/layers/modeling/cached_multi_head_attention.py
+keras_nlp/src/layers/modeling/cached_multi_head_attention_test.py
+keras_nlp/src/layers/modeling/f_net_encoder.py
+keras_nlp/src/layers/modeling/f_net_encoder_test.py
+keras_nlp/src/layers/modeling/masked_lm_head.py
+keras_nlp/src/layers/modeling/masked_lm_head_test.py
+keras_nlp/src/layers/modeling/position_embedding.py
+keras_nlp/src/layers/modeling/position_embedding_test.py
+keras_nlp/src/layers/modeling/sine_position_encoding.py
+keras_nlp/src/layers/modeling/sine_position_encoding_test.py
+keras_nlp/src/layers/modeling/token_and_position_embedding.py
+keras_nlp/src/layers/modeling/token_and_position_embedding_test.py
+keras_nlp/src/layers/modeling/transformer_decoder.py
+keras_nlp/src/layers/modeling/transformer_decoder_test.py
+keras_nlp/src/layers/modeling/transformer_encoder.py
+keras_nlp/src/layers/modeling/transformer_encoder_test.py
+keras_nlp/src/layers/modeling/transformer_layer_utils.py
+keras_nlp/src/layers/modeling/transformer_layer_utils_test.py
+keras_nlp/src/layers/preprocessing/__init__.py
+keras_nlp/src/layers/preprocessing/masked_lm_mask_generator.py
+keras_nlp/src/layers/preprocessing/masked_lm_mask_generator_test.py
+keras_nlp/src/layers/preprocessing/multi_segment_packer.py
+keras_nlp/src/layers/preprocessing/multi_segment_packer_test.py
+keras_nlp/src/layers/preprocessing/preprocessing_layer.py
+keras_nlp/src/layers/preprocessing/random_deletion.py
+keras_nlp/src/layers/preprocessing/random_deletion_test.py
+keras_nlp/src/layers/preprocessing/random_swap.py
+keras_nlp/src/layers/preprocessing/random_swap_test.py
+keras_nlp/src/layers/preprocessing/start_end_packer.py
+keras_nlp/src/layers/preprocessing/start_end_packer_test.py
 keras_nlp/src/metrics/__init__.py
 keras_nlp/src/metrics/bleu.py
 keras_nlp/src/metrics/bleu_test.py
 keras_nlp/src/metrics/edit_distance.py
 keras_nlp/src/metrics/edit_distance_test.py
 keras_nlp/src/metrics/perplexity.py
 keras_nlp/src/metrics/perplexity_test.py
 keras_nlp/src/metrics/rouge_base.py
 keras_nlp/src/metrics/rouge_l.py
 keras_nlp/src/metrics/rouge_l_test.py
 keras_nlp/src/metrics/rouge_n.py
 keras_nlp/src/metrics/rouge_n_test.py
 keras_nlp/src/models/__init__.py
 keras_nlp/src/models/backbone.py
+keras_nlp/src/models/generative_task.py
 keras_nlp/src/models/preprocessor.py
 keras_nlp/src/models/task.py
 keras_nlp/src/models/task_test.py
 keras_nlp/src/models/albert/__init__.py
 keras_nlp/src/models/albert/albert_backbone.py
 keras_nlp/src/models/albert/albert_backbone_test.py
 keras_nlp/src/models/albert/albert_classifier.py
@@ -78,16 +87,18 @@
 keras_nlp/src/models/bart/__init__.py
 keras_nlp/src/models/bart/bart_backbone.py
 keras_nlp/src/models/bart/bart_backbone_test.py
 keras_nlp/src/models/bart/bart_preprocessor.py
 keras_nlp/src/models/bart/bart_preprocessor_test.py
 keras_nlp/src/models/bart/bart_presets.py
 keras_nlp/src/models/bart/bart_presets_test.py
+keras_nlp/src/models/bart/bart_seq_2_seq_lm.py
 keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor.py
 keras_nlp/src/models/bart/bart_seq_2_seq_lm_preprocessor_test.py
+keras_nlp/src/models/bart/bart_seq_2_seq_lm_test.py
 keras_nlp/src/models/bart/bart_tokenizer.py
 keras_nlp/src/models/bart/bart_tokenizer_test.py
 keras_nlp/src/models/bert/__init__.py
 keras_nlp/src/models/bert/bert_backbone.py
 keras_nlp/src/models/bert/bert_backbone_test.py
 keras_nlp/src/models/bert/bert_classifier.py
 keras_nlp/src/models/bert/bert_classifier_test.py
@@ -158,14 +169,26 @@
 keras_nlp/src/models/gpt2/gpt2_causal_lm_test.py
 keras_nlp/src/models/gpt2/gpt2_preprocessor.py
 keras_nlp/src/models/gpt2/gpt2_preprocessor_test.py
 keras_nlp/src/models/gpt2/gpt2_presets.py
 keras_nlp/src/models/gpt2/gpt2_presets_test.py
 keras_nlp/src/models/gpt2/gpt2_tokenizer.py
 keras_nlp/src/models/gpt2/gpt2_tokenizer_test.py
+keras_nlp/src/models/gpt_neo_x/__init__.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_attention.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_backbone.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_backbone_test.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_causal_lm_preprocessor_test.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_decoder.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_preprocessor.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_preprocessor_test.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_tokenizer.py
+keras_nlp/src/models/gpt_neo_x/gpt_neo_x_tokenizer_test.py
+keras_nlp/src/models/gpt_neo_x/rotary_embedding.py
 keras_nlp/src/models/opt/__init__.py
 keras_nlp/src/models/opt/opt_backbone.py
 keras_nlp/src/models/opt/opt_backbone_test.py
 keras_nlp/src/models/opt/opt_causal_lm.py
 keras_nlp/src/models/opt/opt_causal_lm_preprocessor.py
 keras_nlp/src/models/opt/opt_causal_lm_preprocessor_test.py
 keras_nlp/src/models/opt/opt_causal_lm_test.py
@@ -180,15 +203,14 @@
 keras_nlp/src/models/roberta/roberta_backbone_test.py
 keras_nlp/src/models/roberta/roberta_classifier.py
 keras_nlp/src/models/roberta/roberta_classifier_test.py
 keras_nlp/src/models/roberta/roberta_masked_lm.py
 keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor.py
 keras_nlp/src/models/roberta/roberta_masked_lm_preprocessor_test.py
 keras_nlp/src/models/roberta/roberta_masked_lm_test.py
-keras_nlp/src/models/roberta/roberta_multi_segment_packer.py
 keras_nlp/src/models/roberta/roberta_preprocessor.py
 keras_nlp/src/models/roberta/roberta_preprocessor_test.py
 keras_nlp/src/models/roberta/roberta_presets.py
 keras_nlp/src/models/roberta/roberta_presets_test.py
 keras_nlp/src/models/roberta/roberta_tokenizer.py
 keras_nlp/src/models/roberta/roberta_tokenizer_test.py
 keras_nlp/src/models/t5/__init__.py
@@ -196,18 +218,26 @@
 keras_nlp/src/models/t5/t5_backbone_test.py
 keras_nlp/src/models/t5/t5_layer_norm.py
 keras_nlp/src/models/t5/t5_multi_head_attention.py
 keras_nlp/src/models/t5/t5_tokenizer.py
 keras_nlp/src/models/t5/t5_tokenizer_test.py
 keras_nlp/src/models/t5/t5_transformer_layer.py
 keras_nlp/src/models/whisper/__init__.py
+keras_nlp/src/models/whisper/whisper_audio_feature_extractor.py
+keras_nlp/src/models/whisper/whisper_audio_feature_extractor_test.py
 keras_nlp/src/models/whisper/whisper_backbone.py
 keras_nlp/src/models/whisper/whisper_backbone_test.py
 keras_nlp/src/models/whisper/whisper_decoder.py
 keras_nlp/src/models/whisper/whisper_encoder.py
+keras_nlp/src/models/whisper/whisper_preprocessor.py
+keras_nlp/src/models/whisper/whisper_preprocessor_test.py
+keras_nlp/src/models/whisper/whisper_presets.py
+keras_nlp/src/models/whisper/whisper_presets_test.py
+keras_nlp/src/models/whisper/whisper_tokenizer.py
+keras_nlp/src/models/whisper/whisper_tokenizer_test.py
 keras_nlp/src/models/xlm_roberta/__init__.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_backbone_test.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_classifier_test.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm.py
 keras_nlp/src/models/xlm_roberta/xlm_roberta_masked_lm_preprocessor.py
@@ -232,14 +262,15 @@
 keras_nlp/src/samplers/serialization.py
 keras_nlp/src/samplers/serialization_test.py
 keras_nlp/src/samplers/top_k_sampler.py
 keras_nlp/src/samplers/top_k_sampler_test.py
 keras_nlp/src/samplers/top_p_sampler.py
 keras_nlp/src/samplers/top_p_sampler_test.py
 keras_nlp/src/tests/__init__.py
+keras_nlp/src/tests/test_case.py
 keras_nlp/src/tests/doc_tests/__init__.py
 keras_nlp/src/tests/doc_tests/docstring_lib.py
 keras_nlp/src/tests/doc_tests/docstring_test.py
 keras_nlp/src/tests/doc_tests/fenced_docstring_lib.py
 keras_nlp/src/tokenizers/__init__.py
 keras_nlp/src/tokenizers/byte_pair_tokenizer.py
 keras_nlp/src/tokenizers/byte_pair_tokenizer_test.py
@@ -260,10 +291,10 @@
 keras_nlp/src/utils/__init__.py
 keras_nlp/src/utils/keras_utils.py
 keras_nlp/src/utils/keras_utils_test.py
 keras_nlp/src/utils/pipeline_model.py
 keras_nlp/src/utils/pipeline_model_test.py
 keras_nlp/src/utils/python_utils.py
 keras_nlp/src/utils/python_utils_test.py
-keras_nlp/src/utils/tf_utils.py
-keras_nlp/src/utils/tf_utils_test.py
+keras_nlp/src/utils/tensor_utils.py
+keras_nlp/src/utils/tensor_utils_test.py
 keras_nlp/tokenizers/__init__.py
```

### Comparing `keras-nlp-0.5.2/setup.cfg` & `keras-nlp-0.6.0.dev0/setup.cfg`

 * *Files identical despite different names*

### Comparing `keras-nlp-0.5.2/setup.py` & `keras-nlp-0.6.0.dev0/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -47,17 +47,20 @@
     long_description_content_type="text/markdown",
     version=get_version("keras_nlp/__init__.py"),
     url="https://github.com/keras-team/keras-nlp",
     author="Keras team",
     author_email="keras-nlp@google.com",
     license="Apache License 2.0",
     install_requires=[
+        "keras-core",
         "absl-py",
         "numpy",
         "packaging",
+        "regex",
+        "rich",
         # Don't require tensorflow-text on MacOS, there are no binaries for ARM.
         # Also, we rely on tensorflow *transitively* through tensorflow-text.
         # This avoid a slowdown during `pip install keras-nlp` where pip would
         # download many version of both libraries to find compatible versions.
         "tensorflow-text; platform_system != 'Darwin'",
     ],
     extras_require={
```

