# Comparing `tmp/azure-search-documents-11.4.0b3.zip` & `tmp/azure-search-documents-11.4.0b4.zip`

## zipinfo {}

```diff
@@ -1,196 +1,200 @@
-Zip file size: 407356 bytes, number of entries: 194
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/samples/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/tests/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/
--rw-rw-r--  2.0 unx       38 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/setup.cfg
--rw-rw-r--  2.0 unx    17436 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/README.md
--rw-rw-r--  2.0 unx    18337 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/PKG-INFO
--rw-rw-r--  2.0 unx     2351 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/setup.py
--rw-rw-r--  2.0 unx      193 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/MANIFEST.in
--rw-rw-r--  2.0 unx     1073 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/LICENSE
--rw-rw-r--  2.0 unx    10342 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/CHANGELOG.md
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/samples/async_samples/
--rw-rw-r--  2.0 unx     3905 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_indexers_operations.py
--rw-rw-r--  2.0 unx     2242 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_authentication.py
--rw-rw-r--  2.0 unx     1625 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_suggestions.py
--rw-rw-r--  2.0 unx     1554 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_analyze_text.py
--rw-rw-r--  2.0 unx     1578 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_simple_query.py
--rw-rw-r--  2.0 unx     1558 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_autocomplete.py
--rw-rw-r--  2.0 unx     5772 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/README.md
--rw-rw-r--  2.0 unx     2349 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_semantic_search.py
--rw-rw-r--  2.0 unx     1755 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_filter_query.py
--rw-rw-r--  2.0 unx     1580 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_facet_query.py
--rw-rw-r--  2.0 unx     3246 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_index_alias_crud_operations.py
--rw-rw-r--  2.0 unx     1584 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_get_document.py
--rw-rw-r--  2.0 unx     2159 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_crud_operations.py
--rw-rw-r--  2.0 unx     3599 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_index_crud_operations.py
--rw-rw-r--  2.0 unx     1821 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_buffered_sender.py
--rw-rw-r--  2.0 unx     2842 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_data_source_operations.py
--rw-rw-r--  2.0 unx     2892 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_synonym_map_operations.py
--rw-rw-r--  2.0 unx     5960 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/sample_indexer_datasource_skillset.py
--rw-rw-r--  2.0 unx     3471 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_index_alias_crud_operations_async.py
--rw-rw-r--  2.0 unx     1716 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_get_document_async.py
--rw-rw-r--  2.0 unx     2356 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_crud_operations_async.py
--rw-rw-r--  2.0 unx     1751 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_suggestions_async.py
--rw-rw-r--  2.0 unx     1654 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_analyze_text_async.py
--rw-rw-r--  2.0 unx     2538 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_synonym_map_operations_async.py
--rw-rw-r--  2.0 unx     1897 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_filter_query_async.py
--rw-rw-r--  2.0 unx     2380 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_authentication_async.py
--rw-rw-r--  2.0 unx     4362 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_indexers_operations_async.py
--rw-rw-r--  2.0 unx     1706 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_facet_query_async.py
--rw-rw-r--  2.0 unx     3073 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_data_source_operations_async.py
--rw-rw-r--  2.0 unx     1931 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_buffered_sender_async.py
--rw-rw-r--  2.0 unx     1674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_autocomplete_async.py
--rw-rw-r--  2.0 unx     1700 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_simple_query_async.py
--rw-rw-r--  2.0 unx     3819 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_index_crud_operations_async.py
--rw-rw-r--  2.0 unx     2449 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/samples/async_samples/sample_semantic_search_async.py
--rw-rw-r--  2.0 unx     8392 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/SOURCES.txt
--rw-rw-r--  2.0 unx        1 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/dependency_links.txt
--rw-rw-r--  2.0 unx        1 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/not-zip-safe
--rw-rw-r--  2.0 unx        6 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/top_level.txt
--rw-rw-r--  2.0 unx    18337 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/PKG-INFO
--rw-rw-r--  2.0 unx       59 b- defN 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure_search_documents.egg-info/requires.txt
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/tests/async_tests/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/tests/perfstress_tests/
--rw-rw-r--  2.0 unx    12131 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client_skillset_live.py
--rw-rw-r--  2.0 unx     7421 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_indexer_client_live.py
--rw-rw-r--  2.0 unx     7678 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client_live.py
--rw-rw-r--  2.0 unx     6813 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client_data_source_live.py
--rw-rw-r--  2.0 unx     1292 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/conftest.py
--rw-rw-r--  2.0 unx     5909 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_queries.py
--rw-rw-r--  2.0 unx     5160 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_client_search_live.py
--rw-rw-r--  2.0 unx     5048 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_regex_flags.py
--rw-rw-r--  2.0 unx     5705 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client_synonym_map_live.py
--rw-rw-r--  2.0 unx     4068 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client_alias_live.py
--rw-rw-r--  2.0 unx     3099 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_index_field_helpers.py
--rw-rw-r--  2.0 unx     2186 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_index_documents_batch.py
--rw-rw-r--  2.0 unx     6741 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_client_buffered_sender_live.py
--rw-rw-r--  2.0 unx     5016 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_index_client.py
--rw-rw-r--  2.0 unx    29781 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/search_service_preparer.py
--rw-rw-r--  2.0 unx     1938 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_client_basic_live.py
--rw-rw-r--  2.0 unx     6583 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_client_index_document_live.py
--rw-rw-r--  2.0 unx     6879 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_buffered_sender.py
--rw-rw-r--  2.0 unx    15131 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/test_search_client.py
--rw-rw-r--  2.0 unx     4392 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_alias_live_async.py
--rw-rw-r--  2.0 unx     8004 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_indexer_client_live_async.py
--rw-rw-r--  2.0 unx     6499 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_search_live_async.py
--rw-rw-r--  2.0 unx     6058 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_synonym_map_live_async.py
--rw-rw-r--  2.0 unx    10578 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_skillset_live_async.py
--rw-rw-r--  2.0 unx     6887 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_buffered_sender_async.py
--rw-rw-r--  2.0 unx     1362 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_async.py
--rw-rw-r--  2.0 unx     6272 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_data_source_live_async.py
--rw-rw-r--  2.0 unx     6903 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_index_document_live_async.py
--rw-rw-r--  2.0 unx     7178 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_buffered_sender_live_async.py
--rw-rw-r--  2.0 unx     3940 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_async.py
--rw-rw-r--  2.0 unx     2157 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_basic_live_async.py
--rw-rw-r--  2.0 unx     8120 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_live_async.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/perfstress_tests/__init__.py
--rw-rw-r--  2.0 unx     2282 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/perfstress_tests/search_documents.py
--rw-rw-r--  2.0 unx     2224 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/perfstress_tests/autocomplete.py
--rw-rw-r--  2.0 unx     2189 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/tests/perfstress_tests/suggest.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/
--rw-rw-r--  2.0 unx      264 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/
--rw-rw-r--  2.0 unx      264 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/models/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/_generated/
--rw-rw-r--  2.0 unx     1822 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/__init__.py
--rw-rw-r--  2.0 unx      450 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_api_versions.py
--rw-rw-r--  2.0 unx    33577 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_search_client.py
--rw-rw-r--  2.0 unx      769 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_headers_mixin.py
--rw-rw-r--  2.0 unx     5402 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_paging.py
--rw-rw-r--  2.0 unx     3339 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_queries.py
--rw-rw-r--  2.0 unx     2127 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender_base.py
--rw-rw-r--  2.0 unx     5984 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_index_documents_batch.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/py.typed
--rw-rw-r--  2.0 unx    14877 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender.py
--rw-rw-r--  2.0 unx     1829 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_utils.py
--rw-rw-r--  2.0 unx      252 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_version.py
--rw-rw-r--  2.0 unx      491 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_search_documents_error.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/
--rw-rw-r--  2.0 unx     1479 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/__init__.py
--rw-rw-r--  2.0 unx    26369 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_index_client.py
--rw-rw-r--  2.0 unx     3154 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_utils.py
--rw-rw-r--  2.0 unx    31423 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_indexer_client.py
--rw-rw-r--  2.0 unx     1479 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/__init__.py
--rw-rw-r--  2.0 unx    27291 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_index_client.py
--rw-rw-r--  2.0 unx    30278 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_indexer_client.py
--rw-rw-r--  2.0 unx    10169 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/__init__.py
--rw-rw-r--  2.0 unx      614 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_edm.py
--rw-rw-r--  2.0 unx    42501 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_index.py
--rw-rw-r--  2.0 unx    45996 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_models.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/
--rw-rw-r--  2.0 unx      756 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/__init__.py
--rw-rw-r--  2.0 unx     5056 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_search_service_client.py
--rw-rw-r--  2.0 unx    77450 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_serialization.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_patch.py
--rw-rw-r--  2.0 unx     2775 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_configuration.py
--rw-rw-r--  2.0 unx     1605 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_vendor.py
--rw-rw-r--  2.0 unx    35217 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py
--rw-rw-r--  2.0 unx     1234 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/__init__.py
--rw-rw-r--  2.0 unx    44117 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py
--rw-rw-r--  2.0 unx    34616 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_aliases_operations.py
--rw-rw-r--  2.0 unx    49169 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexes_operations.py
--rw-rw-r--  2.0 unx    56803 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexers_operations.py
--rw-rw-r--  2.0 unx     5393 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_patch.py
--rw-rw-r--  2.0 unx    33571 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/
--rw-rw-r--  2.0 unx      756 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/__init__.py
--rw-rw-r--  2.0 unx     5122 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_search_service_client.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_patch.py
--rw-rw-r--  2.0 unx     2741 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_configuration.py
--rw-rw-r--  2.0 unx      954 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_vendor.py
--rw-rw-r--  2.0 unx    28149 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py
--rw-rw-r--  2.0 unx     1234 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/__init__.py
--rw-rw-r--  2.0 unx    35360 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py
--rw-rw-r--  2.0 unx    28209 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py
--rw-rw-r--  2.0 unx    39891 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py
--rw-rw-r--  2.0 unx    44444 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py
--rw-rw-r--  2.0 unx     4441 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    26812 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py
--rw-rw-r--  2.0 unx    16018 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/__init__.py
--rw-rw-r--  2.0 unx   474847 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_models_py3.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_patch.py
--rw-rw-r--  2.0 unx    67720 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_search_service_client_enums.py
--rw-rw-r--  2.0 unx     1553 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/__init__.py
--rw-rw-r--  2.0 unx     5035 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/_paging.py
--rw-rw-r--  2.0 unx    34342 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_client_async.py
--rw-rw-r--  2.0 unx      673 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/_timer.py
--rw-rw-r--  2.0 unx     5735 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/_index_documents_batch_async.py
--rw-rw-r--  2.0 unx    15198 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_indexing_buffered_sender_async.py
--rw-rw-r--  2.0 unx     1887 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/models/__init__.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/
--rw-rw-r--  2.0 unx      750 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/__init__.py
--rw-rw-r--  2.0 unx     3817 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/_search_index_client.py
--rw-rw-r--  2.0 unx    77450 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/_serialization.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/_patch.py
--rw-rw-r--  2.0 unx     3007 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/_configuration.py
--rw-rw-r--  2.0 unx     1066 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/_vendor.py
--rw-rw-r--  2.0 unx      701 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/__init__.py
--rw-rw-r--  2.0 unx    67222 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_documents_operations.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_patch.py
-drwxrwxr-x  2.0 unx        0 b- stor 23-Feb-06 18:00 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/
--rw-rw-r--  2.0 unx      750 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/__init__.py
--rw-rw-r--  2.0 unx     3863 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_search_index_client.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_patch.py
--rw-rw-r--  2.0 unx     2973 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_configuration.py
--rw-rw-r--  2.0 unx      701 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/__init__.py
--rw-rw-r--  2.0 unx    49648 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_documents_operations.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_patch.py
--rw-rw-r--  2.0 unx    12624 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_search_index_client_enums.py
--rw-rw-r--  2.0 unx     2839 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/__init__.py
--rw-rw-r--  2.0 unx    90041 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_models_py3.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Feb-06 17:58 azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_patch.py
-194 files, 2191963 bytes uncompressed, 362156 bytes compressed:  83.5%
+Zip file size: 425680 bytes, number of entries: 198
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/samples/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/tests/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/
+-rw-rw-r--  2.0 unx       38 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/setup.cfg
+-rw-rw-r--  2.0 unx    10508 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/CHANGELOG.md
+-rw-rw-r--  2.0 unx    21853 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/README.md
+-rw-rw-r--  2.0 unx       52 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/pyproject.toml
+-rw-rw-r--  2.0 unx     2365 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/setup.py
+-rw-rw-r--  2.0 unx     4081 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/TROUBLESHOOTING.md
+-rw-rw-r--  2.0 unx      193 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/MANIFEST.in
+-rw-rw-r--  2.0 unx    22754 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/PKG-INFO
+-rw-rw-r--  2.0 unx     1073 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/LICENSE
+-rw-rw-r--  2.0 unx        6 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/top_level.txt
+-rw-rw-r--  2.0 unx        1 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/not-zip-safe
+-rw-rw-r--  2.0 unx       59 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/requires.txt
+-rw-rw-r--  2.0 unx     8510 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/SOURCES.txt
+-rw-rw-r--  2.0 unx        1 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/dependency_links.txt
+-rw-rw-r--  2.0 unx    22754 b- defN 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure_search_documents.egg-info/PKG-INFO
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/samples/async_samples/
+-rw-rw-r--  2.0 unx     2899 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_synonym_map_operations.py
+-rw-rw-r--  2.0 unx     2823 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_data_source_operations.py
+-rw-rw-r--  2.0 unx     1759 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_filter_query.py
+-rw-rw-r--  2.0 unx     6100 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/README.md
+-rw-rw-r--  2.0 unx     1785 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_buffered_sender.py
+-rw-rw-r--  2.0 unx     2373 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_semantic_search.py
+-rw-rw-r--  2.0 unx     6753 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_vector_search.py
+-rw-rw-r--  2.0 unx     3875 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_indexers_operations.py
+-rw-rw-r--  2.0 unx     3272 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_index_alias_crud_operations.py
+-rw-rw-r--  2.0 unx     2246 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_authentication.py
+-rw-rw-r--  2.0 unx     5964 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_indexer_datasource_skillset.py
+-rw-rw-r--  2.0 unx     1581 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_simple_query.py
+-rw-rw-r--  2.0 unx     1561 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_autocomplete.py
+-rw-rw-r--  2.0 unx     1627 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_suggestions.py
+-rw-rw-r--  2.0 unx     1587 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_get_document.py
+-rw-rw-r--  2.0 unx     1557 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_analyze_text.py
+-rw-rw-r--  2.0 unx     1583 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_facet_query.py
+-rw-rw-r--  2.0 unx     2163 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_crud_operations.py
+-rw-rw-r--  2.0 unx     3648 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/sample_index_crud_operations.py
+-rw-rw-r--  2.0 unx     2385 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_authentication_async.py
+-rw-rw-r--  2.0 unx     1753 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_suggestions_async.py
+-rw-rw-r--  2.0 unx     7081 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_vector_search_async.py
+-rw-rw-r--  2.0 unx     4350 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_indexers_operations_async.py
+-rw-rw-r--  2.0 unx     1708 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_facet_query_async.py
+-rw-rw-r--  2.0 unx     3870 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_index_crud_operations_async.py
+-rw-rw-r--  2.0 unx     1656 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_analyze_text_async.py
+-rw-rw-r--  2.0 unx     1718 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_get_document_async.py
+-rw-rw-r--  2.0 unx     3080 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_data_source_operations_async.py
+-rw-rw-r--  2.0 unx     2544 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_synonym_map_operations_async.py
+-rw-rw-r--  2.0 unx     1702 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_simple_query_async.py
+-rw-rw-r--  2.0 unx     3498 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_index_alias_crud_operations_async.py
+-rw-rw-r--  2.0 unx     2347 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_semantic_search_async.py
+-rw-rw-r--  2.0 unx     1896 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_buffered_sender_async.py
+-rw-rw-r--  2.0 unx     1900 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_filter_query_async.py
+-rw-rw-r--  2.0 unx     2361 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_crud_operations_async.py
+-rw-rw-r--  2.0 unx     1676 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/samples/async_samples/sample_autocomplete_async.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/tests/async_tests/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/tests/perfstress_tests/
+-rw-rw-r--  2.0 unx     4583 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_regex_flags.py
+-rw-rw-r--  2.0 unx     2164 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_index_documents_batch.py
+-rw-rw-r--  2.0 unx     4980 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client.py
+-rw-rw-r--  2.0 unx     6740 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_client_buffered_sender_live.py
+-rw-rw-r--  2.0 unx     4068 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client_alias_live.py
+-rw-rw-r--  2.0 unx     6583 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_client_index_document_live.py
+-rw-rw-r--  2.0 unx     1227 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/conftest.py
+-rw-rw-r--  2.0 unx     4976 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_client_search_live.py
+-rw-rw-r--  2.0 unx    14512 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_client.py
+-rw-rw-r--  2.0 unx     6841 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client_data_source_live.py
+-rw-rw-r--  2.0 unx    11683 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client_skillset_live.py
+-rw-rw-r--  2.0 unx     5734 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client_synonym_map_live.py
+-rw-rw-r--  2.0 unx     6365 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_buffered_sender.py
+-rw-rw-r--  2.0 unx     7372 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_indexer_client_live.py
+-rw-rw-r--  2.0 unx     3103 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_index_field_helpers.py
+-rw-rw-r--  2.0 unx     1938 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_client_basic_live.py
+-rw-rw-r--  2.0 unx     7246 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_search_index_client_live.py
+-rw-rw-r--  2.0 unx    29355 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/search_service_preparer.py
+-rw-rw-r--  2.0 unx     5887 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/test_queries.py
+-rw-rw-r--  2.0 unx     6255 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_search_live_async.py
+-rw-rw-r--  2.0 unx     3942 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_async.py
+-rw-rw-r--  2.0 unx     1364 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_async.py
+-rw-rw-r--  2.0 unx     4392 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_alias_live_async.py
+-rw-rw-r--  2.0 unx    10245 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_skillset_live_async.py
+-rw-rw-r--  2.0 unx     7177 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_buffered_sender_live_async.py
+-rw-rw-r--  2.0 unx     6298 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_data_source_live_async.py
+-rw-rw-r--  2.0 unx     7689 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_live_async.py
+-rw-rw-r--  2.0 unx     2157 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_basic_live_async.py
+-rw-rw-r--  2.0 unx     6902 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_index_document_live_async.py
+-rw-rw-r--  2.0 unx     7955 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_indexer_client_live_async.py
+-rw-rw-r--  2.0 unx     6385 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_buffered_sender_async.py
+-rw-rw-r--  2.0 unx     6087 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_synonym_map_live_async.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/perfstress_tests/__init__.py
+-rw-rw-r--  2.0 unx     2252 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/perfstress_tests/suggest.py
+-rw-rw-r--  2.0 unx     2247 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/perfstress_tests/search_documents.py
+-rw-rw-r--  2.0 unx     2287 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/tests/perfstress_tests/autocomplete.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/
+-rw-rw-r--  2.0 unx      264 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/
+-rw-rw-r--  2.0 unx      264 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/_generated/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/aio/
+-rw-rw-r--  2.0 unx      491 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_search_documents_error.py
+-rw-rw-r--  2.0 unx     1822 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/__init__.py
+-rw-rw-r--  2.0 unx    14785 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender.py
+-rw-rw-r--  2.0 unx     6063 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_index_documents_batch.py
+-rw-rw-r--  2.0 unx      498 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_api_versions.py
+-rw-rw-r--  2.0 unx    36116 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_search_client.py
+-rw-rw-r--  2.0 unx      769 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_headers_mixin.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/py.typed
+-rw-rw-r--  2.0 unx     2087 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender_base.py
+-rw-rw-r--  2.0 unx     3518 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_queries.py
+-rw-rw-r--  2.0 unx     1825 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_utils.py
+-rw-rw-r--  2.0 unx      252 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_version.py
+-rw-rw-r--  2.0 unx     5590 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_paging.py
+-rw-rw-r--  2.0 unx     1986 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/models/__init__.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/
+-rw-rw-r--  2.0 unx     1479 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/__init__.py
+-rw-rw-r--  2.0 unx    26062 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_index_client.py
+-rw-rw-r--  2.0 unx    30990 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_indexer_client.py
+-rw-rw-r--  2.0 unx     3140 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_utils.py
+-rw-rw-r--  2.0 unx    10333 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/__init__.py
+-rw-rw-r--  2.0 unx    44098 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_models.py
+-rw-rw-r--  2.0 unx      624 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_edm.py
+-rw-rw-r--  2.0 unx    43360 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_index.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/
+-rw-rw-r--  2.0 unx      742 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/__init__.py
+-rw-rw-r--  2.0 unx     5032 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_search_service_client.py
+-rw-rw-r--  2.0 unx    78836 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_serialization.py
+-rw-rw-r--  2.0 unx     1720 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_vendor.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_patch.py
+-rw-rw-r--  2.0 unx     2467 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_configuration.py
+-rw-rw-r--  2.0 unx   490136 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_models_py3.py
+-rw-rw-r--  2.0 unx    16331 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/__init__.py
+-rw-rw-r--  2.0 unx    70564 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_search_service_client_enums.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_patch.py
+-rw-rw-r--  2.0 unx     1220 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/__init__.py
+-rw-rw-r--  2.0 unx    34800 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_aliases_operations.py
+-rw-rw-r--  2.0 unx    33802 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py
+-rw-rw-r--  2.0 unx    49565 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexes_operations.py
+-rw-rw-r--  2.0 unx     5049 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_patch.py
+-rw-rw-r--  2.0 unx    35474 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py
+-rw-rw-r--  2.0 unx    57126 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexers_operations.py
+-rw-rw-r--  2.0 unx    44430 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/
+-rw-rw-r--  2.0 unx      742 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/__init__.py
+-rw-rw-r--  2.0 unx     5161 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_search_service_client.py
+-rw-rw-r--  2.0 unx      955 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_vendor.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_patch.py
+-rw-rw-r--  2.0 unx     2477 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_configuration.py
+-rw-rw-r--  2.0 unx     1220 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx    28586 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py
+-rw-rw-r--  2.0 unx    27236 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py
+-rw-rw-r--  2.0 unx    40542 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py
+-rw-rw-r--  2.0 unx     4160 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx    28599 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py
+-rw-rw-r--  2.0 unx    45076 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py
+-rw-rw-r--  2.0 unx    35901 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py
+-rw-rw-r--  2.0 unx     1479 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/__init__.py
+-rw-rw-r--  2.0 unx    27028 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_index_client.py
+-rw-rw-r--  2.0 unx    30044 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_indexer_client.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/
+-rw-rw-r--  2.0 unx      736 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/__init__.py
+-rw-rw-r--  2.0 unx     3793 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/_search_index_client.py
+-rw-rw-r--  2.0 unx    78836 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/_serialization.py
+-rw-rw-r--  2.0 unx     1200 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/_vendor.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/_patch.py
+-rw-rw-r--  2.0 unx     2699 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/_configuration.py
+-rw-rw-r--  2.0 unx   103482 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_models_py3.py
+-rw-rw-r--  2.0 unx     3704 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/__init__.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_patch.py
+-rw-rw-r--  2.0 unx    15706 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_search_index_client_enums.py
+-rw-rw-r--  2.0 unx    68985 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_documents_operations.py
+-rw-rw-r--  2.0 unx      687 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/__init__.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_patch.py
+drwxrwxr-x  2.0 unx        0 b- stor 23-Jul-10 15:20 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/
+-rw-rw-r--  2.0 unx      736 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/__init__.py
+-rw-rw-r--  2.0 unx     3902 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_search_index_client.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_patch.py
+-rw-rw-r--  2.0 unx     2709 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_configuration.py
+-rw-rw-r--  2.0 unx    51145 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_documents_operations.py
+-rw-rw-r--  2.0 unx      687 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/__init__.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_patch.py
+-rw-rw-r--  2.0 unx    15161 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_indexing_buffered_sender_async.py
+-rw-rw-r--  2.0 unx    37005 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_client_async.py
+-rw-rw-r--  2.0 unx     1553 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/__init__.py
+-rw-rw-r--  2.0 unx     5886 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/_index_documents_batch_async.py
+-rw-rw-r--  2.0 unx      673 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/_timer.py
+-rw-rw-r--  2.0 unx     5174 b- defN 23-Jul-10 15:19 azure-search-documents-11.4.0b4/azure/search/documents/aio/_paging.py
+198 files, 2268035 bytes uncompressed, 379692 bytes compressed:  83.3%
```

## zipnote {}

```diff
@@ -1,583 +1,595 @@
-Filename: azure-search-documents-11.4.0b3/
+Filename: azure-search-documents-11.4.0b4/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/
+Filename: azure-search-documents-11.4.0b4/samples/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/
+Filename: azure-search-documents-11.4.0b4/tests/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/
+Filename: azure-search-documents-11.4.0b4/azure/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/setup.cfg
+Filename: azure-search-documents-11.4.0b4/setup.cfg
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/README.md
+Filename: azure-search-documents-11.4.0b4/CHANGELOG.md
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/PKG-INFO
+Filename: azure-search-documents-11.4.0b4/README.md
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/setup.py
+Filename: azure-search-documents-11.4.0b4/pyproject.toml
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/MANIFEST.in
+Filename: azure-search-documents-11.4.0b4/setup.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/LICENSE
+Filename: azure-search-documents-11.4.0b4/TROUBLESHOOTING.md
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/CHANGELOG.md
+Filename: azure-search-documents-11.4.0b4/MANIFEST.in
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/
+Filename: azure-search-documents-11.4.0b4/PKG-INFO
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_indexers_operations.py
+Filename: azure-search-documents-11.4.0b4/LICENSE
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_authentication.py
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/top_level.txt
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_suggestions.py
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/not-zip-safe
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_analyze_text.py
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/requires.txt
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_simple_query.py
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/SOURCES.txt
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_autocomplete.py
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/dependency_links.txt
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/README.md
+Filename: azure-search-documents-11.4.0b4/azure_search_documents.egg-info/PKG-INFO
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_semantic_search.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_filter_query.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_synonym_map_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_facet_query.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_data_source_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_index_alias_crud_operations.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_filter_query.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_get_document.py
+Filename: azure-search-documents-11.4.0b4/samples/README.md
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_crud_operations.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_buffered_sender.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_index_crud_operations.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_semantic_search.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_buffered_sender.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_vector_search.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_data_source_operations.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_indexers_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_synonym_map_operations.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_index_alias_crud_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/sample_indexer_datasource_skillset.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_authentication.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_index_alias_crud_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_indexer_datasource_skillset.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_get_document_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_simple_query.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_crud_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_autocomplete.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_suggestions_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_suggestions.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_analyze_text_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_get_document.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_synonym_map_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_analyze_text.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_filter_query_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_facet_query.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_authentication_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_crud_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_indexers_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/sample_index_crud_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_facet_query_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_authentication_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_data_source_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_suggestions_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_buffered_sender_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_vector_search_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_autocomplete_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_indexers_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_simple_query_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_facet_query_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_index_crud_operations_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_index_crud_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/samples/async_samples/sample_semantic_search_async.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_analyze_text_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/SOURCES.txt
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_get_document_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/dependency_links.txt
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_data_source_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/not-zip-safe
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_synonym_map_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/top_level.txt
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_simple_query_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/PKG-INFO
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_index_alias_crud_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure_search_documents.egg-info/requires.txt
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_semantic_search_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_buffered_sender_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/perfstress_tests/
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_filter_query_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client_skillset_live.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_crud_operations_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_indexer_client_live.py
+Filename: azure-search-documents-11.4.0b4/samples/async_samples/sample_autocomplete_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client_live.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client_data_source_live.py
+Filename: azure-search-documents-11.4.0b4/tests/perfstress_tests/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/conftest.py
+Filename: azure-search-documents-11.4.0b4/tests/test_regex_flags.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_queries.py
+Filename: azure-search-documents-11.4.0b4/tests/test_index_documents_batch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_client_search_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_regex_flags.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_client_buffered_sender_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client_synonym_map_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client_alias_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client_alias_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_client_index_document_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_index_field_helpers.py
+Filename: azure-search-documents-11.4.0b4/tests/conftest.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_index_documents_batch.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_client_search_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_client_buffered_sender_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_index_client.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client_data_source_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/search_service_preparer.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client_skillset_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_client_basic_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client_synonym_map_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_client_index_document_live.py
+Filename: azure-search-documents-11.4.0b4/tests/test_buffered_sender.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_buffered_sender.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_indexer_client_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/test_search_client.py
+Filename: azure-search-documents-11.4.0b4/tests/test_index_field_helpers.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_alias_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_client_basic_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_indexer_client_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/test_search_index_client_live.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_search_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/search_service_preparer.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_synonym_map_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/test_queries.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_skillset_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_search_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_buffered_sender_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_data_source_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_alias_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_index_document_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_skillset_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_buffered_sender_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_buffered_sender_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_data_source_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_basic_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_live_async.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_basic_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/perfstress_tests/__init__.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_index_document_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/perfstress_tests/search_documents.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_indexer_client_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/perfstress_tests/autocomplete.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_buffered_sender_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/tests/perfstress_tests/suggest.py
+Filename: azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_synonym_map_live_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/
+Filename: azure-search-documents-11.4.0b4/tests/perfstress_tests/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/__init__.py
+Filename: azure-search-documents-11.4.0b4/tests/perfstress_tests/suggest.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/
+Filename: azure-search-documents-11.4.0b4/tests/perfstress_tests/search_documents.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/__init__.py
+Filename: azure-search-documents-11.4.0b4/tests/perfstress_tests/autocomplete.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/
+Filename: azure-search-documents-11.4.0b4/azure/search/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/
+Filename: azure-search-documents-11.4.0b4/azure/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/models/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/
+Filename: azure-search-documents-11.4.0b4/azure/search/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/models/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_api_versions.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_search_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_headers_mixin.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_paging.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_search_documents_error.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_queries.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender_base.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_index_documents_batch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_index_documents_batch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/py.typed
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_api_versions.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_search_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_utils.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_headers_mixin.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_version.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/py.typed
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_search_documents_error.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender_base.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_queries.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_utils.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_version.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_paging.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_index_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/models/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_utils.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_indexer_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_index_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_indexer_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_index_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_indexer_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_edm.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_utils.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_index.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_models.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_models.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_edm.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_index.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_search_service_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_serialization.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_search_service_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_configuration.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_serialization.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_vendor.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_vendor.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_configuration.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_models_py3.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_aliases_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexes_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_search_service_client_enums.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexers_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_aliases_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexes_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_search_service_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_configuration.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexers_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_vendor.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_search_service_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_vendor.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_configuration.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_models_py3.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_search_service_client_enums.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/_paging.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_client_async.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/_timer.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_index_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/_index_documents_batch_async.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_indexer_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_indexing_buffered_sender_async.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/models/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/_search_index_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/_serialization.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/_search_index_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/_vendor.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/_serialization.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/_configuration.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/_configuration.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_models_py3.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/_vendor.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_documents_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_search_index_client_enums.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_documents_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_search_index_client.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_configuration.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_search_index_client.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_documents_operations.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_configuration.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_documents_operations.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_search_index_client_enums.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/__init__.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/__init__.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_patch.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_models_py3.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_indexing_buffered_sender_async.py
 Comment: 
 
-Filename: azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_patch.py
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_client_async.py
+Comment: 
+
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/__init__.py
+Comment: 
+
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/_index_documents_batch_async.py
+Comment: 
+
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/_timer.py
+Comment: 
+
+Filename: azure-search-documents-11.4.0b4/azure/search/documents/aio/_paging.py
 Comment: 
 
 Zip file comment:
```

## Comparing `azure-search-documents-11.4.0b3/README.md` & `azure-search-documents-11.4.0b4/README.md`

 * *Files 16% similar despite different names*

```diff
@@ -30,19 +30,20 @@
 * Create and manage search indexes.
 * Upload and update documents in the search index.
 * Create and manage indexers that pull data from Azure into an index.
 * Create and manage skillsets that add AI enrichment to data ingestion.
 * Create and manage analyzers for advanced text analysis or multi-lingual content.
 * Optimize results through scoring profiles to factor in business logic or freshness.
 
-[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents) |
-[Package (PyPI)](https://pypi.org/project/azure-search-documents/) |
-[API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html) |
-[Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search) |
-[Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
+[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents)
+| [Package (PyPI)](https://pypi.org/project/azure-search-documents/)
+| [Package (Conda)](https://anaconda.org/microsoft/azure-search-documents/)
+| [API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html)
+| [Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search)
+| [Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
 
 ## _Disclaimer_
 
 _Azure SDK Python packages support for Python 2.7 has ended 01 January 2022. For more information and questions, please refer to https://github.com/Azure/azure-sdk-for-python/issues/20691_
 
 ## Getting started
 
@@ -67,50 +68,75 @@
 ```
 
 See [choosing a pricing tier](https://docs.microsoft.com/azure/search/search-sku-tier)
  for more information about available options.
 
 ### Authenticate the client
 
-All requests to a search service need an api-key that was generated specifically
-for your service. [The api-key is the sole mechanism for authenticating access to
-your search service endpoint.](https://docs.microsoft.com/azure/search/search-security-api-keys)
-You can obtain your api-key from the
-[Azure portal](https://portal.azure.com/) or via the Azure CLI:
+To interact with the Search service, you'll need to create an instance of the appropriate client class: `SearchClient` for searching indexed documents, `SearchIndexClient` for managing indexes, or `SearchIndexerClient` for crawling data sources and loading search documents into an index. To instantiate a client object, you'll need an **endpoint** and an **API key**. You can refer to the documentation for more information on [supported authenticating approaches](https://learn.microsoft.com/azure/search/search-security-overview#authentication) with the Search service.
+
+#### Get an API Key
+
+You can get the **endpoint** and an **API key** from the Search service in the [Azure Portal](https://portal.azure.com/). Please refer the [documentation](https://docs.microsoft.com/azure/search/search-security-api-keys) for instructions on how to get an API key.
+
+Alternatively, you can use the following [Azure CLI](https://learn.microsoft.com/cli/azure/) command to retrieve the API key from the Search service:
 
 ```Powershell
 az search admin-key show --service-name <mysearch> --resource-group <mysearch-rg>
 ```
 
 There are two types of keys used to access your search service: **admin**
 *(read-write)* and **query** *(read-only)* keys.  Restricting access and
 operations in client apps is essential to safeguarding the search assets on your
 service.  Always use a query key rather than an admin key for any query
 originating from a client app.
 
 *Note: The example Azure CLI snippet above retrieves an admin key so it's easier
 to get started exploring APIs, but it should be managed carefully.*
 
-We can use the api-key to create a new `SearchClient`.
+#### Create a SearchClient
+
+To instantiate the `SearchClient`, you'll need the **endpoint**, **API key** and **index name**:
+
+<!-- SNIPPET:sample_authentication.create_search_client_with_key -->
 
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "nycjobs"
-# Get the service endpoint and API key from the environment
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+key = os.getenv("AZURE_SEARCH_API_KEY")
 
-# Create a client
-credential = AzureKeyCredential(key)
-client = SearchClient(endpoint=endpoint,
-                      index_name=index_name,
-                      credential=credential)
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
+#### Create a client using Azure Active Directory authentication
+
+You can also create a `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient` using Azure Active Directory (AAD) authentication. Your user or service principal must be assigned the "Search Index Data Reader" role.
+Using the [DefaultAzureCredential](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#defaultazurecredential) you can authenticate a service using Managed Identity or a service principal, authenticate as a developer working on an application, and more all without changing code. Please refer the [documentation](https://learn.microsoft.com/azure/search/search-security-rbac?tabs=config-svc-portal%2Croles-portal%2Ctest-portal%2Ccustom-role-portal%2Cdisable-keys-portal) for instructions on how to connect to Azure Cognitive Search using Azure role-based access control (Azure RBAC).
+
+Before you can use the `DefaultAzureCredential`, or any credential type from [Azure.Identity](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md), you'll first need to [install the Azure.Identity package](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#install-the-package).
+
+To use `DefaultAzureCredential` with a client ID and secret, you'll need to set the `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET` environment variables; alternatively, you can pass those values
+to the `ClientSecretCredential` also in Azure.Identity.
+
+Make sure you use the right namespace for `DefaultAzureCredential` at the top of your source file:
+
+```python
+from azure.identity import DefaultAzureCredential
+from azure.search.documents import SearchClient
+
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+credential = DefaultAzureCredential()
+
+search_client = SearchClient(service_endpoint, index_name, credential)
 ```
 
 ## Key concepts
 
 An Azure Cognitive Search service contains one or more indexes that provide
 persistent storage of searchable data in the form of JSON documents.  _(If
 you're brand new to search, you can make a very rough analogy between
@@ -134,14 +160,28 @@
   * [Declare custom synonym maps to expand or rewrite queries](https://docs.microsoft.com/rest/api/searchservice/synonym-map-operations)
   * Most of the `SearchServiceClient` functionality is not yet available in our current preview
 
 * `SearchIndexerClient` allows you to:
   * [Start indexers to automatically crawl data sources](https://docs.microsoft.com/rest/api/searchservice/indexer-operations)
   * [Define AI powered Skillsets to transform and enrich your data](https://docs.microsoft.com/rest/api/searchservice/skillset-operations)
 
+Azure Cognitive Search provides two powerful features: **Semantic Search** and **Vector Search**.
+
+**Semantic Search** enhances the quality of search results for text-based queries. By enabling Semantic Search on your search service, you can improve the relevance of search results in two ways:
+- It applies secondary ranking to the initial result set, promoting the most semantically relevant results to the top.
+- It extracts and returns captions and answers in the response, which can be displayed on a search page to enhance the user's search experience.
+
+To learn more about Semantic Search, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview).
+
+**Vector Search** is an information retrieval technique that overcomes the limitations of traditional keyword-based search. Instead of relying solely on lexical analysis and matching individual query terms, Vector Search utilizes machine learning models to capture the contextual meaning of words and phrases. It represents documents and queries as vectors in a high-dimensional space called an embedding. By understanding the intent behind the query, Vector Search can deliver more relevant results that align with the user's requirements, even if the exact terms are not present in the document. Moreover, Vector Search can be applied to various types of content, including images and videos, not just text.
+
+To learn how to index vector fields and perform vector search, you can refer to the [sample](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py). This sample provides detailed guidance on indexing vector fields and demonstrates how to perform vector search.
+
+Additionally, for more comprehensive information about Vector Search, including its concepts and usage, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview). The documentation provides in-depth explanations and guidance on leveraging the power of Vector Search in Azure Cognitive Search.
+
 _The `Azure.Search.Documents` client library (v1) is a brand new offering for
 Python developers who want to use search technology in their applications.  There
 is an older, fully featured `Microsoft.Azure.Search` client library (v10) with
 many similar looking APIs, so please be careful to avoid confusion when
 exploring online resources._
 
 ## Examples
@@ -196,89 +236,65 @@
 
 ### Creating an index
 
 You can use the `SearchIndexClient` to create a search index. Fields can be
 defined using convenient `SimpleField`, `SearchableField`, or `ComplexField`
 models. Indexes can also define suggesters, lexical analyzers, and more.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents.indexes import SearchIndexClient
-from azure.search.documents.indexes.models import (
-    ComplexField,
-    CorsOptions,
-    SearchIndex,
-    ScoringProfile,
-    SearchFieldDataType,
-    SimpleField,
-    SearchableField
-)
-
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_index_crud_operations.create_index -->
 
-# Create a service client
-client = SearchIndexClient(endpoint, AzureKeyCredential(key))
-
-# Create the index
+```python
 name = "hotels"
 fields = [
-        SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-        SimpleField(name="baseRate", type=SearchFieldDataType.Double),
-        SearchableField(name="description", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
+    SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
+    SimpleField(name="baseRate", type=SearchFieldDataType.Double),
+    SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
+    ComplexField(
+        name="address",
+        fields=[
             SimpleField(name="streetAddress", type=SearchFieldDataType.String),
             SimpleField(name="city", type=SearchFieldDataType.String),
-        ])
-    ]
+        ],
+        collection=True,
+    ),
+]
 cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
 scoring_profiles = []
-
-index = SearchIndex(
-    name=name,
-    fields=fields,
-    scoring_profiles=scoring_profiles,
-    cors_options=cors_options)
+index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
 result = client.create_index(index)
 ```
 
+<!-- END SNIPPET -->
 
 ### Adding documents to your index
 
 You can `Upload`, `Merge`, `MergeOrUpload`, and `Delete` multiple documents from
 an index in a single batched request.  There are
 [a few special rules for merging](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents#document-actions)
 to be aware of.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents import SearchClient
-
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_crud_operations.upload_document -->
 
+```python
 DOCUMENT = {
-    'Category': 'Hotel',
-    'hotelId': '1000',
-    'rating': 4.0,
-    'rooms': [],
-    'hotelName': 'Azure Inn',
+    "Category": "Hotel",
+    "HotelId": "1000",
+    "Rating": 4.0,
+    "Rooms": [],
+    "HotelName": "Azure Inn",
 }
 
-search_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
-
 result = search_client.upload_documents(documents=[DOCUMENT])
 
 print("Upload of new document succeeded: {}".format(result[0].succeeded))
 ```
 
+<!-- END SNIPPET -->
+
 ### Authenticate in a National Cloud
 
 To authenticate in a [National Cloud](https://docs.microsoft.com/azure/active-directory/develop/authentication-national-cloud), you will need to make the following additions to your client configuration:
 
 - Set the `AuthorityHost` in the credential options or via the `AZURE_AUTHORITY_HOST` environment variable
 - Set the `audience` in `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient`
 
@@ -299,57 +315,58 @@
 ### Retrieving a specific document from your index
 
 In addition to querying for documents using keywords and optional filters,
 you can retrieve a specific document from your index if you already know the
 key. You could get the key from a query, for example, and want to show more
 information about it or navigate your customer to that document.
 
+<!-- SNIPPET:sample_get_document.get_document -->
+
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
-
-client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-result = client.get_document(key="1")
+result = search_client.get_document(key="23")
 
-print("Details for hotel '1' are:")
+print("Details for hotel '23' are:")
 print("        Name: {}".format(result["HotelName"]))
 print("      Rating: {}".format(result["Rating"]))
 print("    Category: {}".format(result["Category"]))
 ```
 
+<!-- END SNIPPET -->
 
 ### Async APIs
+
 This library includes a complete async API. To use it, you must
 first install an async transport, such as [aiohttp](https://pypi.org/project/aiohttp/).
 See
 [azure-core documentation](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md#transport)
 for more information.
 
+<!-- SNIPPET:sample_simple_query_async.simple_query_async -->
 
-```py
+```python
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchClient
 
-client = SearchClient(endpoint, index_name, AzureKeyCredential(api_key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-async with client:
-  results = await client.search(search_text="hotel")
-  async for result in results:
-    print("{}: {})".format(result["hotelId"], result["hotelName"]))
-
-...
+async with search_client:
+    results = await search_client.search(search_text="spa")
 
+    print("Hotels containing 'spa' in the name (or other fields):")
+    async for result in results:
+        print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
 ```
 
+<!-- END SNIPPET -->
+
 ## Troubleshooting
 
 ### General
 
 The Azure Cognitive Search client will raise exceptions defined in [Azure Core][azure_core].
 
 ### Logging
```

## Comparing `azure-search-documents-11.4.0b3/PKG-INFO` & `azure-search-documents-11.4.0b4/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azure-search-documents
-Version: 11.4.0b3
+Version: 11.4.0b4
 Summary: Microsoft Azure Cognitive Search Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents
 Author: Microsoft Corporation
 Author-email: ascl@microsoft.com
 License: MIT License
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python
@@ -52,19 +52,20 @@
 * Create and manage search indexes.
 * Upload and update documents in the search index.
 * Create and manage indexers that pull data from Azure into an index.
 * Create and manage skillsets that add AI enrichment to data ingestion.
 * Create and manage analyzers for advanced text analysis or multi-lingual content.
 * Optimize results through scoring profiles to factor in business logic or freshness.
 
-[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents) |
-[Package (PyPI)](https://pypi.org/project/azure-search-documents/) |
-[API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html) |
-[Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search) |
-[Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
+[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents)
+| [Package (PyPI)](https://pypi.org/project/azure-search-documents/)
+| [Package (Conda)](https://anaconda.org/microsoft/azure-search-documents/)
+| [API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html)
+| [Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search)
+| [Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
 
 ## _Disclaimer_
 
 _Azure SDK Python packages support for Python 2.7 has ended 01 January 2022. For more information and questions, please refer to https://github.com/Azure/azure-sdk-for-python/issues/20691_
 
 ## Getting started
 
@@ -89,50 +90,75 @@
 ```
 
 See [choosing a pricing tier](https://docs.microsoft.com/azure/search/search-sku-tier)
  for more information about available options.
 
 ### Authenticate the client
 
-All requests to a search service need an api-key that was generated specifically
-for your service. [The api-key is the sole mechanism for authenticating access to
-your search service endpoint.](https://docs.microsoft.com/azure/search/search-security-api-keys)
-You can obtain your api-key from the
-[Azure portal](https://portal.azure.com/) or via the Azure CLI:
+To interact with the Search service, you'll need to create an instance of the appropriate client class: `SearchClient` for searching indexed documents, `SearchIndexClient` for managing indexes, or `SearchIndexerClient` for crawling data sources and loading search documents into an index. To instantiate a client object, you'll need an **endpoint** and an **API key**. You can refer to the documentation for more information on [supported authenticating approaches](https://learn.microsoft.com/azure/search/search-security-overview#authentication) with the Search service.
+
+#### Get an API Key
+
+You can get the **endpoint** and an **API key** from the Search service in the [Azure Portal](https://portal.azure.com/). Please refer the [documentation](https://docs.microsoft.com/azure/search/search-security-api-keys) for instructions on how to get an API key.
+
+Alternatively, you can use the following [Azure CLI](https://learn.microsoft.com/cli/azure/) command to retrieve the API key from the Search service:
 
 ```Powershell
 az search admin-key show --service-name <mysearch> --resource-group <mysearch-rg>
 ```
 
 There are two types of keys used to access your search service: **admin**
 *(read-write)* and **query** *(read-only)* keys.  Restricting access and
 operations in client apps is essential to safeguarding the search assets on your
 service.  Always use a query key rather than an admin key for any query
 originating from a client app.
 
 *Note: The example Azure CLI snippet above retrieves an admin key so it's easier
 to get started exploring APIs, but it should be managed carefully.*
 
-We can use the api-key to create a new `SearchClient`.
+#### Create a SearchClient
+
+To instantiate the `SearchClient`, you'll need the **endpoint**, **API key** and **index name**:
+
+<!-- SNIPPET:sample_authentication.create_search_client_with_key -->
 
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "nycjobs"
-# Get the service endpoint and API key from the environment
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+key = os.getenv("AZURE_SEARCH_API_KEY")
 
-# Create a client
-credential = AzureKeyCredential(key)
-client = SearchClient(endpoint=endpoint,
-                      index_name=index_name,
-                      credential=credential)
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
+#### Create a client using Azure Active Directory authentication
+
+You can also create a `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient` using Azure Active Directory (AAD) authentication. Your user or service principal must be assigned the "Search Index Data Reader" role.
+Using the [DefaultAzureCredential](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#defaultazurecredential) you can authenticate a service using Managed Identity or a service principal, authenticate as a developer working on an application, and more all without changing code. Please refer the [documentation](https://learn.microsoft.com/azure/search/search-security-rbac?tabs=config-svc-portal%2Croles-portal%2Ctest-portal%2Ccustom-role-portal%2Cdisable-keys-portal) for instructions on how to connect to Azure Cognitive Search using Azure role-based access control (Azure RBAC).
+
+Before you can use the `DefaultAzureCredential`, or any credential type from [Azure.Identity](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md), you'll first need to [install the Azure.Identity package](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#install-the-package).
+
+To use `DefaultAzureCredential` with a client ID and secret, you'll need to set the `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET` environment variables; alternatively, you can pass those values
+to the `ClientSecretCredential` also in Azure.Identity.
+
+Make sure you use the right namespace for `DefaultAzureCredential` at the top of your source file:
+
+```python
+from azure.identity import DefaultAzureCredential
+from azure.search.documents import SearchClient
+
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+credential = DefaultAzureCredential()
+
+search_client = SearchClient(service_endpoint, index_name, credential)
 ```
 
 ## Key concepts
 
 An Azure Cognitive Search service contains one or more indexes that provide
 persistent storage of searchable data in the form of JSON documents.  _(If
 you're brand new to search, you can make a very rough analogy between
@@ -156,14 +182,28 @@
   * [Declare custom synonym maps to expand or rewrite queries](https://docs.microsoft.com/rest/api/searchservice/synonym-map-operations)
   * Most of the `SearchServiceClient` functionality is not yet available in our current preview
 
 * `SearchIndexerClient` allows you to:
   * [Start indexers to automatically crawl data sources](https://docs.microsoft.com/rest/api/searchservice/indexer-operations)
   * [Define AI powered Skillsets to transform and enrich your data](https://docs.microsoft.com/rest/api/searchservice/skillset-operations)
 
+Azure Cognitive Search provides two powerful features: **Semantic Search** and **Vector Search**.
+
+**Semantic Search** enhances the quality of search results for text-based queries. By enabling Semantic Search on your search service, you can improve the relevance of search results in two ways:
+- It applies secondary ranking to the initial result set, promoting the most semantically relevant results to the top.
+- It extracts and returns captions and answers in the response, which can be displayed on a search page to enhance the user's search experience.
+
+To learn more about Semantic Search, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview).
+
+**Vector Search** is an information retrieval technique that overcomes the limitations of traditional keyword-based search. Instead of relying solely on lexical analysis and matching individual query terms, Vector Search utilizes machine learning models to capture the contextual meaning of words and phrases. It represents documents and queries as vectors in a high-dimensional space called an embedding. By understanding the intent behind the query, Vector Search can deliver more relevant results that align with the user's requirements, even if the exact terms are not present in the document. Moreover, Vector Search can be applied to various types of content, including images and videos, not just text.
+
+To learn how to index vector fields and perform vector search, you can refer to the [sample](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py). This sample provides detailed guidance on indexing vector fields and demonstrates how to perform vector search.
+
+Additionally, for more comprehensive information about Vector Search, including its concepts and usage, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview). The documentation provides in-depth explanations and guidance on leveraging the power of Vector Search in Azure Cognitive Search.
+
 _The `Azure.Search.Documents` client library (v1) is a brand new offering for
 Python developers who want to use search technology in their applications.  There
 is an older, fully featured `Microsoft.Azure.Search` client library (v10) with
 many similar looking APIs, so please be careful to avoid confusion when
 exploring online resources._
 
 ## Examples
@@ -218,89 +258,65 @@
 
 ### Creating an index
 
 You can use the `SearchIndexClient` to create a search index. Fields can be
 defined using convenient `SimpleField`, `SearchableField`, or `ComplexField`
 models. Indexes can also define suggesters, lexical analyzers, and more.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents.indexes import SearchIndexClient
-from azure.search.documents.indexes.models import (
-    ComplexField,
-    CorsOptions,
-    SearchIndex,
-    ScoringProfile,
-    SearchFieldDataType,
-    SimpleField,
-    SearchableField
-)
-
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_index_crud_operations.create_index -->
 
-# Create a service client
-client = SearchIndexClient(endpoint, AzureKeyCredential(key))
-
-# Create the index
+```python
 name = "hotels"
 fields = [
-        SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-        SimpleField(name="baseRate", type=SearchFieldDataType.Double),
-        SearchableField(name="description", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
+    SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
+    SimpleField(name="baseRate", type=SearchFieldDataType.Double),
+    SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
+    ComplexField(
+        name="address",
+        fields=[
             SimpleField(name="streetAddress", type=SearchFieldDataType.String),
             SimpleField(name="city", type=SearchFieldDataType.String),
-        ])
-    ]
+        ],
+        collection=True,
+    ),
+]
 cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
 scoring_profiles = []
-
-index = SearchIndex(
-    name=name,
-    fields=fields,
-    scoring_profiles=scoring_profiles,
-    cors_options=cors_options)
+index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
 result = client.create_index(index)
 ```
 
+<!-- END SNIPPET -->
 
 ### Adding documents to your index
 
 You can `Upload`, `Merge`, `MergeOrUpload`, and `Delete` multiple documents from
 an index in a single batched request.  There are
 [a few special rules for merging](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents#document-actions)
 to be aware of.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents import SearchClient
-
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_crud_operations.upload_document -->
 
+```python
 DOCUMENT = {
-    'Category': 'Hotel',
-    'hotelId': '1000',
-    'rating': 4.0,
-    'rooms': [],
-    'hotelName': 'Azure Inn',
+    "Category": "Hotel",
+    "HotelId": "1000",
+    "Rating": 4.0,
+    "Rooms": [],
+    "HotelName": "Azure Inn",
 }
 
-search_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
-
 result = search_client.upload_documents(documents=[DOCUMENT])
 
 print("Upload of new document succeeded: {}".format(result[0].succeeded))
 ```
 
+<!-- END SNIPPET -->
+
 ### Authenticate in a National Cloud
 
 To authenticate in a [National Cloud](https://docs.microsoft.com/azure/active-directory/develop/authentication-national-cloud), you will need to make the following additions to your client configuration:
 
 - Set the `AuthorityHost` in the credential options or via the `AZURE_AUTHORITY_HOST` environment variable
 - Set the `audience` in `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient`
 
@@ -321,57 +337,58 @@
 ### Retrieving a specific document from your index
 
 In addition to querying for documents using keywords and optional filters,
 you can retrieve a specific document from your index if you already know the
 key. You could get the key from a query, for example, and want to show more
 information about it or navigate your customer to that document.
 
+<!-- SNIPPET:sample_get_document.get_document -->
+
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
-
-client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-result = client.get_document(key="1")
+result = search_client.get_document(key="23")
 
-print("Details for hotel '1' are:")
+print("Details for hotel '23' are:")
 print("        Name: {}".format(result["HotelName"]))
 print("      Rating: {}".format(result["Rating"]))
 print("    Category: {}".format(result["Category"]))
 ```
 
+<!-- END SNIPPET -->
 
 ### Async APIs
+
 This library includes a complete async API. To use it, you must
 first install an async transport, such as [aiohttp](https://pypi.org/project/aiohttp/).
 See
 [azure-core documentation](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md#transport)
 for more information.
 
+<!-- SNIPPET:sample_simple_query_async.simple_query_async -->
 
-```py
+```python
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchClient
 
-client = SearchClient(endpoint, index_name, AzureKeyCredential(api_key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-async with client:
-  results = await client.search(search_text="hotel")
-  async for result in results:
-    print("{}: {})".format(result["hotelId"], result["hotelName"]))
-
-...
+async with search_client:
+    results = await search_client.search(search_text="spa")
 
+    print("Hotels containing 'spa' in the name (or other fields):")
+    async for result in results:
+        print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
 ```
 
+<!-- END SNIPPET -->
+
 ## Troubleshooting
 
 ### General
 
 The Azure Cognitive Search client will raise exceptions defined in [Azure Core][azure_core].
 
 ### Logging
```

## Comparing `azure-search-documents-11.4.0b3/setup.py` & `azure-search-documents-11.4.0b4/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,52 +18,53 @@
 
 # a-b-c => a/b/c
 PACKAGE_FOLDER_PATH = PACKAGE_NAME.replace("-", "/")
 # a-b-c => a.b.c
 NAMESPACE_NAME = PACKAGE_NAME.replace("-", ".")
 
 # Version extraction inspired from 'requests'
-with open(os.path.join(PACKAGE_FOLDER_PATH, '_version.py'), 'r') as fd:
-    version = re.search(r'^VERSION\s*=\s*[\'"]([^\'"]*)[\'"]',
-                        fd.read(), re.MULTILINE).group(1)
+with open(os.path.join(PACKAGE_FOLDER_PATH, "_version.py"), "r") as fd:
+    version = re.search(r'^VERSION\s*=\s*[\'"]([^\'"]*)[\'"]', fd.read(), re.MULTILINE).group(1)
 
 if not version:
-    raise RuntimeError('Cannot find version information')
+    raise RuntimeError("Cannot find version information")
 
 setup(
     name=PACKAGE_NAME,
     version=version,
     include_package_data=True,
-    description='Microsoft {} Client Library for Python'.format(PACKAGE_PPRINT_NAME),
-    long_description=open('README.md', 'r').read(),
-    long_description_content_type='text/markdown',
-    license='MIT License',
-    author='Microsoft Corporation',
-    author_email='ascl@microsoft.com',
-    url='https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents',
+    description="Microsoft {} Client Library for Python".format(PACKAGE_PPRINT_NAME),
+    long_description=open("README.md", "r").read(),
+    long_description_content_type="text/markdown",
+    license="MIT License",
+    author="Microsoft Corporation",
+    author_email="ascl@microsoft.com",
+    url="https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents",
     classifiers=[
         "Development Status :: 4 - Beta",
-        'Programming Language :: Python',
-        'Programming Language :: Python :: 3 :: Only',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.7',
-        'Programming Language :: Python :: 3.8',
-        'Programming Language :: Python :: 3.9',
-        'Programming Language :: Python :: 3.10',
-        'Programming Language :: Python :: 3.11',
-        'License :: OSI Approved :: MIT License',
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3 :: Only",
+        "Programming Language :: Python :: 3",
+        "Programming Language :: Python :: 3.7",
+        "Programming Language :: Python :: 3.8",
+        "Programming Language :: Python :: 3.9",
+        "Programming Language :: Python :: 3.10",
+        "Programming Language :: Python :: 3.11",
+        "License :: OSI Approved :: MIT License",
     ],
     zip_safe=False,
-    packages=find_packages(exclude=[
-        'samples',
-        'tests',
-        # Exclude packages that will be covered by PEP420 or nspkg
-        'azure',
-        'azure.search',
-    ]),
+    packages=find_packages(
+        exclude=[
+            "samples",
+            "tests",
+            # Exclude packages that will be covered by PEP420 or nspkg
+            "azure",
+            "azure.search",
+        ]
+    ),
     python_requires=">=3.7",
     install_requires=[
         "azure-core<2.0.0,>=1.24.0",
         "azure-common~=1.1",
         "isodate>=0.6.0",
     ],
 )
```

## Comparing `azure-search-documents-11.4.0b3/LICENSE` & `azure-search-documents-11.4.0b4/LICENSE`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/CHANGELOG.md` & `azure-search-documents-11.4.0b4/CHANGELOG.md`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,19 @@
 # Release History
 
+## 11.4.0b4 (2023-07-11)
+
+### Features Added
+
+- Added `VectorSearch` support.
+
+### Breaking Changes
+
+- Deprecated `SentimentSkillV1` and `EntityRecognitionSkillV1`.
+
 ## 11.4.0b3 (2023-02-07)
 
 ### Features Added
 
 - Added the semantic reranker score and captions on `SearchResult`.(thanks to @LucasVascovici for the contribution)
 
 ## 11.4.0b2 (2022-11-08)
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_indexers_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_indexers_operations.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,94 +27,97 @@
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.models import (
     SearchIndexerDataContainer,
     SearchIndexerDataSourceConnection,
     SearchIndex,
     SearchIndexer,
     SimpleField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient
 
 indexers_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
 
+
 def create_indexer():
     # create an index
     index_name = "indexer-hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-        SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+        SimpleField(name="baseRate", type=SearchFieldDataType.Double),
     ]
     index = SearchIndex(name=index_name, fields=fields)
     ind_client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
     ind_client.create_index(index)
 
     # [START create_indexer]
     # create a datasource
-    container = SearchIndexerDataContainer(name='searchcontainer')
+    container = SearchIndexerDataContainer(name="searchcontainer")
     data_source_connection = SearchIndexerDataSourceConnection(
-        name="indexer-datasource",
-        type="azureblob",
-        connection_string=connection_string,
-        container=container
+        name="indexer-datasource", type="azureblob", connection_string=connection_string, container=container
     )
     data_source = indexers_client.create_data_source_connection(data_source_connection)
 
     # create an indexer
     indexer = SearchIndexer(
-        name="sample-indexer",
-        data_source_name="indexer-datasource",
-        target_index_name="indexer-hotels"
+        name="sample-indexer", data_source_name="indexer-datasource", target_index_name="indexer-hotels"
     )
     result = indexers_client.create_indexer(indexer)
     print("Create new Indexer - sample-indexer")
     # [END create_indexer]
 
+
 def list_indexers():
     # [START list_indexer]
     result = indexers_client.get_indexers()
     names = [x.name for x in result]
     print("Found {} Indexers in the service: {}".format(len(result), ", ".join(names)))
     # [END list_indexer]
 
+
 def get_indexer():
     # [START get_indexer]
     result = indexers_client.get_indexer("sample-indexer")
     print("Retrived Indexer 'sample-indexer'")
     return result
     # [END get_indexer]
 
+
 def get_indexer_status():
     # [START get_indexer_status]
     result = indexers_client.get_indexer_status("sample-indexer")
     print("Retrived Indexer status for 'sample-indexer'")
     return result
     # [END get_indexer_status]
 
+
 def run_indexer():
     # [START run_indexer]
     result = indexers_client.run_indexer("sample-indexer")
     print("Ran the Indexer 'sample-indexer'")
     return result
     # [END run_indexer]
 
+
 def reset_indexer():
     # [START reset_indexer]
     result = indexers_client.reset_indexer("sample-indexer")
     print("Reset the Indexer 'sample-indexer'")
     return result
     # [END reset_indexer]
 
+
 def delete_indexer():
     # [START delete_indexer]
     indexers_client.delete_indexer("sample-indexer")
     print("Indexer 'sample-indexer' successfully deleted")
     # [END delete_indexer]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     create_indexer()
     list_indexers()
     get_indexer()
     get_indexer_status()
     run_indexer()
     reset_indexer()
     delete_indexer()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_authentication.py` & `azure-search-documents-11.4.0b4/samples/sample_authentication.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,14 +19,15 @@
     1) AZURE_SEARCH_SERVICE_ENDPOINT - the endpoint of your Azure Cognitive Search service
     2) AZURE_SEARCH_INDEX_NAME - the name of your search index (e.g. "hotels-sample-index")
     3) AZURE_SEARCH_API_KEY - your search API key
 """
 
 import os
 
+
 def authentication_with_api_key_credential():
     # [START create_search_client_with_key]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
@@ -35,21 +36,23 @@
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
     # [END create_search_client_with_key]
 
     result = search_client.get_document_count()
 
     print("There are {} documents in the {} search index.".format(result, repr(index_name)))
 
+
 def authentication_service_client_with_api_key_credential():
     # [START create_search_service_client_with_key]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.indexes import SearchIndexClient
 
     service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     key = os.getenv("AZURE_SEARCH_API_KEY")
 
     search_client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
     # [END create_search_service_client_with_key]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     authentication_with_api_key_credential()
-    authentication_service_client_with_api_key_credential()
+    authentication_service_client_with_api_key_credential()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_suggestions.py` & `azure-search-documents-11.4.0b4/samples/sample_suggestions.py`

 * *Files 7% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def suggest_query():
     # [START suggest_query]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -37,9 +38,10 @@
 
     print("Search suggestions for 'coffee'")
     for result in results:
         hotel = search_client.get_document(key=result["HotelId"])
         print("    Text: {} for Hotel: {}".format(repr(result["text"]), hotel["HotelName"]))
     # [END suggest_query]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     suggest_query()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_analyze_text.py` & `azure-search-documents-11.4.0b4/samples/sample_analyze_text.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,23 +21,25 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def simple_analyze_text():
     # [START simple_analyze_text]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.indexes import SearchIndexClient
     from azure.search.documents.indexes.models import AnalyzeTextOptions
 
     client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
     analyze_request = AnalyzeTextOptions(text="One's <two/>", analyzer_name="standard.lucene")
 
     result = client.analyze_text(index_name, analyze_request)
     print(result.as_dict())
     # [END simple_analyze_text]
 
-if __name__ == '__main__':
-    simple_analyze_text()
+
+if __name__ == "__main__":
+    simple_analyze_text()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_simple_query.py` & `azure-search-documents-11.4.0b4/samples/sample_simple_query.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,23 +22,25 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def simple_text_query():
     # [START simple_query]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
     results = search_client.search(search_text="spa")
 
     print("Hotels containing 'spa' in the name (or other fields):")
     for result in results:
         print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
     # [END simple_query]
 
-if __name__ == '__main__':
-    simple_text_query()
+
+if __name__ == "__main__":
+    simple_text_query()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_autocomplete.py` & `azure-search-documents-11.4.0b4/samples/sample_autocomplete.py`

 * *Files 8% similar despite different names*

```diff
@@ -22,23 +22,25 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def autocomplete_query():
     # [START autocomplete_query]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
     results = search_client.autocomplete(search_text="bo", suggester_name="sg")
 
     print("Autocomplete suggestions for 'bo'")
     for result in results:
         print("    Completion: {}".format(result["text"]))
     # [END autocomplete_query]
 
-if __name__ == '__main__':
-    autocomplete_query()
+
+if __name__ == "__main__":
+    autocomplete_query()
```

## Comparing `azure-search-documents-11.4.0b3/samples/README.md` & `azure-search-documents-11.4.0b4/samples/README.md`

 * *Files 14% similar despite different names*

```diff
@@ -38,14 +38,16 @@
 
 * CRUD operations for indexers: [sample_indexers_operations.py](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_indexers_operations.py) ([async version](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/async_samples/sample_indexers_operations_async.py))
 
 * General workflow of indexer, datasource and index: [sample_indexer_datasource_skillset.py](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_indexer_datasource_skillset.py)
 
 * Semantic search: [sample_semantic_search.py](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_semantic_search.py)
 
+* Vector search: [sample_vector_search.py](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py) ([async version](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/async_samples/sample_vector_search_async.py))
+
 ## Prerequisites
 
 * Python 3.7 or later is required to use this package
 * You must have an [Azure subscription](https://azure.microsoft.com/free/)
 * You must create the "Hotels" sample index [in the Azure Portal](https://docs.microsoft.com/azure/search/search-get-started-portal)
 
 ## Setup
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_semantic_search.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_semantic_search_async.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,62 +3,62 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: sample_semantic_search.py
+FILE: sample_semantic_search_async.py
 DESCRIPTION:
     This sample demonstrates how to use semantic search.
 USAGE:
-    python sample_semantic_search.py
+    python sample_semantic_search_async.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_SEARCH_SERVICE_ENDPOINT - the endpoint of your Azure Cognitive Search service
     2) AZURE_SEARCH_INDEX_NAME - the name of your search index (e.g. "hotels-sample-index")
     3) AZURE_SEARCH_API_KEY - your search API key
 """
 
 import os
+import asyncio
 
-def speller():
-    # [START speller]
+
+async def speller():
+    # [START speller_async]
     from azure.core.credentials import AzureKeyCredential
-    from azure.search.documents import SearchClient
+    from azure.search.documents.aio import SearchClient
 
     endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
     api_key = os.getenv("AZURE_SEARCH_API_KEY")
 
     credential = AzureKeyCredential(api_key)
-    client = SearchClient(endpoint=endpoint,
-                          index_name=index_name,
-                          credential=credential)
-    results = list(client.search(search_text="luxury", query_language="en-us", query_speller="lexicon"))
+    client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)
+    results = await client.search(search_text="luxury", query_language="en-us", query_speller="lexicon")
 
-    for result in results:
+    async for result in results:
         print("{}\n{}\n)".format(result["HotelId"], result["HotelName"]))
-    # [END speller]
+    # [END speller_async]
 
-def semantic_ranking():
-    # [START semantic_ranking]
+
+async def semantic_ranking():
+    # [START semantic_ranking_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
     api_key = os.getenv("AZURE_SEARCH_API_KEY")
 
     credential = AzureKeyCredential(api_key)
-    client = SearchClient(endpoint=endpoint,
-                          index_name=index_name,
-                          credential=credential)
+    client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)
     results = list(client.search(search_text="luxury", query_type="semantic", query_language="en-us"))
 
     for result in results:
         print("{}\n{}\n)".format(result["HotelId"], result["HotelName"]))
-    # [END semantic_ranking]
+    # [END semantic_ranking_async]
+
 
-if __name__ == '__main__':
-    speller()
-    semantic_ranking()
+if __name__ == "__main__":
+    asyncio.run(speller())
+    asyncio.run(semantic_ranking())
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_filter_query.py` & `azure-search-documents-11.4.0b4/samples/sample_filter_query.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,29 +22,31 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def filter_query():
     # [START filter_query]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
     select = ("HotelName", "Rating")
     results = search_client.search(
         search_text="WiFi",
         filter="Address/StateProvince eq 'FL' and Address/Country eq 'USA'",
         select=",".join(select),
-        order_by="Rating desc"
+        order_by="Rating desc",
     )
 
     print("Florida hotels containing 'WiFi', sorted by Rating:")
     for result in results:
         print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
     # [END filter_query]
 
-if __name__ == '__main__':
-    filter_query()
+
+if __name__ == "__main__":
+    filter_query()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_facet_query.py` & `azure-search-documents-11.4.0b4/samples/sample_facet_query.py`

 * *Files 4% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def filter_query():
     # [START facet_query]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -38,9 +39,10 @@
     facets = results.get_facets()
 
     print("Catgory facet counts for hotels:")
     for facet in facets["Category"]:
         print("    {}".format(facet))
     # [END facet_query]
 
-if __name__ == '__main__':
-    filter_query()
+
+if __name__ == "__main__":
+    filter_query()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_index_alias_crud_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_index_alias_crud_operations.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,72 +29,77 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexClient
 from azure.search.documents.indexes.models import (
     ComplexField,
     CorsOptions,
     ScoringProfile,
-    SearchAlias, 
-    SearchIndex, 
-    SimpleField, 
+    SearchAlias,
+    SearchIndex,
+    SimpleField,
     SearchableField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 def create_alias():
     # [START create_alias]
-    alias = SearchAlias(name = alias_name, indexes = [index_name])
+    alias = SearchAlias(name=alias_name, indexes=[index_name])
     result = client.create_alias(alias)
-    # [END create_index]
+    # [END create_alias]
+
 
 def get_alias():
     # [START get_alias]
     result = client.get_alias(alias_name)
     # [END get_alias]
 
+
 def update_alias():
     # [START update_alias]
     new_index_name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
         SearchableField(name="hotelName", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-            SimpleField(name="state", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+                SimpleField(name="state", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-    scoring_profile = ScoringProfile(
-        name="MyProfile"
-    )
+    scoring_profile = ScoringProfile(name="MyProfile")
     scoring_profiles = []
     scoring_profiles.append(scoring_profile)
     index = SearchIndex(
-        name=new_index_name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+        name=new_index_name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options
+    )
 
     result_index = client.create_or_update_index(index=index)
 
-    alias = SearchAlias(name = alias_name, indexes = [new_index_name])
+    alias = SearchAlias(name=alias_name, indexes=[new_index_name])
     result = client.create_or_update_alias(alias)
 
     # [END update_alias]
 
+
 def delete_alias():
     # [START delete_alias]
 
     client.delete_alias(alias_name)
     # [END delete_alias]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     create_alias()
     get_alias()
     update_alias()
     delete_alias()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_get_document.py` & `azure-search-documents-11.4.0b4/samples/sample_get_document.py`

 * *Files 8% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 
 import os
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 def get_document():
     # [START get_document]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -37,9 +38,10 @@
 
     print("Details for hotel '23' are:")
     print("        Name: {}".format(result["HotelName"]))
     print("      Rating: {}".format(result["Rating"]))
     print("    Category: {}".format(result["Category"]))
     # [END get_document]
 
-if __name__ == '__main__':
-    get_document()
+
+if __name__ == "__main__":
+    get_document()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_crud_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_crud_operations.py`

 * *Files 9% similar despite different names*

```diff
@@ -27,40 +27,44 @@
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
 search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
+
 def upload_document():
     # [START upload_document]
     DOCUMENT = {
-        'Category': 'Hotel',
-        'HotelId': '1000',
-        'Rating': 4.0,
-        'Rooms': [],
-        'HotelName': 'Azure Inn',
+        "Category": "Hotel",
+        "HotelId": "1000",
+        "Rating": 4.0,
+        "Rooms": [],
+        "HotelName": "Azure Inn",
     }
 
     result = search_client.upload_documents(documents=[DOCUMENT])
 
     print("Upload of new document succeeded: {}".format(result[0].succeeded))
     # [END upload_document]
 
+
 def merge_document():
     # [START merge_document]
     result = search_client.merge_documents(documents=[{"HotelId": "1000", "Rating": 4.5}])
 
     print("Merge into new document succeeded: {}".format(result[0].succeeded))
     # [END merge_document]
 
+
 def delete_document():
     # [START delete_document]
     result = search_client.delete_documents(documents=[{"HotelId": "1000"}])
 
     print("Delete new document succeeded: {}".format(result[0].succeeded))
     # [END delete_document]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     upload_document()
     merge_document()
     delete_document()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_index_crud_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_index_crud_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,76 +34,79 @@
     SearchFieldDataType,
     SimpleField,
     SearchableField,
 )
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 def create_index():
     # [START create_index]
     name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
     scoring_profiles = []
-    index = SearchIndex(
-        name=name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+    index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
     result = client.create_index(index)
     # [END create_index]
 
+
 def get_index():
     # [START get_index]
     name = "hotels"
     result = client.get_index(name)
     # [END get_index]
 
+
 def update_index():
     # [START update_index]
     name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
         SearchableField(name="hotelName", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-            SimpleField(name="state", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+                SimpleField(name="state", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-    scoring_profile = ScoringProfile(
-        name="MyProfile"
-    )
+    scoring_profile = ScoringProfile(name="MyProfile")
     scoring_profiles = []
     scoring_profiles.append(scoring_profile)
-    index = SearchIndex(
-        name=name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+    index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
     result = client.create_or_update_index(index=index)
     # [END update_index]
 
+
 def delete_index():
     # [START delete_index]
     name = "hotels"
     client.delete_index(name)
     # [END delete_index]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     create_index()
     get_index()
     update_index()
     delete_index()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_buffered_sender.py` & `azure-search-documents-11.4.0b4/samples/sample_buffered_sender.py`

 * *Files 10% similar despite different names*

```diff
@@ -27,27 +27,25 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchIndexingBufferedSender
 
 
 def sample_batching_client():
     DOCUMENT = {
-        'Category': 'Hotel',
-        'HotelId': '1000',
-        'Rating': 4.0,
-        'Rooms': [],
-        'HotelName': 'Azure Inn',
+        "Category": "Hotel",
+        "HotelId": "1000",
+        "Rating": 4.0,
+        "Rooms": [],
+        "HotelName": "Azure Inn",
     }
 
-    with SearchIndexingBufferedSender(
-            service_endpoint,
-            index_name,
-            AzureKeyCredential(key)) as batch_client:
+    with SearchIndexingBufferedSender(service_endpoint, index_name, AzureKeyCredential(key)) as batch_client:
         # add upload actions
         batch_client.upload_documents(documents=[DOCUMENT])
         # add merge actions
         batch_client.merge_documents(documents=[{"HotelId": "1000", "Rating": 4.5}])
         # add delete actions
         batch_client.delete_documents(documents=[{"HotelId": "1000"}])
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     sample_batching_client()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_data_source_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_data_source_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,45 +26,47 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexerClient
 from azure.search.documents.indexes.models import SearchIndexerDataContainer, SearchIndexerDataSourceConnection
 
 client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
 
+
 def create_data_source_connection():
     # [START create_data_source_connection]
-    container = SearchIndexerDataContainer(name='searchcontainer')
+    container = SearchIndexerDataContainer(name="searchcontainer")
     data_source_connection = SearchIndexerDataSourceConnection(
-        name="sample-data-source-connection",
-        type="azureblob",
-        connection_string=connection_string,
-        container=container
+        name="sample-data-source-connection", type="azureblob", connection_string=connection_string, container=container
     )
     result = client.create_data_source_connection(data_source_connection)
     print(result)
     print("Create new Data Source Connection - sample-data-source-connection")
     # [END create_data_source_connection]
 
+
 def list_data_source_connections():
     # [START list_data_source_connection]
     result = client.get_data_source_connections()
     names = [ds.name for ds in result]
     print("Found {} Data Source Connections in the service: {}".format(len(result), ", ".join(names)))
     # [END list_data_source_connection]
 
+
 def get_data_source_connection():
     # [START get_data_source_connection]
     result = client.get_data_source_connection("sample-data-source-connection")
     print("Retrived Data Source Connection 'sample-data-source-connection'")
     # [END get_data_source_connection]
 
+
 def delete_data_source_connection():
     # [START delete_data_source_connection]
     client.delete_data_source_connection("sample-data-source-connection")
     print("Data Source Connection 'sample-data-source-connection' successfully deleted")
     # [END delete_data_source_connection]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     create_data_source_connection()
     list_data_source_connections()
     get_data_source_connection()
     delete_data_source_connection()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_synonym_map_operations.py` & `azure-search-documents-11.4.0b4/samples/sample_synonym_map_operations.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,57 +25,64 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexClient
 from azure.search.documents.indexes.models import SynonymMap
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 def create_synonym_map():
     # [START create_synonym_map]
     synonyms = [
         "USA, United States, United States of America",
         "Washington, Wash. => WA",
     ]
     synonym_map = SynonymMap(name="test-syn-map", synonyms=synonyms)
     result = client.create_synonym_map(synonym_map)
     print("Create new Synonym Map 'test-syn-map succeeded")
     # [END create_synonym_map]
 
+
 def create_synonym_map_from_file():
     # [START create_synonym_map_from_file]
     from os.path import dirname, join, realpath
+
     CWD = dirname(realpath(__file__))
     file_path = join(CWD, "synonym_map.txt")
     with open(file_path, "r") as f:
         solr_format_synonyms = f.read()
         synonyms = solr_format_synonyms.split("\n")
         synonym_map = SynonymMap(name="test-syn-map", synonyms=synonyms)
         result = client.create_synonym_map(synonym_map)
         print("Create new Synonym Map 'test-syn-map succeeded")
     # [END create_synonym_map_from_file]
 
+
 def get_synonym_maps():
     # [START get_synonym_maps]
     result = client.get_synonym_maps()
     names = [x.name for x in result]
     print("Found {} Synonym Maps in the service: {}".format(len(result), ", ".join(names)))
     # [END get_synonym_maps]
 
+
 def get_synonym_map():
     # [START get_synonym_map]
     result = client.get_synonym_map("test-syn-map")
     print("Retrived Synonym Map 'test-syn-map' with synonyms")
     for syn in result.synonyms:
         print("    {}".format(syn))
     # [END get_synonym_map]
 
+
 def delete_synonym_map():
     # [START delete_synonym_map]
     client.delete_synonym_map("test-syn-map")
     print("Synonym Map 'test-syn-map' deleted")
     # [END delete_synonym_map]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     create_synonym_map()
     get_synonym_maps()
     get_synonym_map()
     delete_synonym_map()
```

## Comparing `azure-search-documents-11.4.0b3/samples/sample_indexer_datasource_skillset.py` & `azure-search-documents-11.4.0b4/samples/sample_indexer_datasource_skillset.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,17 +32,28 @@
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.models import (
-    SearchIndexerDataContainer, SearchIndex, SearchIndexer, SimpleField, SearchFieldDataType,
-    EntityRecognitionSkill, InputFieldMappingEntry, OutputFieldMappingEntry, SearchIndexerSkillset,
-    CorsOptions, IndexingSchedule, SearchableField, IndexingParameters, SearchIndexerDataSourceConnection
+    SearchIndexerDataContainer,
+    SearchIndex,
+    SearchIndexer,
+    SimpleField,
+    SearchFieldDataType,
+    EntityRecognitionSkill,
+    InputFieldMappingEntry,
+    OutputFieldMappingEntry,
+    SearchIndexerSkillset,
+    CorsOptions,
+    IndexingSchedule,
+    SearchableField,
+    IndexingParameters,
+    SearchIndexerDataSourceConnection,
 )
 from azure.search.documents.indexes import SearchIndexerClient, SearchIndexClient
 
 
 def _create_index():
     name = "hotel-index"
 
@@ -58,46 +69,43 @@
         SimpleField(name="lastRenovationDate", type=SearchFieldDataType.String),
         SimpleField(name="rating", type=SearchFieldDataType.Int64, sortable=True),
         SimpleField(name="location", type=SearchFieldDataType.GeographyPoint),
     ]
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
 
     # pass in the name, fields and cors options and create the index
-    index = SearchIndex(
-        name=name,
-        fields=fields,
-        cors_options=cors_options)
+    index = SearchIndex(name=name, fields=fields, cors_options=cors_options)
     index_client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
     result = index_client.create_index(index)
     return result
 
+
 def _create_datasource():
     # Here we create a datasource. As mentioned in the description we have stored it in
     # "searchcontainer"
     ds_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
-    container = SearchIndexerDataContainer(name='searchcontainer')
+    container = SearchIndexerDataContainer(name="searchcontainer")
     data_source_connection = SearchIndexerDataSourceConnection(
-        name="hotel-datasource",
-        type="azureblob",
-        connection_string=connection_string,
-        container=container
+        name="hotel-datasource", type="azureblob", connection_string=connection_string, container=container
     )
     data_source = ds_client.create_data_source_connection(data_source_connection)
     return data_source
 
+
 def _create_skillset():
     client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
     inp = InputFieldMappingEntry(name="text", source="/document/lastRenovationDate")
     output = OutputFieldMappingEntry(name="dateTimes", target_name="RenovatedDate")
     s = EntityRecognitionSkill(name="merge-skill", inputs=[inp], outputs=[output])
 
-    skillset = SearchIndexerSkillset(name='hotel-data-skill', skills=[s], description="example skillset")
+    skillset = SearchIndexerSkillset(name="hotel-data-skill", skills=[s], description="example skillset")
     result = client.create_skillset(skillset)
     return result
 
+
 def sample_indexer_workflow():
     # Now that we have a datasource and an index, we can create an indexer.
 
     skillset_name = _create_skillset().name
     print("Skillset is created")
 
     ds_name = _create_datasource().name
@@ -109,19 +117,19 @@
     # we pass the data source, skillsets and targeted index to build an indexer
     parameters = IndexingParameters(configuration={"parsingMode": "jsonArray"})
     indexer = SearchIndexer(
         name="hotel-data-indexer",
         data_source_name=ds_name,
         target_index_name=ind_name,
         skillset_name=skillset_name,
-        parameters=parameters
+        parameters=parameters,
     )
 
     indexer_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
-    indexer_client.create_indexer(indexer) # create the indexer
+    indexer_client.create_indexer(indexer)  # create the indexer
 
     # to get an indexer
     result = indexer_client.get_indexer("hotel-data-indexer")
     print(result)
 
     # To run an indexer, we can use run_indexer()
     indexer_client.run_indexer(result.name)
@@ -133,9 +141,10 @@
     updated_indexer = indexer_client.create_or_update_indexer(result)
 
     print(updated_indexer)
 
     # get the status of an indexer
     indexer_client.get_indexer_status(updated_indexer.name)
 
-if __name__=="__main__":
+
+if __name__ == "__main__":
     sample_indexer_workflow()
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_index_alias_crud_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_index_alias_crud_operations_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -30,76 +30,82 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.aio import SearchIndexClient
 from azure.search.documents.indexes.models import (
     ComplexField,
     CorsOptions,
     ScoringProfile,
-    SearchAlias, 
-    SearchIndex, 
-    SimpleField, 
+    SearchAlias,
+    SearchIndex,
+    SimpleField,
     SearchableField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 async def create_alias():
     # [START create_alias_async]
-    alias = SearchAlias(name = alias_name, indexes = [index_name])
+    alias = SearchAlias(name=alias_name, indexes=[index_name])
     result = await client.create_alias(alias)
     # [END create_alias_async]
 
+
 async def get_alias():
     # [START get_alias_async]
     result = await client.get_alias(alias_name)
     # [END get_alias_async]
 
+
 async def update_alias():
     # [START update_alias_async]
     new_index_name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
         SearchableField(name="hotelName", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-            SimpleField(name="state", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+                SimpleField(name="state", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-    scoring_profile = ScoringProfile(
-        name="MyProfile"
-    )
+    scoring_profile = ScoringProfile(name="MyProfile")
     scoring_profiles = []
     scoring_profiles.append(scoring_profile)
     index = SearchIndex(
-        name=new_index_name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+        name=new_index_name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options
+    )
 
     result_index = await client.create_or_update_index(index=index)
 
-    alias = SearchAlias(name = alias_name, indexes = [new_index_name])
+    alias = SearchAlias(name=alias_name, indexes=[new_index_name])
     result = await client.create_or_update_alias(alias)
 
     # [END update_alias_async]
 
+
 async def delete_alias():
     # [START delete_alias_async]
 
     await client.delete_alias(alias_name)
     # [END delete_alias_async]
 
+
 async def main():
     await create_alias()
     await get_alias()
     await update_alias()
     await delete_alias()
     await client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_get_document_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_get_document_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def autocomplete_query():
     # [START get_document_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -40,9 +41,10 @@
 
         print("Details for hotel '23' are:")
         print("        Name: {}".format(result["HotelName"]))
         print("      Rating: {}".format(result["Rating"]))
         print("    Category: {}".format(result["Category"]))
     # [END get_document_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(autocomplete_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_crud_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_crud_operations_async.py`

 * *Files 11% similar despite different names*

```diff
@@ -28,44 +28,49 @@
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchClient
 
 search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
+
 async def upload_document():
     # [START upload_document_async]
     DOCUMENT = {
-        'Category': 'Hotel',
-        'HotelId': '1000',
-        'Rating': 4.0,
-        'Rooms': [],
-        'HotelName': 'Azure Inn',
+        "Category": "Hotel",
+        "HotelId": "1000",
+        "Rating": 4.0,
+        "Rooms": [],
+        "HotelName": "Azure Inn",
     }
 
     result = await search_client.upload_documents(documents=[DOCUMENT])
 
     print("Upload of new document succeeded: {}".format(result[0].succeeded))
     # [END upload_document_async]
 
+
 async def merge_document():
     # [START merge_document_async]
     result = await search_client.upload_documents(documents=[{"HotelId": "1000", "Rating": 4.5}])
 
     print("Merge into new document succeeded: {}".format(result[0].succeeded))
     # [END merge_document_async]
 
+
 async def delete_document():
     # [START delete_document_async]
     result = await search_client.upload_documents(documents=[{"HotelId": "1000"}])
 
     print("Delete new document succeeded: {}".format(result[0].succeeded))
     # [END delete_document_async]
 
+
 async def main():
     await upload_document()
     await merge_document()
     await delete_document()
     await search_client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_suggestions_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_suggestions_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def suggest_query():
     # [START suggest_query_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -40,9 +41,10 @@
 
         print("Search suggestions for 'coffee'")
         for result in results:
             hotel = await search_client.get_document(key=result["HotelId"])
             print("    Text: {} for Hotel: {}".format(repr(result["text"]), hotel["HotelName"]))
     # [END suggest_query_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(suggest_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_analyze_text_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_analyze_text_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 import os
 import asyncio
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def simple_analyze_text():
     # [START simple_analyze_text_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.indexes.aio import SearchIndexClient
     from azure.search.documents.indexes.models import AnalyzeTextOptions
 
     client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
@@ -37,9 +38,10 @@
     analyze_request = AnalyzeTextOptions(text="One's <two/>", analyzer_name="standard.lucene")
 
     async with client:
         result = await client.analyze_text(index_name, analyze_request)
         print(result.as_dict())
     # [END simple_analyze_text_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(simple_analyze_text())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_synonym_map_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_synonym_map_operations_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -26,48 +26,54 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.aio import SearchIndexClient
 from azure.search.documents.indexes.models import SynonymMap
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 async def create_synonym_map():
     # [START create_synonym_map_async]
     synonyms = [
         "USA, United States, United States of America",
         "Washington, Wash. => WA",
     ]
     synonym_map = SynonymMap(name="test-syn-map", synonyms=synonyms)
     result = await client.create_synonym_map(synonym_map)
     print("Create new Synonym Map 'test-syn-map succeeded")
     # [END create_synonym_map_async]
 
+
 async def get_synonym_maps():
     # [START get_synonym_maps_async]
     result = await client.get_synonym_maps()
     names = [x.name for x in result]
     print("Found {} Synonym Maps in the service: {}".format(len(result), ", ".join(names)))
     # [END get_synonym_maps_async]
 
+
 async def get_synonym_map():
     # [START get_synonym_map_async]
     result = await client.get_synonym_map("test-syn-map")
     print("Retrived Synonym Map 'test-syn-map' with synonyms")
     for syn in result.synonyms:
         print("    {}".format(syn))
     # [END get_synonym_map_async]
 
+
 async def delete_synonym_map():
     # [START delete_synonym_map_async]
     await client.delete_synonym_map("test-syn-map")
     print("Synonym Map 'test-syn-map' deleted")
     # [END delete_synonym_map_async]
 
+
 async def main():
     await create_synonym_map()
     await get_synonym_maps()
     await get_synonym_map()
     await delete_synonym_map()
     await client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_filter_query_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_filter_query_async.py`

 * *Files 6% similar despite different names*

```diff
@@ -24,30 +24,32 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def filter_query():
     # [START filter_query_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
     select = ("HotelName", "Rating")
     async with search_client:
         results = await search_client.search(
             search_text="WiFi",
             filter="Address/StateProvince eq 'FL' and Address/Country eq 'USA'",
             select=",".join(select),
-            order_by="Rating desc"
+            order_by="Rating desc",
         )
 
         print("Florida hotels containing 'WiFi', sorted by Rating:")
         async for result in results:
             print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
     # [END filter_query_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(filter_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_authentication_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_authentication_async.py`

 * *Files 5% similar despite different names*

```diff
@@ -19,36 +19,41 @@
     2) AZURE_SEARCH_INDEX_NAME - the name of your search index (e.g. "hotels-sample-index")
     3) AZURE_SEARCH_API_KEY - your search API key
 """
 
 import asyncio
 import os
 
+
 async def authentication_with_api_key_credential_async():
     # [START create_search_client_with_key_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
+
     service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
     key = os.getenv("AZURE_SEARCH_API_KEY")
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
     # [END create_search_client_with_key_async]
 
     async with search_client:
         result = await search_client.get_document_count()
 
     print("There are {} documents in the {} search index.".format(result, repr(index_name)))
 
+
 async def authentication_service_client_with_api_key_credential_async():
     # [START create_search_service_with_key_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.indexes.aio import SearchIndexClient
+
     service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     key = os.getenv("AZURE_SEARCH_API_KEY")
 
     client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
     # [END create_search_service_with_key_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(authentication_with_api_key_credential_async())
     asyncio.run(authentication_service_client_with_api_key_credential_async())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_indexers_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_indexers_operations_async.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,99 +28,105 @@
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.models import (
     SearchIndexerDataContainer,
     SearchIndexerDataSourceConnection,
     SearchIndex,
     SearchIndexer,
     SimpleField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 from azure.search.documents.indexes.aio import SearchIndexerClient, SearchIndexClient
 
 indexers_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
 
+
 async def create_indexer():
     # create an index
     index_name = "async-indexer-hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-        SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+        SimpleField(name="baseRate", type=SearchFieldDataType.Double),
     ]
     index = SearchIndex(name=index_name, fields=fields)
     ind_client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
     async with ind_client:
         await ind_client.create_index(index)
 
     # [START create_indexer_async]
     # create a datasource
-    container = SearchIndexerDataContainer(name='searchcontainer')
+    container = SearchIndexerDataContainer(name="searchcontainer")
     data_source_connection = SearchIndexerDataSourceConnection(
-        name="async-indexer-datasource",
-        type="azureblob",
-        connection_string=connection_string,
-        container=container
+        name="async-indexer-datasource", type="azureblob", connection_string=connection_string, container=container
     )
     data_source = await indexers_client.create_data_source_connection(data_source_connection)
 
     # create an indexer
     indexer = SearchIndexer(
         name="async-sample-indexer",
         data_source_name="async-indexer-datasource",
-        target_index_name="async-indexer-hotels"
+        target_index_name="async-indexer-hotels",
     )
     result = await indexers_client.create_indexer(indexer)
     print("Create new Indexer - async-sample-indexer")
     # [END create_indexer_async]
 
+
 async def list_indexers():
     # [START list_indexer_async]
     result = await indexers_client.get_indexers()
     names = [x.name for x in result]
     print("Found {} Indexers in the service: {}".format(len(result), ", ".join(names)))
     # [END list_indexer_async]
 
+
 async def get_indexer():
     # [START get_indexer_async]
     result = await indexers_client.get_indexer("async-sample-indexer")
     print("Retrived Indexer 'async-sample-indexer'")
     return result
     # [END get_indexer_async]
 
+
 async def get_indexer_status():
     # [START get_indexer_status_async]
     result = await indexers_client.get_indexer_status("async-sample-indexer")
     print("Retrived Indexer status for 'async-sample-indexer'")
     return result
     # [END get_indexer_status_async]
 
+
 async def run_indexer():
     # [START run_indexer_async]
     result = await indexers_client.run_indexer("async-sample-indexer")
     print("Ran the Indexer 'async-sample-indexer'")
     return result
     # [END run_indexer_async]
 
+
 async def reset_indexer():
     # [START reset_indexer_async]
     result = await indexers_client.reset_indexer("async-sample-indexer")
     print("Reset the Indexer 'async-sample-indexer'")
     return result
     # [END reset_indexer_async]
 
+
 async def delete_indexer():
     # [START delete_indexer_async]
     await indexers_client.delete_indexer("async-sample-indexer")
     print("Indexer 'async-sample-indexer' successfully deleted")
     # [END delete_indexer_async]
 
+
 async def main():
     await create_indexer()
     await list_indexers()
     await get_indexer()
     await get_indexer_status()
     await run_indexer()
     await reset_indexer()
     await delete_indexer()
     await indexers_client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_facet_query_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_facet_query_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def filter_query():
     # [START facet_query_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -41,9 +42,10 @@
         facets = await results.get_facets()
 
         print("Catgory facet counts for hotels:")
         for facet in facets["Category"]:
             print("    {}".format(facet))
     # [END facet_query_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(filter_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_data_source_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_data_source_operations_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,49 +27,55 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes.models import SearchIndexerDataContainer, SearchIndexerDataSourceConnection
 from azure.search.documents.indexes.aio import SearchIndexerClient
 
 client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))
 
+
 async def create_data_source_connection():
     # [START create_data_source_connection_async]
-    container = SearchIndexerDataContainer(name='searchcontainer')
+    container = SearchIndexerDataContainer(name="searchcontainer")
     data_source = SearchIndexerDataSourceConnection(
         name="async-sample-data-source-connection",
         type="azureblob",
         connection_string=connection_string,
-        container=container
+        container=container,
     )
     result = await client.create_data_source_connection(data_source)
     print("Create new Data Source Connection - async-sample-data-source-connection")
     # [END create_data_source_connection_async]
 
+
 async def list_data_source_connections():
     # [START list_data_source_connection_async]
     result = await client.get_data_source_connections()
     names = [x.name for x in result]
     print("Found {} Data Source Connections in the service: {}".format(len(result), ", ".join(names)))
     # [END list_data_source_connection_async]
 
+
 async def get_data_source_connection():
     # [START get_data_source_connection_async]
     result = await client.get_data_source_connection("async-sample-data-source-connection")
     print("Retrived Data Source Connection 'async-sample-data-source-connection'")
     return result
     # [END get_data_source_connection_async]
 
+
 async def delete_data_source_connection():
     # [START delete_data_source_connection_async]
     await client.delete_data_source_connection("async-sample-data-source-connection")
     print("Data Source Connection 'async-sample-data-source-connection' successfully deleted")
     # [END delete_data_source_connection_async]
 
+
 async def main():
     await create_data_source_connection()
     await list_data_source_connections()
     await get_data_source_connection()
     await delete_data_source_connection()
     await client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_buffered_sender_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_buffered_sender_async.py`

 * *Files 16% similar despite different names*

```diff
@@ -28,30 +28,29 @@
 
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchIndexingBufferedSender
 
 
 async def sample_batching_client():
     DOCUMENT = {
-        'Category': 'Hotel',
-        'HotelId': '1000',
-        'Rating': 4.0,
-        'Rooms': [],
-        'HotelName': 'Azure Inn',
+        "Category": "Hotel",
+        "HotelId": "1000",
+        "Rating": 4.0,
+        "Rooms": [],
+        "HotelName": "Azure Inn",
     }
 
-    async with SearchIndexingBufferedSender(
-            service_endpoint,
-            index_name,
-            AzureKeyCredential(key)) as batch_client:
+    async with SearchIndexingBufferedSender(service_endpoint, index_name, AzureKeyCredential(key)) as batch_client:
         # add upload actions
         await batch_client.upload_documents(documents=[DOCUMENT])
         # add merge actions
         await batch_client.merge_documents(documents=[{"HotelId": "1000", "Rating": 4.5}])
         # add delete actions
         await batch_client.delete_documents(documents=[{"HotelId": "1000"}])
 
+
 async def main():
     await sample_batching_client()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_autocomplete_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_autocomplete_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def autocomplete_query():
     # [START autocomplete_query_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -39,9 +40,10 @@
         results = await search_client.autocomplete(search_text="bo", suggester_name="sg")
 
         print("Autocomplete suggestions for 'bo'")
         for result in results:
             print("    Completion: {}".format(result["text"]))
     # [END autocomplete_query_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(autocomplete_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_simple_query_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_simple_query_async.py`

 * *Files 8% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import asyncio
 
 
 service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
 index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
 key = os.getenv("AZURE_SEARCH_API_KEY")
 
+
 async def simple_text_query():
     # [START simple_query_async]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents.aio import SearchClient
 
     search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
@@ -39,9 +40,10 @@
         results = await search_client.search(search_text="spa")
 
         print("Hotels containing 'spa' in the name (or other fields):")
         async for result in results:
             print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
     # [END simple_query_async]
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(simple_text_query())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_index_crud_operations_async.py` & `azure-search-documents-11.4.0b4/samples/async_samples/sample_index_crud_operations_async.py`

 * *Files 10% similar despite different names*

```diff
@@ -30,87 +30,91 @@
 from azure.search.documents.indexes.models import (
     ComplexField,
     CorsOptions,
     SearchIndex,
     ScoringProfile,
     SearchFieldDataType,
     SimpleField,
-    SearchableField
+    SearchableField,
 )
 
 client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))
 
+
 async def create_index():
     # [START create_index_async]
     name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
 
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
     scoring_profiles = []
-    index = SearchIndex(
-        name=name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+    index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
     result = await client.create_index(index)
     # [END create_index_async]
 
+
 async def get_index():
     # [START get_index_async]
     name = "hotels"
     result = await client.get_index(name)
     # [END get_index_async]
 
+
 async def update_index():
     # [START update_index_async]
     name = "hotels"
     fields = [
         SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
         SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
         SearchableField(name="hotelName", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
-            SimpleField(name="streetAddress", type=SearchFieldDataType.String),
-            SimpleField(name="city", type=SearchFieldDataType.String),
-            SimpleField(name="state", type=SearchFieldDataType.String),
-        ], collection=True)
+        ComplexField(
+            name="address",
+            fields=[
+                SimpleField(name="streetAddress", type=SearchFieldDataType.String),
+                SimpleField(name="city", type=SearchFieldDataType.String),
+                SimpleField(name="state", type=SearchFieldDataType.String),
+            ],
+            collection=True,
+        ),
     ]
 
     cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-    scoring_profile = ScoringProfile(
-        name="MyProfile"
-    )
+    scoring_profile = ScoringProfile(name="MyProfile")
     scoring_profiles = []
     scoring_profiles.append(scoring_profile)
-    index = SearchIndex(
-        name=name,
-        fields=fields,
-        scoring_profiles=scoring_profiles,
-        cors_options=cors_options)
+    index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
     result = await client.create_or_update_index(index=index)
     # [END update_index_async]
 
+
 async def delete_index():
     # [START delete_index_async]
     name = "hotels"
     await client.delete_index(name)
     # [END delete_index_async]
 
+
 async def main():
     await create_index()
     await get_index()
     await update_index()
     await delete_index()
     await client.close()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     asyncio.run(main())
```

## Comparing `azure-search-documents-11.4.0b3/samples/async_samples/sample_semantic_search_async.py` & `azure-search-documents-11.4.0b4/samples/sample_semantic_search.py`

 * *Files 23% similar despite different names*

```diff
@@ -3,64 +3,68 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 """
-FILE: sample_semantic_search_async.py
+FILE: sample_semantic_search.py
 DESCRIPTION:
     This sample demonstrates how to use semantic search.
 USAGE:
-    python sample_semantic_search_async.py
+    python sample_semantic_search.py
 
     Set the environment variables with your own values before running the sample:
     1) AZURE_SEARCH_SERVICE_ENDPOINT - the endpoint of your Azure Cognitive Search service
     2) AZURE_SEARCH_INDEX_NAME - the name of your search index (e.g. "hotels-sample-index")
     3) AZURE_SEARCH_API_KEY - your search API key
 """
 
 import os
-import asyncio
 
-async def speller():
-    # [START speller_async]
+
+def speller():
+    # [START speller]
     from azure.core.credentials import AzureKeyCredential
-    from azure.search.documents.aio import SearchClient
+    from azure.search.documents import SearchClient
 
     endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
     api_key = os.getenv("AZURE_SEARCH_API_KEY")
 
     credential = AzureKeyCredential(api_key)
-    client = SearchClient(endpoint=endpoint,
-                          index_name=index_name,
-                          credential=credential)
-    results = await client.search(search_text="luxury", query_language="en-us", query_speller="lexicon")
+    client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)
+    results = list(client.search(search_text="luxury", query_language="en-us", query_speller="lexicon"))
 
-    async for result in results:
+    for result in results:
         print("{}\n{}\n)".format(result["HotelId"], result["HotelName"]))
-    # [END speller_async]
+    # [END speller]
+
 
-async def semantic_ranking():
-    # [START semantic_ranking_async]
+def semantic_ranking():
+    # [START semantic_ranking]
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
 
     endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
     index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
     api_key = os.getenv("AZURE_SEARCH_API_KEY")
 
     credential = AzureKeyCredential(api_key)
-    client = SearchClient(endpoint=endpoint,
-                          index_name=index_name,
-                          credential=credential)
-    results = list(client.search(search_text="luxury", query_type="semantic", query_language="en-us"))
+    client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)
+    results = list(
+        client.search(
+            search_text="luxury",
+            query_type="semantic",
+            semantic_configuration_name="semantic_config_name",
+            query_language="en-us",
+        )
+    )
 
     for result in results:
         print("{}\n{}\n)".format(result["HotelId"], result["HotelName"]))
-    # [END semantic_ranking_async]
+    # [END semantic_ranking]
 
 
-if __name__ == '__main__':
-    asyncio.run(speller())
-    asyncio.run(semantic_ranking())
+if __name__ == "__main__":
+    speller()
+    semantic_ranking()
```

## Comparing `azure-search-documents-11.4.0b3/azure_search_documents.egg-info/SOURCES.txt` & `azure-search-documents-11.4.0b4/azure_search_documents.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 CHANGELOG.md
 LICENSE
 MANIFEST.in
 README.md
+TROUBLESHOOTING.md
+pyproject.toml
 setup.py
 azure/__init__.py
 azure/search/__init__.py
 azure/search/documents/__init__.py
 azure/search/documents/_api_versions.py
 azure/search/documents/_headers_mixin.py
 azure/search/documents/_index_documents_batch.py
@@ -109,14 +111,15 @@
 samples/sample_index_crud_operations.py
 samples/sample_indexer_datasource_skillset.py
 samples/sample_indexers_operations.py
 samples/sample_semantic_search.py
 samples/sample_simple_query.py
 samples/sample_suggestions.py
 samples/sample_synonym_map_operations.py
+samples/sample_vector_search.py
 samples/async_samples/sample_analyze_text_async.py
 samples/async_samples/sample_authentication_async.py
 samples/async_samples/sample_autocomplete_async.py
 samples/async_samples/sample_buffered_sender_async.py
 samples/async_samples/sample_crud_operations_async.py
 samples/async_samples/sample_data_source_operations_async.py
 samples/async_samples/sample_facet_query_async.py
@@ -125,14 +128,15 @@
 samples/async_samples/sample_index_alias_crud_operations_async.py
 samples/async_samples/sample_index_crud_operations_async.py
 samples/async_samples/sample_indexers_operations_async.py
 samples/async_samples/sample_semantic_search_async.py
 samples/async_samples/sample_simple_query_async.py
 samples/async_samples/sample_suggestions_async.py
 samples/async_samples/sample_synonym_map_operations_async.py
+samples/async_samples/sample_vector_search_async.py
 tests/conftest.py
 tests/search_service_preparer.py
 tests/test_buffered_sender.py
 tests/test_index_documents_batch.py
 tests/test_index_field_helpers.py
 tests/test_queries.py
 tests/test_regex_flags.py
```

## Comparing `azure-search-documents-11.4.0b3/azure_search_documents.egg-info/PKG-INFO` & `azure-search-documents-11.4.0b4/azure_search_documents.egg-info/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azure-search-documents
-Version: 11.4.0b3
+Version: 11.4.0b4
 Summary: Microsoft Azure Cognitive Search Client Library for Python
 Home-page: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents
 Author: Microsoft Corporation
 Author-email: ascl@microsoft.com
 License: MIT License
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python
@@ -52,19 +52,20 @@
 * Create and manage search indexes.
 * Upload and update documents in the search index.
 * Create and manage indexers that pull data from Azure into an index.
 * Create and manage skillsets that add AI enrichment to data ingestion.
 * Create and manage analyzers for advanced text analysis or multi-lingual content.
 * Optimize results through scoring profiles to factor in business logic or freshness.
 
-[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents) |
-[Package (PyPI)](https://pypi.org/project/azure-search-documents/) |
-[API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html) |
-[Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search) |
-[Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
+[Source code](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents)
+| [Package (PyPI)](https://pypi.org/project/azure-search-documents/)
+| [Package (Conda)](https://anaconda.org/microsoft/azure-search-documents/)
+| [API reference documentation](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html)
+| [Product documentation](https://docs.microsoft.com/azure/search/search-what-is-azure-search)
+| [Samples](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples)
 
 ## _Disclaimer_
 
 _Azure SDK Python packages support for Python 2.7 has ended 01 January 2022. For more information and questions, please refer to https://github.com/Azure/azure-sdk-for-python/issues/20691_
 
 ## Getting started
 
@@ -89,50 +90,75 @@
 ```
 
 See [choosing a pricing tier](https://docs.microsoft.com/azure/search/search-sku-tier)
  for more information about available options.
 
 ### Authenticate the client
 
-All requests to a search service need an api-key that was generated specifically
-for your service. [The api-key is the sole mechanism for authenticating access to
-your search service endpoint.](https://docs.microsoft.com/azure/search/search-security-api-keys)
-You can obtain your api-key from the
-[Azure portal](https://portal.azure.com/) or via the Azure CLI:
+To interact with the Search service, you'll need to create an instance of the appropriate client class: `SearchClient` for searching indexed documents, `SearchIndexClient` for managing indexes, or `SearchIndexerClient` for crawling data sources and loading search documents into an index. To instantiate a client object, you'll need an **endpoint** and an **API key**. You can refer to the documentation for more information on [supported authenticating approaches](https://learn.microsoft.com/azure/search/search-security-overview#authentication) with the Search service.
+
+#### Get an API Key
+
+You can get the **endpoint** and an **API key** from the Search service in the [Azure Portal](https://portal.azure.com/). Please refer the [documentation](https://docs.microsoft.com/azure/search/search-security-api-keys) for instructions on how to get an API key.
+
+Alternatively, you can use the following [Azure CLI](https://learn.microsoft.com/cli/azure/) command to retrieve the API key from the Search service:
 
 ```Powershell
 az search admin-key show --service-name <mysearch> --resource-group <mysearch-rg>
 ```
 
 There are two types of keys used to access your search service: **admin**
 *(read-write)* and **query** *(read-only)* keys.  Restricting access and
 operations in client apps is essential to safeguarding the search assets on your
 service.  Always use a query key rather than an admin key for any query
 originating from a client app.
 
 *Note: The example Azure CLI snippet above retrieves an admin key so it's easier
 to get started exploring APIs, but it should be managed carefully.*
 
-We can use the api-key to create a new `SearchClient`.
+#### Create a SearchClient
+
+To instantiate the `SearchClient`, you'll need the **endpoint**, **API key** and **index name**:
+
+<!-- SNIPPET:sample_authentication.create_search_client_with_key -->
 
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "nycjobs"
-# Get the service endpoint and API key from the environment
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+key = os.getenv("AZURE_SEARCH_API_KEY")
 
-# Create a client
-credential = AzureKeyCredential(key)
-client = SearchClient(endpoint=endpoint,
-                      index_name=index_name,
-                      credential=credential)
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
+```
+
+<!-- END SNIPPET -->
+
+#### Create a client using Azure Active Directory authentication
+
+You can also create a `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient` using Azure Active Directory (AAD) authentication. Your user or service principal must be assigned the "Search Index Data Reader" role.
+Using the [DefaultAzureCredential](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#defaultazurecredential) you can authenticate a service using Managed Identity or a service principal, authenticate as a developer working on an application, and more all without changing code. Please refer the [documentation](https://learn.microsoft.com/azure/search/search-security-rbac?tabs=config-svc-portal%2Croles-portal%2Ctest-portal%2Ccustom-role-portal%2Cdisable-keys-portal) for instructions on how to connect to Azure Cognitive Search using Azure role-based access control (Azure RBAC).
+
+Before you can use the `DefaultAzureCredential`, or any credential type from [Azure.Identity](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md), you'll first need to [install the Azure.Identity package](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/README.md#install-the-package).
+
+To use `DefaultAzureCredential` with a client ID and secret, you'll need to set the `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET` environment variables; alternatively, you can pass those values
+to the `ClientSecretCredential` also in Azure.Identity.
+
+Make sure you use the right namespace for `DefaultAzureCredential` at the top of your source file:
+
+```python
+from azure.identity import DefaultAzureCredential
+from azure.search.documents import SearchClient
+
+service_endpoint = os.getenv("AZURE_SEARCH_SERVICE_ENDPOINT")
+index_name = os.getenv("AZURE_SEARCH_INDEX_NAME")
+credential = DefaultAzureCredential()
+
+search_client = SearchClient(service_endpoint, index_name, credential)
 ```
 
 ## Key concepts
 
 An Azure Cognitive Search service contains one or more indexes that provide
 persistent storage of searchable data in the form of JSON documents.  _(If
 you're brand new to search, you can make a very rough analogy between
@@ -156,14 +182,28 @@
   * [Declare custom synonym maps to expand or rewrite queries](https://docs.microsoft.com/rest/api/searchservice/synonym-map-operations)
   * Most of the `SearchServiceClient` functionality is not yet available in our current preview
 
 * `SearchIndexerClient` allows you to:
   * [Start indexers to automatically crawl data sources](https://docs.microsoft.com/rest/api/searchservice/indexer-operations)
   * [Define AI powered Skillsets to transform and enrich your data](https://docs.microsoft.com/rest/api/searchservice/skillset-operations)
 
+Azure Cognitive Search provides two powerful features: **Semantic Search** and **Vector Search**.
+
+**Semantic Search** enhances the quality of search results for text-based queries. By enabling Semantic Search on your search service, you can improve the relevance of search results in two ways:
+- It applies secondary ranking to the initial result set, promoting the most semantically relevant results to the top.
+- It extracts and returns captions and answers in the response, which can be displayed on a search page to enhance the user's search experience.
+
+To learn more about Semantic Search, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview).
+
+**Vector Search** is an information retrieval technique that overcomes the limitations of traditional keyword-based search. Instead of relying solely on lexical analysis and matching individual query terms, Vector Search utilizes machine learning models to capture the contextual meaning of words and phrases. It represents documents and queries as vectors in a high-dimensional space called an embedding. By understanding the intent behind the query, Vector Search can deliver more relevant results that align with the user's requirements, even if the exact terms are not present in the document. Moreover, Vector Search can be applied to various types of content, including images and videos, not just text.
+
+To learn how to index vector fields and perform vector search, you can refer to the [sample](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py). This sample provides detailed guidance on indexing vector fields and demonstrates how to perform vector search.
+
+Additionally, for more comprehensive information about Vector Search, including its concepts and usage, you can refer to the [documentation](https://learn.microsoft.com/azure/search/vector-search-overview). The documentation provides in-depth explanations and guidance on leveraging the power of Vector Search in Azure Cognitive Search.
+
 _The `Azure.Search.Documents` client library (v1) is a brand new offering for
 Python developers who want to use search technology in their applications.  There
 is an older, fully featured `Microsoft.Azure.Search` client library (v10) with
 many similar looking APIs, so please be careful to avoid confusion when
 exploring online resources._
 
 ## Examples
@@ -218,89 +258,65 @@
 
 ### Creating an index
 
 You can use the `SearchIndexClient` to create a search index. Fields can be
 defined using convenient `SimpleField`, `SearchableField`, or `ComplexField`
 models. Indexes can also define suggesters, lexical analyzers, and more.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents.indexes import SearchIndexClient
-from azure.search.documents.indexes.models import (
-    ComplexField,
-    CorsOptions,
-    SearchIndex,
-    ScoringProfile,
-    SearchFieldDataType,
-    SimpleField,
-    SearchableField
-)
-
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_index_crud_operations.create_index -->
 
-# Create a service client
-client = SearchIndexClient(endpoint, AzureKeyCredential(key))
-
-# Create the index
+```python
 name = "hotels"
 fields = [
-        SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-        SimpleField(name="baseRate", type=SearchFieldDataType.Double),
-        SearchableField(name="description", type=SearchFieldDataType.String),
-        ComplexField(name="address", fields=[
+    SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
+    SimpleField(name="baseRate", type=SearchFieldDataType.Double),
+    SearchableField(name="description", type=SearchFieldDataType.String, collection=True),
+    ComplexField(
+        name="address",
+        fields=[
             SimpleField(name="streetAddress", type=SearchFieldDataType.String),
             SimpleField(name="city", type=SearchFieldDataType.String),
-        ])
-    ]
+        ],
+        collection=True,
+    ),
+]
 cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
 scoring_profiles = []
-
-index = SearchIndex(
-    name=name,
-    fields=fields,
-    scoring_profiles=scoring_profiles,
-    cors_options=cors_options)
+index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
 
 result = client.create_index(index)
 ```
 
+<!-- END SNIPPET -->
 
 ### Adding documents to your index
 
 You can `Upload`, `Merge`, `MergeOrUpload`, and `Delete` multiple documents from
 an index in a single batched request.  There are
 [a few special rules for merging](https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents#document-actions)
 to be aware of.
 
-```python
-import os
-from azure.core.credentials import AzureKeyCredential
-from azure.search.documents import SearchClient
-
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
+<!-- SNIPPET:sample_crud_operations.upload_document -->
 
+```python
 DOCUMENT = {
-    'Category': 'Hotel',
-    'hotelId': '1000',
-    'rating': 4.0,
-    'rooms': [],
-    'hotelName': 'Azure Inn',
+    "Category": "Hotel",
+    "HotelId": "1000",
+    "Rating": 4.0,
+    "Rooms": [],
+    "HotelName": "Azure Inn",
 }
 
-search_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
-
 result = search_client.upload_documents(documents=[DOCUMENT])
 
 print("Upload of new document succeeded: {}".format(result[0].succeeded))
 ```
 
+<!-- END SNIPPET -->
+
 ### Authenticate in a National Cloud
 
 To authenticate in a [National Cloud](https://docs.microsoft.com/azure/active-directory/develop/authentication-national-cloud), you will need to make the following additions to your client configuration:
 
 - Set the `AuthorityHost` in the credential options or via the `AZURE_AUTHORITY_HOST` environment variable
 - Set the `audience` in `SearchClient`, `SearchIndexClient`, or `SearchIndexerClient`
 
@@ -321,57 +337,58 @@
 ### Retrieving a specific document from your index
 
 In addition to querying for documents using keywords and optional filters,
 you can retrieve a specific document from your index if you already know the
 key. You could get the key from a query, for example, and want to show more
 information about it or navigate your customer to that document.
 
+<!-- SNIPPET:sample_get_document.get_document -->
+
 ```python
-import os
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents import SearchClient
 
-index_name = "hotels"
-endpoint = os.environ["SEARCH_ENDPOINT"]
-key = os.environ["SEARCH_API_KEY"]
-
-client = SearchClient(endpoint, index_name, AzureKeyCredential(key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-result = client.get_document(key="1")
+result = search_client.get_document(key="23")
 
-print("Details for hotel '1' are:")
+print("Details for hotel '23' are:")
 print("        Name: {}".format(result["HotelName"]))
 print("      Rating: {}".format(result["Rating"]))
 print("    Category: {}".format(result["Category"]))
 ```
 
+<!-- END SNIPPET -->
 
 ### Async APIs
+
 This library includes a complete async API. To use it, you must
 first install an async transport, such as [aiohttp](https://pypi.org/project/aiohttp/).
 See
 [azure-core documentation](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md#transport)
 for more information.
 
+<!-- SNIPPET:sample_simple_query_async.simple_query_async -->
 
-```py
+```python
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchClient
 
-client = SearchClient(endpoint, index_name, AzureKeyCredential(api_key))
+search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))
 
-async with client:
-  results = await client.search(search_text="hotel")
-  async for result in results:
-    print("{}: {})".format(result["hotelId"], result["hotelName"]))
-
-...
+async with search_client:
+    results = await search_client.search(search_text="spa")
 
+    print("Hotels containing 'spa' in the name (or other fields):")
+    async for result in results:
+        print("    Name: {} (rating {})".format(result["HotelName"], result["Rating"]))
 ```
 
+<!-- END SNIPPET -->
+
 ## Troubleshooting
 
 ### General
 
 The Azure Cognitive Search client will raise exceptions defined in [Azure Core][azure_core].
 
 ### Logging
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client_skillset_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client_skillset_live.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,29 +6,29 @@
 
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from azure.core.credentials import AzureKeyCredential
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     EntityLinkingSkill,
     EntityRecognitionSkill,
     EntityRecognitionSkillVersion,
     InputFieldMappingEntry,
     OutputFieldMappingEntry,
     SearchIndexerSkillset,
     SentimentSkill,
-    SentimentSkillVersion
+    SentimentSkillVersion,
 )
 from azure.search.documents.indexes import SearchIndexerClient
 
 
 class TestSearchSkillset(AzureRecordedTestCase):
-
+    @pytest.mark.skip("The skills are deprecated")
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_skillset_crud(self, api_key, endpoint):
         client = SearchIndexerClient(endpoint, api_key)
         self._test_create_skillset_validation()
         self._test_create_skillset(client)
@@ -41,68 +41,84 @@
         self._test_delete_skillset(client)
 
     def _test_create_skillset_validation(self):
         name = "test-ss-validation"
         with pytest.raises(ValueError) as err:
             client = SearchIndexerClient("fake_endpoint", AzureKeyCredential("fake_key"))
 
-            s1 = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                        outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
-                                        description="Skill Version 1",
-                                        model_version="1",
-                                        include_typeless_entities=True)
-
-            s2 = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                        outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
-                                        skill_version=EntityRecognitionSkillVersion.LATEST,
-                                        description="Skill Version 3",
-                                        model_version="3",
-                                        include_typeless_entities=True)
-            s3 = SentimentSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
-                                skill_version=SentimentSkillVersion.V1,
-                                description="Sentiment V1",
-                                include_opinion_mining=True)
+            s1 = EntityRecognitionSkill(
+                inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+                outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
+                description="Skill Version 1",
+                model_version="1",
+                include_typeless_entities=True,
+            )
+
+            s2 = EntityRecognitionSkill(
+                inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+                outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
+                skill_version=EntityRecognitionSkillVersion.LATEST,
+                description="Skill Version 3",
+                model_version="3",
+                include_typeless_entities=True,
+            )
+            s3 = SentimentSkill(
+                inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+                outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
+                skill_version=SentimentSkillVersion.V1,
+                description="Sentiment V1",
+                include_opinion_mining=True,
+            )
             skillset = SearchIndexerSkillset(name=name, skills=list([s1, s2, s3]), description="desc")
             client.create_skillset(skillset)
-        assert 'include_typeless_entities' in str(err.value)
-        assert 'model_version' in str(err.value)
-        assert 'include_opinion_mining' in str(err.value)
+        assert "include_typeless_entities" in str(err.value)
+        assert "model_version" in str(err.value)
+        assert "include_opinion_mining" in str(err.value)
 
     def _test_create_skillset(self, client):
         name = "test-ss-create"
-        s1 = EntityRecognitionSkill(name="skill1",
-                                    inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                    outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
-                                    description="Skill Version 1",
-                                    include_typeless_entities=True)
-
-        s2 = EntityRecognitionSkill(name="skill2",
-                                    inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                    outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
-                                    skill_version=EntityRecognitionSkillVersion.LATEST,
-                                    description="Skill Version 3",
-                                    model_version="3")
-        s3 = SentimentSkill(name="skill3",
-                            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                            outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
-                            skill_version=SentimentSkillVersion.V1,
-                            description="Sentiment V1")
-
-        s4 = SentimentSkill(name="skill4",
-                            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                            outputs=[OutputFieldMappingEntry(name="confidenceScores", target_name="scoreS4")],
-                            skill_version=SentimentSkillVersion.V3,
-                            description="Sentiment V3",
-                            include_opinion_mining=True)
-
-        s5 = EntityLinkingSkill(name="skill5",
-                                inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                outputs=[OutputFieldMappingEntry(name="entities", target_name="entitiesS5")],
-                                minimum_precision=0.5)
+        s1 = EntityRecognitionSkill(
+            name="skill1",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
+            description="Skill Version 1",
+            include_typeless_entities=True,
+        )
+
+        s2 = EntityRecognitionSkill(
+            name="skill2",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
+            skill_version=EntityRecognitionSkillVersion.LATEST,
+            description="Skill Version 3",
+            model_version="3",
+        )
+        s3 = SentimentSkill(
+            name="skill3",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
+            skill_version=SentimentSkillVersion.V1,
+            description="Sentiment V1",
+        )
+
+        s4 = SentimentSkill(
+            name="skill4",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="confidenceScores", target_name="scoreS4")],
+            skill_version=SentimentSkillVersion.V3,
+            description="Sentiment V3",
+            include_opinion_mining=True,
+        )
+
+        s5 = EntityLinkingSkill(
+            name="skill5",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="entities", target_name="entitiesS5")],
+            minimum_precision=0.5,
+        )
 
         skillset = SearchIndexerSkillset(name=name, skills=list([s1, s2, s3, s4, s5]), description="desc")
 
         dict_skills = [skill.as_dict() for skill in skillset.skills]
         skillset.skills = dict_skills
 
         result = client.create_skillset(skillset)
@@ -124,45 +140,51 @@
         assert result.skills[4].minimum_precision == 0.5
 
         assert len(client.get_skillsets()) == 1
         client.reset_skills(result, [x.name for x in result.skills])
 
     def _test_get_skillset(self, client):
         name = "test-ss-get"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
         skillset = SearchIndexerSkillset(name=name, skills=list([s]), description="desc")
         client.create_skillset(skillset)
         result = client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc"
         assert result.e_tag
         assert len(result.skills) == 1
         assert isinstance(result.skills[0], EntityRecognitionSkill)
 
     def _test_get_skillsets(self, client):
         name1 = "test-ss-list-1"
         name2 = "test-ss-list-2"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name1, skills=list([s]), description="desc1")
         client.create_skillset(skillset1)
         skillset2 = SearchIndexerSkillset(name=name2, skills=list([s]), description="desc2")
         client.create_skillset(skillset2)
         result = client.get_skillsets()
         assert isinstance(result, list)
         assert all(isinstance(x, SearchIndexerSkillset) for x in result)
         assert set(x.name for x in result).intersection([name1, name2]) == set([name1, name2])
 
     def _test_create_or_update_skillset(self, client):
         name = "test-ss-create-or-update"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         client.create_or_update_skillset(skillset1)
         expected_count = len(client.get_skillsets())
         skillset2 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc2")
         client.create_or_update_skillset(skillset2)
         assert len(client.get_skillsets()) == expected_count
@@ -170,16 +192,18 @@
         result = client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc2"
 
     def _test_create_or_update_skillset_inplace(self, client):
         name = "test-ss-create-or-update-inplace"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         ss = client.create_or_update_skillset(skillset1)
         expected_count = len(client.get_skillsets())
         skillset2 = SearchIndexerSkillset(name=name, skills=[s], description="desc2", skillset=ss)
         client.create_or_update_skillset(skillset2)
         assert len(client.get_skillsets()) == expected_count
@@ -187,29 +211,33 @@
         result = client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc2"
 
     def _test_create_or_update_skillset_if_unchanged(self, client):
         name = "test-ss-create-or-update-unchanged"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         ss = client.create_or_update_skillset(skillset1)
-        
-        ss.e_tag = 'changed_etag'
+
+        ss.e_tag = "changed_etag"
 
         with pytest.raises(HttpResponseError):
             client.create_or_update_skillset(ss, match_condition=MatchConditions.IfNotModified)
 
     def _test_delete_skillset_if_unchanged(self, client):
         name = "test-ss-deleted-unchanged"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset = SearchIndexerSkillset(name=name, skills=list([s]), description="desc")
 
         result = client.create_skillset(skillset)
         etag = result.e_tag
 
         skillset = SearchIndexerSkillset(name=name, skills=list([s]), description="updated")
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_indexer_client_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_indexer_client_live.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,23 +5,25 @@
 # --------------------------------------------------------------------------
 
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient
 from azure.search.documents.indexes.models import (
-    SearchIndex, SearchIndexer, SearchIndexerDataContainer,
-    SearchIndexerDataSourceConnection)
+    SearchIndex,
+    SearchIndexer,
+    SearchIndexerDataContainer,
+    SearchIndexerDataSourceConnection,
+)
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchIndexerClientTest(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_search_indexers(self, endpoint, api_key, **kwargs):
         storage_cs = kwargs.get("search_storage_connection_string")
         container_name = kwargs.get("search_storage_container_name")
         client = SearchIndexerClient(endpoint, api_key)
@@ -38,25 +40,19 @@
         self._test_delete_indexer_if_unchanged(client, index_client, storage_cs, container_name)
 
     def _prepare_indexer(self, client, index_client, storage_cs, name, container_name):
         data_source_connection = SearchIndexerDataSourceConnection(
             name=f"{name}-ds",
             type="azureblob",
             connection_string=storage_cs,
-            container=SearchIndexerDataContainer(name=container_name)
+            container=SearchIndexerDataContainer(name=container_name),
         )
         ds = client.create_data_source_connection(data_source_connection)
 
-        fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        }]
+        fields = [{"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False}]
         index = SearchIndex(name=f"{name}-hotels", fields=fields)
         ind = index_client.create_index(index)
         return SearchIndexer(name=name, data_source_name=ds.name, target_index_name=ind.name)
 
     def _test_create_indexer(self, client, index_client, storage_cs, container_name):
         name = "create"
         indexer = self._prepare_indexer(client, index_client, storage_cs, name, container_name)
@@ -104,22 +100,22 @@
         assert result.description == "updated"
 
     def _test_reset_indexer(self, client, index_client, storage_cs, container_name):
         name = "reset"
         indexer = self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         client.create_indexer(indexer)
         client.reset_indexer(name)
-        assert (client.get_indexer_status(name)).last_result.status.lower() in ('inprogress', 'reset')
+        assert (client.get_indexer_status(name)).last_result.status.lower() in ("inprogress", "reset")
 
     def _test_run_indexer(self, client, index_client, storage_cs, container_name):
         name = "run"
         indexer = self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         client.create_indexer(indexer)
         client.run_indexer(name)
-        assert (client.get_indexer_status(name)).status == 'running'
+        assert (client.get_indexer_status(name)).status == "running"
 
     def _test_get_indexer_status(self, client, index_client, storage_cs, container_name):
         name = "get-status"
         indexer = self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         client.create_indexer(indexer)
         status = client.get_indexer_status(name)
         assert status.status is not None
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client_live.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,29 +3,28 @@
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     AnalyzeTextOptions,
     CorsOptions,
     SearchIndex,
     ScoringProfile,
     SimpleField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 from azure.search.documents.indexes import SearchIndexClient
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchIndexClient(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema=None, index_batch=None)
     @recorded_by_proxy
     def test_search_index_client(self, api_key, endpoint, index_name):
         client = SearchIndexClient(endpoint, api_key)
         index_name = "hotels"
         self._test_get_service_statistics(client)
@@ -60,95 +59,68 @@
 
     def _test_get_index(self, client, index_name):
         result = client.get_index(index_name)
         assert result.name == index_name
 
     def _test_get_index_statistics(self, client, index_name):
         result = client.get_index_statistics(index_name)
-        assert set(result.keys()) == {'document_count', 'storage_size'}
+        assert set(result.keys()) == {"document_count", "storage_size"}
 
     def _test_create_index(self, client, index_name):
         fields = [
             SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-            SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+            SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         ]
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
         index = SearchIndex(
-            name=index_name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+            name=index_name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options
+        )
         result = client.create_index(index)
         assert result.name == index_name
         assert result.scoring_profiles[0].name == scoring_profile.name
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
 
     def _test_create_or_update_index(self, client):
         name = "hotels-cou"
         fields = [
             SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-            SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+            SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         ]
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
         scoring_profiles = []
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = client.create_or_update_index(index=index)
         assert len(result.scoring_profiles) == 0
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = client.create_or_update_index(index=index)
         assert result.scoring_profiles[0].name == scoring_profile.name
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
 
     def _test_create_or_update_indexes_if_unchanged(self, client):
         # First create an index
         name = "hotels-coa-unchanged"
         fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        },
-        {
-          "name": "baseRate",
-          "type": "Edm.Double"
-        }]
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+            {"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False},
+            {"name": "baseRate", "type": "Edm.Double"},
+        ]
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = client.create_index(index)
         etag = result.e_tag
         # get e tag  and update
         index.scoring_profiles = []
         client.create_or_update_index(index)
 
         index.e_tag = etag
@@ -160,35 +132,22 @@
         result = client.analyze_text(index_name, analyze_request)
         assert len(result.tokens) == 2
 
     def _test_delete_indexes_if_unchanged(self, client):
         # First create an index
         name = "hotels-del-unchanged"
         fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        },
-        {
-          "name": "baseRate",
-          "type": "Edm.Double"
-        }]
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+            {"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False},
+            {"name": "baseRate", "type": "Edm.Double"},
+        ]
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = client.create_index(index)
         etag = result.e_tag
         # get e tag  and update
         index.scoring_profiles = []
         client.create_or_update_index(index)
 
         index.e_tag = etag
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client_data_source_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client_data_source_live.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,29 +7,26 @@
 import pytest
 
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     SearchIndexerDataSourceConnection,
     SearchIndexerDataContainer,
 )
 from azure.search.documents.indexes import SearchIndexerClient
 
-class TestSearchClientDataSources(AzureRecordedTestCase):
 
+class TestSearchClientDataSources(AzureRecordedTestCase):
     def _create_data_source_connection(self, cs, name):
-        container = SearchIndexerDataContainer(name='searchcontainer')
+        container = SearchIndexerDataContainer(name="searchcontainer")
         data_source_connection = SearchIndexerDataSourceConnection(
-            name=name,
-            type="azureblob",
-            connection_string=cs,
-            container=container
+            name=name, type="azureblob", connection_string=cs, container=container
         )
         return data_source_connection
 
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_data_source(self, endpoint, api_key, **kwargs):
@@ -94,41 +91,45 @@
         etag = created.e_tag
 
         # Now update the data source connection
         data_source_connection.description = "updated"
         client.create_or_update_data_source_connection(data_source_connection)
 
         # prepare data source connection
-        data_source_connection.e_tag = etag # reset to the original data source connection
+        data_source_connection.e_tag = etag  # reset to the original data source connection
         data_source_connection.description = "changed"
         with pytest.raises(HttpResponseError):
-            client.create_or_update_data_source_connection(data_source_connection, match_condition=MatchConditions.IfNotModified)
+            client.create_or_update_data_source_connection(
+                data_source_connection, match_condition=MatchConditions.IfNotModified
+            )
 
     def _test_delete_datasource_if_unchanged(self, client, storage_cs):
         ds_name = "delunch"
         data_source_connection = self._create_data_source_connection(storage_cs, ds_name)
         created = client.create_data_source_connection(data_source_connection)
         etag = created.e_tag
 
         # Now update the data source connection
         data_source_connection.description = "updated"
         client.create_or_update_data_source_connection(data_source_connection)
 
         # prepare data source connection
-        data_source_connection.e_tag = etag # reset to the original data source connection
+        data_source_connection.e_tag = etag  # reset to the original data source connection
         with pytest.raises(HttpResponseError):
             client.delete_data_source_connection(data_source_connection, match_condition=MatchConditions.IfNotModified)
 
     def _test_delete_datasource_string_if_unchanged(self, client, storage_cs):
         ds_name = "delstrunch"
         data_source_connection = self._create_data_source_connection(storage_cs, ds_name)
         created = client.create_data_source_connection(data_source_connection)
         etag = created.e_tag
 
         # Now update the data source connection
         data_source_connection.description = "updated"
         client.create_or_update_data_source_connection(data_source_connection)
 
         # prepare data source connection
-        data_source_connection.e_tag = etag # reset to the original data source connection
+        data_source_connection.e_tag = etag  # reset to the original data source connection
         with pytest.raises(ValueError):
-            client.delete_data_source_connection(data_source_connection.name, match_condition=MatchConditions.IfNotModified)
+            client.delete_data_source_connection(
+                data_source_connection.name, match_condition=MatchConditions.IfNotModified
+            )
```

## Comparing `azure-search-documents-11.4.0b3/tests/conftest.py` & `azure-search-documents-11.4.0b4/tests/conftest.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,26 +10,18 @@
 from devtools_testutils.sanitizers import add_remove_header_sanitizer, add_general_regex_sanitizer
 
 # Ignore async tests for Python < 3.5
 collect_ignore = []
 if sys.version_info < (3, 5):
     collect_ignore.append("async_tests")
 
+
 @pytest.fixture(scope="session", autouse=True)
 def add_sanitizers(test_proxy):
     add_remove_header_sanitizer(headers="api-key")
 
     # Ensure all search service endpoint names are mocked to "test-service"
-    add_general_regex_sanitizer(
-        value="://fakesearchendpoint.search.windows.net",
-        regex=r"://(.+).search.windows.net"
-    )
+    add_general_regex_sanitizer(value="://fakesearchendpoint.search.windows.net", regex=r"://(.+).search.windows.net")
     # Remove storage connection strings from recordings
-    add_general_regex_sanitizer(
-        value="AccountKey=FAKE;",
-        regex=r"AccountKey=([^;]+);"
-    )
+    add_general_regex_sanitizer(value="AccountKey=FAKE;", regex=r"AccountKey=([^;]+);")
     # Remove storage account names from recordings
-    add_general_regex_sanitizer(
-        value="AccountName=fakestoragecs;",
-        regex=r"AccountName=([^;]+);"
-    )
+    add_general_regex_sanitizer(value="AccountName=fakestoragecs;", regex=r"AccountName=([^;]+);")
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_queries.py` & `azure-search-documents-11.4.0b4/tests/test_queries.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,17 +38,15 @@
 
     def test_filter(self):
         query = AutocompleteQuery(search_text="text", suggester_name="sg")
         assert query.request.filter is None
         query.filter("expr0")
         assert query.request.filter == "expr0"
 
-        query = AutocompleteQuery(
-            search_text="text", suggester_name="sg", filter="expr1"
-        )
+        query = AutocompleteQuery(search_text="text", suggester_name="sg", filter="expr1")
         assert query.request.filter == "expr1"
         query.filter("expr2")
         assert query.request.filter == "expr2"
 
 
 class TestSearchQuery:
     def test_init(self):
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_client_search_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_client_search_live.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 from azure.search.documents import SearchClient
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchClient(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_search_client(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         self._test_get_search_simple(client)
         self._test_get_search_simple_with_top(client)
@@ -40,46 +39,38 @@
         assert len(results) == 3
 
         results = list(client.search(search_text="motel", top=3))
         assert len(results) == 2
 
     def _test_get_search_filter(self, client):
         select = ["hotelName", "category", "description"]
-        results = list(client.search(
-            search_text="WiFi",
-            filter="category eq 'Budget'",
-            select=",".join(select),
-            order_by="hotelName desc"
-        ))
-        assert [x["hotelName"] for x in results] == sorted(
-            [x["hotelName"] for x in results], reverse=True
+        results = list(
+            client.search(
+                search_text="WiFi", filter="category eq 'Budget'", select=",".join(select), order_by="hotelName desc"
+            )
         )
+        assert [x["hotelName"] for x in results] == sorted([x["hotelName"] for x in results], reverse=True)
         expected = {
             "category",
             "hotelName",
             "description",
             "@search.score",
             "@search.reranker_score",
             "@search.highlights",
             "@search.captions",
         }
         assert all(set(x) == expected for x in results)
         assert all(x["category"] == "Budget" for x in results)
 
     def _test_get_search_filter_array(self, client):
         select = ["hotelName", "category", "description"]
-        results = list(client.search(
-            search_text="WiFi",
-            filter="category eq 'Budget'",
-            select=select,
-            order_by="hotelName desc"
-        ))
-        assert [x["hotelName"] for x in results] == sorted(
-            [x["hotelName"] for x in results], reverse=True
+        results = list(
+            client.search(search_text="WiFi", filter="category eq 'Budget'", select=select, order_by="hotelName desc")
         )
+        assert [x["hotelName"] for x in results] == sorted([x["hotelName"] for x in results], reverse=True)
         expected = {
             "category",
             "hotelName",
             "description",
             "@search.score",
             "@search.reranker_score",
             "@search.highlights",
@@ -107,18 +98,15 @@
     def _test_get_search_facets_none(self, client):
         select = ("hotelName", "category", "description")
         results = client.search(search_text="WiFi", select=",".join(select))
         assert results.get_facets() is None
 
     def _test_get_search_facets_result(self, client):
         select = ("hotelName", "category", "description")
-        results = client.search(search_text="WiFi",
-                                facets=["category"],
-                                select=",".join(select)
-                                )
+        results = client.search(search_text="WiFi", facets=["category"], select=",".join(select))
         assert results.get_facets() == {
             "category": [
                 {"value": "Budget", "count": 4},
                 {"value": "Luxury", "count": 1},
             ]
         }
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_regex_flags.py` & `azure-search-documents-11.4.0b4/tests/test_regex_flags.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,143 +5,93 @@
 
 from azure.search.documents.indexes.models import SearchIndex, RegexFlags, PatternAnalyzer, PatternTokenizer
 from azure.search.documents.indexes._generated.models import (
     PatternAnalyzer as _PatternAnalyzer,
     PatternTokenizer as _PatternTokenizer,
 )
 
+
 def test_unpack_search_index():
-    pattern_analyzer = _PatternAnalyzer(
-            name="test_analyzer",
-            flags="CANON_EQ"
-        )
+    pattern_analyzer = _PatternAnalyzer(name="test_analyzer", flags="CANON_EQ")
     analyzers = []
     analyzers.append(pattern_analyzer)
-    pattern_tokenizer = _PatternTokenizer(
-        name="test_tokenizer",
-        flags="CANON_EQ"
-    )
+    pattern_tokenizer = _PatternTokenizer(name="test_tokenizer", flags="CANON_EQ")
     tokenizers = []
     tokenizers.append(pattern_tokenizer)
-    index = SearchIndex(
-        name="test",
-        fields=None,
-        analyzers=analyzers,
-        tokenizers=tokenizers
-    )
+    index = SearchIndex(name="test", fields=None, analyzers=analyzers, tokenizers=tokenizers)
     result = SearchIndex._from_generated(index)
     assert isinstance(result.analyzers[0], PatternAnalyzer)
     assert isinstance(result.analyzers[0].flags, list)
     assert result.analyzers[0].flags[0] == "CANON_EQ"
     assert isinstance(result.tokenizers[0], PatternTokenizer)
     assert isinstance(result.tokenizers[0].flags, list)
     assert result.tokenizers[0].flags[0] == "CANON_EQ"
 
+
 def test_multi_unpack_search_index():
-    pattern_analyzer = _PatternAnalyzer(
-            name="test_analyzer",
-            flags="CANON_EQ|MULTILINE"
-        )
+    pattern_analyzer = _PatternAnalyzer(name="test_analyzer", flags="CANON_EQ|MULTILINE")
     analyzers = []
     analyzers.append(pattern_analyzer)
-    pattern_tokenizer = _PatternTokenizer(
-        name="test_tokenizer",
-        flags="CANON_EQ|MULTILINE"
-    )
+    pattern_tokenizer = _PatternTokenizer(name="test_tokenizer", flags="CANON_EQ|MULTILINE")
     tokenizers = []
     tokenizers.append(pattern_tokenizer)
-    index = SearchIndex(
-        name="test",
-        fields=None,
-        analyzers=analyzers,
-        tokenizers=tokenizers
-    )
+    index = SearchIndex(name="test", fields=None, analyzers=analyzers, tokenizers=tokenizers)
     result = SearchIndex._from_generated(index)
     assert isinstance(result.analyzers[0], PatternAnalyzer)
     assert isinstance(result.analyzers[0].flags, list)
     assert result.analyzers[0].flags[0] == "CANON_EQ"
     assert result.analyzers[0].flags[1] == "MULTILINE"
     assert isinstance(result.tokenizers[0], PatternTokenizer)
     assert isinstance(result.tokenizers[0].flags, list)
     assert result.tokenizers[0].flags[0] == "CANON_EQ"
     assert result.tokenizers[0].flags[1] == "MULTILINE"
 
+
 def test_unpack_search_index_enum():
-    pattern_analyzer = _PatternAnalyzer(
-            name="test_analyzer",
-            flags=RegexFlags.canon_eq
-        )
+    pattern_analyzer = _PatternAnalyzer(name="test_analyzer", flags=RegexFlags.canon_eq)
     analyzers = []
     analyzers.append(pattern_analyzer)
-    pattern_tokenizer = _PatternTokenizer(
-        name="test_tokenizer",
-        flags=RegexFlags.canon_eq
-    )
+    pattern_tokenizer = _PatternTokenizer(name="test_tokenizer", flags=RegexFlags.canon_eq)
     tokenizers = []
     tokenizers.append(pattern_tokenizer)
-    index = SearchIndex(
-        name="test",
-        fields=None,
-        analyzers=analyzers,
-        tokenizers=tokenizers
-    )
+    index = SearchIndex(name="test", fields=None, analyzers=analyzers, tokenizers=tokenizers)
     result = SearchIndex._from_generated(index)
     assert isinstance(result.analyzers[0], PatternAnalyzer)
     assert isinstance(result.analyzers[0].flags, list)
     assert result.analyzers[0].flags[0] == "CANON_EQ"
     assert isinstance(result.tokenizers[0], PatternTokenizer)
     assert isinstance(result.tokenizers[0].flags, list)
     assert result.tokenizers[0].flags[0] == "CANON_EQ"
 
+
 def test_pack_search_index():
-    pattern_analyzer = PatternAnalyzer(
-            name="test_analyzer",
-            flags=["CANON_EQ"]
-        )
+    pattern_analyzer = PatternAnalyzer(name="test_analyzer", flags=["CANON_EQ"])
     analyzers = []
     analyzers.append(pattern_analyzer)
-    pattern_tokenizer = PatternTokenizer(
-        name="test_tokenizer",
-        flags=["CANON_EQ"]
-    )
+    pattern_tokenizer = PatternTokenizer(name="test_tokenizer", flags=["CANON_EQ"])
     tokenizers = []
     tokenizers.append(pattern_tokenizer)
-    index = SearchIndex(
-        name="test",
-        fields=None,
-        analyzers=analyzers,
-        tokenizers=tokenizers
-    )
+    index = SearchIndex(name="test", fields=None, analyzers=analyzers, tokenizers=tokenizers)
     result = index._to_generated()
     assert isinstance(result.analyzers[0], _PatternAnalyzer)
     assert isinstance(result.analyzers[0].flags, str)
     assert result.analyzers[0].flags == "CANON_EQ"
     assert isinstance(result.tokenizers[0], _PatternTokenizer)
     assert isinstance(result.tokenizers[0].flags, str)
     assert result.tokenizers[0].flags == "CANON_EQ"
 
+
 def test_multi_pack_search_index():
-    pattern_analyzer = PatternAnalyzer(
-            name="test_analyzer",
-            flags=["CANON_EQ", "MULTILINE"]
-        )
+    pattern_analyzer = PatternAnalyzer(name="test_analyzer", flags=["CANON_EQ", "MULTILINE"])
     analyzers = []
     analyzers.append(pattern_analyzer)
-    pattern_tokenizer = PatternTokenizer(
-        name="test_analyzer",
-        flags=["CANON_EQ", "MULTILINE"]
-    )
+    pattern_tokenizer = PatternTokenizer(name="test_analyzer", flags=["CANON_EQ", "MULTILINE"])
     tokenizers = []
     tokenizers.append(pattern_tokenizer)
-    index = SearchIndex(
-        name="test",
-        fields=None,
-        analyzers=analyzers,
-        tokenizers=tokenizers
-    )
+    index = SearchIndex(name="test", fields=None, analyzers=analyzers, tokenizers=tokenizers)
     result = index._to_generated()
     assert isinstance(result.analyzers[0], _PatternAnalyzer)
     assert isinstance(result.analyzers[0].flags, str)
     assert result.analyzers[0].flags == "CANON_EQ|MULTILINE"
     assert isinstance(result.tokenizers[0], _PatternTokenizer)
     assert isinstance(result.tokenizers[0].flags, str)
     assert result.tokenizers[0].flags == "CANON_EQ|MULTILINE"
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client_synonym_map_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client_synonym_map_live.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,14 @@
 from azure.search.documents.indexes.models import SynonymMap
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchClientSynonymMaps(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_synonym_map(self, endpoint, api_key):
         client = SearchIndexClient(endpoint, api_key)
         self._test_create_synonym_map(client)
         self._test_delete_synonym_map(client)
@@ -65,17 +64,19 @@
             "USA, United States, United States of America",
             "Washington, Wash. => WA",
         ]
         synonym_map = SynonymMap(name=name, synonyms=synonyms)
         result = client.create_synonym_map(synonym_map)
         etag = result.e_tag
 
-        synonym_map.synonyms = "\n".join([
-            "Washington, Wash. => WA",
-        ])
+        synonym_map.synonyms = "\n".join(
+            [
+                "Washington, Wash. => WA",
+            ]
+        )
         client.create_or_update_synonym_map(synonym_map)
 
         result.e_tag = etag
         with pytest.raises(HttpResponseError):
             client.delete_synonym_map(result, match_condition=MatchConditions.IfNotModified)
         client.delete_synonym_map(name)
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client_alias_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client_alias_live.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/tests/test_index_field_helpers.py` & `azure-search-documents-11.4.0b4/tests/test_index_field_helpers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 # ------------------------------------
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 # ------------------------------------
 
 from azure.search.documents.indexes.models import ComplexField, SearchableField, SimpleField, SearchFieldDataType
 
+
 def test_edm_contents():
     assert SearchFieldDataType.String == "Edm.String"
     assert SearchFieldDataType.Int32 == "Edm.Int32"
     assert SearchFieldDataType.Int64 == "Edm.Int64"
     assert SearchFieldDataType.Double == "Edm.Double"
     assert SearchFieldDataType.Boolean == "Edm.Boolean"
     assert SearchFieldDataType.DateTimeOffset == "Edm.DateTimeOffset"
     assert SearchFieldDataType.GeographyPoint == "Edm.GeographyPoint"
     assert SearchFieldDataType.ComplexType == "Edm.ComplexType"
     assert SearchFieldDataType.Collection("foo") == "Collection(foo)"
 
+
 class TestComplexField:
     def test_single(self):
         fld = ComplexField(name="foo", fields=[])
         assert fld.name == "foo"
         assert fld.type == SearchFieldDataType.ComplexType
 
         assert fld.sortable is None
@@ -41,14 +43,15 @@
         assert fld.searchable is None
         assert fld.filterable is None
         assert fld.analyzer_name is None
         assert fld.search_analyzer_name is None
         assert fld.index_analyzer_name is None
         assert fld.synonym_map_names is None
 
+
 class TestSimplexField:
     def test_defaults(self):
         fld = SimpleField(name="foo", type=SearchFieldDataType.Double)
         assert fld.name == "foo"
         assert fld.type == SearchFieldDataType.Double
         assert fld.hidden == False
         assert fld.sortable == False
@@ -57,14 +60,15 @@
         assert fld.filterable == False
 
         assert fld.analyzer_name is None
         assert fld.search_analyzer_name is None
         assert fld.index_analyzer_name is None
         assert fld.synonym_map_names is None
 
+
 class TestSearchableField:
     def test_defaults(self):
         fld = SearchableField(name="foo", collection=True)
         assert fld.name == "foo"
         assert fld.type == SearchFieldDataType.Collection(SearchFieldDataType.String)
         assert fld.hidden == False
         assert fld.sortable == False
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_index_documents_batch.py` & `azure-search-documents-11.4.0b4/tests/test_index_documents_batch.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,14 +58,12 @@
 
         method(["doc4", "doc5"])
         assert len(batch.actions) == 5
 
         method(("doc6", "doc7"))
         assert len(batch.actions) == 7
 
-        assert all(
-            action.action_type == METHOD_MAP[method_name] for action in batch.actions
-        )
+        assert all(action.action_type == METHOD_MAP[method_name] for action in batch.actions)
         assert all(type(action) == IndexAction for action in batch.actions)
 
         expected = ["doc{}".format(i) for i in range(1, 8)]
         assert [action.additional_properties for action in batch.actions] == expected
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_client_buffered_sender_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_client_buffered_sender_live.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,14 @@
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 TIME_TO_SLEEP = 3
 
 
 class TestSearchIndexingBufferedSender(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_search_client_index_buffered_sender(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         batch_client = SearchIndexingBufferedSender(endpoint, index_name, api_key)
         try:
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_index_client.py` & `azure-search-documents-11.4.0b4/tests/test_search_index_client.py`

 * *Files 7% similar despite different names*

```diff
@@ -34,15 +34,15 @@
             "api-key": "new_api_key",
             "Accept": "application/json;odata.metadata=minimal",
         }
 
     def test_get_search_client(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexClient("endpoint", credential)
-        search_client = client.get_search_client('index')
+        search_client = client.get_search_client("index")
         assert isinstance(search_client, SearchClient)
 
     @mock.patch(
         "azure.search.documents.indexes._generated.operations._search_service_client_operations.SearchServiceClientOperationsMixin.get_service_statistics"
     )
     def test_get_service_statistics(self, mock_get_stats):
         client = SearchIndexClient("endpoint", CREDENTIAL)
@@ -60,18 +60,18 @@
         assert mock_get_stats.called
         assert mock_get_stats.call_args[0] == ()
         assert mock_get_stats.call_args[1] == {"headers": client._headers}
 
     def test_index_endpoint_https(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexClient("endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         client = SearchIndexClient("https://endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         with pytest.raises(ValueError):
             client = SearchIndexClient("http://endpoint", credential)
 
         with pytest.raises(ValueError):
             client = SearchIndexClient(12345, credential)
 
@@ -96,28 +96,25 @@
             "api-key": "new_api_key",
             "Accept": "application/json;odata.metadata=minimal",
         }
 
     def test_indexer_endpoint_https(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexerClient("endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         client = SearchIndexerClient("https://endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         with pytest.raises(ValueError):
             client = SearchIndexerClient("http://endpoint", credential)
 
         with pytest.raises(ValueError):
             client = SearchIndexerClient(12345, credential)
 
     def test_datasource_with_empty_connection_string(self):
-        container = SearchIndexerDataContainer(name='searchcontainer')
+        container = SearchIndexerDataContainer(name="searchcontainer")
         data_source_connection = SearchIndexerDataSourceConnection(
-            name="test",
-            type="azureblob",
-            connection_string="",
-            container=container
+            name="test", type="azureblob", connection_string="", container=container
         )
         packed_data_source_connection = data_source_connection._to_generated()
         assert packed_data_source_connection.credentials.connection_string == "<unchanged>"
```

## Comparing `azure-search-documents-11.4.0b3/tests/search_service_preparer.py` & `azure-search-documents-11.4.0b4/tests/search_service_preparer.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,69 +16,75 @@
 from azure_devtools.scenario_tests.exceptions import AzureTestError
 
 from azure.core.credentials import AzureKeyCredential
 from azure.core.exceptions import ResourceNotFoundError
 
 SERVICE_URL_FMT = "https://{}.{}/indexes?api-version=2021-04-30-Preview"
 TIME_TO_SLEEP = 3
-SEARCH_ENDPOINT_SUFFIX = environ.get("SEARCH_ENDPOINT_SUFFIX", "servicebus.windows.net")
+SEARCH_ENDPOINT_SUFFIX = environ.get("SEARCH_ENDPOINT_SUFFIX", "search.windows.net")
 
 SearchEnvVarPreparer = functools.partial(
     EnvironmentVariableLoader,
     "search",
     search_service_endpoint="https://fakesearchendpoint.search.windows.net",
     search_service_api_key="fakesearchapikey",
     search_service_name="fakesearchendpoint",
     search_query_api_key="fakequeryapikey",
     search_storage_connection_string="DefaultEndpointsProtocol=https;AccountName=fakestoragecs;AccountKey=FAKE;EndpointSuffix=core.windows.net",
-    search_storage_container_name="fakestoragecontainer"
+    search_storage_container_name="fakestoragecontainer",
 )
 
+
 def _load_schema(filename):
     if not filename:
         return None
     cwd = dirname(realpath(__file__))
     return open(join(cwd, filename)).read()
 
+
 def _load_batch(filename):
     if not filename:
         return None
     cwd = dirname(realpath(__file__))
     try:
         return json.load(open(join(cwd, filename)))
     except UnicodeDecodeError:
-        return json.load(open(join(cwd, filename), encoding='utf-8'))
+        return json.load(open(join(cwd, filename), encoding="utf-8"))
+
 
 def _clean_up_indexes(endpoint, api_key):
     from azure.search.documents.indexes import SearchIndexClient
-    client = SearchIndexClient(endpoint, AzureKeyCredential(api_key))
+
+    client = SearchIndexClient(endpoint, AzureKeyCredential(api_key), retry_backoff_factor=60)
 
     # wipe the synonym maps which seem to survive the index
     for map in client.get_synonym_maps():
         client.delete_synonym_map(map.name)
 
-    #wipe out any existing aliases
+    # wipe out any existing aliases
     for alias in client.list_aliases():
         client.delete_alias(alias)
 
     # wipe any existing indexes
     for index in client.list_indexes():
         client.delete_index(index)
 
 
 def _clean_up_indexers(endpoint, api_key):
     from azure.search.documents.indexes import SearchIndexerClient
-    client = SearchIndexerClient(endpoint, AzureKeyCredential(api_key))
+
+    client = SearchIndexerClient(endpoint, AzureKeyCredential(api_key), retry_backoff_factor=60)
     for indexer in client.get_indexers():
         client.delete_indexer(indexer)
     for datasource in client.get_data_source_connection_names():
         client.delete_data_source_connection(datasource)
     for skillset in client.get_skillset_names():
         client.delete_skillset(skillset)
 
+
 def _set_up_index(service_name, endpoint, api_key, schema, index_batch):
     from azure.core.credentials import AzureKeyCredential
     from azure.search.documents import SearchClient
     from azure.search.documents._generated.models import IndexBatch
 
     schema = _load_schema(schema)
     index_batch = _load_batch(index_batch)
@@ -86,31 +92,30 @@
         index_name = json.loads(schema)["name"]
         response = requests.post(
             SERVICE_URL_FMT.format(service_name, SEARCH_ENDPOINT_SUFFIX),
             headers={"Content-Type": "application/json", "api-key": api_key},
             data=schema,
         )
         if response.status_code != 201:
-            raise AzureTestError(
-                "Could not create a search index {}".format(response.status_code)
-            )
-        
+            raise AzureTestError("Could not create a search index {}".format(response.status_code))
+
     # optionally load data into the index
     if index_batch and schema:
         batch = IndexBatch.deserialize(index_batch)
         index_client = SearchClient(endpoint, index_name, AzureKeyCredential(api_key))
         results = index_client.index_documents(batch)
         if not all(result.succeeded for result in results):
             raise AzureTestError("Document upload to search index failed")
 
         # Indexing is asynchronous, so if you get a 200 from the REST API, that only means that the documents are
         # persisted, not that they're searchable yet. The only way to check for searchability is to run queries,
         # and even then things are eventually consistent due to replication. In the Track 1 SDK tests, we "solved"
         # this by using a constant delay between indexing and querying.
         import time
+
         time.sleep(TIME_TO_SLEEP)
 
 
 def _trim_kwargs_from_test_function(fn, kwargs):
     # the next function is the actual test function. the kwargs need to be trimmed so
     # that parameters which are not required will not be passed to it.
     if not getattr(fn, "__is_preparer", False):
@@ -119,61 +124,64 @@
         except AttributeError:
             args, _, kw, _ = inspect.getargspec(fn)  # pylint: disable=deprecated-method
         if kw is None:
             args = set(args)
             for key in [k for k in kwargs if k not in args]:
                 del kwargs[key]
 
-def search_decorator(*, schema, index_batch):
 
+def search_decorator(*, schema, index_batch):
     @wrapt.decorator
     def wrapper(func, _, args, kwargs):
         # set up hotels search index
         test = args[0]
-        api_key = kwargs.get('search_service_api_key')
-        endpoint = kwargs.get('search_service_endpoint')
-        service_name = kwargs.get('search_service_name')
+        api_key = kwargs.get("search_service_api_key")
+        endpoint = kwargs.get("search_service_endpoint")
+        service_name = kwargs.get("search_service_name")
         if test.is_live:
             _clean_up_indexes(endpoint, api_key)
             _set_up_index(service_name, endpoint, api_key, schema, index_batch)
             _clean_up_indexers(endpoint, api_key)
-        index_name = json.loads(_load_schema(schema))['name'] if schema else None
+        index_name = json.loads(_load_schema(schema))["name"] if schema else None
         index_batch_data = _load_batch(index_batch) if index_batch else None
 
         # ensure that the names in the test signatures are in the
         # bag of kwargs
-        kwargs['endpoint'] = endpoint        
-        kwargs['api_key'] = AzureKeyCredential(api_key)
-        kwargs['index_name'] = index_name
-        kwargs['index_batch'] = index_batch_data
+        kwargs["endpoint"] = endpoint
+        kwargs["api_key"] = AzureKeyCredential(api_key)
+        kwargs["index_name"] = index_name
+        kwargs["index_batch"] = index_batch_data
 
         trimmed_kwargs = {k: v for k, v in kwargs.items()}
         _trim_kwargs_from_test_function(func, trimmed_kwargs)
 
         return func(*args, **trimmed_kwargs)
+
     return wrapper
 
+
 # FIXME: DELETE EVERYTHING AFTER THIS LINE BEFORE MERGING
 
 from devtools_testutils import ResourceGroupPreparer, AzureMgmtPreparer
 from devtools_testutils.resource_testcase import RESOURCE_GROUP_PARAM
 import datetime
 
 
 # TODO: Remove this
 class SearchResourceGroupPreparer(ResourceGroupPreparer):
     def create_resource(self, name, **kwargs):
         result = super(SearchResourceGroupPreparer, self).create_resource(name, **kwargs)
         if self.is_live and self._need_creation:
             expiry = datetime.datetime.now() + datetime.timedelta(days=1)
-            resource_group_params = dict(tags={'DeleteAfter': expiry.isoformat()}, location=self.location)
+            resource_group_params = dict(tags={"DeleteAfter": expiry.isoformat()}, location=self.location)
             self.client.resource_groups.create_or_update(name, resource_group_params)
         return result
 
-#TODO: Remove this
+
+# TODO: Remove this
 class SearchServicePreparer(AzureMgmtPreparer):
     def __init__(
         self,
         schema=None,
         index_batch=None,
         name_prefix="search",
         resource_group_parameter_name=RESOURCE_GROUP_PARAM,
@@ -226,17 +234,15 @@
 
         self.mgmt_client = self.create_mgmt_client(SearchManagementClient)
 
         # create the search service
         from azure.mgmt.search.models import SearchService, Sku
 
         service_config = SearchService(location="West US", sku=Sku(name="basic"))
-        resource = self.mgmt_client.services.begin_create_or_update(
-            group_name, self.service_name, service_config
-        )
+        resource = self.mgmt_client.services.begin_create_or_update(group_name, self.service_name, service_config)
 
         retries = 4
         for i in range(retries):
             try:
                 result = resource.result()
                 if result.provisioning_state == ProvisioningState.succeeded:
                     break
@@ -247,40 +253,34 @@
             time.sleep(TIME_TO_SLEEP)
 
         # note the for/else here: will raise an error if we *don't* break
         # above i.e. if result.provisioning state was never "Succeeded"
         else:
             raise AzureTestError("Could not create a search service")
 
-        api_key = self.mgmt_client.admin_keys.get(
-            group_name, self.service_name
-        ).primary_key
+        api_key = self.mgmt_client.admin_keys.get(group_name, self.service_name).primary_key
 
         if self.schema:
             response = requests.post(
                 SERVICE_URL_FMT.format(self.service_name, SEARCH_ENDPOINT_SUFFIX),
                 headers={"Content-Type": "application/json", "api-key": api_key},
                 data=self.schema,
             )
             if response.status_code != 201:
-                raise AzureTestError(
-                    "Could not create a search index {}".format(response.status_code)
-                )
+                raise AzureTestError("Could not create a search index {}".format(response.status_code))
             self.index_name = schema["name"]
 
         # optionally load data into the index
         if self.index_batch and self.schema:
             from azure.core.credentials import AzureKeyCredential
             from azure.search.documents import SearchClient
             from azure.search.documents._generated.models import IndexBatch
 
             batch = IndexBatch.deserialize(self.index_batch)
-            index_client = SearchClient(
-                self.endpoint, self.index_name, AzureKeyCredential(api_key)
-            )
+            index_client = SearchClient(self.endpoint, self.index_name, AzureKeyCredential(api_key))
             results = index_client.index_documents(batch)
             if not all(result.succeeded for result in results):
                 raise AzureTestError("Document upload to search index failed")
 
             # Indexing is asynchronous, so if you get a 200 from the REST API, that only means that the documents are
             # persisted, not that they're searchable yet. The only way to check for searchability is to run queries,
             # and even then things are eventually consistent due to replication. In the Track 1 SDK tests, we "solved"
@@ -298,32 +298,34 @@
     def remove_resource(self, name, **kwargs):
         if not self.is_live:
             return
 
         group_name = self._get_resource_group(**kwargs).name
         self.mgmt_client.services.delete(group_name, self.service_name)
 
+
 # FIXME: DELETE EVERYTHING AFTER THIS LINE BEFORE MERGING
 
 from devtools_testutils import ResourceGroupPreparer, AzureMgmtPreparer
 from devtools_testutils.resource_testcase import RESOURCE_GROUP_PARAM
 import datetime
 
 
 # TODO: Remove this
 class SearchResourceGroupPreparer(ResourceGroupPreparer):
     def create_resource(self, name, **kwargs):
         result = super(SearchResourceGroupPreparer, self).create_resource(name, **kwargs)
         if self.is_live and self._need_creation:
             expiry = datetime.datetime.now() + datetime.timedelta(days=1)
-            resource_group_params = dict(tags={'DeleteAfter': expiry.isoformat()}, location=self.location)
+            resource_group_params = dict(tags={"DeleteAfter": expiry.isoformat()}, location=self.location)
             self.client.resource_groups.create_or_update(name, resource_group_params)
         return result
 
-#TODO: Remove this
+
+# TODO: Remove this
 class SearchServicePreparer(AzureMgmtPreparer):
     def __init__(
         self,
         schema=None,
         index_batch=None,
         name_prefix="search",
         resource_group_parameter_name=RESOURCE_GROUP_PARAM,
@@ -376,17 +378,15 @@
 
         self.mgmt_client = self.create_mgmt_client(SearchManagementClient)
 
         # create the search service
         from azure.mgmt.search.models import SearchService, Sku
 
         service_config = SearchService(location="West US", sku=Sku(name="basic"))
-        resource = self.mgmt_client.services.begin_create_or_update(
-            group_name, self.service_name, service_config
-        )
+        resource = self.mgmt_client.services.begin_create_or_update(group_name, self.service_name, service_config)
 
         retries = 4
         for i in range(retries):
             try:
                 result = resource.result()
                 if result.provisioning_state == ProvisioningState.succeeded:
                     break
@@ -397,40 +397,34 @@
             time.sleep(TIME_TO_SLEEP)
 
         # note the for/else here: will raise an error if we *don't* break
         # above i.e. if result.provisioning state was never "Succeeded"
         else:
             raise AzureTestError("Could not create a search service")
 
-        api_key = self.mgmt_client.admin_keys.get(
-            group_name, self.service_name
-        ).primary_key
+        api_key = self.mgmt_client.admin_keys.get(group_name, self.service_name).primary_key
 
         if self.schema:
             response = requests.post(
                 SERVICE_URL_FMT.format(self.service_name, SEARCH_ENDPOINT_SUFFIX),
                 headers={"Content-Type": "application/json", "api-key": api_key},
                 data=self.schema,
             )
             if response.status_code != 201:
-                raise AzureTestError(
-                    "Could not create a search index {}".format(response.status_code)
-                )
+                raise AzureTestError("Could not create a search index {}".format(response.status_code))
             self.index_name = schema["name"]
 
         # optionally load data into the index
         if self.index_batch and self.schema:
             from azure.core.credentials import AzureKeyCredential
             from azure.search.documents import SearchClient
             from azure.search.documents._generated.models import IndexBatch
 
             batch = IndexBatch.deserialize(self.index_batch)
-            index_client = SearchClient(
-                self.endpoint, self.index_name, AzureKeyCredential(api_key)
-            )
+            index_client = SearchClient(self.endpoint, self.index_name, AzureKeyCredential(api_key))
             results = index_client.index_documents(batch)
             if not all(result.succeeded for result in results):
                 raise AzureTestError("Document upload to search index failed")
 
             # Indexing is asynchronous, so if you get a 200 from the REST API, that only means that the documents are
             # persisted, not that they're searchable yet. The only way to check for searchability is to run queries,
             # and even then things are eventually consistent due to replication. In the Track 1 SDK tests, we "solved"
@@ -448,32 +442,34 @@
     def remove_resource(self, name, **kwargs):
         if not self.is_live:
             return
 
         group_name = self._get_resource_group(**kwargs).name
         self.mgmt_client.services.delete(group_name, self.service_name)
 
+
 # FIXME: DELETE EVERYTHING AFTER THIS LINE BEFORE MERGING
 
 from devtools_testutils import ResourceGroupPreparer, AzureMgmtPreparer
 from devtools_testutils.resource_testcase import RESOURCE_GROUP_PARAM
 import datetime
 
 
 # TODO: Remove this
 class SearchResourceGroupPreparer(ResourceGroupPreparer):
     def create_resource(self, name, **kwargs):
         result = super(SearchResourceGroupPreparer, self).create_resource(name, **kwargs)
         if self.is_live and self._need_creation:
             expiry = datetime.datetime.now() + datetime.timedelta(days=1)
-            resource_group_params = dict(tags={'DeleteAfter': expiry.isoformat()}, location=self.location)
+            resource_group_params = dict(tags={"DeleteAfter": expiry.isoformat()}, location=self.location)
             self.client.resource_groups.create_or_update(name, resource_group_params)
         return result
 
-#TODO: Remove this
+
+# TODO: Remove this
 class SearchServicePreparer(AzureMgmtPreparer):
     def __init__(
         self,
         schema=None,
         index_batch=None,
         name_prefix="search",
         resource_group_parameter_name=RESOURCE_GROUP_PARAM,
@@ -526,17 +522,15 @@
 
         self.mgmt_client = self.create_mgmt_client(SearchManagementClient)
 
         # create the search service
         from azure.mgmt.search.models import SearchService, Sku
 
         service_config = SearchService(location="West US", sku=Sku(name="basic"))
-        resource = self.mgmt_client.services.begin_create_or_update(
-            group_name, self.service_name, service_config
-        )
+        resource = self.mgmt_client.services.begin_create_or_update(group_name, self.service_name, service_config)
 
         retries = 4
         for i in range(retries):
             try:
                 result = resource.result()
                 if result.provisioning_state == ProvisioningState.succeeded:
                     break
@@ -547,40 +541,34 @@
             time.sleep(TIME_TO_SLEEP)
 
         # note the for/else here: will raise an error if we *don't* break
         # above i.e. if result.provisioning state was never "Succeeded"
         else:
             raise AzureTestError("Could not create a search service")
 
-        api_key = self.mgmt_client.admin_keys.get(
-            group_name, self.service_name
-        ).primary_key
+        api_key = self.mgmt_client.admin_keys.get(group_name, self.service_name).primary_key
 
         if self.schema:
             response = requests.post(
                 SERVICE_URL_FMT.format(self.service_name, SEARCH_ENDPOINT_SUFFIX),
                 headers={"Content-Type": "application/json", "api-key": api_key},
                 data=self.schema,
             )
             if response.status_code != 201:
-                raise AzureTestError(
-                    "Could not create a search index {}".format(response.status_code)
-                )
+                raise AzureTestError("Could not create a search index {}".format(response.status_code))
             self.index_name = schema["name"]
 
         # optionally load data into the index
         if self.index_batch and self.schema:
             from azure.core.credentials import AzureKeyCredential
             from azure.search.documents import SearchClient
             from azure.search.documents._generated.models import IndexBatch
 
             batch = IndexBatch.deserialize(self.index_batch)
-            index_client = SearchClient(
-                self.endpoint, self.index_name, AzureKeyCredential(api_key)
-            )
+            index_client = SearchClient(self.endpoint, self.index_name, AzureKeyCredential(api_key))
             results = index_client.index_documents(batch)
             if not all(result.succeeded for result in results):
                 raise AzureTestError("Document upload to search index failed")
 
             # Indexing is asynchronous, so if you get a 200 from the REST API, that only means that the documents are
             # persisted, not that they're searchable yet. The only way to check for searchability is to run queries,
             # and even then things are eventually consistent due to replication. In the Track 1 SDK tests, we "solved"
@@ -598,32 +586,34 @@
     def remove_resource(self, name, **kwargs):
         if not self.is_live:
             return
 
         group_name = self._get_resource_group(**kwargs).name
         self.mgmt_client.services.delete(group_name, self.service_name)
 
+
 # FIXME: DELETE EVERYTHING AFTER THIS LINE BEFORE MERGING
 
 from devtools_testutils import ResourceGroupPreparer, AzureMgmtPreparer
 from devtools_testutils.resource_testcase import RESOURCE_GROUP_PARAM
 import datetime
 
 
 # TODO: Remove this
 class SearchResourceGroupPreparer(ResourceGroupPreparer):
     def create_resource(self, name, **kwargs):
         result = super(SearchResourceGroupPreparer, self).create_resource(name, **kwargs)
         if self.is_live and self._need_creation:
             expiry = datetime.datetime.now() + datetime.timedelta(days=1)
-            resource_group_params = dict(tags={'DeleteAfter': expiry.isoformat()}, location=self.location)
+            resource_group_params = dict(tags={"DeleteAfter": expiry.isoformat()}, location=self.location)
             self.client.resource_groups.create_or_update(name, resource_group_params)
         return result
 
-#TODO: Remove this
+
+# TODO: Remove this
 class SearchServicePreparer(AzureMgmtPreparer):
     def __init__(
         self,
         schema=None,
         index_batch=None,
         name_prefix="search",
         resource_group_parameter_name=RESOURCE_GROUP_PARAM,
@@ -676,17 +666,15 @@
 
         self.mgmt_client = self.create_mgmt_client(SearchManagementClient)
 
         # create the search service
         from azure.mgmt.search.models import SearchService, Sku
 
         service_config = SearchService(location="West US", sku=Sku(name="basic"))
-        resource = self.mgmt_client.services.begin_create_or_update(
-            group_name, self.service_name, service_config
-        )
+        resource = self.mgmt_client.services.begin_create_or_update(group_name, self.service_name, service_config)
 
         retries = 4
         for i in range(retries):
             try:
                 result = resource.result()
                 if result.provisioning_state == ProvisioningState.succeeded:
                     break
@@ -697,40 +685,34 @@
             time.sleep(TIME_TO_SLEEP)
 
         # note the for/else here: will raise an error if we *don't* break
         # above i.e. if result.provisioning state was never "Succeeded"
         else:
             raise AzureTestError("Could not create a search service")
 
-        api_key = self.mgmt_client.admin_keys.get(
-            group_name, self.service_name
-        ).primary_key
+        api_key = self.mgmt_client.admin_keys.get(group_name, self.service_name).primary_key
 
         if self.schema:
             response = requests.post(
                 SERVICE_URL_FMT.format(self.service_name, SEARCH_ENDPOINT_SUFFIX),
                 headers={"Content-Type": "application/json", "api-key": api_key},
                 data=self.schema,
             )
             if response.status_code != 201:
-                raise AzureTestError(
-                    "Could not create a search index {}".format(response.status_code)
-                )
+                raise AzureTestError("Could not create a search index {}".format(response.status_code))
             self.index_name = schema["name"]
 
         # optionally load data into the index
         if self.index_batch and self.schema:
             from azure.core.credentials import AzureKeyCredential
             from azure.search.documents import SearchClient
             from azure.search.documents._generated.models import IndexBatch
 
             batch = IndexBatch.deserialize(self.index_batch)
-            index_client = SearchClient(
-                self.endpoint, self.index_name, AzureKeyCredential(api_key)
-            )
+            index_client = SearchClient(self.endpoint, self.index_name, AzureKeyCredential(api_key))
             results = index_client.index_documents(batch)
             if not all(result.succeeded for result in results):
                 raise AzureTestError("Document upload to search index failed")
 
             # Indexing is asynchronous, so if you get a 200 from the REST API, that only means that the documents are
             # persisted, not that they're searchable yet. The only way to check for searchability is to run queries,
             # and even then things are eventually consistent due to replication. In the Track 1 SDK tests, we "solved"
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_client_basic_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_client_basic_live.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -8,16 +8,16 @@
 
 from azure.core.exceptions import HttpResponseError
 from azure.search.documents import SearchClient
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
-class TestSearchClient(AzureRecordedTestCase):
 
+class TestSearchClient(AzureRecordedTestCase):
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_get_document_count(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         assert client.get_document_count() == 10
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_client_index_document_live.py` & `azure-search-documents-11.4.0b4/tests/test_search_client_index_document_live.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -9,16 +9,16 @@
 from azure.core.exceptions import HttpResponseError
 from azure.search.documents import SearchClient
 from devtools_testutils import AzureRecordedTestCase, recorded_by_proxy
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 TIME_TO_SLEEP = 3
 
-class TestSearchClientIndexDocument(AzureRecordedTestCase):
 
+class TestSearchClientIndexDocument(AzureRecordedTestCase):
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy
     def test_search_client_index_document(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         doc_count = 10
         doc_count = self._test_upload_documents_new(client, doc_count)
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_buffered_sender.py` & `azure-search-documents-11.4.0b4/tests/test_buffered_sender.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 )
 from azure.core.credentials import AzureKeyCredential
 from azure.core.exceptions import HttpResponseError, ServiceResponseTimeoutError
 from azure.search.documents.models import IndexingResult
 
 CREDENTIAL = AzureKeyCredential(key="test_api_key")
 
+
 class TestSearchBatchingClient:
     def test_search_indexing_buffered_sender_kwargs(self):
         with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, window=100) as client:
             assert client._batch_action_count == 512
             assert client._max_retries_per_action == 3
             assert client._auto_flush_interval == 60
             assert client._auto_flush
@@ -35,118 +36,112 @@
             assert len(client.actions) == 7
             actions = client._index_documents_batch.dequeue_actions()
             assert len(client.actions) == 0
             for action in actions:
                 client._index_documents_batch.enqueue_actions(action)
             assert len(client.actions) == 7
 
-
     @mock.patch(
         "azure.search.documents._search_indexing_buffered_sender.SearchIndexingBufferedSender._process_if_needed"
     )
     def test_process_if_needed(self, mock_process_if_needed):
         with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL) as client:
             client.upload_documents(["upload1"])
             client.delete_documents(["delete1", "delete2"])
         assert mock_process_if_needed.called
 
-
-    @mock.patch(
-        "azure.search.documents._search_indexing_buffered_sender.SearchIndexingBufferedSender._cleanup"
-    )
+    @mock.patch("azure.search.documents._search_indexing_buffered_sender.SearchIndexingBufferedSender._cleanup")
     def test_context_manager(self, mock_cleanup):
         with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False) as client:
             client.upload_documents(["upload1"])
             client.delete_documents(["delete1", "delete2"])
         assert mock_cleanup.called
 
     def test_flush(self):
         DOCUMENT = {
-            'Category': 'Hotel',
-            'HotelId': '1000',
-            'Rating': 4.0,
-            'Rooms': [],
-            'HotelName': 'Azure Inn',
+            "Category": "Hotel",
+            "HotelId": "1000",
+            "Rating": 4.0,
+            "Rooms": [],
+            "HotelName": "Azure Inn",
         }
-        with mock.patch.object(SearchIndexingBufferedSender, "_index_documents_actions", side_effect=HttpResponseError("Error")):
+        with mock.patch.object(
+            SearchIndexingBufferedSender, "_index_documents_actions", side_effect=HttpResponseError("Error")
+        ):
             with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False) as client:
                 client._index_key = "HotelId"
                 client.upload_documents([DOCUMENT])
                 client.flush()
                 assert len(client.actions) == 0
 
     def test_callback_new(self):
         on_new = mock.Mock()
-        with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False, on_new=on_new) as client:
+        with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_new=on_new
+        ) as client:
             client.upload_documents(["upload1"])
             assert on_new.called
 
     def test_callback_error(self):
         def mock_fail_index_documents(actions, timeout=86400):
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 400
                 result.succeeded = False
                 self.uploaded = self.uploaded + len(actions) - 1
                 return [result]
 
         on_error = mock.Mock()
-        with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_error=on_error) as client:
+        with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_error=on_error
+        ) as client:
             client._index_documents_actions = mock_fail_index_documents
             client._index_key = "id"
             client.upload_documents({"id": 0})
             client.flush()
             assert on_error.called
 
     def test_callback_error_on_timeout(self):
         def mock_fail_index_documents(actions, timeout=86400):
             import time
+
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 400
                 result.succeeded = False
                 self.uploaded = self.uploaded + len(actions) - 1
                 time.sleep(1)
                 return [result]
 
         on_error = mock.Mock()
-        with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_error=on_error) as client:
+        with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_error=on_error
+        ) as client:
             client._index_documents_actions = mock_fail_index_documents
             client._index_key = "id"
-            client.upload_documents([{"id": 0},{"id": 1}])
+            client.upload_documents([{"id": 0}, {"id": 1}])
             with pytest.raises(ServiceResponseTimeoutError):
                 client.flush(timeout=-1)
             assert on_error.call_count == 2
 
     def test_callback_progress(self):
         def mock_successful_index_documents(actions, timeout=86400):
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 200
                 result.succeeded = True
                 return [result]
 
         on_progress = mock.Mock()
         on_remove = mock.Mock()
-        with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_progress=on_progress,
-                                               on_remove=on_remove) as client:
+        with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_progress=on_progress, on_remove=on_remove
+        ) as client:
             client._index_documents_actions = mock_successful_index_documents
             client._index_key = "id"
             client.upload_documents({"id": 0})
             client.flush()
             assert on_progress.called
             assert on_remove.called
```

## Comparing `azure-search-documents-11.4.0b3/tests/test_search_client.py` & `azure-search-documents-11.4.0b4/tests/test_search_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,17 +27,15 @@
 CRUD_METHOD_NAMES = [
     "upload_documents",
     "delete_documents",
     "merge_documents",
     "merge_or_upload_documents",
 ]
 
-CRUD_METHOD_MAP = dict(
-    zip(CRUD_METHOD_NAMES, ["upload", "delete", "merge", "mergeOrUpload"])
-)
+CRUD_METHOD_MAP = dict(zip(CRUD_METHOD_NAMES, ["upload", "delete", "merge", "mergeOrUpload"]))
 
 
 class Test_odata:
     def test_const(self):
         assert odata("no escapes") == "no escapes"
 
     def test_numbers(self):
@@ -89,33 +87,26 @@
             "api-key": "test_api_key",
             "Accept": "application/json;odata.metadata=none",
             "foo": "bar",
         }
 
     def test_repr(self):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
-        assert repr(client) == "<SearchClient [endpoint={}, index={}]>".format(
-            repr("endpoint"), repr("index name")
-        )
+        assert repr(client) == "<SearchClient [endpoint={}, index={}]>".format(repr("endpoint"), repr("index name"))
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.count"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.count")
     def test_get_document_count(self, mock_count):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         client.get_document_count()
         assert mock_count.called
         assert mock_count.call_args[0] == ()
         assert len(mock_count.call_args[1]) == 1
         assert mock_count.call_args[1]["headers"] == client._headers
 
-
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.get"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.get")
     def test_get_document(self, mock_get):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         client.get_document("some_key")
         assert mock_get.called
         assert mock_get.call_args[0] == ()
         assert len(mock_get.call_args[1]) == 3
         assert mock_get.call_args[1]["headers"] == client._headers
@@ -128,60 +119,45 @@
         assert mock_get.called
         assert mock_get.call_args[0] == ()
         assert len(mock_get.call_args[1]) == 3
         assert mock_get.call_args[1]["headers"] == client._headers
         assert mock_get.call_args[1]["key"] == "some_key"
         assert mock_get.call_args[1]["selected_fields"] == "foo"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post")
     def test_search_query_argument(self, mock_search_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         result = client.search(search_text="search text")
         assert isinstance(result, ItemPaged)
         assert result._page_iterator_class is SearchPageIterator
         search_result = SearchDocumentsResult()
         search_result.results = [SearchResult(additional_properties={"key": "val"})]
         mock_search_post.return_value = search_result
         assert not mock_search_post.called
         next(result)
         assert mock_search_post.called
         assert mock_search_post.call_args[0] == ()
-        assert (
-            mock_search_post.call_args[1]["search_request"].search_text == "search text"
-        )
+        assert mock_search_post.call_args[1]["search_request"].search_text == "search text"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.suggest_post"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.suggest_post")
     def test_suggest_query_argument(self, mock_suggest_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
-        result = client.suggest(
-            search_text="search text", suggester_name="sg"
-        )
+        result = client.suggest(search_text="search text", suggester_name="sg")
         assert mock_suggest_post.called
         assert mock_suggest_post.call_args[0] == ()
         assert mock_suggest_post.call_args[1]["headers"] == client._headers
-        assert (
-            mock_suggest_post.call_args[1]["suggest_request"].search_text
-            == "search text"
-        )
+        assert mock_suggest_post.call_args[1]["suggest_request"].search_text == "search text"
 
     def test_suggest_bad_argument(self):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         with pytest.raises(TypeError) as e:
             client.suggest("bad_query")
-            assert str(e) == "Expected a SuggestQuery for 'query', but got {}".format(
-                repr("bad_query")
-            )
+            assert str(e) == "Expected a SuggestQuery for 'query', but got {}".format(repr("bad_query"))
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post")
     def test_get_count_reset_continuation_token(self, mock_search_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         result = client.search(search_text="search text")
         assert isinstance(result, ItemPaged)
         assert result._page_iterator_class is SearchPageIterator
         search_result = SearchDocumentsResult()
         search_result.results = [SearchResult(additional_properties={"key": "val"})]
@@ -192,39 +168,30 @@
         assert not result._first_page_iterator_instance.continuation_token
 
     @mock.patch(
         "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.autocomplete_post"
     )
     def test_autocomplete_query_argument(self, mock_autocomplete_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
-        result = client.autocomplete(
-            search_text="search text", suggester_name="sg"
-        )
+        result = client.autocomplete(search_text="search text", suggester_name="sg")
         assert mock_autocomplete_post.called
         assert mock_autocomplete_post.call_args[0] == ()
         assert mock_autocomplete_post.call_args[1]["headers"] == client._headers
-        assert (
-            mock_autocomplete_post.call_args[1]["autocomplete_request"].search_text
-            == "search text"
-        )
+        assert mock_autocomplete_post.call_args[1]["autocomplete_request"].search_text == "search text"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.count"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.count")
     def test_get_document_count_v2020_06_30(self, mock_count):
         client = SearchClient("endpoint", "index name", CREDENTIAL, api_version=ApiVersion.V2020_06_30)
         client.get_document_count()
         assert mock_count.called
         assert mock_count.call_args[0] == ()
         assert len(mock_count.call_args[1]) == 1
         assert mock_count.call_args[1]["headers"] == client._headers
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.get"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.get")
     def test_get_document_v2020_06_30(self, mock_get):
         client = SearchClient("endpoint", "index name", CREDENTIAL, api_version=ApiVersion.V2020_06_30)
         client.get_document("some_key")
         assert mock_get.called
         assert mock_get.call_args[0] == ()
         assert len(mock_get.call_args[1]) == 3
         assert mock_get.call_args[1]["headers"] == client._headers
@@ -237,103 +204,74 @@
         assert mock_get.called
         assert mock_get.call_args[0] == ()
         assert len(mock_get.call_args[1]) == 3
         assert mock_get.call_args[1]["headers"] == client._headers
         assert mock_get.call_args[1]["key"] == "some_key"
         assert mock_get.call_args[1]["selected_fields"] == "foo"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.search_post")
     def test_search_query_argument_v2020_06_30(self, mock_search_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL, api_version=ApiVersion.V2020_06_30)
         result = client.search(search_text="search text")
         assert isinstance(result, ItemPaged)
         assert result._page_iterator_class is SearchPageIterator
         search_result = SearchDocumentsResult()
         search_result.results = [SearchResult(additional_properties={"key": "val"})]
         mock_search_post.return_value = search_result
         assert not mock_search_post.called
         next(result)
         assert mock_search_post.called
         assert mock_search_post.call_args[0] == ()
-        assert (
-                mock_search_post.call_args[1]["search_request"].search_text == "search text"
-        )
+        assert mock_search_post.call_args[1]["search_request"].search_text == "search text"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.suggest_post"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.suggest_post")
     def test_suggest_query_argument_v2020_06_30(self, mock_suggest_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL, api_version=ApiVersion.V2020_06_30)
-        result = client.suggest(
-            search_text="search text", suggester_name="sg"
-        )
+        result = client.suggest(search_text="search text", suggester_name="sg")
         assert mock_suggest_post.called
         assert mock_suggest_post.call_args[0] == ()
         assert mock_suggest_post.call_args[1]["headers"] == client._headers
-        assert (
-                mock_suggest_post.call_args[1]["suggest_request"].search_text
-                == "search text"
-        )
+        assert mock_suggest_post.call_args[1]["suggest_request"].search_text == "search text"
 
     @mock.patch(
         "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.autocomplete_post"
     )
     def test_autocomplete_query_argument_v2020_06_30(self, mock_autocomplete_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL, api_version=ApiVersion.V2020_06_30)
-        result = client.autocomplete(
-            search_text="search text", suggester_name="sg"
-        )
+        result = client.autocomplete(search_text="search text", suggester_name="sg")
         assert mock_autocomplete_post.called
         assert mock_autocomplete_post.call_args[0] == ()
         assert mock_autocomplete_post.call_args[1]["headers"] == client._headers
-        assert (
-                mock_autocomplete_post.call_args[1]["autocomplete_request"].search_text
-                == "search text"
-        )
+        assert mock_autocomplete_post.call_args[1]["autocomplete_request"].search_text == "search text"
 
     def test_autocomplete_bad_argument(self):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         with pytest.raises(TypeError) as e:
             client.autocomplete("bad_query")
-            assert str(
-                e
-            ) == "Expected a AutocompleteQuery for 'query', but got {}".format(
-                repr("bad_query")
-            )
+            assert str(e) == "Expected a AutocompleteQuery for 'query', but got {}".format(repr("bad_query"))
 
-    @pytest.mark.parametrize(
-        "arg", [[], ["doc1"], ["doc1", "doc2"]], ids=lambda x: str(len(x)) + " docs"
-    )
+    @pytest.mark.parametrize("arg", [[], ["doc1"], ["doc1", "doc2"]], ids=lambda x: str(len(x)) + " docs")
     @pytest.mark.parametrize("method_name", CRUD_METHOD_NAMES)
     def test_add_method(self, arg, method_name):
-        with mock.patch.object(
-            SearchClient, "index_documents", return_value=None
-        ) as mock_index_documents:
+        with mock.patch.object(SearchClient, "index_documents", return_value=None) as mock_index_documents:
             client = SearchClient("endpoint", "index name", CREDENTIAL)
 
             method = getattr(client, method_name)
             method(arg, extra="foo")
 
             assert mock_index_documents.called
             assert len(mock_index_documents.call_args[0]) == 1
             batch = mock_index_documents.call_args[0][0]
             assert isinstance(batch, IndexDocumentsBatch)
-            assert all(
-                action.action_type == CRUD_METHOD_MAP[method_name]
-                for action in batch.actions
-            )
+            assert all(action.action_type == CRUD_METHOD_MAP[method_name] for action in batch.actions)
             assert [action.additional_properties for action in batch.actions] == arg
             assert mock_index_documents.call_args[1]["headers"] == client._headers
             assert mock_index_documents.call_args[1]["extra"] == "foo"
 
-    @mock.patch(
-        "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.index"
-    )
+    @mock.patch("azure.search.documents._generated.operations._documents_operations.DocumentsOperations.index")
     def test_index_documents(self, mock_index):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
 
         batch = IndexDocumentsBatch()
         actions = batch.add_upload_actions("upload1")
         assert len(actions) == 1
         for x in actions:
@@ -353,13 +291,15 @@
         assert mock_index.called
         assert mock_index.call_args[0] == ()
         assert len(mock_index.call_args[1]) == 4
         assert mock_index.call_args[1]["headers"] == client._headers
         assert mock_index.call_args[1]["extra"] == "foo"
 
     def test_request_too_large_error(self):
-        with mock.patch.object(SearchClient, "_index_documents_actions", side_effect=RequestEntityTooLargeError("Error")):
+        with mock.patch.object(
+            SearchClient, "_index_documents_actions", side_effect=RequestEntityTooLargeError("Error")
+        ):
             client = SearchClient("endpoint", "index name", CREDENTIAL)
             batch = IndexDocumentsBatch()
             batch.add_upload_actions("upload1")
             with pytest.raises(RequestEntityTooLargeError):
                 client.index_documents(batch, extra="foo")
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_alias_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_alias_live_async.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_indexer_client_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_indexer_client_live_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,24 +5,26 @@
 # --------------------------------------------------------------------------
 
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from azure.search.documents.indexes.aio import SearchIndexClient, SearchIndexerClient
 from azure.search.documents.indexes.models import (
-    SearchIndex, SearchIndexer, SearchIndexerDataContainer,
-    SearchIndexerDataSourceConnection)
+    SearchIndex,
+    SearchIndexer,
+    SearchIndexerDataContainer,
+    SearchIndexerDataSourceConnection,
+)
 from devtools_testutils import AzureRecordedTestCase
 from devtools_testutils.aio import recorded_by_proxy_async
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchIndexerClientTestAsync(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_search_indexers(self, endpoint, api_key, **kwargs):
         storage_cs = kwargs.get("search_storage_connection_string")
         container_name = kwargs.get("search_storage_container_name")
         client = SearchIndexerClient(endpoint, api_key)
@@ -41,25 +43,19 @@
                 await self._test_delete_indexer_if_unchanged(client, index_client, storage_cs, container_name)
 
     async def _prepare_indexer(self, client, index_client, storage_cs, name, container_name):
         data_source_connection = SearchIndexerDataSourceConnection(
             name=f"{name}-ds",
             type="azureblob",
             connection_string=storage_cs,
-            container=SearchIndexerDataContainer(name=container_name)
+            container=SearchIndexerDataContainer(name=container_name),
         )
         ds = await client.create_data_source_connection(data_source_connection)
 
-        fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        }]
+        fields = [{"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False}]
         index = SearchIndex(name=f"{name}-hotels", fields=fields)
         ind = await index_client.create_index(index)
         return SearchIndexer(name=name, data_source_name=ds.name, target_index_name=ind.name)
 
     async def _test_create_indexer(self, client, index_client, storage_cs, container_name):
         name = "create"
         indexer = await self._prepare_indexer(client, index_client, storage_cs, name, container_name)
@@ -107,22 +103,22 @@
         assert result.description == "updated"
 
     async def _test_reset_indexer(self, client, index_client, storage_cs, container_name):
         name = "reset"
         indexer = await self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         await client.create_indexer(indexer)
         await client.reset_indexer(name)
-        assert (await client.get_indexer_status(name)).last_result.status.lower() in ('inprogress', 'reset')
+        assert (await client.get_indexer_status(name)).last_result.status.lower() in ("inprogress", "reset")
 
     async def _test_run_indexer(self, client, index_client, storage_cs, container_name):
         name = "run"
         indexer = await self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         await client.create_indexer(indexer)
         await client.run_indexer(name)
-        assert (await client.get_indexer_status(name)).status == 'running'
+        assert (await client.get_indexer_status(name)).status == "running"
 
     async def _test_get_indexer_status(self, client, index_client, storage_cs, container_name):
         name = "get-status"
         indexer = await self._prepare_indexer(client, index_client, storage_cs, name, container_name)
         await client.create_indexer(indexer)
         status = await client.get_indexer_status(name)
         assert status.status is not None
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_search_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_search_live_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestClientTestAsync(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_search_client(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         async with client:
             await self._test_get_search_simple(client)
@@ -54,23 +53,18 @@
             results.append(x)
         assert len(results) == 2
 
     async def _test_get_search_filter(self, client):
         results = []
         select = ["hotelName", "category", "description"]
         async for x in await client.search(
-                search_text="WiFi",
-                filter="category eq 'Budget'",
-                select=",".join(select),
-                order_by="hotelName desc"
+            search_text="WiFi", filter="category eq 'Budget'", select=",".join(select), order_by="hotelName desc"
         ):
             results.append(x)
-        assert [x["hotelName"] for x in results] == sorted(
-            [x["hotelName"] for x in results], reverse=True
-        )
+        assert [x["hotelName"] for x in results] == sorted([x["hotelName"] for x in results], reverse=True)
         expected = {
             "category",
             "hotelName",
             "description",
             "@search.score",
             "@search.reranker_score",
             "@search.highlights",
@@ -79,23 +73,18 @@
         assert all(set(x) == expected for x in results)
         assert all(x["category"] == "Budget" for x in results)
 
     async def _test_get_search_filter_array(self, client):
         results = []
         select = ["hotelName", "category", "description"]
         async for x in await client.search(
-                search_text="WiFi",
-                filter="category eq 'Budget'",
-                select=select,
-                order_by="hotelName desc"
+            search_text="WiFi", filter="category eq 'Budget'", select=select, order_by="hotelName desc"
         ):
             results.append(x)
-        assert [x["hotelName"] for x in results] == sorted(
-            [x["hotelName"] for x in results], reverse=True
-        )
+        assert [x["hotelName"] for x in results] == sorted([x["hotelName"] for x in results], reverse=True)
         expected = {
             "category",
             "hotelName",
             "description",
             "@search.score",
             "@search.reranker_score",
             "@search.highlights",
@@ -118,27 +107,20 @@
         results = await client.search(search_text="hotel", minimum_coverage=50.0)
         cov = await results.get_coverage()
         assert isinstance(cov, float)
         assert cov >= 50.0
 
     async def _test_get_search_facets_none(self, client):
         select = ("hotelName", "category", "description")
-        results = await client.search(
-            search_text="WiFi",
-            select=",".join(select)
-        )
+        results = await client.search(search_text="WiFi", select=",".join(select))
         assert await results.get_facets() is None
 
     async def _test_get_search_facets_result(self, client):
         select = ("hotelName", "category", "description")
-        results = await client.search(
-            search_text="WiFi",
-            facets=["category"],
-            select=",".join(select)
-        )
+        results = await client.search(search_text="WiFi", facets=["category"], select=",".join(select))
         assert await results.get_facets() == {
             "category": [
                 {"value": "Budget", "count": 4},
                 {"value": "Luxury", "count": 1},
             ]
         }
 
@@ -159,11 +141,10 @@
     async def test_search_client_large(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         async with client:
             await self._test_get_search_simple_large(client)
 
     async def _test_get_search_simple_large(self, client):
         results = []
-        async for x in await client.search(search_text = ''):
+        async for x in await client.search(search_text=""):
             results.append(x)
         assert len(results) == 60
-
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_synonym_map_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_synonym_map_live_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 
 class TestSearchClientSynonymMaps(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_synonym_map(self, endpoint, api_key):
         client = SearchIndexClient(endpoint, api_key)
         async with client:
             await self._test_create_synonym_map(client)
@@ -67,17 +66,19 @@
             "USA, United States, United States of America",
             "Washington, Wash. => WA",
         ]
         synonym_map = SynonymMap(name=name, synonyms=synonyms)
         result = await client.create_synonym_map(synonym_map)
         etag = result.e_tag
 
-        synonym_map.synonyms = "\n".join([
-            "Washington, Wash. => WA",
-        ])
+        synonym_map.synonyms = "\n".join(
+            [
+                "Washington, Wash. => WA",
+            ]
+        )
         await client.create_or_update_synonym_map(synonym_map)
 
         result.e_tag = etag
         with pytest.raises(HttpResponseError):
             await client.delete_synonym_map(result, match_condition=MatchConditions.IfNotModified)
         await client.delete_synonym_map(name)
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_skillset_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_skillset_live_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,29 +6,29 @@
 
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     EntityLinkingSkill,
     EntityRecognitionSkill,
     EntityRecognitionSkillVersion,
     InputFieldMappingEntry,
     OutputFieldMappingEntry,
     SearchIndexerSkillset,
     SentimentSkill,
-    SentimentSkillVersion
+    SentimentSkillVersion,
 )
 from azure.search.documents.indexes.aio import SearchIndexerClient
 
 
 class TestSearchClientSkillsets(AzureRecordedTestCase):
-
+    @pytest.mark.skip("The skills are deprecated")
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_skillset_crud(self, api_key, endpoint):
         client = SearchIndexerClient(endpoint, api_key)
         async with client:
             await self._test_create_skillset(client)
@@ -38,46 +38,56 @@
             await self._test_create_or_update_skillset_if_unchanged(client)
             await self._test_create_or_update_skillset_inplace(client)
             await self._test_delete_skillset_if_unchanged(client)
             await self._test_delete_skillset(client)
 
     async def _test_create_skillset(self, client):
         name = "test-ss-create"
-        s1 = EntityRecognitionSkill(name="skill1",
-                                    inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                    outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
-                                    description="Skill Version 1",
-                                    model_version="1",
-                                    include_typeless_entities=True)
-
-        s2 = EntityRecognitionSkill(name="skill2",
-                                    inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                    outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
-                                    skill_version=EntityRecognitionSkillVersion.LATEST,
-                                    description="Skill Version 3",
-                                    model_version="3",
-                                    include_typeless_entities=True)
-        s3 = SentimentSkill(name="skill3",
-                            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                            outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
-                            skill_version=SentimentSkillVersion.V1,
-                            description="Sentiment V1",
-                            include_opinion_mining=True)
-
-        s4 = SentimentSkill(name="skill4",
-                            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                            outputs=[OutputFieldMappingEntry(name="confidenceScores", target_name="scoreS4")],
-                            skill_version=SentimentSkillVersion.V3,
-                            description="Sentiment V3",
-                            include_opinion_mining=True)
-
-        s5 = EntityLinkingSkill(name="skill5",
-                                inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                outputs=[OutputFieldMappingEntry(name="entities", target_name="entitiesS5")],
-                                minimum_precision=0.5)
+        s1 = EntityRecognitionSkill(
+            name="skill1",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS1")],
+            description="Skill Version 1",
+            model_version="1",
+            include_typeless_entities=True,
+        )
+
+        s2 = EntityRecognitionSkill(
+            name="skill2",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizationsS2")],
+            skill_version=EntityRecognitionSkillVersion.LATEST,
+            description="Skill Version 3",
+            model_version="3",
+            include_typeless_entities=True,
+        )
+        s3 = SentimentSkill(
+            name="skill3",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="score", target_name="scoreS3")],
+            skill_version=SentimentSkillVersion.V1,
+            description="Sentiment V1",
+            include_opinion_mining=True,
+        )
+
+        s4 = SentimentSkill(
+            name="skill4",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="confidenceScores", target_name="scoreS4")],
+            skill_version=SentimentSkillVersion.V3,
+            description="Sentiment V3",
+            include_opinion_mining=True,
+        )
+
+        s5 = EntityLinkingSkill(
+            name="skill5",
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="entities", target_name="entitiesS5")],
+            minimum_precision=0.5,
+        )
 
         skillset = SearchIndexerSkillset(name=name, skills=list([s1, s2, s3, s4, s5]), description="desc")
         result = await client.create_skillset(skillset)
 
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc"
@@ -95,45 +105,51 @@
         assert result.skills[4].minimum_precision == 0.5
 
         assert len(await client.get_skillsets()) == 1
         await client.reset_skills(result, [x.name for x in result.skills])
 
     async def _test_get_skillset(self, client):
         name = "test-ss-get"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
         skillset = SearchIndexerSkillset(name=name, skills=list([s]), description="desc")
         await client.create_skillset(skillset)
         result = await client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc"
         assert result.e_tag
         assert len(result.skills) == 1
         assert isinstance(result.skills[0], EntityRecognitionSkill)
 
     async def _test_get_skillsets(self, client):
         name1 = "test-ss-list-1"
         name2 = "test-ss-list-2"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name1, skills=list([s]), description="desc1")
         await client.create_skillset(skillset1)
         skillset2 = SearchIndexerSkillset(name=name2, skills=list([s]), description="desc2")
         await client.create_skillset(skillset2)
         result = await client.get_skillsets()
         assert isinstance(result, list)
         assert all(isinstance(x, SearchIndexerSkillset) for x in result)
         assert set(x.name for x in result).intersection([name1, name2]) == set([name1, name2])
 
     async def _test_create_or_update_skillset(self, client):
         name = "test-ss-create-or-update"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         await client.create_or_update_skillset(skillset1)
         expected_count = len(await client.get_skillsets())
         skillset2 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc2")
         await client.create_or_update_skillset(skillset2)
         assert len(await client.get_skillsets()) == expected_count
@@ -141,16 +157,18 @@
         result = await client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc2"
 
     async def _test_create_or_update_skillset_inplace(self, client):
         name = "test-ss-create-or-update-inplace"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         ss = await client.create_or_update_skillset(skillset1)
         expected_count = len(await client.get_skillsets())
         skillset2 = SearchIndexerSkillset(name=name, skills=[s], description="desc2", skillset=ss)
         await client.create_or_update_skillset(skillset2)
         assert len(await client.get_skillsets()) == expected_count
@@ -158,29 +176,33 @@
         result = await client.get_skillset(name)
         assert isinstance(result, SearchIndexerSkillset)
         assert result.name == name
         assert result.description == "desc2"
 
     async def _test_create_or_update_skillset_if_unchanged(self, client):
         name = "test-ss-create-or-update-unchanged"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="desc1")
         ss = await client.create_or_update_skillset(skillset1)
-        
-        ss.e_tag = 'changed_etag'
-        
+
+        ss.e_tag = "changed_etag"
+
         with pytest.raises(HttpResponseError):
             await client.create_or_update_skillset(ss, match_condition=MatchConditions.IfNotModified)
 
     async def _test_delete_skillset_if_unchanged(self, client):
         name = "test-ss-deleted-unchanged"
-        s = EntityRecognitionSkill(inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
-                                   outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")])
+        s = EntityRecognitionSkill(
+            inputs=[InputFieldMappingEntry(name="text", source="/document/content")],
+            outputs=[OutputFieldMappingEntry(name="organizations", target_name="organizations")],
+        )
 
         skillset = SearchIndexerSkillset(name=name, skills=list([s]), description="desc")
         result = await client.create_skillset(skillset)
         etag = result.e_tag
 
         skillset1 = SearchIndexerSkillset(name=name, skills=list([s]), description="updated")
         updated = await client.create_or_update_skillset(skillset1)
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_buffered_sender_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_buffered_sender_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,139 +9,134 @@
 )
 from azure.core.credentials import AzureKeyCredential
 from azure.core.exceptions import HttpResponseError, ServiceResponseTimeoutError
 from azure.search.documents.models import IndexingResult
 
 CREDENTIAL = AzureKeyCredential(key="test_api_key")
 
+
 class TestSearchBatchingClientAsync:
     async def test_search_indexing_buffered_sender_kwargs(self):
         async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, window=100) as client:
             assert client._batch_action_count == 512
             assert client._max_retries_per_action == 3
             assert client._auto_flush_interval == 60
             assert client._auto_flush
 
-
     async def test_batch_queue(self):
         async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False) as client:
             assert client._index_documents_batch
             await client.upload_documents(["upload1"])
             await client.delete_documents(["delete1", "delete2"])
             await client.merge_documents(["merge1", "merge2", "merge3"])
             await client.merge_or_upload_documents(["merge_or_upload1"])
             assert len(client.actions) == 7
             actions = await client._index_documents_batch.dequeue_actions()
             assert len(client.actions) == 0
             await client._index_documents_batch.enqueue_actions(actions)
             assert len(client.actions) == 7
 
-
     @mock.patch(
         "azure.search.documents.aio._search_indexing_buffered_sender_async.SearchIndexingBufferedSender._process_if_needed"
     )
     async def test_process_if_needed(self, mock_process_if_needed):
         async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL) as client:
             await client.upload_documents(["upload1"])
             await client.delete_documents(["delete1", "delete2"])
         assert mock_process_if_needed.called
 
-
     @mock.patch(
         "azure.search.documents.aio._search_indexing_buffered_sender_async.SearchIndexingBufferedSender._cleanup"
     )
     async def test_context_manager(self, mock_cleanup):
         async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False) as client:
             await client.upload_documents(["upload1"])
             await client.delete_documents(["delete1", "delete2"])
         assert mock_cleanup.called
 
     async def test_flush(self):
         DOCUMENT = {
-            'Category': 'Hotel',
-            'HotelId': '1000',
-            'Rating': 4.0,
-            'Rooms': [],
-            'HotelName': 'Azure Inn',
+            "Category": "Hotel",
+            "HotelId": "1000",
+            "Rating": 4.0,
+            "Rooms": [],
+            "HotelName": "Azure Inn",
         }
-        with mock.patch.object(SearchIndexingBufferedSender, "_index_documents_actions", side_effect=HttpResponseError("Error")):
+        with mock.patch.object(
+            SearchIndexingBufferedSender, "_index_documents_actions", side_effect=HttpResponseError("Error")
+        ):
             async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False) as client:
                 client._index_key = "HotelId"
                 await client.upload_documents([DOCUMENT])
                 await client.flush()
                 assert len(client.actions) == 0
 
     async def test_callback_new(self):
         on_new = mock.AsyncMock()
-        async with SearchIndexingBufferedSender("endpoint", "index name", CREDENTIAL, auto_flush=False, on_new=on_new) as client:
+        async with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_new=on_new
+        ) as client:
             await client.upload_documents(["upload1"])
             assert on_new.called
 
     async def test_callback_error(self):
         async def mock_fail_index_documents(actions, timeout=86400):
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 400
                 result.succeeded = False
                 self.uploaded = self.uploaded + len(actions) - 1
                 return [result]
 
         on_error = mock.AsyncMock()
-        async with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_error=on_error) as client:
+        async with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_error=on_error
+        ) as client:
             client._index_documents_actions = mock_fail_index_documents
             client._index_key = "id"
             await client.upload_documents({"id": 0})
             await client.flush()
             assert on_error.called
 
     async def test_callback_error_on_timeout(self):
         async def mock_fail_index_documents(actions, timeout=86400):
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 400
                 result.succeeded = False
                 self.uploaded = self.uploaded + len(actions) - 1
                 time.sleep(1)
                 return [result]
 
         on_error = mock.AsyncMock()
-        async with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_error=on_error) as client:
+        async with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_error=on_error
+        ) as client:
             client._index_documents_actions = mock_fail_index_documents
             client._index_key = "id"
-            await client.upload_documents([{"id": 0},{"id": 1}])
+            await client.upload_documents([{"id": 0}, {"id": 1}])
             with pytest.raises(ServiceResponseTimeoutError):
                 await client.flush(timeout=-1)
             assert on_error.call_count == 2
 
     async def test_callback_progress(self):
         async def mock_successful_index_documents(actions, timeout=86400):
             if len(actions) > 0:
                 result = IndexingResult()
-                result.key = actions[0].additional_properties.get('id')
+                result.key = actions[0].additional_properties.get("id")
                 result.status_code = 200
                 result.succeeded = True
                 return [result]
 
         on_progress = mock.AsyncMock()
         on_remove = mock.AsyncMock()
-        async with SearchIndexingBufferedSender("endpoint",
-                                               "index name",
-                                               CREDENTIAL,
-                                               auto_flush=False,
-                                               on_progress=on_progress,
-                                               on_remove=on_remove) as client:
+        async with SearchIndexingBufferedSender(
+            "endpoint", "index name", CREDENTIAL, auto_flush=False, on_progress=on_progress, on_remove=on_remove
+        ) as client:
             client._index_documents_actions = mock_successful_index_documents
             client._index_key = "id"
             await client.upload_documents({"id": 0})
             await client.flush()
             assert on_progress.called
             assert on_remove.called
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,22 +6,23 @@
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents._generated.models import SearchDocumentsResult, SearchResult
 from azure.search.documents.aio import SearchClient
 from azure.search.documents.aio._search_client_async import AsyncSearchPageIterator
 
 CREDENTIAL = AzureKeyCredential(key="test_api_key")
 
+
 class TestSearchClientAsync:
     @mock.patch(
         "azure.search.documents._generated.aio.operations._documents_operations.DocumentsOperations.search_post"
     )
     async def test_get_count_reset_continuation_token(self, mock_search_post):
         client = SearchClient("endpoint", "index name", CREDENTIAL)
         result = await client.search(search_text="search text")
         assert result._page_iterator_class is AsyncSearchPageIterator
         search_result = SearchDocumentsResult()
         search_result.results = [SearchResult(additional_properties={"key": "val"})]
         mock_search_post.return_value = search_result
         await result.__anext__()
         result._first_page_iterator_instance.continuation_token = "fake token"
         await result.get_count()
-        assert not result._first_page_iterator_instance.continuation_token
+        assert not result._first_page_iterator_instance.continuation_token
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_data_source_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_data_source_live_async.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,30 +7,26 @@
 import pytest
 from azure.core import MatchConditions
 from azure.core.exceptions import HttpResponseError
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     SearchIndexerDataSourceConnection,
     SearchIndexerDataContainer,
 )
 from azure.search.documents.indexes.aio import SearchIndexerClient
 
 
 class TestSearchClientDataSourcesAsync(AzureRecordedTestCase):
-
     def _create_data_source_connection(self, cs, name):
-        container = SearchIndexerDataContainer(name='searchcontainer')
+        container = SearchIndexerDataContainer(name="searchcontainer")
         data_source_connection = SearchIndexerDataSourceConnection(
-            name=name,
-            type="azureblob",
-            connection_string=cs,
-            container=container
+            name=name, type="azureblob", connection_string=cs, container=container
         )
         return data_source_connection
 
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_data_source(self, endpoint, api_key, **kwargs):
@@ -95,26 +91,30 @@
         etag = created.e_tag
 
         # Now update the data source connection
         data_source_connection.description = "updated"
         await client.create_or_update_data_source_connection(data_source_connection)
 
         # prepare data source connection
-        data_source_connection.e_tag = etag # reset to the original data source connection
+        data_source_connection.e_tag = etag  # reset to the original data source connection
         data_source_connection.description = "changed"
         with pytest.raises(HttpResponseError):
-            await client.create_or_update_data_source_connection(data_source_connection, match_condition=MatchConditions.IfNotModified)
+            await client.create_or_update_data_source_connection(
+                data_source_connection, match_condition=MatchConditions.IfNotModified
+            )
 
     async def _test_delete_datasource_if_unchanged(self, client, storage_cs):
         ds_name = "delunch"
         data_source_connection = self._create_data_source_connection(storage_cs, ds_name)
         created = await client.create_data_source_connection(data_source_connection)
         etag = created.e_tag
 
         # Now update the data source connection
         data_source_connection.description = "updated"
         await client.create_or_update_data_source_connection(data_source_connection)
 
         # prepare data source connection
-        data_source_connection.e_tag = etag # reset to the original data source connection
+        data_source_connection.e_tag = etag  # reset to the original data source connection
         with pytest.raises(HttpResponseError):
-            await client.delete_data_source_connection(data_source_connection, match_condition=MatchConditions.IfNotModified)
+            await client.delete_data_source_connection(
+                data_source_connection, match_condition=MatchConditions.IfNotModified
+            )
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_index_document_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_index_document_live_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 from devtools_testutils.aio import recorded_by_proxy_async
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 TIME_TO_SLEEP = 3
 
 
 class TestSearchClientDocumentsAsync(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_search_client_index_document(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         doc_count = 10
         async with client:
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_buffered_sender_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_buffered_sender_live_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 from devtools_testutils.aio import recorded_by_proxy_async
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
 TIME_TO_SLEEP = 3
 
 
 class TestSearchIndexingBufferedSenderAsync(AzureRecordedTestCase):
-
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_search_client_index_buffered_sender(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         batch_client = SearchIndexingBufferedSender(endpoint, index_name, api_key)
         try:
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,27 +9,29 @@
 from azure_devtools.scenario_tests.utilities import trim_kwargs_from_test_function
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.aio import SearchClient
 from azure.search.documents.indexes.aio import SearchIndexClient, SearchIndexerClient
 
 CREDENTIAL = AzureKeyCredential(key="test_api_key")
 
+
 def await_prepared_test(test_fn):
     """Synchronous wrapper for async test methods. Used to avoid making changes
     upstream to AbstractPreparer (which doesn't await the functions it wraps)
     """
 
     @functools.wraps(test_fn)
     def run(test_class_instance, *args, **kwargs):
         trim_kwargs_from_test_function(test_fn, kwargs)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(test_fn(test_class_instance, **kwargs))
 
     return run
 
+
 class TestSearchIndexClient:
     def test_index_init(self):
         client = SearchIndexClient("endpoint", CREDENTIAL)
         assert client._headers == {
             "api-key": "test_api_key",
             "Accept": "application/json;odata.metadata=minimal",
         }
@@ -46,24 +48,24 @@
             "api-key": "new_api_key",
             "Accept": "application/json;odata.metadata=minimal",
         }
 
     def test_get_search_client(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexClient("endpoint", credential)
-        search_client = client.get_search_client('index')
+        search_client = client.get_search_client("index")
         assert isinstance(search_client, SearchClient)
 
     def test_index_endpoint_https(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexClient("endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         client = SearchIndexClient("https://endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         with pytest.raises(ValueError):
             client = SearchIndexClient("http://endpoint", credential)
 
         with pytest.raises(ValueError):
             client = SearchIndexClient(12345, credential)
 
@@ -88,17 +90,17 @@
             "api-key": "new_api_key",
             "Accept": "application/json;odata.metadata=minimal",
         }
 
     def test_indexer_endpoint_https(self):
         credential = AzureKeyCredential(key="old_api_key")
         client = SearchIndexerClient("endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         client = SearchIndexerClient("https://endpoint", credential)
-        assert client._endpoint.startswith('https')
+        assert client._endpoint.startswith("https")
 
         with pytest.raises(ValueError):
             client = SearchIndexerClient("http://endpoint", credential)
 
         with pytest.raises(ValueError):
             client = SearchIndexerClient(12345, credential)
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_client_basic_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_client_basic_live_async.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -8,16 +8,16 @@
 from azure.core.exceptions import HttpResponseError
 from azure.search.documents.aio import SearchClient
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 
-class TestSearchClientAsync(AzureRecordedTestCase):
 
+class TestSearchClientAsync(AzureRecordedTestCase):
     @SearchEnvVarPreparer()
     @search_decorator(schema="hotel_schema.json", index_batch="hotel_small.json")
     @recorded_by_proxy_async
     async def test_get_document_count(self, endpoint, api_key, index_name):
         client = SearchClient(endpoint, index_name, api_key)
         async with client:
             assert await client.get_document_count() == 10
```

## Comparing `azure-search-documents-11.4.0b3/tests/async_tests/test_search_index_client_live_async.py` & `azure-search-documents-11.4.0b4/tests/async_tests/test_search_index_client_live_async.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,25 +8,25 @@
 from azure.core.exceptions import HttpResponseError
 from azure.core import MatchConditions
 from devtools_testutils.aio import recorded_by_proxy_async
 from devtools_testutils import AzureRecordedTestCase
 
 from search_service_preparer import SearchEnvVarPreparer, search_decorator
 from azure.search.documents.indexes.aio import SearchIndexClient
-from azure.search.documents.indexes.models import(
+from azure.search.documents.indexes.models import (
     AnalyzeTextOptions,
     CorsOptions,
     SearchIndex,
     ScoringProfile,
     SimpleField,
-    SearchFieldDataType
+    SearchFieldDataType,
 )
 
-class TestSearchIndexClientAsync(AzureRecordedTestCase):
 
+class TestSearchIndexClientAsync(AzureRecordedTestCase):
     @SearchEnvVarPreparer()
     @search_decorator(schema=None, index_batch=None)
     @recorded_by_proxy_async
     async def test_search_index_client(self, api_key, endpoint, index_name):
         client = SearchIndexClient(endpoint, api_key)
         index_name = "hotels"
         async with client:
@@ -51,28 +51,24 @@
         result = client.list_indexes()
         with pytest.raises(StopAsyncIteration):
             await result.__anext__()
 
     async def _test_create_index(self, client, index_name):
         fields = fields = [
             SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-            SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+            SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         ]
 
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
         index = SearchIndex(
-            name=index_name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+            name=index_name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options
+        )
         result = await client.create_index(index)
         assert result.name == "hotels"
         assert result.scoring_profiles[0].name == scoring_profile.name
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
 
     async def _test_list_indexes(self, client, index_name):
@@ -84,109 +80,73 @@
 
     async def _test_get_index(self, client, index_name):
         result = await client.get_index(index_name)
         assert result.name == index_name
 
     async def _test_get_index_statistics(self, client, index_name):
         result = await client.get_index_statistics(index_name)
-        assert set(result.keys()) == {'document_count', 'storage_size'}
+        assert set(result.keys()) == {"document_count", "storage_size"}
 
     async def _test_delete_indexes_if_unchanged(self, client):
         # First create an index
         name = "hotels-del-unchanged"
         fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        },
-        {
-          "name": "baseRate",
-          "type": "Edm.Double"
-        }]
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+            {"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False},
+            {"name": "baseRate", "type": "Edm.Double"},
+        ]
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = await client.create_index(index)
         etag = result.e_tag
         # get eTag and update
         index.scoring_profiles = []
         await client.create_or_update_index(index)
 
         index.e_tag = etag
         with pytest.raises(HttpResponseError):
             await client.delete_index(index, match_condition=MatchConditions.IfNotModified)
 
     async def _test_create_or_update_index(self, client):
         name = "hotels-cou"
         fields = fields = [
             SimpleField(name="hotelId", type=SearchFieldDataType.String, key=True),
-            SimpleField(name="baseRate", type=SearchFieldDataType.Double)
+            SimpleField(name="baseRate", type=SearchFieldDataType.Double),
         ]
 
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
         scoring_profiles = []
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = await client.create_or_update_index(index=index)
         assert len(result.scoring_profiles) == 0
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = await client.create_or_update_index(index=index)
         assert result.scoring_profiles[0].name == scoring_profile.name
         assert result.cors_options.allowed_origins == cors_options.allowed_origins
         assert result.cors_options.max_age_in_seconds == cors_options.max_age_in_seconds
 
     async def _test_create_or_update_indexes_if_unchanged(self, client):
         # First create an index
         name = "hotels-coa-unchanged"
         fields = [
-        {
-          "name": "hotelId",
-          "type": "Edm.String",
-          "key": True,
-          "searchable": False
-        },
-        {
-          "name": "baseRate",
-          "type": "Edm.Double"
-        }]
-        scoring_profile = ScoringProfile(
-            name="MyProfile"
-        )
+            {"name": "hotelId", "type": "Edm.String", "key": True, "searchable": False},
+            {"name": "baseRate", "type": "Edm.Double"},
+        ]
+        scoring_profile = ScoringProfile(name="MyProfile")
         scoring_profiles = []
         scoring_profiles.append(scoring_profile)
         cors_options = CorsOptions(allowed_origins=["*"], max_age_in_seconds=60)
-        index = SearchIndex(
-            name=name,
-            fields=fields,
-            scoring_profiles=scoring_profiles,
-            cors_options=cors_options)
+        index = SearchIndex(name=name, fields=fields, scoring_profiles=scoring_profiles, cors_options=cors_options)
         result = await client.create_index(index)
         etag = result.e_tag
         # get eTag and update
         index.scoring_profiles = []
         await client.create_or_update_index(index)
 
         index.e_tag = etag
```

## Comparing `azure-search-documents-11.4.0b3/tests/perfstress_tests/search_documents.py` & `azure-search-documents-11.4.0b4/tests/perfstress_tests/search_documents.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,32 +21,31 @@
         key = os.getenv("AZURE_SEARCH_API_KEY")
         self.service_client = SyncClient(service_endpoint, index_name, AzureKeyCredential(api_key))
         self.async_service_client = AsyncClient(service_endpoint, index_name, AzureKeyCredential(api_key))
 
     @staticmethod
     def add_arguments(parser):
         super(SearchDocumentsTest, SearchDocumentsTest).add_arguments(parser)
-        parser.add_argument('--num-documents', nargs='?', type=int,
-                            help='The number of results expect to be returned.',
-                            default=-1)
+        parser.add_argument(
+            "--num-documents", nargs="?", type=int, help="The number of results expect to be returned.", default=-1
+        )
 
     async def global_setup(self):
         await super().global_setup()
 
     async def close(self):
         await self.async_service_client.close()
         await super().close()
 
     def run_sync(self):
         if self.args.num_documents == -1:
             results = len(self.service_client.search(search_text="luxury"))
         else:
             results = len(self.service_client.search(search_text="luxury", top=self.args.num_documents))
 
-
     async def run_async(self):
         if self.args.num_documents == -1:
             results = await self.async_service_client.search(search_text="luxury")
         else:
             results = await self.async_service_client.search(search_text="luxury", top=self.args.num_documents)
         count = 0
         async for result in results:
```

## Comparing `azure-search-documents-11.4.0b3/tests/perfstress_tests/autocomplete.py` & `azure-search-documents-11.4.0b4/tests/perfstress_tests/autocomplete.py`

 * *Files 9% similar despite different names*

```diff
@@ -24,23 +24,28 @@
     async def close(self):
         await self.async_service_client.close()
         await super().close()
 
     @staticmethod
     def add_arguments(parser):
         super(AutoCompleteTest, AutoCompleteTest).add_arguments(parser)
-        parser.add_argument('--num-documents', nargs='?', type=int,
-                            help='The number of results expect to be returned.',
-                            default=-1)
+        parser.add_argument(
+            "--num-documents", nargs="?", type=int, help="The number of results expect to be returned.", default=-1
+        )
 
     def run_sync(self):
         if self.args.num_documents == -1:
             results = len(self.service_client.autocomplete(search_text="mot", suggester_name="sg"))
         else:
-            results = len(self.service_client.autocomplete(search_text="mot", suggester_name="sg", top=self.args.num_documents))
+            results = len(
+                self.service_client.autocomplete(search_text="mot", suggester_name="sg", top=self.args.num_documents)
+            )
 
     async def run_async(self):
         if self.args.num_documents == -1:
             results = len(await self.async_service_client.autocomplete(search_text="mot", suggester_name="sg"))
         else:
-            results = len(await self.async_service_client.autocomplete(search_text="mot", suggester_name="sg", top=self.args.num_documents))
-
+            results = len(
+                await self.async_service_client.autocomplete(
+                    search_text="mot", suggester_name="sg", top=self.args.num_documents
+                )
+            )
```

## Comparing `azure-search-documents-11.4.0b3/tests/perfstress_tests/suggest.py` & `azure-search-documents-11.4.0b4/tests/perfstress_tests/suggest.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,23 +24,28 @@
     async def close(self):
         await self.async_service_client.close()
         await super().close()
 
     @staticmethod
     def add_arguments(parser):
         super(SuggestTest, SuggestTest).add_arguments(parser)
-        parser.add_argument('--num-documents', nargs='?', type=int,
-                            help='The number of results expect to be returned.',
-                            default=-1)
+        parser.add_argument(
+            "--num-documents", nargs="?", type=int, help="The number of results expect to be returned.", default=-1
+        )
 
     def run_sync(self):
         if self.args.num_documents == -1:
             results = len(self.service_client.suggest(search_text="mot", suggester_name="sg"))
         else:
-            results = len(self.service_client.suggest(search_text="mot", suggester_name="sg", top=self.args.num_documents))
+            results = len(
+                self.service_client.suggest(search_text="mot", suggester_name="sg", top=self.args.num_documents)
+            )
 
     async def run_async(self):
         if self.args.num_documents == -1:
             results = len(await self.async_service_client.suggest(search_text="mot", suggester_name="sg"))
         else:
-            results = len(await self.async_service_client.suggest(search_text="mot", suggester_name="sg", top=self.args.num_documents))
-
+            results = len(
+                await self.async_service_client.suggest(
+                    search_text="mot", suggester_name="sg", top=self.args.num_documents
+                )
+            )
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_search_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_search_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,23 +17,27 @@
     QueryAnswerType,
     QueryCaptionType,
     QueryLanguage,
     QuerySpellerType,
     QueryType,
     SearchMode,
     ScoringStatistics,
+    Vector,
+    SemanticErrorHandling,
+    QueryDebugMode,
 )
 from ._search_documents_error import RequestEntityTooLargeError
 from ._index_documents_batch import IndexDocumentsBatch
 from ._paging import SearchItemPaged, SearchPageIterator
 from ._queries import AutocompleteQuery, SearchQuery, SuggestQuery
 from ._headers_mixin import HeadersMixin
 from ._utils import get_authentication_policy
 from ._version import SDK_MONIKER
 
+
 class SearchClient(HeadersMixin):
     """A client to interact with an existing Azure search index.
 
     :param endpoint: The URL endpoint of an Azure search service
     :type endpoint: str
     :param index_name: The name of the index to connect to
     :type index_name: str
@@ -53,19 +57,15 @@
             :dedent: 4
             :caption: Creating the SearchClient with an API key.
     """
 
     _ODATA_ACCEPT = "application/json;odata.metadata=none"  # type: str
 
     def __init__(
-            self,
-            endpoint: str,
-            index_name: str,
-            credential: Union[AzureKeyCredential, TokenCredential],
-            **kwargs
+        self, endpoint: str, index_name: str, credential: Union[AzureKeyCredential, TokenCredential], **kwargs: Any
     ) -> None:
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._endpoint = endpoint
         self._index_name = index_name
         self._credential = credential
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
@@ -86,127 +86,133 @@
                 authentication_policy=authentication_policy,
                 sdk_moniker=SDK_MONIKER,
                 api_version=self._api_version,
                 **kwargs
             )
 
     def __repr__(self) -> str:
-        return "<SearchClient [endpoint={}, index={}]>".format(
-            repr(self._endpoint), repr(self._index_name)
-        )[:1024]
+        return "<SearchClient [endpoint={}, index={}]>".format(repr(self._endpoint), repr(self._index_name))[:1024]
 
     def close(self) -> None:
         """Close the :class:`~azure.search.documents.SearchClient` session."""
         return self._client.close()
 
     @distributed_trace
     def get_document_count(self, **kwargs: Any) -> int:
         """Return the number of documents in the Azure search index.
 
+        :return: The count of documents in the index
         :rtype: int
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         return int(self._client.documents.count(**kwargs))
 
     @distributed_trace
     def get_document(self, key: str, selected_fields: Optional[List[str]] = None, **kwargs: Any) -> Dict:
         """Retrieve a document from the Azure search index by its key.
 
         :param key: The primary key value for the document to retrieve
         :type key: str
-        :param selected_fields: a allowlist of fields to include in the results
-        :type selected_fields: List[str]
-        :rtype:  Dict
+        :param selected_fields: an allow-list of fields to include in the results
+        :type selected_fields: list[str]
+        :return: The document as stored in the Azure search index
+        :rtype:  dict
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_get_document.py
                 :start-after: [START get_document]
                 :end-before: [END get_document]
                 :language: python
                 :dedent: 4
                 :caption: Get a specific document from the search index.
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        result = self._client.documents.get(
-            key=key, selected_fields=selected_fields, **kwargs
-        )
+        result = self._client.documents.get(key=key, selected_fields=selected_fields, **kwargs)
         return cast(dict, result)
 
     @distributed_trace
     def search(
-            self,
-            search_text: str,
-            *,
-            include_total_count: Optional[bool] = None,
-            facets: Optional[List[str]] = None,
-            filter: Optional[str] = None,
-            highlight_fields: Optional[str] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            order_by: Optional[List[str]] = None,
-            query_type: Optional[Union[str, QueryType]] = None,
-            scoring_parameters: Optional[List[str]] = None,
-            scoring_profile: Optional[str] = None,
-            search_fields: Optional[List[str]] = None,
-            search_mode: Optional[Union[str, SearchMode]] = None,
-            query_language: Optional[Union[str, QueryLanguage]] = None,
-            query_speller: Optional[Union[str, QuerySpellerType]] = None,
-            query_answer: Optional[Union[str, QueryAnswerType]] = None,
-            query_answer_count: Optional[int] = None,
-            query_caption: Optional[Union[str, QueryCaptionType]] = None,
-            query_caption_highlight: Optional[bool] = None,
-            semantic_fields: Optional[List[str]] = None,
-            semantic_configuration_name: Optional[str] = None,
-            select: Optional[List[str]] = None,
-            skip: Optional[int] = None,
-            top: Optional[int] = None,
-            scoring_statistics: Optional[Union[str, ScoringStatistics]] = None,
-            session_id: Optional[str] = None,
-            **kwargs: Any) -> SearchItemPaged[Dict]:
+        self,
+        search_text: str,
+        *,
+        include_total_count: Optional[bool] = None,
+        facets: Optional[List[str]] = None,
+        filter: Optional[str] = None,
+        highlight_fields: Optional[str] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        order_by: Optional[List[str]] = None,
+        query_type: Optional[Union[str, QueryType]] = None,
+        scoring_parameters: Optional[List[str]] = None,
+        scoring_profile: Optional[str] = None,
+        search_fields: Optional[List[str]] = None,
+        search_mode: Optional[Union[str, SearchMode]] = None,
+        query_language: Optional[Union[str, QueryLanguage]] = None,
+        query_speller: Optional[Union[str, QuerySpellerType]] = None,
+        query_answer: Optional[Union[str, QueryAnswerType]] = None,
+        query_answer_count: Optional[int] = None,
+        query_answer_threshold: Optional[float] = None,
+        query_caption: Optional[Union[str, QueryCaptionType]] = None,
+        query_caption_highlight: Optional[bool] = None,
+        semantic_fields: Optional[List[str]] = None,
+        semantic_configuration_name: Optional[str] = None,
+        select: Optional[List[str]] = None,
+        skip: Optional[int] = None,
+        top: Optional[int] = None,
+        scoring_statistics: Optional[Union[str, ScoringStatistics]] = None,
+        session_id: Optional[str] = None,
+        vector: Optional[List[float]] = None,
+        top_k: Optional[int] = None,
+        vector_fields: Optional[str] = None,
+        semantic_error_handling: Optional[Union[str, SemanticErrorHandling]] = None,
+        semantic_max_wait_in_milliseconds: Optional[int] = None,
+        debug: Optional[Union[str, QueryDebugMode]] = None,
+        **kwargs: Any
+    ) -> SearchItemPaged[Dict]:
         # pylint:disable=too-many-locals, disable=redefined-builtin
         """Search the Azure search index for documents.
 
         :param str search_text: A full-text search query expression; Use "*" or omit this parameter to
          match all documents.
         :keyword bool include_total_count: A value that specifies whether to fetch the total count of
          results. Default is false. Setting this value to true may have a performance impact. Note that
          the count returned is an approximation.
-        :keyword List[str] facets: The list of facet expressions to apply to the search query. Each facet
+        :keyword list[str] facets: The list of facet expressions to apply to the search query. Each facet
          expression contains a field name, optionally followed by a comma-separated list of name:value
          pairs.
         :keyword str filter: The OData $filter expression to apply to the search query.
         :keyword str highlight_fields: The comma-separated list of field names to use for hit highlights.
          Only searchable fields can be used for hit highlighting.
         :keyword str highlight_post_tag: A string tag that is appended to hit highlights. Must be set with
          highlightPreTag. Default is </em>.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. Default is <em>.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by a search query in order for the query to be reported as a success. This
          parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 100.
-        :keyword List[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
+        :keyword list[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
          expression can be either a field name or a call to either the geo.distance() or the
          search.score() functions. Each expression can be followed by asc to indicate ascending, and
          desc to indicate descending. The default is ascending order. Ties will be broken by the match
          scores of documents. If no OrderBy is specified, the default sort order is descending by
          document match score. There can be at most 32 $orderby clauses.
         :keyword query_type: A value that specifies the syntax of the search query. The default is
          'simple'. Use 'full' if your query uses the Lucene query syntax. Possible values include:
          'simple', 'full', "semantic".
         :paramtype query_type: str or ~azure.search.documents.models.QueryType
-        :keyword List[str] scoring_parameters: The list of parameter values to be used in scoring functions (for
+        :keyword list[str] scoring_parameters: The list of parameter values to be used in scoring functions (for
          example, referencePointParameter) using the format name-values. For example, if the scoring
          profile defines a function with a parameter called 'mylocation' the parameter string would be
          "mylocation--122.2,44.8" (without the quotes).
         :keyword str scoring_profile: The name of a scoring profile to evaluate match scores for matching
          documents in order to sort the results.
-        :keyword List[str] search_fields: The list of field names to which to scope the full-text search. When
+        :keyword list[str] search_fields: The list of field names to which to scope the full-text search. When
          using fielded search (fieldName:searchExpression) in a full Lucene query, the field names of
          each fielded search expression take precedence over any field names listed in this parameter.
         :keyword search_mode: A value that specifies whether any or all of the search terms must be
          matched in order to count the document as a match. Possible values include: 'any', 'all'.
         :paramtype search_mode: str or ~azure.search.documents.models.SearchMode
         :keyword query_language: The language of the search query. Possible values include: "none", "en-us",
          "en-gb", "en-in", "en-ca", "en-au", "fr-fr", "fr-ca", "de-de", "es-es", "es-mx", "zh-cn",
@@ -222,26 +228,28 @@
         :paramtype query_speller: str or ~azure.search.documents.models.QuerySpellerType
         :keyword query_answer: This parameter is only valid if the query type is 'semantic'. If set,
          the query returns answers extracted from key passages in the highest ranked documents.
          Possible values include: "none", "extractive".
         :paramtype query_answer: str or ~azure.search.documents.models.QueryAnswerType
         :keyword int query_answer_count: This parameter is only valid if the query type is 'semantic' and
          query answer is 'extractive'. Configures the number of answers returned. Default count is 1.
+        :keyword float query_answer_threshold: This parameter is only valid if the query type is 'semantic' and
+         query answer is 'extractive'. Configures the number of confidence threshold. Default count is 0.7.
         :keyword query_caption: This parameter is only valid if the query type is 'semantic'. If set, the
          query returns captions extracted from key passages in the highest ranked documents.
          Defaults to 'None'. Possible values include: "none", "extractive".
         :paramtype query_caption: str or ~azure.search.documents.models.QueryCaptionType
         :keyword bool query_caption_highlight: This parameter is only valid if the query type is 'semantic' when
          query caption is set to 'extractive'. Determines whether highlighting is enabled.
          Defaults to 'true'.
-        :keyword List[str] semantic_fields: The list of field names used for semantic search.
+        :keyword list[str] semantic_fields: The list of field names used for semantic search.
         :keyword semantic_configuration_name: The name of the semantic configuration that will be used when
          processing documents for queries of type semantic.
         :paramtype semantic_configuration_name: str
-        :keyword List[str] select: The list of fields to retrieve. If unspecified, all fields marked as retrievable
+        :keyword list[str] select: The list of fields to retrieve. If unspecified, all fields marked as retrievable
          in the schema are included.
         :keyword int skip: The number of search results to skip. This value cannot be greater than 100,000.
          If you need to scan documents in sequence, but cannot use $skip due to this limitation,
          consider using $orderby on a totally-ordered key and $filter with a range query instead.
         :keyword int top: The number of search results to retrieve. This can be used in conjunction with
          $skip to implement client-side paging of search results. If results are truncated due to
          server-side paging, the response will include a continuation token that can be used to issue
@@ -254,14 +262,30 @@
         :paramtype scoring_statistics: str or ~azure.search.documents.models.ScoringStatistics
         :keyword str session_id: A value to be used to create a sticky session, which can help getting more
          consistent results. As long as the same sessionId is used, a best-effort attempt will be made
          to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
          interfere with the load balancing of the requests across replicas and adversely affect the
          performance of the search service. The value used as sessionId cannot start with a '_'
          character.
+        :keyword semantic_error_handling: Allows the user to choose whether a semantic call should fail
+         completely (default / current behavior), or to return partial results. Known values are:
+         "partial" and "fail".
+        :paramtype semantic_error_handling: str or ~azure.search.documents.models.SemanticErrorHandling
+        :keyword int semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount of
+         time it takes for semantic enrichment to finish processing before the request fails.
+        :keyword debug: Enables a debugging tool that can be used to further explore your Semantic search
+         results. Known values are: "disabled", "speller", "semantic", and "all".
+        :paramtype debug: str or ~azure.search.documents.models.QueryDebugMode
+        :keyword vector: The vector representation of a search query.
+        :paramtype vector: List[float]
+        :keyword top_k: Number of nearest neighbors to return as top hits.
+        :paramtype top_k: int
+        :keyword vector_fields: Vector Fields of type Collection(Edm.Single) to be included in the vector
+          searched.
+        :paramtype vector_fields: str
         :rtype:  SearchItemPaged[Dict]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_simple_query.py
                 :start-after: [START simple_query]
                 :end-before: [END simple_query]
@@ -287,23 +311,27 @@
                 :dedent: 4
                 :caption: Get search result facets.
         """
         include_total_result_count = include_total_count
         filter_arg = filter
         search_fields_str = ",".join(search_fields) if search_fields else None
 
-        answers = query_answer if not query_answer_count else '{}|count-{}'.format(
-            query_answer, query_answer_count
-        )
+        answers = query_answer if not query_answer_count else "{}|count-{}".format(query_answer, query_answer_count)
+        answers = answers if not query_answer_threshold else "{}|threshold-{}".format(answers, query_answer_threshold)
 
-        captions = query_caption if not query_caption_highlight else '{}|highlight-{}'.format(
-            query_caption, query_caption_highlight
+        captions = (
+            query_caption
+            if not query_caption_highlight
+            else "{}|highlight-{}".format(query_caption, query_caption_highlight)
         )
 
         semantic_configuration = semantic_configuration_name
+        vector_option = None
+        if vector or top_k or vector_fields:
+            vector_option = Vector(value=vector, top_k=top_k, fields=vector_fields)
 
         query = SearchQuery(
             search_text=search_text,
             include_total_result_count=include_total_result_count,
             facets=facets,
             filter=filter_arg,
             highlight_fields=highlight_fields,
@@ -322,40 +350,43 @@
             captions=captions,
             semantic_fields=",".join(semantic_fields) if semantic_fields else None,
             semantic_configuration=semantic_configuration,
             select=select if isinstance(select, str) else None,
             skip=skip,
             top=top,
             session_id=session_id,
-            scoring_statistics=scoring_statistics
+            scoring_statistics=scoring_statistics,
+            vector=vector_option,
+            semantic_error_handling=semantic_error_handling,
+            semantic_max_wait_in_milliseconds=semantic_max_wait_in_milliseconds,
+            debug=debug,
         )
         if isinstance(select, list):
             query.select(select)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         kwargs["api_version"] = self._api_version
-        return SearchItemPaged(
-            self._client, query, kwargs, page_iterator_class=SearchPageIterator
-        )
+        return SearchItemPaged(self._client, query, kwargs, page_iterator_class=SearchPageIterator)
 
     @distributed_trace
     def suggest(
-            self,
-            search_text: str,
-            suggester_name: str,
-            *,
-            use_fuzzy_matching: Optional[bool] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            order_by: Optional[List[str]] = None,
-            search_fields: Optional[List[str]] = None,
-            select: Optional[List[str]] = None,
-            top: Optional[int] = None,
-            **kwargs) -> List[Dict]:
+        self,
+        search_text: str,
+        suggester_name: str,
+        *,
+        use_fuzzy_matching: Optional[bool] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        order_by: Optional[List[str]] = None,
+        search_fields: Optional[List[str]] = None,
+        select: Optional[List[str]] = None,
+        top: Optional[int] = None,
+        **kwargs
+    ) -> List[Dict]:
         """Get search suggestion results from the Azure search index.
 
         :param str search_text: Required. The search text to use to suggest documents. Must be at least 1
         character, and no more than 100 characters.
         :param str suggester_name: Required. The name of the suggester as specified in the suggesters
         collection that's part of the index definition.
         :keyword str filter: An OData expression that filters the documents considered for suggestions.
@@ -368,27 +399,28 @@
          highlightPreTag. If omitted, hit highlighting of suggestions is disabled.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. If omitted, hit highlighting of suggestions is disabled.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by a suggestions query in order for the query to be reported as a success. This
          parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 80.
-        :keyword List[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
+        :keyword list[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
          expression can be either a field name or a call to either the geo.distance() or the
          search.score() functions. Each expression can be followed by asc to indicate ascending, or desc
          to indicate descending. The default is ascending order. Ties will be broken by the match scores
          of documents. If no $orderby is specified, the default sort order is descending by document
          match score. There can be at most 32 $orderby clauses.
-        :keyword List[str] search_fields: The list of field names to search for the specified search text. Target
+        :keyword list[str] search_fields: The list of field names to search for the specified search text. Target
          fields must be included in the specified suggester.
-        :keyword List[str] select: The list of fields to retrieve. If unspecified, only the key field will be
+        :keyword list[str] select: The list of fields to retrieve. If unspecified, only the key field will be
          included in the results.
         :keyword int top: The number of suggestions to retrieve. The value must be a number between 1 and
          100. The default is 5.
-        :rtype:  List[Dict]
+        :return: List of documents.
+        :rtype:  list[Dict]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_suggestions.py
                 :start-after: [START suggest_query]
                 :end-before: [END suggest_query]
                 :language: python
@@ -409,34 +441,33 @@
             search_fields=search_fields_str,
             select=select if isinstance(select, str) else None,
             top=top,
         )
         if isinstance(select, list):
             query.select(select)
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        response = self._client.documents.suggest_post(
-            suggest_request=query.request, **kwargs
-        )
+        response = self._client.documents.suggest_post(suggest_request=query.request, **kwargs)
         results = [r.as_dict() for r in response.results]
         return results
 
     @distributed_trace
     def autocomplete(
-            self,
-            search_text: str,
-            suggester_name: str,
-            *,
-            mode: Optional[Union[str, AutocompleteMode]] = None,
-            use_fuzzy_matching: Optional[bool] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            search_fields: Optional[List[str]] = None,
-            top: Optional[int] = None,
-            **kwargs) -> List[Dict]:
+        self,
+        search_text: str,
+        suggester_name: str,
+        *,
+        mode: Optional[Union[str, AutocompleteMode]] = None,
+        use_fuzzy_matching: Optional[bool] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        search_fields: Optional[List[str]] = None,
+        top: Optional[int] = None,
+        **kwargs
+    ) -> List[Dict]:
         """Get search auto-completion results from the Azure search index.
 
         :param str search_text: The search text on which to base autocomplete results.
         :param str suggester_name: The name of the suggester as specified in the suggesters
         collection that's part of the index definition.
         :keyword mode: Specifies the mode for Autocomplete. The default is 'oneTerm'. Use
          'twoTerms' to get shingles and 'oneTermWithContext' to use the current context while producing
@@ -453,15 +484,15 @@
          highlightPreTag. If omitted, hit highlighting is disabled.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. If omitted, hit highlighting is disabled.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by an autocomplete query in order for the query to be reported as a success.
          This parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 80.
-        :keyword List[str] search_fields: The list of field names to consider when querying for auto-completed
+        :keyword list[str] search_fields: The list of field names to consider when querying for auto-completed
          terms. Target fields must be included in the specified suggester.
         :keyword int top: The number of auto-completed terms to retrieve. This must be a value between 1 and
          100. The default is 5.
         :rtype:  List[Dict]
 
         .. admonition:: Example:
 
@@ -485,30 +516,30 @@
             highlight_pre_tag=highlight_pre_tag,
             minimum_coverage=minimum_coverage,
             search_fields=search_fields_str,
             top=top,
         )
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        response = self._client.documents.autocomplete_post(
-            autocomplete_request=query.request, **kwargs
-        )
+        response = self._client.documents.autocomplete_post(autocomplete_request=query.request, **kwargs)
         results = [r.as_dict() for r in response.results]
         return results
 
+    # pylint:disable=client-method-missing-tracing-decorator
     def upload_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Upload documents to the Azure search index.
 
         An upload action is similar to an "upsert" where the document will be
         inserted if it is new and updated/replaced if it exists. All fields are
         replaced in the update case.
 
         :param documents: A list of documents to upload.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_crud_operations.py
                 :start-after: [START upload_document]
                 :end-before: [END upload_document]
                 :language: python
@@ -518,29 +549,31 @@
         batch = IndexDocumentsBatch()
         batch.add_upload_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator, delete-operation-wrong-return-type
     def delete_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Delete documents from the Azure search index
 
         Delete removes the specified document from the index. Any field you
         specify in a delete operation, other than the key field, will be
         ignored. If you want to remove an individual field from a document, use
         `merge_documents` instead and set the field explicitly to None.
 
         Delete operations are idempotent. That is, even if a document key does
         not exist in the index, attempting a delete operation with that key will
         result in a 200 status code.
 
         :param documents: A list of documents to delete.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_crud_operations.py
                 :start-after: [START delete_document]
                 :end-before: [END delete_document]
                 :language: python
@@ -550,25 +583,27 @@
         batch = IndexDocumentsBatch()
         batch.add_delete_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator
     def merge_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Merge documents in to existing documents in the Azure search index.
 
         Merge updates an existing document with the specified fields. If the
         document doesn't exist, the merge will fail. Any field you specify in a
         merge will replace the existing field in the document. This also applies
         to collections of primitive and complex types.
 
         :param documents: A list of documents to merge.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_crud_operations.py
                 :start-after: [START merge_document]
                 :end-before: [END merge_document]
                 :language: python
@@ -578,77 +613,74 @@
         batch = IndexDocumentsBatch()
         batch.add_merge_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator
     def merge_or_upload_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Merge documents in to existing documents in the Azure search index,
         or upload them if they do not yet exist.
 
         This action behaves like `merge_documents` if a document with the given
         key already exists in the index. If the document does not exist, it
         behaves like `upload_documents` with a new document.
 
         :param documents: A list of documents to merge or upload.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
         """
         batch = IndexDocumentsBatch()
         batch.add_merge_or_upload_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
     @distributed_trace
     def index_documents(self, batch: IndexDocumentsBatch, **kwargs: Any) -> List[IndexingResult]:
         """Specify a document operations to perform as a batch.
 
         :param batch: A batch of document operations to perform.
         :type batch: IndexDocumentsBatch
-        :rtype:  List[IndexingResult]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
         :raises :class:`~azure.search.documents.RequestEntityTooLargeError`
         """
         return self._index_documents_actions(actions=batch.actions, **kwargs)
 
     def _index_documents_actions(self, actions: List[IndexAction], **kwargs: Any) -> List[IndexingResult]:
         error_map = {413: RequestEntityTooLargeError}
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         batch = IndexBatch(actions=actions)
         try:
-            batch_response = self._client.documents.index(
-                batch=batch, error_map=error_map, **kwargs
-            )
+            batch_response = self._client.documents.index(batch=batch, error_map=error_map, **kwargs)
             return cast(List[IndexingResult], batch_response.results)
         except RequestEntityTooLargeError:
             if len(actions) == 1:
                 raise
             pos = round(len(actions) / 2)
             batch_response_first_half = self._index_documents_actions(
                 actions=actions[:pos], error_map=error_map, **kwargs
             )
             if batch_response_first_half:
-                result_first_half = cast(
-                    List[IndexingResult], batch_response_first_half.results
-                )
+                result_first_half = cast(List[IndexingResult], batch_response_first_half.results)
             else:
                 result_first_half = []
             batch_response_second_half = self._index_documents_actions(
                 actions=actions[pos:], error_map=error_map, **kwargs
             )
             if batch_response_second_half:
-                result_second_half = cast(
-                    List[IndexingResult], batch_response_second_half.results
-                )
+                result_second_half = cast(List[IndexingResult], batch_response_second_half.results)
             else:
                 result_second_half = []
             return result_first_half.extend(result_second_half)
 
-    def __enter__(self):
+    def __enter__(self) -> "SearchClient":
         self._client.__enter__()  # pylint:disable=no-member
         return self
 
-    def __exit__(self, *args):
+    def __exit__(self, *args) -> None:
         self._client.__exit__(*args)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_headers_mixin.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_headers_mixin.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_paging.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_paging.py`

 * *Files 4% similar despite different names*

```diff
@@ -55,46 +55,56 @@
 
     def _first_iterator_instance(self):
         if self._first_page_iterator_instance is None:
             self._first_page_iterator_instance = self.by_page()
         return self._first_page_iterator_instance
 
     def get_facets(self) -> Optional[Dict]:
-        """Return any facet results if faceting was requested."""
+        """Return any facet results if faceting was requested.
+
+        :return: facet results
+        :rtype: dict or None
+        """
         return self._first_iterator_instance().get_facets()
 
     def get_coverage(self) -> float:
         """Return the coverage percentage, if `minimum_coverage` was
         specificied for the query.
 
+        :return: coverage percentage
+        :rtype: float
         """
         return self._first_iterator_instance().get_coverage()
 
     def get_count(self) -> int:
         """Return the count of results if `include_total_count` was
         set for the query.
 
+        :return: count of results
+        :rtype: int
         """
         return self._first_iterator_instance().get_count()
 
     def get_answers(self) -> Optional[List[AnswerResult]]:
-        """Return answers."""
+        """Return answers.
+
+        :return: answers
+        :rtype: list[~azure.search.documents.models.AnswerResult] or None
+        """
         return self._first_iterator_instance().get_answers()
 
 
 # The pylint error silenced below seems spurious, as the inner wrapper does, in
 # fact, become a method of the class when it is applied.
 def _ensure_response(f):
     # pylint:disable=protected-access
     def wrapper(self, *args, **kw):
         if self._current_page is None:
             self._response = self._get_next(self.continuation_token)
-            self.continuation_token, self._current_page = self._extract_data(
-                self._response
-            )
+            self.continuation_token, self._current_page = self._extract_data(self._response)
         return f(self, *args, **kw)
 
     return wrapper
 
 
 class SearchPageIterator(PageIterator):
     def __init__(self, client, initial_query, kwargs, continuation_token=None) -> None:
@@ -107,28 +117,22 @@
         self._initial_query = initial_query
         self._kwargs = kwargs
         self._facets = None
         self._api_version = kwargs.pop("api_version", "2020-06-30")
 
     def _get_next_cb(self, continuation_token):
         if continuation_token is None:
-            return self._client.documents.search_post(
-                search_request=self._initial_query.request, **self._kwargs
-            )
+            return self._client.documents.search_post(search_request=self._initial_query.request, **self._kwargs)
 
         _next_link, next_page_request = unpack_continuation_token(continuation_token)
 
-        return self._client.documents.search_post(
-            search_request=next_page_request, **self._kwargs
-        )
+        return self._client.documents.search_post(search_request=next_page_request, **self._kwargs)
 
     def _extract_data_cb(self, response):
-        continuation_token = pack_continuation_token(
-            response, api_version=self._api_version
-        )
+        continuation_token = pack_continuation_token(response, api_version=self._api_version)
         results = [convert_search_result(r) for r in response.results]
         return continuation_token, results
 
     @_ensure_response
     def get_facets(self):
         self.continuation_token = None
         facets = self._response.facets
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_queries.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_queries.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,37 +5,37 @@
 # --------------------------------------------------------------------------
 from typing import Type, Union
 from ._generated.models import AutocompleteRequest, SearchRequest, SuggestRequest
 
 
 class _QueryBase:
 
-    _request_type: Union[Type[AutocompleteRequest], Type[SearchRequest], Type[SuggestRequest]] = (
-        None
-    )
+    _request_type: Union[Type[AutocompleteRequest], Type[SearchRequest], Type[SuggestRequest]] = None
 
     def __init__(self, **kwargs) -> None:
         self._request = self._request_type(**kwargs)  # pylint:disable=not-callable
 
     def __repr__(self) -> str:
-        return "<{} [{}]>".format(self.__class__.__name__, self._request.search_text)[
-            :1024
-        ]
+        return "<{} [{}]>".format(self.__class__.__name__, self._request.search_text)[:1024]
 
     def filter(self, expression: str) -> None:
         """Add a `filter` expression for the search results.
 
         :param expression: An ODate expression of for the query filter.
         :type expression: str
         """
         self._request.filter = expression
 
     @property
-    def request(self):
-        """The service request for this operation."""
+    def request(self) -> Union[Type[AutocompleteRequest], Type[SearchRequest], Type[SuggestRequest]]:
+        """The service request for this operation.
+
+        :return: The service request for this operation.
+        :rtype: AutocompleteRequest or SearchRequest or SuggestRequest
+        """
         return self._request
 
 
 class AutocompleteQuery(_QueryBase):
     """Represent an autocomplete query again an Azure Search index."""
 
     _request_type = AutocompleteRequest
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender_base.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender_base.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,24 +16,24 @@
 
     _ODATA_ACCEPT: str = "application/json;odata.metadata=none"
     _DEFAULT_AUTO_FLUSH_INTERVAL = 60
     _DEFAULT_INITIAL_BATCH_ACTION_COUNT = 512
     _DEFAULT_MAX_RETRIES = 3
 
     def __init__(
-            self,
-            endpoint: str,
-            index_name: str,
-            credential: Union[AzureKeyCredential, TokenCredential],
-            *,
-            auto_flush: bool = True,
-            initial_batch_action_count: int = _DEFAULT_INITIAL_BATCH_ACTION_COUNT,
-            auto_flush_interval: int = _DEFAULT_AUTO_FLUSH_INTERVAL,
-            max_retries_per_action: int = _DEFAULT_MAX_RETRIES,
-            **kwargs: Any
+        self,
+        endpoint: str,
+        index_name: str,
+        credential: Union[AzureKeyCredential, TokenCredential],
+        *,
+        auto_flush: bool = True,
+        initial_batch_action_count: int = _DEFAULT_INITIAL_BATCH_ACTION_COUNT,
+        auto_flush_interval: int = _DEFAULT_AUTO_FLUSH_INTERVAL,
+        max_retries_per_action: int = _DEFAULT_MAX_RETRIES,
+        **kwargs: Any
     ) -> None:
 
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._auto_flush = auto_flush
         self._batch_action_count = initial_batch_action_count
         self._auto_flush_interval = auto_flush_interval
         if self._auto_flush_interval <= 0:
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_index_documents_batch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_index_documents_batch.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 
 
 def _flatten_args(args: Union[List[Dict], List[List[Dict]]]) -> List[Dict]:
     if len(args) == 1 and isinstance(args[0], (list, tuple)):
         return args[0]
     return args
 
+
 class IndexDocumentsBatch:
     """Represent a batch of update operations for documents in an Azure
     Search index.
 
     Index operations are performed in the order in which they are added
     to the batch.
 
@@ -41,17 +42,15 @@
          a single list of documents, or documents as individual parameters.
         :type documents: Dict or List[Dict]
         :return: the added actions
         :rtype: List[IndexAction]
         """
         return self._extend_batch(_flatten_args(documents), "upload")
 
-    def add_delete_actions(
-        self, *documents: Union[List[Dict], List[List[Dict]]], **kwargs: Any
-    ) -> List[IndexAction]:
+    def add_delete_actions(self, *documents: Union[List[Dict], List[List[Dict]]], **kwargs: Any) -> List[IndexAction]:
         # pylint: disable=unused-argument
         """Add documents to delete to the Azure search index.
 
         Delete removes the specified document from the index. Any field you
         specify in a delete operation, other than the key field, will be
         ignored. If you want to remove an individual field from a document, use
         `merge_documents` instead and set the field explicitly to None.
@@ -64,17 +63,15 @@
          a single list of documents, or documents as individual parameters.
         :type documents: Dict or List[Dict]
         :return: the added actions
         :rtype: List[IndexAction]
         """
         return self._extend_batch(_flatten_args(documents), "delete")
 
-    def add_merge_actions(
-        self, *documents: Union[List[Dict], List[List[Dict]]], **kwargs: Any
-    ) -> List[IndexAction]:
+    def add_merge_actions(self, *documents: Union[List[Dict], List[List[Dict]]], **kwargs: Any) -> List[IndexAction]:
         # pylint: disable=unused-argument
         """Add documents to merge in to existing documents in the Azure search
         index.
 
         Merge updates an existing document with the specified fields. If the
         document doesn't exist, the merge will fail. Any field you specify in a
         merge will replace the existing field in the document. This also applies
@@ -115,34 +112,34 @@
         :rtype: List[IndexAction]
         """
         return list(self._actions)
 
     def dequeue_actions(self, **kwargs: Any) -> List[IndexAction]:  # pylint: disable=unused-argument
         """Get the list of currently configured index actions and clear it.
 
+        :return: the current actions
         :rtype: List[IndexAction]
         """
         with self._lock:
             result = list(self._actions)
             self._actions = []
         return result
 
-    def enqueue_actions(
-            self, new_actions: Union[IndexAction, List[IndexAction]], **kwargs: Any
-    ) -> None:
+    def enqueue_actions(self, new_actions: Union[IndexAction, List[IndexAction]], **kwargs: Any) -> None:
         # pylint: disable=unused-argument
-        """Enqueue a list of index actions to index."""
+        """Enqueue a list of index actions to index.
+
+        :param new_actions: the actions to enqueue
+        :type new_actions: IndexAction or List[IndexAction]
+        """
         if isinstance(new_actions, IndexAction):
             with self._lock:
                 self._actions.append(new_actions)
         else:
             with self._lock:
                 self._actions.extend(new_actions)
 
     def _extend_batch(self, documents: List[Dict], action_type: str) -> List[IndexAction]:
-        new_actions = [
-            IndexAction(additional_properties=document, action_type=action_type)
-            for document in documents
-        ]
+        new_actions = [IndexAction(additional_properties=document, action_type=action_type) for document in documents]
         with self._lock:
             self._actions.extend(new_actions)
         return new_actions
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_search_indexing_buffered_sender.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_search_indexing_buffered_sender.py`

 * *Files 2% similar despite different names*

```diff
@@ -49,15 +49,15 @@
      audience is not considered when using a shared key. If audience is not provided, the public cloud audience
      will be assumed.
     """
 
     # pylint: disable=too-many-instance-attributes
 
     def __init__(
-            self, endpoint: str, index_name: str, credential: Union[AzureKeyCredential, TokenCredential], **kwargs: Any
+        self, endpoint: str, index_name: str, credential: Union[AzureKeyCredential, TokenCredential], **kwargs: Any
     ) -> None:
         super(SearchIndexingBufferedSender, self).__init__(
             endpoint=endpoint, index_name=index_name, credential=credential, **kwargs
         )
         self._index_documents_batch = IndexDocumentsBatch()
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
@@ -98,15 +98,15 @@
             repr(self._endpoint), repr(self._index_name)
         )[:1024]
 
     @property
     def actions(self) -> List[IndexAction]:
         """The list of currently index actions in queue to index.
 
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return self._index_documents_batch.actions
 
     @distributed_trace
     def close(self, **kwargs) -> None:  # pylint: disable=unused-argument
         """Close the :class:`~azure.search.documents.SearchClient` session."""
         self._cleanup(flush=True)
@@ -155,19 +155,15 @@
 
         self._reset_timer()
 
         try:
             results = self._index_documents_actions(actions=actions, timeout=timeout)
             for result in results:
                 try:
-                    action = next(
-                        x
-                        for x in actions
-                        if x.additional_properties.get(self._index_key) == result.key
-                    )
+                    action = next(x for x in actions if x.additional_properties.get(self._index_key) == result.key)
                     if result.succeeded:
                         self._callback_succeed(action)
                     elif is_retryable_status_code(result.status_code):
                         self._retry_action(action)
                         has_error = True
                     else:
                         self._callback_fail(action)
@@ -207,118 +203,111 @@
             self._timer.start()
 
     @distributed_trace
     def upload_documents(self, documents: List[Dict], **kwargs) -> None:  # pylint: disable=unused-argument
         """Queue upload documents actions.
 
         :param documents: A list of documents to upload.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = self._index_documents_batch.add_upload_actions(documents)
         self._callback_new(actions)
         self._process_if_needed()
 
     @distributed_trace
     def delete_documents(self, documents: List[Dict], **kwargs) -> None:  # pylint: disable=unused-argument
         """Queue delete documents actions
 
         :param documents: A list of documents to delete.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = self._index_documents_batch.add_delete_actions(documents)
         self._callback_new(actions)
         self._process_if_needed()
 
     @distributed_trace
     def merge_documents(self, documents: List[Dict], **kwargs) -> None:  # pylint: disable=unused-argument
         """Queue merge documents actions
 
         :param documents: A list of documents to merge.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = self._index_documents_batch.add_merge_actions(documents)
         self._callback_new(actions)
         self._process_if_needed()
 
     @distributed_trace
-    def merge_or_upload_documents(
-        self, documents: List[Dict], **kwargs
-    ) -> None:
+    def merge_or_upload_documents(self, documents: List[Dict], **kwargs) -> None:
         # pylint: disable=unused-argument
         """Queue merge documents or upload documents actions
 
         :param documents: A list of documents to merge or upload.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = self._index_documents_batch.add_merge_or_upload_actions(documents)
         self._callback_new(actions)
         self._process_if_needed()
 
     @distributed_trace
     def index_documents(self, batch: IndexDocumentsBatch, **kwargs) -> List[IndexingResult]:
         """Specify a document operations to perform as a batch.
 
         :param batch: A batch of document operations to perform.
         :type batch: IndexDocumentsBatch
-        :rtype:  List[IndexingResult]
+        :return: Indexing result of each action in the batch.
+        :rtype:  list[IndexingResult]
         :raises :class:`~azure.search.documents.RequestEntityTooLargeError`
         """
         return self._index_documents_actions(actions=batch.actions, **kwargs)
 
     def _index_documents_actions(self, actions: List[IndexAction], **kwargs) -> List[IndexingResult]:
         error_map = {413: RequestEntityTooLargeError}
 
         timeout = kwargs.pop("timeout", 86400)
         begin_time = int(time.time())
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         batch = IndexBatch(actions=actions)
         try:
-            batch_response = self._client.documents.index(
-                batch=batch, error_map=error_map, **kwargs
-            )
+            batch_response = self._client.documents.index(batch=batch, error_map=error_map, **kwargs)
             return cast(List[IndexingResult], batch_response.results)
-        except RequestEntityTooLargeError:
+        except RequestEntityTooLargeError as ex:
             if len(actions) == 1:
                 raise
             pos = round(len(actions) / 2)
             if pos < self._batch_action_count:
                 self._index_documents_batch = pos
             now = int(time.time())
             remaining = timeout - (now - begin_time)
             if remaining < 0:
-                raise ServiceResponseTimeoutError("Service response time out")
+                raise ServiceResponseTimeoutError("Service response time out") from ex
             batch_response_first_half = self._index_documents_actions(
                 actions=actions[:pos], error_map=error_map, timeout=remaining, **kwargs
             )
             if len(batch_response_first_half) > 0:
-                result_first_half = cast(
-                    List[IndexingResult], batch_response_first_half.results
-                )
+                result_first_half = cast(List[IndexingResult], batch_response_first_half.results)
             else:
                 result_first_half = []
             now = int(time.time())
             remaining = timeout - (now - begin_time)
             if remaining < 0:
-                raise ServiceResponseTimeoutError("Service response time out")
+                raise ServiceResponseTimeoutError("Service response time out") from ex
             batch_response_second_half = self._index_documents_actions(
                 actions=actions[pos:], error_map=error_map, timeout=remaining, **kwargs
             )
             if len(batch_response_second_half) > 0:
-                result_second_half = cast(
-                    List[IndexingResult], batch_response_second_half.results
-                )
+                result_second_half = cast(List[IndexingResult], batch_response_second_half.results)
             else:
                 result_second_half = []
             return result_first_half.extend(result_second_half)
 
-    def __enter__(self):
+    def __enter__(self) -> "SearchIndexingBufferedSender":
         self._client.__enter__()  # pylint:disable=no-member
         return self
 
-    def __exit__(self, *args):
+    def __exit__(self, *args) -> None:
         self.close()
         self._client.__exit__(*args)
 
     def _retry_action(self, action: IndexAction) -> None:
         if not self._index_key:
             self._callback_fail(action)
             return
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_utils.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,44 +10,41 @@
 
 
 def is_retryable_status_code(status_code: int) -> bool:
     return status_code in [422, 409, 503]
 
 
 def get_authentication_policy(credential, *, is_async: bool = False, **kwargs):
-    audience = kwargs.get('audience', None)
+    audience = kwargs.get("audience", None)
     if not audience:
         audience = DEFAULT_AUDIENCE
-    scope = audience.rstrip('/') + '/.default'
+    scope = audience.rstrip("/") + "/.default"
     _policy = BearerTokenCredentialPolicy if not is_async else AsyncBearerTokenCredentialPolicy
-    authentication_policy = _policy(
-        credential, scope
-    )
+    authentication_policy = _policy(credential, scope)
     return authentication_policy
 
 
 def odata(statement: str, **kwargs: Any) -> str:
     """Escape an OData query string.
 
     The statement to prepare should include fields to substitute given inside
     braces, e.g. `{somevar}` and then pass the corresponding value as a keyword
     argument, e.g. `somevar=10`.
 
     :param statement: An OData query string to prepare
     :type statement: str
+    :return: The prepared OData query string
     :rtype: str
 
     .. admonition:: Example:
 
         >>> odata("name eq {name} and age eq {age}", name="O'Neil", age=37)
         "name eq 'O''Neil' and age eq 37"
 
 
     """
-    kw = dict(kwargs)
-    for key in kw:
-        value = kw[key]
+    for key, value in kwargs.items():
         if isinstance(value, str):
             value = value.replace("'", "''")
-            if "'{{{}}}'".format(key) not in statement:
-                kw[key] = "'{}'".format(value)
-    return statement.format(**kw)
+            if f"'{{{key}}}'" not in statement:
+                kwargs[key] = f"'{value}'"
+    return statement.format(**kwargs)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_index_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_index_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,18 +42,15 @@
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._endpoint = normalize_endpoint(endpoint)
         self._credential = credential
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
             self._aad = False
             self._client: _SearchServiceClient = _SearchServiceClient(
-                endpoint=endpoint,
-                sdk_moniker=SDK_MONIKER,
-                api_version=self._api_version,
-                **kwargs
+                endpoint=endpoint, sdk_moniker=SDK_MONIKER, api_version=self._api_version, **kwargs
             )
         else:
             self._aad = True
             authentication_policy = get_authentication_policy(credential, audience=audience)
             self._client: _SearchServiceClient = _SearchServiceClient(
                 endpoint=endpoint,
                 authentication_policy=authentication_policy,
@@ -74,54 +71,51 @@
         return self._client.close()
 
     def get_search_client(self, index_name: str, **kwargs: Any) -> SearchClient:
         """Return a client to perform operations on Search
 
         :param index_name: The name of the Search Index
         :type index_name: str
+        :return: SearchClient object
         :rtype: ~azure.search.documents.SearchClient
 
         """
         return SearchClient(self._endpoint, index_name, self._credential, **kwargs)
 
     @distributed_trace
     def list_indexes(self, *, select: Optional[List[str]] = None, **kwargs: Any) -> ItemPaged[SearchIndex]:
         """List the indexes in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
-        :paramtype select: List[str]
+        :paramtype select: list[str]
         :return: List of indexes
         :rtype: ~azure.core.paging.ItemPaged[~azure.search.documents.indexes.models.SearchIndex]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         # pylint:disable=protected-access
-        return self._client.indexes.list(
-            cls=lambda objs: [SearchIndex._from_generated(x) for x in objs], **kwargs
-        )
+        return self._client.indexes.list(cls=lambda objs: [SearchIndex._from_generated(x) for x in objs], **kwargs)
 
     @distributed_trace
     def list_index_names(self, **kwargs: Any) -> ItemPaged[str]:
         """List the index names in an Azure Search service.
 
         :return: List of index names
         :rtype: ~azure.core.paging.ItemPaged[str]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
 
-        return self._client.indexes.list(
-            cls=lambda objs: [x.name for x in objs], **kwargs
-        )
+        return self._client.indexes.list(cls=lambda objs: [x.name for x in objs], **kwargs)
 
     @distributed_trace
     def get_index(self, name: str, **kwargs: Any) -> SearchIndex:
         """
 
         :param name: The name of the index to retrieve.
         :type name: str
@@ -156,19 +150,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.indexes.get_statistics(index_name, **kwargs)
         return result.as_dict()
 
     @distributed_trace
     def delete_index(
-            self,
-            index: Union[str, SearchIndex],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        index: Union[str, SearchIndex],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Deletes a search index and all the documents it contains. The model must be
         provided instead of the name to use the access conditions.
 
         :param index: The index name or object to delete.
         :type index: str or ~azure.search.documents.indexes.models.SearchIndex
         :keyword match_condition: The match condition to use upon the etag
@@ -187,17 +181,15 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(index, match_condition)
         kwargs.update(access_condition)
         try:
             index_name = index.name
         except AttributeError:
             index_name = index
-        self._client.indexes.delete(
-            index_name=index_name, error_map=error_map, **kwargs
-        )
+        self._client.indexes.delete(index_name=index_name, error_map=error_map, **kwargs)
 
     @distributed_trace
     def create_index(self, index: SearchIndex, **kwargs: Any) -> SearchIndex:
         """Creates a new search index.
 
         :param index: The index object.
         :type index: ~azure.search.documents.indexes.models.SearchIndex
@@ -217,20 +209,20 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         patched_index = index._to_generated()  # pylint:disable=protected-access
         result = self._client.indexes.create(patched_index, **kwargs)
         return SearchIndex._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def create_or_update_index(
-            self,
-            index: SearchIndex,
-            allow_index_downtime: Optional[bool] = None,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        index: SearchIndex,
+        allow_index_downtime: Optional[bool] = None,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
         :param index: The index object.
         :type index: ~azure.search.documents.indexes.models.SearchIndex
         :param allow_index_downtime: Allows new analyzers, tokenizers, token filters, or char filters
          to be added to an index by taking the index offline for at least a few seconds. This
@@ -303,51 +295,51 @@
     @distributed_trace
     def get_synonym_maps(self, *, select: Optional[List[str]] = None, **kwargs) -> List[SynonymMap]:
         """List the Synonym Maps in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
-        :paramtype select: List[str]
+        :paramtype select: list[str]
         :return: List of synonym maps
-        :rtype: List[~azure.search.documents.indexes.models.SynonymMap]
+        :rtype: list[~azure.search.documents.indexes.models.SynonymMap]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/sample_synonym_map_operations.py
                 :start-after: [START get_synonym_maps]
                 :end-before: [END get_synonym_maps]
                 :language: python
                 :dedent: 4
                 :caption: List Synonym Maps
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = self._client.synonym_maps.list(**kwargs)
         # pylint:disable=protected-access
         return [SynonymMap._from_generated(x) for x in result.synonym_maps]
 
     @distributed_trace
     def get_synonym_map_names(self, **kwargs: Any) -> List[str]:
         """List the Synonym Map names in an Azure Search service.
 
         :return: List of synonym maps
-        :rtype: List[str]
+        :rtype: list[str]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.synonym_maps.list(**kwargs)
         return [x.name for x in result.synonym_maps]
 
     @distributed_trace
-    def get_synonym_map(self, name: str, **kwargs: Any)  -> SynonymMap:
+    def get_synonym_map(self, name: str, **kwargs: Any) -> SynonymMap:
         """Retrieve a named Synonym Map in an Azure Search service
 
         :param name: The name of the Synonym Map to get
         :type name: str
         :return: The retrieved Synonym Map
         :rtype: :class:`~azure.search.documents.indexes.models.SynonymMap`
         :raises: :class:`~azure.core.exceptions.ResourceNotFoundError`
@@ -364,19 +356,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.synonym_maps.get(name, **kwargs)
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def delete_synonym_map(
-            self,
-            synonym_map: Union[str, SynonymMap],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        synonym_map: Union[str, SynonymMap],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Delete a named Synonym Map in an Azure Search service. To use access conditions,
         the SynonymMap model must be provided instead of the name. It is enough to provide
         the name of the synonym map to delete unconditionally.
 
         :param name: The synonym map name or object to delete
         :type name: str or ~azure.search.documents.indexes.models.SynonymMap
@@ -398,17 +390,15 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(synonym_map, match_condition)
         kwargs.update(access_condition)
         try:
             name = synonym_map.name
         except AttributeError:
             name = synonym_map
-        self._client.synonym_maps.delete(
-            synonym_map_name=name, error_map=error_map, **kwargs
-        )
+        self._client.synonym_maps.delete(synonym_map_name=name, error_map=error_map, **kwargs)
 
     @distributed_trace
     def create_synonym_map(self, synonym_map: SynonymMap, **kwargs: Any) -> SynonymMap:
         """Create a new Synonym Map in an Azure Search service
 
         :param synonym_map: The Synonym Map object
         :type synonym_map: ~azure.search.documents.indexes.models.SynonymMap
@@ -422,93 +412,91 @@
                 :end-before: [END create_synonym_map]
                 :language: python
                 :dedent: 4
                 :caption: Create a Synonym Map
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        patched_synonym_map = (
-            synonym_map._to_generated()  # pylint:disable=protected-access
-        )
+        patched_synonym_map = synonym_map._to_generated()  # pylint:disable=protected-access
         result = self._client.synonym_maps.create(patched_synonym_map, **kwargs)
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def create_or_update_synonym_map(
-            self,
-            synonym_map: SynonymMap,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> SynonymMap:
+        self,
+        synonym_map: SynonymMap,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> SynonymMap:
         """Create a new Synonym Map in an Azure Search service, or update an
         existing one.
 
         :param synonym_map: The Synonym Map object
         :type synonym_map: ~azure.search.documents.indexes.models.SynonymMap
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :return: The created or updated Synonym Map
         :rtype: ~azure.search.documents.indexes.models.SynonymMap
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(synonym_map, match_condition)
         kwargs.update(access_condition)
-        patched_synonym_map = (
-            synonym_map._to_generated()  # pylint:disable=protected-access
-        )
+        patched_synonym_map = synonym_map._to_generated()  # pylint:disable=protected-access
         result = self._client.synonym_maps.create_or_update(
             synonym_map_name=synonym_map.name,
             synonym_map=patched_synonym_map,
             prefer="return=representation",
             error_map=error_map,
             **kwargs
         )
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def get_service_statistics(self, **kwargs: Any) -> Dict:
-        """Get service level statistics for a search service."""
+        """Get service level statistics for a search service.
+
+        :return: Service statistics result.
+        :rtype: dict
+        """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.get_service_statistics(**kwargs)
         return result.as_dict()
 
     @distributed_trace
     def list_aliases(self, *, select: Optional[List[str]] = None, **kwargs: Any) -> ItemPaged[SearchAlias]:
         """List the aliases in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
-        :paramtype select: List[str]
+        :paramtype select: list[str]
         :return: List of Aliases
         :rtype: ~azure.core.paging.ItemPaged[~azure.search.documents.indexes.models.SearchAlias]
         :raises: ~azure.core.exceptions.HttpResponseError
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         # pylint:disable=protected-access
         return self._client.aliases.list(**kwargs)
 
     @distributed_trace
     def list_alias_names(self, **kwargs: Any) -> ItemPaged[str]:
         """List the alias names in an Azure Search service.
 
         :return: List of alias names
         :rtype: ~azure.core.paging.ItemPaged[str]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
 
-        return self._client.aliases.list(
-            cls=lambda objs: [x.name for x in objs], **kwargs
-        )
+        return self._client.aliases.list(cls=lambda objs: [x.name for x in objs], **kwargs)
 
     @distributed_trace
     def get_alias(self, name: str, **kwargs: Any) -> SearchAlias:
         """
 
         :param name: The name of the alias to retrieve.
         :type name: str
@@ -518,19 +506,20 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.aliases.get(name, **kwargs)
         return result
 
     @distributed_trace
     def delete_alias(
-            self,
-            alias: Union[str, SearchAlias],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> None:
+        self,
+        alias: Union[str, SearchAlias],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> None:
         """Deletes a search alias and its associated mapping to an index. This operation is permanent,
         with no recovery option. The mapped index is untouched by this operation
 
         :param alias: The alias name or object to delete.
         :type alias: str or ~azure.search.documents.indexes.models.SearchAlias
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
@@ -547,17 +536,15 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(alias, match_condition)
         kwargs.update(access_condition)
         try:
             alias_name = alias.name
         except AttributeError:
             alias_name = alias
-        self._client.aliases.delete(
-            alias_name=alias_name, error_map=error_map, **kwargs
-        )
+        self._client.aliases.delete(alias_name=alias_name, error_map=error_map, **kwargs)
 
     @distributed_trace
     def create_alias(self, alias: SearchAlias, **kwargs: Any) -> SearchAlias:
         """Creates a new search alias.
 
         :param alias: The alias object.
         :type alias: ~azure.search.documents.indexes.models.SearchAlias
@@ -576,19 +563,16 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.aliases.create(alias, **kwargs)
         return result  # pylint:disable=protected-access
 
     @distributed_trace
     def create_or_update_alias(
-            self,
-            alias: SearchAlias,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> SearchAlias:
+        self, alias: SearchAlias, *, match_condition: MatchConditions = MatchConditions.Unconditionally, **kwargs: Any
+    ) -> SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
         :param alias: The definition of the alias to create or update.
         :type alias: ~azure.search.documents.indexes.models.SearchAlias
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :return: The index created or updated
@@ -607,14 +591,10 @@
                 :dedent: 4
                 :caption: Updating an alias.
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(alias, match_condition)
         kwargs.update(access_condition)
         result = self._client.aliases.create_or_update(
-            alias_name=alias.name,
-            alias=alias,
-            prefer="return=representation",
-            error_map=error_map,
-            **kwargs
+            alias_name=alias.name, alias=alias, prefer="return=representation", error_map=error_map, **kwargs
         )
         return result  # pylint:disable=protected-access
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_utils.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,16 +39,15 @@
         return if_none_match
     if match_condition == MatchConditions.IfMissing:
         return "*"
     return None
 
 
 def get_access_conditions(
-        model: Any,
-        match_condition: MatchConditions = MatchConditions.Unconditionally
+    model: Any, match_condition: MatchConditions = MatchConditions.Unconditionally
 ) -> Tuple[Dict[int, Any], Dict[str, bool]]:
     error_map = {401: ClientAuthenticationError, 404: ResourceNotFoundError}
 
     if isinstance(model, str):
         if match_condition is not MatchConditions.Unconditionally:
             raise ValueError("A model must be passed to use access conditions")
         return error_map, {}
@@ -62,22 +61,20 @@
             error_map[304] = ResourceNotModifiedError
             error_map[412] = ResourceNotModifiedError
         if match_condition == MatchConditions.IfPresent:
             error_map[412] = ResourceNotFoundError
         if match_condition == MatchConditions.IfMissing:
             error_map[412] = ResourceExistsError
         return error_map, dict(if_match=if_match, if_none_match=if_none_match)
-    except AttributeError:
-        raise ValueError("Unable to get e_tag from the model")
+    except AttributeError as ex:
+        raise ValueError("Unable to get e_tag from the model") from ex
 
 
 def normalize_endpoint(endpoint):
     try:
         if not endpoint.lower().startswith("http"):
             endpoint = "https://" + endpoint
         elif not endpoint.lower().startswith("https"):
-            raise ValueError(
-                "Bearer token authentication is not permitted for non-TLS protected (non-https) URLs."
-            )
+            raise ValueError("Bearer token authentication is not permitted for non-TLS protected (non-https) URLs.")
         return endpoint
-    except AttributeError:
-        raise ValueError("Endpoint must be a string.")
+    except AttributeError as ex:
+        raise ValueError("Endpoint must be a string.") from ex
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_search_indexer_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_search_indexer_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
     get_access_conditions,
     normalize_endpoint,
 )
 from .models import (
     SearchIndexerSkillset,
     EntityRecognitionSkillVersion,
     SearchIndexerDataSourceConnection,
-    SentimentSkillVersion
+    SentimentSkillVersion,
 )
 from .._api_versions import DEFAULT_VERSION
 from .._headers_mixin import HeadersMixin
 from .._utils import get_authentication_policy
 from .._version import SDK_MONIKER
 
 
@@ -46,35 +46,32 @@
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._endpoint = normalize_endpoint(endpoint)
         self._credential = credential
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
             self._aad = False
             self._client: _SearchServiceClient = _SearchServiceClient(
-                endpoint=endpoint,
-                sdk_moniker=SDK_MONIKER,
-                api_version=self._api_version,
-                **kwargs
+                endpoint=endpoint, sdk_moniker=SDK_MONIKER, api_version=self._api_version, **kwargs
             )
         else:
             self._aad = True
             authentication_policy = get_authentication_policy(credential, audience=audience)
             self._client: _SearchServiceClient = _SearchServiceClient(
                 endpoint=endpoint,
                 authentication_policy=authentication_policy,
                 sdk_moniker=SDK_MONIKER,
                 api_version=self._api_version,
                 **kwargs
             )
 
-    def __enter__(self):
+    def __enter__(self) -> "SearchIndexerClient":
         self._client.__enter__()
         return self
 
-    def __exit__(self, *args):
+    def __exit__(self, *args) -> None:
         return self._client.__exit__(*args)
 
     def close(self) -> None:
         """Close the :class:`~azure.search.documents.indexes.SearchIndexerClient` session."""
         return self._client.close()
 
     @distributed_trace
@@ -97,19 +94,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.indexers.create(indexer, **kwargs)
         return result
 
     @distributed_trace
     def create_or_update_indexer(
-            self,
-            indexer: SearchIndexer,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        indexer: SearchIndexer,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
         :param indexer: The definition of the indexer to create or update.
         :type indexer: ~azure.search.documents.indexes.models.SearchIndexer
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
@@ -170,15 +167,15 @@
                 :end-before: [END list_indexer]
                 :language: python
                 :dedent: 4
                 :caption: List all the SearchIndexers
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = self._client.indexers.list(**kwargs)
         return result.indexers
 
     @distributed_trace
     def get_indexer_names(self, **kwargs: Any) -> Sequence[str]:
         """Lists all indexer names available for a search service.
 
@@ -196,19 +193,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.indexers.list(**kwargs)
         return [x.name for x in result.indexers]
 
     @distributed_trace
     def delete_indexer(
-            self,
-            indexer: Union[str, SearchIndexer],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        indexer: Union[str, SearchIndexer],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Deletes an indexer. To use access conditions, the SearchIndexer model
         must be provided instead of the name. It is enough to provide
         the name of the indexer to delete unconditionally.
 
         :param indexer: The indexer to delete.
         :type indexer: str or ~azure.search.documents.indexes.models.SearchIndexer
@@ -278,15 +275,15 @@
                 :caption: Reset a SearchIndexer's change tracking state
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         self._client.indexers.reset(name, **kwargs)
 
     @distributed_trace
     def reset_documents(
-            self, indexer: Union[str, SearchIndexer], keys_or_ids: DocumentKeysOrIds, **kwargs: Any
+        self, indexer: Union[str, SearchIndexer], keys_or_ids: DocumentKeysOrIds, **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
         :param indexer: The indexer to reset documents for.
         :type indexer: str or ~azure.search.documents.indexes.models.SearchIndexer
         :param keys_or_ids:
         :type keys_or_ids: ~azure.search.documents.indexes.models.DocumentKeysOrIds
@@ -325,15 +322,15 @@
                 :caption: Get a SearchIndexer's status
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         return self._client.indexers.get_status(name, **kwargs)
 
     @distributed_trace
     def create_data_source_connection(
-            self, data_source_connection: SearchIndexerDataSourceConnection, **kwargs: Any
+        self, data_source_connection: SearchIndexerDataSourceConnection, **kwargs: Any
     ) -> SearchIndexerDataSourceConnection:
         """Creates a new data source connection.
 
         :param data_source_connection: The definition of the data source connection to create.
         :type data_source_connection: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
         :return: The created SearchIndexerDataSourceConnection
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
@@ -351,40 +348,36 @@
         # pylint:disable=protected-access
         packed_data_source = data_source_connection._to_generated()
         result = self._client.data_sources.create(packed_data_source, **kwargs)
         return SearchIndexerDataSourceConnection._from_generated(result)
 
     @distributed_trace
     def create_or_update_data_source_connection(
-            self,
-            data_source_connection: SearchIndexerDataSourceConnection,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> SearchIndexerDataSourceConnection:
+        self,
+        data_source_connection: SearchIndexerDataSourceConnection,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> SearchIndexerDataSourceConnection:
         """Creates a new data source connection or updates a data source connection if it already exists.
         :param data_source_connection: The definition of the data source connection to create or update.
         :type data_source_connection: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :keyword skip_indexer_reset_requirement_for_cache: Ignores cache reset requirements.
         :paramtype skip_indexer_reset_requirement_for_cache: bool
         :return: The created SearchIndexerDataSourceConnection
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
         """
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        error_map, access_condition = get_access_conditions(
-            data_source_connection,
-            match_condition
-        )
+        error_map, access_condition = get_access_conditions(data_source_connection, match_condition)
         kwargs.update(access_condition)
         name = data_source_connection.name
-        packed_data_source = (
-            data_source_connection._to_generated()  # pylint:disable=protected-access
-        )
+        packed_data_source = data_source_connection._to_generated()  # pylint:disable=protected-access
         result = self._client.data_sources.create_or_update(
             data_source_name=name,
             data_source=packed_data_source,
             prefer="return=representation",
             error_map=error_map,
             **kwargs
         )
@@ -407,21 +400,19 @@
                 :end-before: [END get_data_source_connection]
                 :language: python
                 :dedent: 4
                 :caption: Retrieve a SearchIndexerDataSourceConnection
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.data_sources.get(name, **kwargs)
-        return SearchIndexerDataSourceConnection._from_generated(  # pylint:disable=protected-access
-            result
-        )
+        return SearchIndexerDataSourceConnection._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def get_data_source_connections(
-            self, *, select: Optional[List[str]] = None, **kwargs: Any
+        self, *, select: Optional[List[str]] = None, **kwargs: Any
     ) -> Sequence[SearchIndexerDataSourceConnection]:
         """Lists all data source connections available for a search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
         :paramtype select: List[str]
@@ -435,21 +426,18 @@
                 :end-before: [END list_data_source_connection]
                 :language: python
                 :dedent: 4
                 :caption: List all the SearchIndexerDataSourceConnections
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = self._client.data_sources.list(**kwargs)
         # pylint:disable=protected-access
-        return [
-            SearchIndexerDataSourceConnection._from_generated(x)
-            for x in result.data_sources
-        ]
+        return [SearchIndexerDataSourceConnection._from_generated(x) for x in result.data_sources]
 
     @distributed_trace
     def get_data_source_connection_names(self, **kwargs: Any) -> Sequence[str]:
         """Lists all data source connection names available for a search service.
 
         :return: List of all the data source connection names.
         :rtype: List[str]
@@ -457,19 +445,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.data_sources.list(**kwargs)
         return [x.name for x in result.data_sources]
 
     @distributed_trace
     def delete_data_source_connection(
-            self,
-            data_source_connection: Union[str, SearchIndexerDataSourceConnection],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        data_source_connection: Union[str, SearchIndexerDataSourceConnection],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Deletes a data source connection. To use access conditions, the SearchIndexerDataSourceConnection
         model must be provided instead of the name. It is enough to provide the name of the data source connection
         to delete unconditionally
 
         :param data_source_connection: The data source connection to delete.
         :type data_source_connection: str or ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
@@ -484,29 +472,25 @@
                 :start-after: [START delete_data_source_connection]
                 :end-before: [END delete_data_source_connection]
                 :language: python
                 :dedent: 4
                 :caption: Delete a SearchIndexerDataSourceConnection
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        error_map, access_condition = get_access_conditions(
-            data_source_connection,
-            match_condition
-        )
+        error_map, access_condition = get_access_conditions(data_source_connection, match_condition)
         kwargs.update(access_condition)
         try:
             name = data_source_connection.name
         except AttributeError:
             name = data_source_connection
-        self._client.data_sources.delete(
-            data_source_name=name, error_map=error_map, **kwargs
-        )
+        self._client.data_sources.delete(data_source_name=name, error_map=error_map, **kwargs)
 
     @distributed_trace
     def get_skillsets(self, *, select: Optional[List[str]] = None, **kwargs: Any) -> List[SearchIndexerSkillset]:
+        # pylint:disable=protected-access
         """List the SearchIndexerSkillsets in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
         :paramtype select: List[str]
         :return: List of SearchIndexerSkillsets
@@ -521,17 +505,17 @@
                 :language: python
                 :dedent: 4
                 :caption: List SearchIndexerSkillsets
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = self._client.skillsets.list(**kwargs)
-        return [SearchIndexerSkillset._from_generated(skillset) for skillset in result.skillsets]  # pylint:disable=protected-access
+        return [SearchIndexerSkillset._from_generated(skillset) for skillset in result.skillsets]
 
     @distributed_trace
     def get_skillset_names(self, **kwargs: Any) -> List[str]:
         """List the SearchIndexerSkillset names in an Azure Search service.
 
         :return: List of SearchIndexerSkillset names
         :rtype: List[str]
@@ -564,19 +548,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = self._client.skillsets.get(name, **kwargs)
         return SearchIndexerSkillset._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def delete_skillset(
-            self,
-            skillset: Union[str, SearchIndexerSkillset],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        skillset: Union[str, SearchIndexerSkillset],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Delete a named SearchIndexerSkillset in an Azure Search service. To use access conditions,
         the SearchIndexerSkillset model must be provided instead of the name. It is enough to provide
         the name of the skillset to delete unconditionally
 
         :param skillset: The SearchIndexerSkillset to delete
         :type skillset: str or ~azure.search.documents.indexes.models.SearchIndexerSkillset
@@ -590,26 +574,25 @@
                 :end-before: [END delete_skillset]
                 :language: python
                 :dedent: 4
                 :caption: Delete a SearchIndexerSkillset
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        error_map, access_condition = get_access_conditions(
-            skillset, match_condition
-        )
+        error_map, access_condition = get_access_conditions(skillset, match_condition)
         kwargs.update(access_condition)
         try:
             name = skillset.name
         except AttributeError:
             name = skillset
         self._client.skillsets.delete(name, error_map=error_map, **kwargs)
 
     @distributed_trace
     def create_skillset(self, skillset: SearchIndexerSkillset, **kwargs: Any) -> SearchIndexerSkillset:
+        # pylint:disable=protected-access
         """Create a new SearchIndexerSkillset in an Azure Search service
 
         :param skillset: The SearchIndexerSkillset object to create
         :type skillset: ~azure.search.documents.indexes.models.SearchIndexerSkillset
         :return: The created SearchIndexerSkillset
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerSkillset
 
@@ -621,27 +604,28 @@
                 :language: python
                 :dedent: 4
                 :caption: Create a SearchIndexerSkillset
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         _validate_skillset(skillset)
-        skillset = skillset._to_generated() if hasattr(skillset, '_to_generated') else skillset  # pylint:disable=protected-access
+        skillset = skillset._to_generated() if hasattr(skillset, "_to_generated") else skillset
 
         result = self._client.skillsets.create(skillset, **kwargs)
         return SearchIndexerSkillset._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
     def create_or_update_skillset(
-            self,
-            skillset: SearchIndexerSkillset,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        skillset: SearchIndexerSkillset,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndexerSkillset:
+        # pylint:disable=protected-access
         """Create a new SearchIndexerSkillset in an Azure Search service, or update an
         existing one.
 
         :param skillset: The SearchIndexerSkillset object to create or update
         :type skillset: ~azure.search.documents.indexes.models.SearchIndexerSkillset
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
@@ -651,34 +635,30 @@
          detection.
         :paramtype disable_cache_reprocessing_change_detection: bool
         :return: The created or updated SearchIndexerSkillset
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerSkillset
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        error_map, access_condition = get_access_conditions(
-            skillset, match_condition
-        )
+        error_map, access_condition = get_access_conditions(skillset, match_condition)
         kwargs.update(access_condition)
         _validate_skillset(skillset)
-        skillset = skillset._to_generated() if hasattr(skillset, '_to_generated') else skillset  # pylint:disable=protected-access
+        skillset = skillset._to_generated() if hasattr(skillset, "_to_generated") else skillset
 
         result = self._client.skillsets.create_or_update(
             skillset_name=skillset.name,
             skillset=skillset,
             prefer="return=representation",
             error_map=error_map,
             **kwargs
         )
         return SearchIndexerSkillset._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace
-    def reset_skills(
-            self, skillset: Union[str, SearchIndexerSkillset], skill_names: List[str], **kwargs: Any
-    ) -> None:
+    def reset_skills(self, skillset: Union[str, SearchIndexerSkillset], skill_names: List[str], **kwargs: Any) -> None:
         """Reset an existing skillset in a search service.
 
         :param skillset: The SearchIndexerSkillset to reset
         :type skillset: str or ~azure.search.documents.indexes.models.SearchIndexerSkillset
         :param skill_names: the names of skills to be reset.
         :type skill_names: List[str]
         :return: None, or the result of cls(response)
@@ -690,47 +670,45 @@
             name = skillset.name
         except AttributeError:
             name = skillset
         names = SkillNames(skill_names=skill_names)
         return self._client.skillsets.reset_skills(skillset_name=name, skill_names=names, **kwargs)
 
 
-def _validate_skillset(skillset):
+def _validate_skillset(skillset: SearchIndexerSkillset):
     """Validates any multi-version skills in the skillset to verify that unsupported
-    parameters are not supplied by the user.
+       parameters are not supplied by the user.
+
+    :param skillset: The skillset to validate
+    :type skillset: ~azure.search.documents.indexes.models.SearchIndexerSkillset
     """
-    skills = getattr(skillset, 'skills', None)
+    skills = getattr(skillset, "skills", None)
     if not skills:
         return
 
     error_strings = []
     for skill in skills:
         try:
-            skill_version = skill.get('skill_version')
+            skill_version = skill.get("skill_version")
         except AttributeError:
-            skill_version = getattr(skill, 'skill_version', None)
+            skill_version = getattr(skill, "skill_version", None)
         if not skill_version:
             continue
-
-        if skill_version == SentimentSkillVersion.V1:
-            unsupported = ['model_version', 'include_opinion_mining']
-        elif skill_version == SentimentSkillVersion.V3:
+        if skill_version == SentimentSkillVersion.V3:
             unsupported = []
-        elif skill_version == EntityRecognitionSkillVersion.V1:
-            unsupported = ['model_version']
         elif skill_version == EntityRecognitionSkillVersion.V3:
-            unsupported = ['include_typeless_entities']
+            unsupported = ["include_typeless_entities"]
 
         errors = []
         for item in unsupported:
             try:
                 if skill.get(item, None):
                     errors.append(item)
             except AttributeError:
                 if skill.__dict__.get(item, None):
                     errors.append(item)
         if errors:
-            error_strings.append("Unsupported parameters for skill version {}: {}".format(
-                skill_version, ", ".join(errors))
+            error_strings.append(
+                "Unsupported parameters for skill version {}: {}".format(skill_version, ", ".join(errors))
             )
     if error_strings:
         raise ValueError("\n".join(error_strings))
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_index_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_index_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,21 +17,15 @@
     get_access_conditions,
     normalize_endpoint,
 )
 from ..._api_versions import DEFAULT_VERSION
 from ..._headers_mixin import HeadersMixin
 from ..._utils import get_authentication_policy
 from ..._version import SDK_MONIKER
-from ..models import (
-    SearchIndex,
-    SynonymMap,
-    SearchAlias,
-    AnalyzeResult,
-    AnalyzeTextOptions
-)
+from ..models import SearchIndex, SynonymMap, SearchAlias, AnalyzeResult, AnalyzeTextOptions
 
 
 class SearchIndexClient(HeadersMixin):  # pylint:disable=too-many-public-methods
     """A client to interact with Azure search service Indexes.
 
     :param endpoint: The URL endpoint of an Azure search service
     :type endpoint: str
@@ -41,98 +35,89 @@
     :keyword str audience: sets the Audience to use for authentication with Azure Active Directory (AAD). The
      audience is not considered when using a shared key. If audience is not provided, the public cloud audience
      will be assumed.
     """
 
     _ODATA_ACCEPT: str = "application/json;odata.metadata=minimal"
 
-    def __init__(
-            self,
-            endpoint: str,
-            credential: Union[AzureKeyCredential, AsyncTokenCredential],
-            **kwargs
-    ) -> None:
+    def __init__(self, endpoint: str, credential: Union[AzureKeyCredential, AsyncTokenCredential], **kwargs) -> None:
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._endpoint = normalize_endpoint(endpoint)
         self._credential = credential
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
             self._aad = False
             self._client: _SearchServiceClient = _SearchServiceClient(
-                endpoint=endpoint,
-                sdk_moniker=SDK_MONIKER,
-                api_version=self._api_version,
-                **kwargs
+                endpoint=endpoint, sdk_moniker=SDK_MONIKER, api_version=self._api_version, **kwargs
             )
         else:
             self._aad = True
             authentication_policy = get_authentication_policy(credential, audience=audience, is_async=True)
             self._client: _SearchServiceClient = _SearchServiceClient(
                 endpoint=endpoint,
                 authentication_policy=authentication_policy,
                 sdk_moniker=SDK_MONIKER,
                 api_version=self._api_version,
                 **kwargs
             )
 
-    async def __aenter__(self):
+    async def __aenter__(self) -> "SearchIndexClient":
         await self._client.__aenter__()  # pylint:disable=no-member
         return self
 
-    async def __aexit__(self, *args):
+    async def __aexit__(self, *args: Any) -> None:
         return await self._client.__aexit__(*args)
 
     async def close(self) -> None:
-        """Close the :class:`~azure.search.documents.indexes.aio.SearchIndexClient` session."""
+        """Close the :class:`~azure.search.documents.indexes.aio.SearchIndexClient` session.
+
+        :return: None
+        :rtype: None
+        """
         return await self._client.close()
 
     def get_search_client(self, index_name: str, **kwargs: Any) -> SearchClient:
         """Return a client to perform operations on Search.
 
         :param index_name: The name of the Search Index
         :type index_name: str
+        :return: SearchClient
         :rtype: ~azure.search.documents.aio.SearchClient
         """
         return SearchClient(self._endpoint, index_name, self._credential, **kwargs)
 
     @distributed_trace
     def list_indexes(self, *, select: Optional[List[str]] = None, **kwargs) -> AsyncItemPaged[SearchIndex]:
         """List the indexes in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
-        :paramtype select: List[str]
+        :paramtype select: list[str]
         :return: List of indexes
         :rtype: ~azure.core.async_paging.AsyncItemPaged[:class:`~azure.search.documents.indexes.models.SearchIndex`]
         :raises: ~azure.core.exceptions.HttpResponseError
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         # pylint:disable=protected-access
-        return self._client.indexes.list(
-            cls=lambda objs: [SearchIndex._from_generated(x) for x in objs], **kwargs
-        )
+        return self._client.indexes.list(cls=lambda objs: [SearchIndex._from_generated(x) for x in objs], **kwargs)
 
     @distributed_trace
     def list_index_names(self, **kwargs: Any) -> AsyncItemPaged[str]:
         """List the index names in an Azure Search service.
 
         :return: List of index names
         :rtype: ~azure.core.async_paging.AsyncItemPaged[str]
         :raises: ~azure.core.exceptions.HttpResponseError
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
 
-        return self._client.indexes.list(
-            cls=lambda objs: [x.name for x in objs], **kwargs
-        )
+        return self._client.indexes.list(cls=lambda objs: [x.name for x in objs], **kwargs)
 
     @distributed_trace_async
     async def get_index(self, name: str, **kwargs: Any) -> SearchIndex:
         """
 
         :param name: The name of the index to retrieve.
         :type name: str
@@ -159,15 +144,14 @@
         and storage usage.
 
         :param index_name: The name of the index to retrieve.
         :type index_name: str
         :return: Statistics for the given index, including a document count and storage usage.
         :rtype: Dict
         :raises: ~azure.core.exceptions.HttpResponseError
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.indexes.get_statistics(index_name, **kwargs)
         return result.as_dict()
 
     @distributed_trace_async
     async def delete_index(self, index: Union[str, SearchIndex], **kwargs: Any) -> None:
@@ -194,17 +178,15 @@
             index, kwargs.pop("match_condition", MatchConditions.Unconditionally)
         )
         kwargs.update(access_condition)
         try:
             index_name = index.name
         except AttributeError:
             index_name = index
-        await self._client.indexes.delete(
-            index_name=index_name, error_map=error_map, **kwargs
-        )
+        await self._client.indexes.delete(index_name=index_name, error_map=error_map, **kwargs)
 
     @distributed_trace_async
     async def create_index(self, index: SearchIndex, **kwargs: Any) -> SearchIndex:
         """Creates a new search index.
 
         :param index: The index object.
         :type index: :class:`~azure.search.documents.indexes.models.SearchIndex`
@@ -224,20 +206,20 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         patched_index = index._to_generated()  # pylint:disable=protected-access
         result = await self._client.indexes.create(patched_index, **kwargs)
         return result
 
     @distributed_trace_async
     async def create_or_update_index(
-            self,
-            index: SearchIndex,
-            allow_index_downtime: Optional[bool] = None,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        index: SearchIndex,
+        allow_index_downtime: Optional[bool] = None,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
         :param index: The index object.
         :type index: :class:`~azure.search.documents.indexes.models.SearchIndex`
         :param allow_index_downtime: Allows new analyzers, tokenizers, token filters, or char filters
          to be added to an index by taking the index offline for at least a few seconds. This
@@ -327,15 +309,15 @@
                 :language: python
                 :dedent: 4
                 :caption: List Synonym Maps
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = await self._client.synonym_maps.list(**kwargs)
         # pylint:disable=protected-access
         return [SynonymMap._from_generated(x) for x in result.synonym_maps]
 
     @distributed_trace_async
     async def get_synonym_map_names(self, **kwargs: Any) -> List[str]:
         """List the Synonym Map names in an Azure Search service.
@@ -371,19 +353,20 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.synonym_maps.get(name, **kwargs)
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace_async
     async def delete_synonym_map(
-            self,
-            synonym_map: Union[str, SynonymMap],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> None:
+        self,
+        synonym_map: Union[str, SynonymMap],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> None:
         """Delete a named Synonym Map in an Azure Search service. To use access conditions,
         the SynonymMap model must be provided instead of the name. It is enough to provide
         the name of the synonym map to delete unconditionally.
 
         :param name: The synonym map name or object to delete
         :type name: str or ~azure.search.documents.indexes.models.SynonymMap
         :keyword match_condition: The match condition to use upon the etag
@@ -405,17 +388,15 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(synonym_map, match_condition)
         kwargs.update(access_condition)
         try:
             name = synonym_map.name
         except AttributeError:
             name = synonym_map
-        await self._client.synonym_maps.delete(
-            synonym_map_name=name, error_map=error_map, **kwargs
-        )
+        await self._client.synonym_maps.delete(synonym_map_name=name, error_map=error_map, **kwargs)
 
     @distributed_trace_async
     async def create_synonym_map(self, synonym_map: SynonymMap, **kwargs: Any) -> SynonymMap:
         """Create a new Synonym Map in an Azure Search service
 
         :param synonym_map: The Synonym Map object
         :type synonym_map: :class:`~azure.search.documents.indexes.models.SynonymMap`
@@ -429,115 +410,113 @@
                 :end-before: [END create_synonym_map_async]
                 :language: python
                 :dedent: 4
                 :caption: Create a Synonym Map
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        patched_synonym_map = (
-            synonym_map._to_generated()  # pylint:disable=protected-access
-        )
+        patched_synonym_map = synonym_map._to_generated()  # pylint:disable=protected-access
         result = await self._client.synonym_maps.create(patched_synonym_map, **kwargs)
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace_async
     async def create_or_update_synonym_map(
-            self,
-            synonym_map: SynonymMap,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> SynonymMap:
+        self,
+        synonym_map: SynonymMap,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> SynonymMap:
         """Create a new Synonym Map in an Azure Search service, or update an
         existing one.
 
         :param synonym_map: The Synonym Map object
         :type synonym_map: :class:`~azure.search.documents.indexes.models.SynonymMap`
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :return: The created or updated Synonym Map
         :rtype: :class:`~azure.search.documents.indexes.models.SynonymMap`
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(synonym_map, match_condition)
         kwargs.update(access_condition)
-        patched_synonym_map = (
-            synonym_map._to_generated()  # pylint:disable=protected-access
-        )
+        patched_synonym_map = synonym_map._to_generated()  # pylint:disable=protected-access
         result = await self._client.synonym_maps.create_or_update(
             synonym_map_name=synonym_map.name,
             synonym_map=patched_synonym_map,
             prefer="return=representation",
             error_map=error_map,
             **kwargs
         )
         return SynonymMap._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace_async
     async def get_service_statistics(self, **kwargs) -> Dict:
-        """Get service level statistics for a search service."""
+        """Get service level statistics for a search service.
+
+        :return: Service statistics result
+        :rtype: dict
+        """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.get_service_statistics(**kwargs)
         return result.as_dict()
 
     @distributed_trace
     def list_aliases(self, *, select: Optional[List[str]] = None, **kwargs) -> AsyncItemPaged[SearchAlias]:
         """List the aliases in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
-        :paramtype select: List[str]
+        :paramtype select: list[str]
         :return: List of Aliases
         :rtype: ~azure.core.paging.AsyncItemPaged[~azure.search.documents.indexes.models.SearchAlias]
         :raises: ~azure.core.exceptions.HttpResponseError
-
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         # pylint:disable=protected-access
         return self._client.aliases.list(**kwargs)
 
     @distributed_trace
     def list_alias_names(self, **kwargs) -> AsyncItemPaged[str]:
         """List the alias names in an Azure Search service.
 
         :return: List of alias names
         :rtype: ~azure.core.paging.AsyncItemPaged[str]
         :raises: ~azure.core.exceptions.HttpResponseError
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
 
-        return self._client.aliases.list(
-            cls=lambda objs: [x.name for x in objs], **kwargs
-        )
+        return self._client.aliases.list(cls=lambda objs: [x.name for x in objs], **kwargs)
 
-    @distributed_trace
+    @distributed_trace_async
     async def get_alias(self, name: str, **kwargs) -> SearchAlias:
         """
 
         :param name: The name of the alias to retrieve.
         :type name: str
         :return: SearchAlias object
         :rtype: ~azure.search.documents.indexes.models.SearchAlias
         :raises: ~azure.core.exceptions.HttpResponseError
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.aliases.get(name, **kwargs)
         return result
 
-    @distributed_trace
+    @distributed_trace_async
     async def delete_alias(
-            self,
-            alias: Union[str, SearchAlias],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> None:
+        self,
+        alias: Union[str, SearchAlias],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> None:
         """Deletes a search alias and its associated mapping to an index.
         This operation is permanent, with no recovery option. The mapped index is untouched by this operation
         :param alias: The alias name or object to delete.
         :type alias: str or ~azure.search.documents.indexes.models.SearchAlias
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :raises: ~azure.core.exceptions.HttpResponseError
@@ -554,19 +533,17 @@
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(alias, match_condition)
         kwargs.update(access_condition)
         try:
             alias_name = alias.name
         except AttributeError:
             alias_name = alias
-        await self._client.aliases.delete(
-            alias_name=alias_name, error_map=error_map, **kwargs
-        )
+        await self._client.aliases.delete(alias_name=alias_name, error_map=error_map, **kwargs)
 
-    @distributed_trace
+    @distributed_trace_async
     async def create_alias(self, alias: SearchIndex, **kwargs: Any) -> SearchAlias:
         """Creates a new search alias.
         :param alias: The alias object.
         :type alias: ~azure.search.documents.indexes.models.SearchAlias
         :return: The alias created
         :rtype: ~azure.search.documents.indexes.models.SearchAlias
         :raises: ~azure.core.exceptions.HttpResponseError
@@ -580,21 +557,17 @@
                 :dedent: 4
                 :caption: Create an alias.
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.aliases.create(alias, **kwargs)
         return result  # pylint:disable=protected-access
 
-    @distributed_trace
+    @distributed_trace_async
     async def create_or_update_alias(
-            self,
-            alias: SearchAlias,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self, alias: SearchAlias, *, match_condition: MatchConditions = MatchConditions.Unconditionally, **kwargs: Any
     ) -> SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
         :param alias: The definition of the alias to create or update.
         :type alias: ~azure.search.documents.indexes.models.SearchAlias
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
@@ -615,14 +588,10 @@
                 :dedent: 4
                 :caption: Update an alias.
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(alias, match_condition)
         kwargs.update(access_condition)
         result = await self._client.aliases.create_or_update(
-            alias_name=alias.name,
-            alias=alias,
-            prefer="return=representation",
-            error_map=error_map,
-            **kwargs
+            alias_name=alias.name, alias=alias, prefer="return=representation", error_map=error_map, **kwargs
         )
         return result  # pylint:disable=protected-access
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/aio/_search_indexer_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/aio/_search_indexer_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,52 +34,48 @@
     :keyword str audience: sets the Audience to use for authentication with Azure Active Directory (AAD). The
      audience is not considered when using a shared key. If audience is not provided, the public cloud audience
      will be assumed.
     """
 
     _ODATA_ACCEPT: str = "application/json;odata.metadata=minimal"
 
-    def __init__(
-            self,
-            endpoint: str,
-            credential: Union[AzureKeyCredential, AsyncTokenCredential],
-            **kwargs
-    ) -> None:
+    def __init__(self, endpoint: str, credential: Union[AzureKeyCredential, AsyncTokenCredential], **kwargs) -> None:
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._endpoint = normalize_endpoint(endpoint)  # type: str
         self._credential = credential
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
             self._aad = False
             self._client: _SearchServiceClient = _SearchServiceClient(
-                endpoint=endpoint,
-                sdk_moniker=SDK_MONIKER,
-                api_version=self._api_version,
-                **kwargs
+                endpoint=endpoint, sdk_moniker=SDK_MONIKER, api_version=self._api_version, **kwargs
             )
         else:
             self._aad = True
             authentication_policy = get_authentication_policy(credential, audience=audience, is_async=True)
             self._client: _SearchServiceClient = _SearchServiceClient(
                 endpoint=endpoint,
                 authentication_policy=authentication_policy,
                 sdk_moniker=SDK_MONIKER,
                 api_version=self._api_version,
                 **kwargs
             )
 
-    async def __aenter__(self):
+    async def __aenter__(self) -> "SearchIndexerClient":
         await self._client.__aenter__()
         return self
 
-    async def __aexit__(self, *args):
+    async def __aexit__(self, *args) -> None:
         return await self._client.__aexit__(*args)
 
     async def close(self) -> None:
-        """Close the :class:`~azure.search.documents.indexes.aio.SearchIndexerClient` session."""
+        """Close the :class:`~azure.search.documents.indexes.aio.SearchIndexerClient` session.
+
+        :return: None
+        :rtype: None
+        """
         return await self._client.close()
 
     @distributed_trace_async
     async def create_indexer(self, indexer: SearchIndexer, **kwargs: Any) -> SearchIndexer:
         """Creates a new SearchIndexer.
 
         :param indexer: The definition of the indexer to create.
@@ -98,19 +94,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.indexers.create(indexer, **kwargs)
         return result
 
     @distributed_trace_async
     async def create_or_update_indexer(
-            self,
-            indexer: SearchIndexer,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        indexer: SearchIndexer,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndexer:
         """Creates a new indexer or updates a indexer if it already exists.
 
         :param indexer: The definition of the indexer to create or update.
         :type indexer: ~azure.search.documents.indexes.models.SearchIndexer
         :keyword skip_indexer_reset_requirement_for_cache: Ignores cache reset requirements.
         :paramtype skip_indexer_reset_requirement_for_cache: bool
@@ -169,37 +165,37 @@
                 :end-before: [END list_indexer_async]
                 :language: python
                 :dedent: 4
                 :caption: List all the SearchIndexers
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = await self._client.indexers.list(**kwargs)
         return result.indexers
 
     @distributed_trace_async
     async def get_indexer_names(self, **kwargs) -> Sequence[str]:
         """Lists all indexer names available for a search service.
 
         :return: List of all the SearchIndexer names.
-        :rtype: `List[str]`
-
+        :rtype: list[str]
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.indexers.list(**kwargs)
         return [x.name for x in result.indexers]
 
     @distributed_trace_async
     async def delete_indexer(
-            self,
-            indexer: Union[str, SearchIndexer],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any) -> None:
+        self,
+        indexer: Union[str, SearchIndexer],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
+    ) -> None:
         """Deletes an indexer. To use access conditions, the SearchIndexer model
         must be provided instead of the name. It is enough to provide
         the name of the indexer to delete unconditionally.
 
         :param name: The name or the indexer object to delete.
         :type name: str or ~azure.search.documents.indexes.models.SearchIndexer
         :keyword match_condition: The match condition to use upon the etag
@@ -268,15 +264,15 @@
                 :caption: Reset a SearchIndexer's change tracking state
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         await self._client.indexers.reset(name, **kwargs)
 
     @distributed_trace_async
     async def reset_documents(
-            self, indexer: Union[str, SearchIndexer], keys_or_ids: DocumentKeysOrIds, **kwargs: Any
+        self, indexer: Union[str, SearchIndexer], keys_or_ids: DocumentKeysOrIds, **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
         :param indexer: The indexer to reset documents for.
         :type indexer: str or ~azure.search.documents.indexes.models.SearchIndexer
         :param keys_or_ids:
         :type keys_or_ids: ~azure.search.documents.indexes.models.DocumentKeysOrIds
@@ -316,15 +312,15 @@
                 :caption: Get a SearchIndexer's status
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         return await self._client.indexers.get_status(name, **kwargs)
 
     @distributed_trace_async
     async def create_data_source_connection(
-            self, data_source_connection: SearchIndexerDataSourceConnection, **kwargs: Any
+        self, data_source_connection: SearchIndexerDataSourceConnection, **kwargs: Any
     ) -> SearchIndexerDataSourceConnection:
         """Creates a new data source connection.
         :param data_source_connection: The definition of the data source connection to create.
         :type data_source_connection: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
         :return: The created SearchIndexerDataSourceConnection
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
 
@@ -341,19 +337,19 @@
         # pylint:disable=protected-access
         packed_data_source = data_source_connection._to_generated()
         result = await self._client.data_sources.create(packed_data_source, **kwargs)
         return SearchIndexerDataSourceConnection._from_generated(result)
 
     @distributed_trace_async
     async def create_or_update_data_source_connection(
-            self,
-            data_source_connection: SearchIndexerDataSourceConnection,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        data_source_connection: SearchIndexerDataSourceConnection,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndexerDataSourceConnection:
         """Creates a new data source connection or updates a data source connection if it already exists.
         :param data_source_connection: The definition of the data source connection to create or update.
         :type data_source_connection: ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
         :keyword skip_indexer_reset_requirement_for_cache: Ignores cache reset requirements.
@@ -377,19 +373,19 @@
             error_map=error_map,
             **kwargs
         )
         return SearchIndexerDataSourceConnection._from_generated(result)
 
     @distributed_trace_async
     async def delete_data_source_connection(
-            self,
-            data_source_connection: Union[str, SearchIndexerDataSourceConnection],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        data_source_connection: Union[str, SearchIndexerDataSourceConnection],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Deletes a data source connection. To use access conditions, the
         SearchIndexerDataSourceConnection model must be provided instead of the name.
         It is enough to provide the name of the data source connection to delete unconditionally
 
         :param data_source_connection: The data source connection to delete.
         :type data_source_connection: str or ~azure.search.documents.indexes.models.SearchIndexerDataSourceConnection
@@ -413,21 +409,19 @@
             match_condition,
         )
         kwargs.update(access_condition)
         try:
             name = data_source_connection.name
         except AttributeError:
             name = data_source_connection
-        await self._client.data_sources.delete(
-            data_source_name=name, error_map=error_map, **kwargs
-        )
+        await self._client.data_sources.delete(data_source_name=name, error_map=error_map, **kwargs)
 
     @distributed_trace_async
     async def get_data_source_connection(
-            self, name: str, *, select: Optional[List[str]] = None, **kwargs: Any
+        self, name: str, *, select: Optional[List[str]] = None, **kwargs: Any
     ) -> SearchIndexerDataSourceConnection:
         """Retrieves a data source connection definition.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
         :paramtype select: List[str]
@@ -441,15 +435,15 @@
                 :end-before: [END get_data_source_connection_async]
                 :language: python
                 :dedent: 4
                 :caption: Retrieve a SearchIndexerDataSourceConnection
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = await self._client.data_sources.get(name, **kwargs)
         # pylint:disable=protected-access
         return SearchIndexerDataSourceConnection._from_generated(result)
 
     @distributed_trace_async
     async def get_data_source_connections(self, **kwargs: Any) -> Sequence[SearchIndexerDataSourceConnection]:
         """Lists all data source connections available for a search service.
@@ -465,35 +459,31 @@
                 :language: python
                 :dedent: 4
                 :caption: List all SearchIndexerDataSourceConnections
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.data_sources.list(**kwargs)
         # pylint:disable=protected-access
-        return [
-            SearchIndexerDataSourceConnection._from_generated(x)
-            for x in result.data_sources
-        ]
+        return [SearchIndexerDataSourceConnection._from_generated(x) for x in result.data_sources]
 
     @distributed_trace_async
     async def get_data_source_connection_names(self, **kwargs) -> Sequence[str]:
         """Lists all data source connection names available for a search service.
 
         :return: List of all the data source connection names.
         :rtype: List[str]
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.data_sources.list(**kwargs)
         return [x.name for x in result.data_sources]
 
     @distributed_trace_async
-    async def get_skillsets(
-            self, *, select: Optional[List[str]] = None, **kwargs
-    ) -> List[SearchIndexerSkillset]:
+    async def get_skillsets(self, *, select: Optional[List[str]] = None, **kwargs) -> List[SearchIndexerSkillset]:
+        # pylint:disable=protected-access
         """List the SearchIndexerSkillsets in an Azure Search service.
 
         :keyword select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          list of JSON property names, or '*' for all properties. The default is all
          properties.
         :paramtype select: List[str]
         :return: List of SearchIndexerSkillsets
@@ -508,17 +498,17 @@
                 :language: python
                 :dedent: 4
                 :caption: List SearchIndexerSkillsets
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         if select:
-            kwargs['select'] = ','.join(select)
+            kwargs["select"] = ",".join(select)
         result = await self._client.skillsets.list(**kwargs)
-        return [SearchIndexerSkillset._from_generated(skillset) for skillset in result.skillsets]  # pylint:disable=protected-access
+        return [SearchIndexerSkillset._from_generated(skillset) for skillset in result.skillsets]
 
     @distributed_trace_async
     async def get_skillset_names(self, **kwargs) -> List[str]:
         """List the SearchIndexerSkillset names in an Azure Search service.
 
         :return: List of SearchIndexerSkillset names
         :rtype: List[str]
@@ -551,19 +541,19 @@
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         result = await self._client.skillsets.get(name, **kwargs)
         return SearchIndexerSkillset._from_generated(result)  # pylint:disable=protected-access
 
     @distributed_trace_async
     async def delete_skillset(
-            self,
-            skillset: Union[str, SearchIndexerSkillset],
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        skillset: Union[str, SearchIndexerSkillset],
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> None:
         """Delete a named SearchIndexerSkillset in an Azure Search service. To use access conditions,
         the SearchIndexerSkillset model must be provided instead of the name. It is enough to provide
         the name of the skillset to delete unconditionally
 
         :param skillset: The SearchIndexerSkillset to delete
         :type skillset: str or ~azure.search.documents.indexes.models.SearchIndexerSkillset
@@ -587,14 +577,15 @@
             name = skillset.name
         except AttributeError:
             name = skillset
         await self._client.skillsets.delete(name, error_map=error_map, **kwargs)
 
     @distributed_trace_async
     async def create_skillset(self, skillset: SearchIndexerSkillset, **kwargs: Any) -> SearchIndexerSkillset:
+        # pylint:disable=protected-access
         """Create a new SearchIndexerSkillset in an Azure Search service
 
         :param skillset: The SearchIndexerSkillset object to create
         :type skillset: ~azure.search.documents.indexes.models.SearchIndexerSkillset
         :return: The created SearchIndexerSkillset
         :rtype: ~azure.search.documents.indexes.models.SearchIndexerSkillset
 
@@ -605,26 +596,27 @@
                 :end-before: [END create_skillset]
                 :language: python
                 :dedent: 4
                 :caption: Create a SearchIndexerSkillset
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        skillset = skillset._to_generated() if hasattr(skillset, '_to_generated') else skillset  # pylint:disable=protected-access
+        skillset = skillset._to_generated() if hasattr(skillset, "_to_generated") else skillset
         result = await self._client.skillsets.create(skillset, **kwargs)
-        return SearchIndexerSkillset._from_generated(result)  # pylint:disable=protected-access
+        return SearchIndexerSkillset._from_generated(result)
 
     @distributed_trace_async
     async def create_or_update_skillset(
-            self,
-            skillset: SearchIndexerSkillset,
-            *,
-            match_condition: MatchConditions = MatchConditions.Unconditionally,
-            **kwargs: Any
+        self,
+        skillset: SearchIndexerSkillset,
+        *,
+        match_condition: MatchConditions = MatchConditions.Unconditionally,
+        **kwargs: Any
     ) -> SearchIndexerSkillset:
+        # pylint:disable=protected-access
         """Create a new SearchIndexerSkillset in an Azure Search service, or update an
         existing one.
 
         :param skillset: The SearchIndexerSkillset object to create or update
         :type skillset: :class:`~azure.search.documents.indexes.models.SearchIndexerSkillset`
         :keyword match_condition: The match condition to use upon the etag
         :paramtype match_condition: ~azure.core.MatchConditions
@@ -636,15 +628,15 @@
         :return: The created or updated SearchIndexerSkillset
         :rtype: :class:`~azure.search.documents.indexes.models.SearchIndexerSkillset`
 
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         error_map, access_condition = get_access_conditions(skillset, match_condition)
         kwargs.update(access_condition)
-        skillset = skillset._to_generated() if hasattr(skillset, '_to_generated') else skillset  # pylint:disable=protected-access
+        skillset = skillset._to_generated() if hasattr(skillset, "_to_generated") else skillset
 
         result = await self._client.skillsets.create_or_update(
             skillset_name=skillset.name,
             skillset=skillset,
             prefer="return=representation",
             error_map=error_map,
             **kwargs
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -66,14 +66,15 @@
     EntityLinkingSkill,
     EntityRecognitionSkillLanguage,
     FieldMapping,
     FieldMappingFunction,
     FreshnessScoringFunction,
     FreshnessScoringParameters,
     GetIndexStatisticsResult,
+    HnswParameters,
     ImageAnalysisSkill,
     ImageAnalysisSkillLanguage,
     ImageDetail,
     IndexingSchedule,
     IndexingParameters,
     IndexerExecutionStatus,
     IndexerStatus,
@@ -165,14 +166,16 @@
     TokenFilter,
     TokenFilterName,
     TruncateTokenFilter,
     UaxUrlEmailTokenizer,
     UniqueTokenFilter,
     WebApiSkill,
     VisualFeature,
+    VectorSearch,
+    VectorSearchAlgorithmConfiguration,
     WordDelimiterTokenFilter,
 )
 from ._models import (
     AnalyzeTextOptions,
     CustomAnalyzer,
     EntityRecognitionSkill,
     EntityRecognitionSkillVersion,
@@ -184,29 +187,31 @@
     SentimentSkill,
     SentimentSkillVersion,
     SynonymMap,
 )
 
 SearchFieldDataType = _edm
 
+
 class KeywordTokenizer(KeywordTokenizerV2):
     pass
 
+
 class PathHierarchyTokenizer(PathHierarchyTokenizerV2):
     pass
 
+
 class SimilarityAlgorithm(Similarity):
     pass
 
+
 class SearchSuggester(Suggester):
     pass
 
 
-
-
 __all__ = (
     "SearchAlias",
     "AzureMachineLearningSkill",
     "AnalyzeTextOptions",
     "AnalyzeResult",
     "AnalyzedTokenInfo",
     "AsciiFoldingTokenFilter",
@@ -241,14 +246,15 @@
     "EntityRecognitionSkillLanguage",
     "EntityRecognitionSkillVersion",
     "FieldMapping",
     "FieldMappingFunction",
     "FreshnessScoringFunction",
     "FreshnessScoringParameters",
     "GetIndexStatisticsResult",
+    "HnswParameters",
     "ImageAnalysisSkill",
     "ImageAnalysisSkillLanguage",
     "ImageDetail",
     "IndexingSchedule",
     "IndexingParameters",
     "IndexerExecutionStatus",
     "IndexerStatus",
@@ -352,11 +358,13 @@
     "TokenCharacterKind",
     "TokenFilter",
     "TokenFilterName",
     "TruncateTokenFilter",
     "UaxUrlEmailTokenizer",
     "UniqueTokenFilter",
     "VisualFeature",
+    "VectorSearch",
+    "VectorSearchAlgorithmConfiguration",
     "WebApiSkill",
     "WordDelimiterTokenFilter",
     "SearchFieldDataType",
 )
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_edm.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_edm.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,17 +3,17 @@
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 
 String = "Edm.String"
 Int32 = "Edm.Int32"
 Int64 = "Edm.Int64"
+Single = "Edm.Single"
 Double = "Edm.Double"
 Boolean = "Edm.Boolean"
 DateTimeOffset = "Edm.DateTimeOffset"
 GeographyPoint = "Edm.GeographyPoint"
 ComplexType = "Edm.ComplexType"
 
 
-def Collection(typ):
-    # type (str) -> str
+def Collection(typ: str) -> str:
     return "Collection({})".format(typ)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_index.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_index.py`

 * *Files 2% similar despite different names*

```diff
@@ -162,14 +162,17 @@
      supported. Assigning a synonym map to a field ensures that query terms targeting that field are
      expanded at query-time using the rules in the synonym map. This attribute can be changed on
      existing fields. Must be null or an empty collection for complex fields.
     :paramtype synonym_map_names: list[str]
     :keyword fields: A list of sub-fields if this is a field of type Edm.ComplexType or
      Collection(Edm.ComplexType). Must be null or empty for simple fields.
     :paramtype fields: list[~azure.search.documents.models.SearchField]
+    :keyword int vector_search_dimensions: The dimensionality of the vector field.
+    :keyword str vector_search_configuration: The name of the vector search algorithm configuration
+     that specifies the algorithm and optional parameters for searching the vector field.
     """
 
     _validation = {
         "name": {"required": True},
         "type": {"required": True},
     }
 
@@ -184,14 +187,16 @@
         "facetable": {"key": "facetable", "type": "bool"},
         "analyzer_name": {"key": "analyzerName", "type": "str"},
         "search_analyzer_name": {"key": "searchAnalyzerName", "type": "str"},
         "index_analyzer_name": {"key": "indexAnalyzerName", "type": "str"},
         "normalizer_name": {"key": "normalizerName", "type": "str"},
         "synonym_map_names": {"key": "synonymMapNames", "type": "[str]"},
         "fields": {"key": "fields", "type": "[SearchField]"},
+        "vector_search_dimensions": {"key": "vectorSearchDimensions", "type": "int"},
+        "vector_search_configuration": {"key": "vectorSearchConfiguration", "type": "str"},
     }
 
     def __init__(self, **kwargs):
         super(SearchField, self).__init__(**kwargs)
         self.name = kwargs["name"]
         self.type = kwargs["type"]
         self.key = kwargs.get("key", None)
@@ -202,14 +207,16 @@
         self.facetable = kwargs.get("facetable", None)
         self.analyzer_name = kwargs.get("analyzer_name", None)
         self.search_analyzer_name = kwargs.get("search_analyzer_name", None)
         self.index_analyzer_name = kwargs.get("index_analyzer_name", None)
         self.normalizer_name = kwargs.get("normalizer_name", None)
         self.synonym_map_names = kwargs.get("synonym_map_names", None)
         self.fields = kwargs.get("fields", None)
+        self.vector_search_dimensions = kwargs.get("vector_search_dimensions", None)
+        self.vector_search_configuration = kwargs.get("vector_search_configuration", None)
 
     def _to_generated(self):
         fields = [pack_search_field(x) for x in self.fields] if self.fields else None
         retrievable = not self.hidden if self.hidden is not None else None
         return _SearchField(
             name=self.name,
             type=self.type,
@@ -221,31 +228,25 @@
             facetable=self.facetable,
             analyzer=self.analyzer_name,
             search_analyzer=self.search_analyzer_name,
             index_analyzer=self.index_analyzer_name,
             normalizer=self.normalizer_name,
             synonym_maps=self.synonym_map_names,
             fields=fields,
+            dimensions=self.vector_search_dimensions,
+            vector_search_configuration=self.vector_search_configuration,
         )
 
     @classmethod
     def _from_generated(cls, search_field):
         if not search_field:
             return None
         # pylint:disable=protected-access
-        fields = (
-            [SearchField._from_generated(x) for x in search_field.fields]
-            if search_field.fields
-            else None
-        )
-        hidden = (
-            not search_field.retrievable
-            if search_field.retrievable is not None
-            else None
-        )
+        fields = [SearchField._from_generated(x) for x in search_field.fields] if search_field.fields else None
+        hidden = not search_field.retrievable if search_field.retrievable is not None else None
         try:
             normalizer = search_field.normalizer_name
         except AttributeError:
             normalizer = None
         return cls(
             name=search_field.name,
             type=search_field.type,
@@ -257,14 +258,16 @@
             facetable=search_field.facetable,
             analyzer_name=search_field.analyzer,
             search_analyzer_name=search_field.search_analyzer,
             index_analyzer_name=search_field.index_analyzer,
             normalizer_name=normalizer,
             synonym_map_names=search_field.synonym_maps,
             fields=fields,
+            vector_search_dimensions=search_field.dimensions,
+            vector_search_configuration=search_field.vector_search_configuration,
         )
 
 
 def SimpleField(**kw):
     # type: (**Any) -> SearchField
     """Configure a simple field for an Azure Search Index
 
@@ -355,15 +358,15 @@
      queries. filterable differs from searchable in how strings are handled. Fields that are
      filterable do not undergo word-breaking, so comparisons are for exact matches only. For example,
      if you set such a field f to "sunny day", $filter=f eq 'sunny' will find no matches, but
      $filter=f eq 'sunny day' will. Default is False.
     :paramtype filterable: bool
     :keyword sortable: A value indicating whether to enable the field to be referenced in $orderby
      expressions. By default Azure Cognitive Search sorts results by score, but in many experiences
-     users will want to sort by fields in the documents.  The default is true False.
+     users will want to sort by fields in the documents.  The default is False.
     :paramtype sortable: bool
     :keyword facetable: A value indicating whether to enable the field to be referenced in facet
      queries. Typically used in a presentation of search results that includes hit count by category
      (for example, search for digital cameras and see hits by brand, by megapixels, by price, and so
      on). Default is False.
     :paramtype facetable: bool
     :keyword analyzer_name: The name of the analyzer to use for the field. This option can't be set together
@@ -549,14 +552,15 @@
         "normalizers": {"key": "normalizers", "type": "[LexicalNormalizer]"},
         "encryption_key": {
             "key": "encryptionKey",
             "type": "SearchResourceEncryptionKey",
         },
         "similarity": {"key": "similarity", "type": "SimilarityAlgorithm"},
         "semantic_settings": {"key": "semantic", "type": "SemanticSettings"},
+        "vector_search": {"key": "vectorSearch", "type": "VectorSearch"},
         "e_tag": {"key": "@odata\\.etag", "type": "str"},
     }
 
     def __init__(self, **kwargs):
         super(SearchIndex, self).__init__(**kwargs)
         self.name = kwargs["name"]
         self.fields = kwargs["fields"]
@@ -568,28 +572,25 @@
         self.tokenizers = kwargs.get("tokenizers", None)
         self.token_filters = kwargs.get("token_filters", None)
         self.char_filters = kwargs.get("char_filters", None)
         self.normalizers = kwargs.get("normalizers", None)
         self.encryption_key = kwargs.get("encryption_key", None)
         self.similarity = kwargs.get("similarity", None)
         self.semantic_settings = kwargs.get("semantic_settings", None)
+        self.vector_search = kwargs.get("vector_search", None)
         self.e_tag = kwargs.get("e_tag", None)
 
     def _to_generated(self):
         if self.analyzers:
-            analyzers = [
-                pack_analyzer(x) for x in self.analyzers  # type: ignore
-            ]  # mypy: ignore
+            analyzers = [pack_analyzer(x) for x in self.analyzers]  # type: ignore  # mypy: ignore
         else:
             analyzers = None
         if self.tokenizers:
             tokenizers = [
-                x._to_generated()  # pylint:disable=protected-access
-                if isinstance(x, PatternTokenizer)
-                else x
+                x._to_generated() if isinstance(x, PatternTokenizer) else x  # pylint:disable=protected-access
                 for x in self.tokenizers
             ]
         else:
             tokenizers = None
         if self.fields:
             fields = [pack_search_field(x) for x in self.fields]
         else:
@@ -603,45 +604,40 @@
             suggesters=self.suggesters,
             analyzers=analyzers,
             tokenizers=tokenizers,
             token_filters=self.token_filters,
             char_filters=self.char_filters,
             normalizers=self.normalizers,
             # pylint:disable=protected-access
-            encryption_key=self.encryption_key._to_generated()
-            if self.encryption_key
-            else None,
+            encryption_key=self.encryption_key._to_generated() if self.encryption_key else None,
             similarity=self.similarity,
             semantic_settings=self.semantic_settings,
             e_tag=self.e_tag,
+            vector_search=self.vector_search,
         )
 
     @classmethod
     def _from_generated(cls, search_index):
         if not search_index:
             return None
         if search_index.analyzers:
-            analyzers = [
-                unpack_analyzer(x) for x in search_index.analyzers  # type: ignore
-            ]
+            analyzers = [unpack_analyzer(x) for x in search_index.analyzers]  # type: ignore
         else:
             analyzers = None
         if search_index.tokenizers:
             tokenizers = [
                 PatternTokenizer._from_generated(x)  # pylint:disable=protected-access
                 if isinstance(x, _PatternTokenizer)
                 else x
                 for x in search_index.tokenizers
             ]
         else:
             tokenizers = None
         if search_index.fields:
-            fields = [
-                SearchField._from_generated(x) for x in search_index.fields  # pylint:disable=protected-access
-            ]
+            fields = [SearchField._from_generated(x) for x in search_index.fields]  # pylint:disable=protected-access
         else:
             fields = None
         try:
             normalizers = search_index.normalizers
         except AttributeError:
             normalizers = None
         return cls(
@@ -653,20 +649,19 @@
             suggesters=search_index.suggesters,
             analyzers=analyzers,
             tokenizers=tokenizers,
             token_filters=search_index.token_filters,
             char_filters=search_index.char_filters,
             normalizers=normalizers,
             # pylint:disable=protected-access
-            encryption_key=SearchResourceEncryptionKey._from_generated(
-                search_index.encryption_key
-            ),
+            encryption_key=SearchResourceEncryptionKey._from_generated(search_index.encryption_key),
             similarity=search_index.similarity,
             semantic_settings=search_index.semantic_settings,
             e_tag=search_index.e_tag,
+            vector_search=search_index.vector_search,
         )
 
 
 def pack_search_field(search_field):
     # type: (SearchField) -> _SearchField
     if not search_field:
         return None
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/models/_models.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/models/_models.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,23 +8,21 @@
 from azure.core import CaseInsensitiveEnumMeta
 from .._generated import _serialization
 from .._generated.models import (
     LexicalAnalyzer,
     LexicalTokenizer,
     AnalyzeRequest,
     CustomAnalyzer as _CustomAnalyzer,
-    EntityRecognitionSkill as _EntityRecognitionSkillV1,
     EntityRecognitionSkillV3 as _EntityRecognitionSkillV3,
     PatternAnalyzer as _PatternAnalyzer,
     PatternTokenizer as _PatternTokenizer,
     SearchResourceEncryptionKey as _SearchResourceEncryptionKey,
     SearchIndexerDataSource as _SearchIndexerDataSource,
     SearchIndexerSkill,
     SearchIndexerSkillset as _SearchIndexerSkillset,
-    SentimentSkill as _SentimentSkillV1,
     SentimentSkillV3 as _SentimentSkillV3,
     SynonymMap as _SynonymMap,
     DataSourceCredentials,
     AzureActiveDirectoryApplicationCredentials,
 )
 
 
@@ -59,60 +57,55 @@
      this property to null. You can change this property as needed if you want to rotate your
      encryption key; Your skillset definition will be unaffected. Encryption with customer-managed
      keys is not available for free search services, and is only available for paid services created
      on or after January 1, 2019.
     :paramtype encryption_key: ~azure.search.documents.indexes.models.SearchResourceEncryptionKey
     """
 
-    def __init__(
-        self,
-        **kwargs
-    ):
+    def __init__(self, **kwargs):
         super(SearchIndexerSkillset, self).__init__(**kwargs)
 
     def _to_generated(self):
         generated_skills = []
         for skill in self.skills:
-            if hasattr(skill, '_to_generated'):
-                generated_skills.append(skill._to_generated()) # pylint:disable=protected-access
+            if hasattr(skill, "_to_generated"):
+                generated_skills.append(skill._to_generated())  # pylint:disable=protected-access
             else:
                 generated_skills.append(skill)
         assert len(generated_skills) == len(self.skills)
         return _SearchIndexerSkillset(
-            name=getattr(self, 'name', None),
-            description=getattr(self, 'description', None),
+            name=getattr(self, "name", None),
+            description=getattr(self, "description", None),
             skills=generated_skills,
-            cognitive_services_account=getattr(self, 'cognitive_services_account', None),
-            knowledge_store=getattr(self, 'knowledge_store', None),
-            e_tag=getattr(self, 'e_tag', None),
-            encryption_key=getattr(self, 'encryption_key', None)
+            cognitive_services_account=getattr(self, "cognitive_services_account", None),
+            knowledge_store=getattr(self, "knowledge_store", None),
+            e_tag=getattr(self, "e_tag", None),
+            encryption_key=getattr(self, "encryption_key", None),
         )
 
     @classmethod
     def _from_generated(cls, skillset):
         custom_skills = []
         for skill in skillset.skills:
             skill_cls = type(skill)
-            if skill_cls in [_EntityRecognitionSkillV1, _EntityRecognitionSkillV3]:
-                custom_skills.append(EntityRecognitionSkill._from_generated(skill)) # pylint:disable=protected-access
-            elif skill_cls in [_SentimentSkillV1, _SentimentSkillV3]:
-                custom_skills.append(SentimentSkill._from_generated(skill)) # pylint:disable=protected-access
+            if skill_cls in [_EntityRecognitionSkillV3]:
+                custom_skills.append(EntityRecognitionSkill._from_generated(skill))  # pylint:disable=protected-access
+            elif skill_cls in [_SentimentSkillV3]:
+                custom_skills.append(SentimentSkill._from_generated(skill))  # pylint:disable=protected-access
             else:
                 custom_skills.append(skill)
         assert len(skillset.skills) == len(custom_skills)
         kwargs = skillset.as_dict()
-        kwargs['skills'] = custom_skills
+        kwargs["skills"] = custom_skills
         return cls(**kwargs)
 
 
 class EntityRecognitionSkillVersion(str, Enum, metaclass=CaseInsensitiveEnumMeta):
     """Specifies the Entity Recognition skill version to use."""
 
-    #: Use Entity Recognition skill V1.
-    V1 = "#Microsoft.Skills.Text.EntityRecognitionSkill"
     #: Use Entity Recognition skill V3.
     V3 = "#Microsoft.Skills.Text.V3.EntityRecognitionSkill"
     #: Use latest version of Entity Recognition skill.
     LATEST = "#Microsoft.Skills.Text.V3.EntityRecognitionSkill"
 
 
 class EntityRecognitionSkill(SearchIndexerSkill):
@@ -161,100 +154,75 @@
     :paramtype model_version: str
     :keyword skill_version: The version of the skill to use when calling the Text Analytics service.
      It will default to V1 when not specified.
     :paramtype skill_version: ~azure.search.documents.indexes.models.EntityRecognitionSkillVersion
     """
 
     _validation = {
-        'odata_type': {'required': True},
-        'inputs': {'required': True},
-        'outputs': {'required': True},
-        'minimum_precision': {'maximum': 1, 'minimum': 0},
+        "odata_type": {"required": True},
+        "inputs": {"required": True},
+        "outputs": {"required": True},
+        "minimum_precision": {"maximum": 1, "minimum": 0},
     }
 
     _attribute_map = {
-        'odata_type': {'key': '@odata\\.type', 'type': 'str'},
-        'name': {'key': 'name', 'type': 'str'},
-        'description': {'key': 'description', 'type': 'str'},
-        'context': {'key': 'context', 'type': 'str'},
-        'inputs': {'key': 'inputs', 'type': '[InputFieldMappingEntry]'},
-        'outputs': {'key': 'outputs', 'type': '[OutputFieldMappingEntry]'},
-        'categories': {'key': 'categories', 'type': '[str]'},
-        'default_language_code': {'key': 'defaultLanguageCode', 'type': 'str'},
-        'include_typeless_entities': {'key': 'includeTypelessEntities', 'type': 'bool'},
-        'minimum_precision': {'key': 'minimumPrecision', 'type': 'float'},
-        'model_version': {'key': 'modelVersion', 'type': 'str'},
-        'skill_version': {'key': 'skillVersion', 'type': 'str'}
-    }
-
-    def __init__(
-        self,
-        **kwargs
-    ):
+        "odata_type": {"key": "@odata\\.type", "type": "str"},
+        "name": {"key": "name", "type": "str"},
+        "description": {"key": "description", "type": "str"},
+        "context": {"key": "context", "type": "str"},
+        "inputs": {"key": "inputs", "type": "[InputFieldMappingEntry]"},
+        "outputs": {"key": "outputs", "type": "[OutputFieldMappingEntry]"},
+        "categories": {"key": "categories", "type": "[str]"},
+        "default_language_code": {"key": "defaultLanguageCode", "type": "str"},
+        "include_typeless_entities": {"key": "includeTypelessEntities", "type": "bool"},
+        "minimum_precision": {"key": "minimumPrecision", "type": "float"},
+        "model_version": {"key": "modelVersion", "type": "str"},
+        "skill_version": {"key": "skillVersion", "type": "str"},
+    }
+
+    def __init__(self, **kwargs):
         # pop skill_version from kwargs to avoid warning in msrest
-        skill_version = kwargs.pop('skill_version', EntityRecognitionSkillVersion.V1)
+        skill_version = kwargs.pop("skill_version", EntityRecognitionSkillVersion.V3)
 
         super(EntityRecognitionSkill, self).__init__(**kwargs)
         self.skill_version = skill_version
         self.odata_type = self.skill_version  # type: str
-        self.categories = kwargs.get('categories', None)
-        self.default_language_code = kwargs.get('default_language_code', None)
-        self.minimum_precision = kwargs.get('minimum_precision', None)
-        self.include_typeless_entities = kwargs.get('include_typeless_entities', None)
-        self.model_version = kwargs.get('model_version', None)
-
+        self.categories = kwargs.get("categories", None)
+        self.default_language_code = kwargs.get("default_language_code", None)
+        self.minimum_precision = kwargs.get("minimum_precision", None)
+        self.include_typeless_entities = kwargs.get("include_typeless_entities", None)
+        self.model_version = kwargs.get("model_version", None)
 
     def _to_generated(self):
-        if self.skill_version == EntityRecognitionSkillVersion.V1:
-            return _EntityRecognitionSkillV1(
-                inputs=self.inputs,
-                outputs=self.outputs,
-                name=self.name,
-                odata_type=self.odata_type,
-                categories=self.categories,
-                default_language_code=self.default_language_code,
-                include_typeless_entities=self.include_typeless_entities,
-                minimum_precision=self.minimum_precision
-            )
         if self.skill_version in [EntityRecognitionSkillVersion.V3, EntityRecognitionSkillVersion.LATEST]:
             return _EntityRecognitionSkillV3(
                 inputs=self.inputs,
                 outputs=self.outputs,
                 name=self.name,
                 odata_type=self.odata_type,
                 categories=self.categories,
                 default_language_code=self.default_language_code,
                 minimum_precision=self.minimum_precision,
-                model_version=self.model_version
+                model_version=self.model_version,
             )
         return None
 
     @classmethod
     def _from_generated(cls, skill):
         if not skill:
             return None
         kwargs = skill.as_dict()
-        if isinstance(skill, _EntityRecognitionSkillV1):
-            return EntityRecognitionSkill(
-                skill_version=EntityRecognitionSkillVersion.V1,
-                **kwargs
-            )
         if isinstance(skill, _EntityRecognitionSkillV3):
-            return EntityRecognitionSkill(
-                skill_version=EntityRecognitionSkillVersion.V3,
-                **kwargs
-            )
+            return EntityRecognitionSkill(skill_version=EntityRecognitionSkillVersion.V3, **kwargs)
         return None
 
 
 class SentimentSkillVersion(str, Enum, metaclass=CaseInsensitiveEnumMeta):
-    """ Specifies the Sentiment Skill version to use."""
+    """Specifies the Sentiment Skill version to use."""
 
-    #: Use Sentiment skill V1.
-    V1 = "#Microsoft.Skills.Text.SentimentSkill"
     #: Use Sentiment skill V3.
     V3 = "#Microsoft.Skills.Text.V3.SentimentSkill"
     #: Use latest version of Sentiment skill.
     LATEST = "#Microsoft.Skills.Text.V3.SentimentSkill"
 
 
 class SentimentSkill(SearchIndexerSkill):
@@ -300,82 +268,63 @@
     :paramtype model_version: str
     :keyword skill_version: The version of the skill to use when calling the Text Analytics service.
      It will default to V1 when not specified.
     :paramtype skill_version: ~azure.search.documents.indexes.models.SentimentSkillVersion
     """
 
     _validation = {
-        'odata_type': {'required': True},
-        'inputs': {'required': True},
-        'outputs': {'required': True},
+        "odata_type": {"required": True},
+        "inputs": {"required": True},
+        "outputs": {"required": True},
     }
 
     _attribute_map = {
-        'odata_type': {'key': '@odata\\.type', 'type': 'str'},
-        'name': {'key': 'name', 'type': 'str'},
-        'description': {'key': 'description', 'type': 'str'},
-        'context': {'key': 'context', 'type': 'str'},
-        'inputs': {'key': 'inputs', 'type': '[InputFieldMappingEntry]'},
-        'outputs': {'key': 'outputs', 'type': '[OutputFieldMappingEntry]'},
-        'default_language_code': {'key': 'defaultLanguageCode', 'type': 'str'},
-        'include_opinion_mining': {'key': 'includeOpinionMining', 'type': 'bool'},
-        'model_version': {'key': 'modelVersion', 'type': 'str'},
-        'skill_version': {'key': 'skillVersion', 'type': 'str'}
-    }
-
-    def __init__(
-        self,
-        **kwargs
-    ):
+        "odata_type": {"key": "@odata\\.type", "type": "str"},
+        "name": {"key": "name", "type": "str"},
+        "description": {"key": "description", "type": "str"},
+        "context": {"key": "context", "type": "str"},
+        "inputs": {"key": "inputs", "type": "[InputFieldMappingEntry]"},
+        "outputs": {"key": "outputs", "type": "[OutputFieldMappingEntry]"},
+        "default_language_code": {"key": "defaultLanguageCode", "type": "str"},
+        "include_opinion_mining": {"key": "includeOpinionMining", "type": "bool"},
+        "model_version": {"key": "modelVersion", "type": "str"},
+        "skill_version": {"key": "skillVersion", "type": "str"},
+    }
+
+    def __init__(self, **kwargs):
         # pop skill_version from kwargs to avoid warning in msrest
-        skill_version = kwargs.pop('skill_version', SentimentSkillVersion.V1)
+        skill_version = kwargs.pop("skill_version", SentimentSkillVersion.V3)
 
         super(SentimentSkill, self).__init__(**kwargs)
         self.skill_version = skill_version
         self.odata_type = self.skill_version  # type: str
-        self.default_language_code = kwargs.get('default_language_code', None)
-        self.include_opinion_mining = kwargs.get('include_opinion_mining', None)
-        self.model_version = kwargs.get('model_version', None)
+        self.default_language_code = kwargs.get("default_language_code", None)
+        self.include_opinion_mining = kwargs.get("include_opinion_mining", None)
+        self.model_version = kwargs.get("model_version", None)
 
     def _to_generated(self):
-        if self.skill_version == SentimentSkillVersion.V1:
-            return _SentimentSkillV1(
-                inputs=self.inputs,
-                outputs=self.outputs,
-                name=self.name,
-                odata_type=self.odata_type,
-                default_language_code=self.default_language_code
-        )
         if self.skill_version in [SentimentSkillVersion.V3, SentimentSkillVersion.LATEST]:
             return _SentimentSkillV3(
                 inputs=self.inputs,
                 outputs=self.outputs,
                 name=self.name,
                 odata_type=self.odata_type,
                 default_language_code=self.default_language_code,
                 include_opinion_mining=self.include_opinion_mining,
-                model_version=self.model_version
+                model_version=self.model_version,
             )
         return None
 
     @classmethod
     def _from_generated(cls, skill):
         if not skill:
             return None
         kwargs = skill.as_dict()
-        if isinstance(skill, _SentimentSkillV1):
-            return SentimentSkill(
-                skill_version=SentimentSkillVersion.V1,
-                **kwargs
-            )
         if isinstance(skill, _SentimentSkillV3):
-            return SentimentSkill(
-                skill_version=SentimentSkillVersion.V3,
-                **kwargs
-            )
+            return SentimentSkill(skill_version=SentimentSkillVersion.V3, **kwargs)
         return None
 
 
 class AnalyzeTextOptions(_serialization.Model):
     """Specifies some text and analysis components used to break that text into tokens.
 
     All required parameters must be populated in order to send to Azure.
@@ -727,20 +676,16 @@
         )
 
     @classmethod
     def _from_generated(cls, search_resource_encryption_key):
         if not search_resource_encryption_key:
             return None
         if search_resource_encryption_key.access_credentials:
-            application_id = (
-                search_resource_encryption_key.access_credentials.application_id
-            )
-            application_secret = (
-                search_resource_encryption_key.access_credentials.application_secret
-            )
+            application_id = search_resource_encryption_key.access_credentials.application_id
+            application_secret = search_resource_encryption_key.access_credentials.application_secret
         else:
             application_id = None
             application_secret = None
         return cls(
             key_name=search_resource_encryption_key.key_name,
             key_version=search_resource_encryption_key.key_version,
             vault_uri=search_resource_encryption_key.vault_uri,
@@ -817,17 +762,15 @@
     def _from_generated(cls, synonym_map):
         if not synonym_map:
             return None
         return cls(
             name=synonym_map.name,
             synonyms=synonym_map.synonyms.split("\n"),
             # pylint:disable=protected-access
-            encryption_key=SearchResourceEncryptionKey._from_generated(
-                synonym_map.encryption_key
-            ),
+            encryption_key=SearchResourceEncryptionKey._from_generated(synonym_map.encryption_key),
             e_tag=synonym_map.e_tag,
         )
 
 
 class SearchIndexerDataSourceConnection(_serialization.Model):
     """Represents a datasource connection definition, which can be used to configure an indexer.
 
@@ -885,32 +828,28 @@
             "key": "dataChangeDetectionPolicy",
             "type": "DataChangeDetectionPolicy",
         },
         "data_deletion_detection_policy": {
             "key": "dataDeletionDetectionPolicy",
             "type": "DataDeletionDetectionPolicy",
         },
-        'encryption_key': {'key': 'encryptionKey', 'type': 'SearchResourceEncryptionKey'},
+        "encryption_key": {"key": "encryptionKey", "type": "SearchResourceEncryptionKey"},
         "e_tag": {"key": "@odata\\.etag", "type": "str"},
-        'identity': {'key': 'identity', 'type': 'SearchIndexerDataIdentity'},
+        "identity": {"key": "identity", "type": "SearchIndexerDataIdentity"},
     }
 
     def __init__(self, **kwargs):
         super(SearchIndexerDataSourceConnection, self).__init__(**kwargs)
         self.name = kwargs["name"]
         self.description = kwargs.get("description", None)
         self.type = kwargs["type"]
         self.connection_string = kwargs["connection_string"]
         self.container = kwargs["container"]
-        self.data_change_detection_policy = kwargs.get(
-            "data_change_detection_policy", None
-        )
-        self.data_deletion_detection_policy = kwargs.get(
-            "data_deletion_detection_policy", None
-        )
+        self.data_change_detection_policy = kwargs.get("data_change_detection_policy", None)
+        self.data_deletion_detection_policy = kwargs.get("data_deletion_detection_policy", None)
         self.e_tag = kwargs.get("e_tag", None)
         self.encryption_key = kwargs.get("encryption_key", None)
         self.identity = kwargs.get("identity", None)
 
     def _to_generated(self):
         if self.connection_string is None or self.connection_string == "":
             connection_string = "<unchanged>"
@@ -923,52 +862,47 @@
             type=self.type,
             credentials=credentials,
             container=self.container,
             data_change_detection_policy=self.data_change_detection_policy,
             data_deletion_detection_policy=self.data_deletion_detection_policy,
             e_tag=self.e_tag,
             encryption_key=self.encryption_key,
-            identity=self.identity
+            identity=self.identity,
         )
 
     @classmethod
     def _from_generated(cls, search_indexer_data_source):
         if not search_indexer_data_source:
             return None
         connection_string = (
-            search_indexer_data_source.credentials.connection_string
-            if search_indexer_data_source.credentials
-            else None
+            search_indexer_data_source.credentials.connection_string if search_indexer_data_source.credentials else None
         )
         return cls(
             name=search_indexer_data_source.name,
             description=search_indexer_data_source.description,
             type=search_indexer_data_source.type,
             connection_string=connection_string,
             container=search_indexer_data_source.container,
             data_change_detection_policy=search_indexer_data_source.data_change_detection_policy,
             data_deletion_detection_policy=search_indexer_data_source.data_deletion_detection_policy,
             e_tag=search_indexer_data_source.e_tag,
             encryption_key=search_indexer_data_source.encryption_key,
-            identity=search_indexer_data_source.identity
+            identity=search_indexer_data_source.identity,
         )
 
+
 def pack_analyzer(analyzer):
     if not analyzer:
         return None
     if isinstance(analyzer, (PatternAnalyzer, CustomAnalyzer)):
         return analyzer._to_generated()  # pylint:disable=protected-access
     return analyzer
 
 
 def unpack_analyzer(analyzer):
     if not analyzer:
         return None
     if isinstance(analyzer, _PatternAnalyzer):
-        return PatternAnalyzer._from_generated(  # pylint:disable=protected-access
-            analyzer
-        )
+        return PatternAnalyzer._from_generated(analyzer)  # pylint:disable=protected-access
     if isinstance(analyzer, _CustomAnalyzer):
-        return CustomAnalyzer._from_generated(  # pylint:disable=protected-access
-            analyzer
-        )
+        return CustomAnalyzer._from_generated(analyzer)  # pylint:disable=protected-access
     return analyzer
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._search_service_client import SearchServiceClient
 
 try:
     from ._patch import __all__ as _patch_all
-    from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+    from ._patch import *  # pylint: disable=unused-wildcard-import
 except ImportError:
     _patch_all = []
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "SearchServiceClient",
 ]
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_search_service_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_search_service_client.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from copy import deepcopy
 from typing import Any
 
 from azure.core import PipelineClient
 from azure.core.rest import HttpRequest, HttpResponse
 
-from . import models
+from . import models as _models
 from ._configuration import SearchServiceClientConfiguration
 from ._serialization import Deserializer, Serializer
 from .operations import (
     AliasesOperations,
     DataSourcesOperations,
     IndexersOperations,
     IndexesOperations,
@@ -40,27 +40,27 @@
     :vartype synonym_maps: search_service_client.operations.SynonymMapsOperations
     :ivar indexes: IndexesOperations operations
     :vartype indexes: search_service_client.operations.IndexesOperations
     :ivar aliases: AliasesOperations operations
     :vartype aliases: search_service_client.operations.AliasesOperations
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(  # pylint: disable=missing-client-constructor-parameter-credential
         self, endpoint: str, **kwargs: Any
     ) -> None:
         _endpoint = "{endpoint}"
         self._config = SearchServiceClientConfiguration(endpoint=endpoint, **kwargs)
-        self._client = PipelineClient(base_url=_endpoint, config=self._config, **kwargs)
+        self._client: PipelineClient = PipelineClient(base_url=_endpoint, config=self._config, **kwargs)
 
-        client_models = {k: v for k, v in models.__dict__.items() if isinstance(v, type)}
+        client_models = {k: v for k, v in _models.__dict__.items() if isinstance(v, type)}
         self._serialize = Serializer(client_models)
         self._deserialize = Deserializer(client_models)
         self._serialize.client_side_validation = False
         self.data_sources = DataSourcesOperations(self._client, self._config, self._serialize, self._deserialize)
         self.indexers = IndexersOperations(self._client, self._config, self._serialize, self._deserialize)
         self.skillsets = SkillsetsOperations(self._client, self._config, self._serialize, self._deserialize)
         self.synonym_maps = SynonymMapsOperations(self._client, self._config, self._serialize, self._deserialize)
@@ -89,19 +89,16 @@
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
 
         request_copy.url = self._client.format_url(request_copy.url, **path_format_arguments)
         return self._client.send_request(request_copy, **kwargs)
 
-    def close(self):
-        # type: () -> None
+    def close(self) -> None:
         self._client.close()
 
-    def __enter__(self):
-        # type: () -> SearchServiceClient
+    def __enter__(self) -> "SearchServiceClient":
         self._client.__enter__()
         return self
 
-    def __exit__(self, *exc_details):
-        # type: (Any) -> None
+    def __exit__(self, *exc_details: Any) -> None:
         self._client.__exit__(*exc_details)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_serialization.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_serialization.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,56 +21,71 @@
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 # IN THE SOFTWARE.
 #
 # --------------------------------------------------------------------------
 
 # pylint: skip-file
+# pyright: reportUnnecessaryTypeIgnoreComment=false
 
 from base64 import b64decode, b64encode
 import calendar
 import datetime
 import decimal
 import email
 from enum import Enum
 import json
 import logging
 import re
 import sys
 import codecs
+from typing import (
+    Dict,
+    Any,
+    cast,
+    Optional,
+    Union,
+    AnyStr,
+    IO,
+    Mapping,
+    Callable,
+    TypeVar,
+    MutableMapping,
+    Type,
+    List,
+    Mapping,
+)
 
 try:
     from urllib import quote  # type: ignore
 except ImportError:
-    from urllib.parse import quote  # type: ignore
+    from urllib.parse import quote
 import xml.etree.ElementTree as ET
 
-import isodate
-
-from typing import Dict, Any, cast, TYPE_CHECKING
+import isodate  # type: ignore
 
 from azure.core.exceptions import DeserializationError, SerializationError, raise_with_traceback
+from azure.core.serialization import NULL as AzureCoreNull
 
 _BOM = codecs.BOM_UTF8.decode(encoding="utf-8")
 
-if TYPE_CHECKING:
-    from typing import Optional, Union, AnyStr, IO, Mapping
+ModelType = TypeVar("ModelType", bound="Model")
+JSON = MutableMapping[str, Any]
 
 
 class RawDeserializer:
 
     # Accept "text" because we're open minded people...
     JSON_REGEXP = re.compile(r"^(application|text)/([a-z+.]+\+)?json$")
 
     # Name used in context
     CONTEXT_NAME = "deserialized_data"
 
     @classmethod
-    def deserialize_from_text(cls, data, content_type=None):
-        # type: (Optional[Union[AnyStr, IO]], Optional[str]) -> Any
+    def deserialize_from_text(cls, data: Optional[Union[AnyStr, IO]], content_type: Optional[str] = None) -> Any:
         """Decode data according to content-type.
 
         Accept a stream of data as well, but will be load at once in memory for now.
 
         If no content-type, will return the string version (not bytes, not stream)
 
         :param data: Input, could be bytes or stream (will be decoded with UTF8) or text
@@ -128,16 +143,15 @@
                 # The function hack is because Py2.7 messes up with exception
                 # context otherwise.
                 _LOGGER.critical("Wasn't XML not JSON, failing")
                 raise_with_traceback(DeserializationError, "XML is invalid")
         raise DeserializationError("Cannot deserialize content-type: {}".format(content_type))
 
     @classmethod
-    def deserialize_from_http_generics(cls, body_bytes, headers):
-        # type: (Optional[Union[AnyStr, IO]], Mapping) -> Any
+    def deserialize_from_http_generics(cls, body_bytes: Optional[Union[AnyStr, IO]], headers: Mapping) -> Any:
         """Deserialize from HTTP response.
 
         Use bytes and headers to NOT use any requests/aiohttp or whatever
         specific implementation.
         Headers will tested for "content-type"
         """
         # Try to use content-type from headers if available
@@ -156,16 +170,16 @@
         return None
 
 
 try:
     basestring  # type: ignore
     unicode_str = unicode  # type: ignore
 except NameError:
-    basestring = str  # type: ignore
-    unicode_str = str  # type: ignore
+    basestring = str
+    unicode_str = str
 
 _LOGGER = logging.getLogger(__name__)
 
 try:
     _long_type = long  # type: ignore
 except NameError:
     _long_type = int
@@ -184,15 +198,15 @@
 
     def dst(self, dt):
         """No daylight saving for UTC."""
         return datetime.timedelta(hours=1)
 
 
 try:
-    from datetime import timezone as _FixedOffset
+    from datetime import timezone as _FixedOffset  # type: ignore
 except ImportError:  # Python 2.7
 
     class _FixedOffset(datetime.tzinfo):  # type: ignore
         """Fixed offset in minutes east from UTC.
         Copy/pasted from Python doc
         :param datetime.timedelta offset: offset in timedelta format
         """
@@ -215,15 +229,15 @@
         def __getinitargs__(self):
             return (self.__offset,)
 
 
 try:
     from datetime import timezone
 
-    TZ_UTC = timezone.utc  # type: ignore
+    TZ_UTC = timezone.utc
 except ImportError:
     TZ_UTC = UTC()  # type: ignore
 
 _FLATTEN = re.compile(r"(?<!\\)\.")
 
 
 def attribute_transformer(key, attr_desc, value):
@@ -272,79 +286,84 @@
 
 
 class Model(object):
     """Mixin for all client request body/response body models to support
     serialization and deserialization.
     """
 
-    _subtype_map = {}  # type: Dict[str, Dict[str, Any]]
-    _attribute_map = {}  # type: Dict[str, Dict[str, Any]]
-    _validation = {}  # type: Dict[str, Dict[str, Any]]
+    _subtype_map: Dict[str, Dict[str, Any]] = {}
+    _attribute_map: Dict[str, Dict[str, Any]] = {}
+    _validation: Dict[str, Dict[str, Any]] = {}
 
-    def __init__(self, **kwargs):
-        self.additional_properties = {}
+    def __init__(self, **kwargs: Any) -> None:
+        self.additional_properties: Dict[str, Any] = {}
         for k in kwargs:
             if k not in self._attribute_map:
                 _LOGGER.warning("%s is not a known attribute of class %s and will be ignored", k, self.__class__)
             elif k in self._validation and self._validation[k].get("readonly", False):
                 _LOGGER.warning("Readonly attribute %s will be ignored in class %s", k, self.__class__)
             else:
                 setattr(self, k, kwargs[k])
 
-    def __eq__(self, other):
+    def __eq__(self, other: Any) -> bool:
         """Compare objects by comparing all attributes."""
         if isinstance(other, self.__class__):
             return self.__dict__ == other.__dict__
         return False
 
-    def __ne__(self, other):
+    def __ne__(self, other: Any) -> bool:
         """Compare objects by comparing all attributes."""
         return not self.__eq__(other)
 
-    def __str__(self):
+    def __str__(self) -> str:
         return str(self.__dict__)
 
     @classmethod
-    def enable_additional_properties_sending(cls):
+    def enable_additional_properties_sending(cls) -> None:
         cls._attribute_map["additional_properties"] = {"key": "", "type": "{object}"}
 
     @classmethod
-    def is_xml_model(cls):
+    def is_xml_model(cls) -> bool:
         try:
-            cls._xml_map
+            cls._xml_map  # type: ignore
         except AttributeError:
             return False
         return True
 
     @classmethod
     def _create_xml_node(cls):
         """Create XML node."""
         try:
-            xml_map = cls._xml_map
+            xml_map = cls._xml_map  # type: ignore
         except AttributeError:
             xml_map = {}
 
         return _create_xml_node(xml_map.get("name", cls.__name__), xml_map.get("prefix", None), xml_map.get("ns", None))
 
-    def serialize(self, keep_readonly=False, **kwargs):
+    def serialize(self, keep_readonly: bool = False, **kwargs: Any) -> JSON:
         """Return the JSON that would be sent to azure from this model.
 
         This is an alias to `as_dict(full_restapi_key_transformer, keep_readonly=False)`.
 
         If you want XML serialization, you can pass the kwargs is_xml=True.
 
         :param bool keep_readonly: If you want to serialize the readonly attributes
         :returns: A dict JSON compatible object
         :rtype: dict
         """
         serializer = Serializer(self._infer_class_models())
         return serializer._serialize(self, keep_readonly=keep_readonly, **kwargs)
 
-    def as_dict(self, keep_readonly=True, key_transformer=attribute_transformer, **kwargs):
-        """Return a dict that can be JSONify using json.dump.
+    def as_dict(
+        self,
+        keep_readonly: bool = True,
+        key_transformer: Callable[[str, Dict[str, Any], Any], Any] = attribute_transformer,
+        **kwargs: Any
+    ) -> JSON:
+        """Return a dict that can be serialized using json.dump.
 
         Advanced usage might optionally use a callback as parameter:
 
         .. code::python
 
             def my_key_transformer(key, attr_desc, value):
                 return key
@@ -383,41 +402,46 @@
                 raise ValueError("Not Autorest generated code")
         except Exception:
             # Assume it's not Autorest generated (tests?). Add ourselves as dependencies.
             client_models = {cls.__name__: cls}
         return client_models
 
     @classmethod
-    def deserialize(cls, data, content_type=None):
+    def deserialize(cls: Type[ModelType], data: Any, content_type: Optional[str] = None) -> ModelType:
         """Parse a str using the RestAPI syntax and return a model.
 
         :param str data: A str using RestAPI structure. JSON by default.
         :param str content_type: JSON by default, set application/xml if XML.
         :returns: An instance of this model
         :raises: DeserializationError if something went wrong
         """
         deserializer = Deserializer(cls._infer_class_models())
         return deserializer(cls.__name__, data, content_type=content_type)
 
     @classmethod
-    def from_dict(cls, data, key_extractors=None, content_type=None):
+    def from_dict(
+        cls: Type[ModelType],
+        data: Any,
+        key_extractors: Optional[Callable[[str, Dict[str, Any], Any], Any]] = None,
+        content_type: Optional[str] = None,
+    ) -> ModelType:
         """Parse a dict using given key extractor return a model.
 
         By default consider key
         extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor
         and last_rest_key_case_insensitive_extractor)
 
         :param dict data: A dict using RestAPI structure
         :param str content_type: JSON by default, set application/xml if XML.
         :returns: An instance of this model
         :raises: DeserializationError if something went wrong
         """
         deserializer = Deserializer(cls._infer_class_models())
-        deserializer.key_extractors = (
-            [
+        deserializer.key_extractors = (  # type: ignore
+            [  # type: ignore
                 attribute_key_case_insensitive_extractor,
                 rest_key_case_insensitive_extractor,
                 last_rest_key_case_insensitive_extractor,
             ]
             if key_extractors is None
             else key_extractors
         )
@@ -449,15 +473,15 @@
             if subtype_value:
                 # Try to match base class. Can be class name only
                 # (bug to fix in Autorest to support x-ms-discriminator-name)
                 if cls.__name__ == subtype_value:
                     return cls
                 flatten_mapping_type = cls._flatten_subtype(subtype_key, objects)
                 try:
-                    return objects[flatten_mapping_type[subtype_value]]
+                    return objects[flatten_mapping_type[subtype_value]]  # type: ignore
                 except KeyError:
                     _LOGGER.warning(
                         "Subtype value %s has no mapping, use base class %s.",
                         subtype_value,
                         cls.__name__,
                     )
                     break
@@ -517,15 +541,15 @@
         "min_items": lambda x, y: len(x) < y,
         "max_items": lambda x, y: len(x) > y,
         "pattern": lambda x, y: not re.match(y, x, re.UNICODE),
         "unique": lambda x, y: len(x) != len(set(x)),
         "multiple": lambda x, y: x % y != 0,
     }
 
-    def __init__(self, classes=None):
+    def __init__(self, classes: Optional[Mapping[str, Type[ModelType]]] = None):
         self.serialize_type = {
             "iso-8601": Serializer.serialize_iso,
             "rfc-1123": Serializer.serialize_rfc,
             "unix-time": Serializer.serialize_unix,
             "duration": Serializer.serialize_duration,
             "date": Serializer.serialize_date,
             "time": Serializer.serialize_time,
@@ -533,15 +557,15 @@
             "long": Serializer.serialize_long,
             "bytearray": Serializer.serialize_bytearray,
             "base64": Serializer.serialize_base64,
             "object": self.serialize_object,
             "[]": self.serialize_iter,
             "{}": self.serialize_dict,
         }
-        self.dependencies = dict(classes) if classes else {}
+        self.dependencies: Dict[str, Type[ModelType]] = dict(classes) if classes else {}
         self.key_transformer = full_restapi_key_transformer
         self.client_side_validation = True
 
     def _serialize(self, target_obj, data_type=None, **kwargs):
         """Serialize data into a string according to type.
 
         :param target_obj: The data to be serialized.
@@ -601,47 +625,46 @@
                         xml_desc = attr_desc.get("xml", {})
                         xml_name = xml_desc.get("name", attr_desc["key"])
                         xml_prefix = xml_desc.get("prefix", None)
                         xml_ns = xml_desc.get("ns", None)
                         if xml_desc.get("attr", False):
                             if xml_ns:
                                 ET.register_namespace(xml_prefix, xml_ns)
-                                xml_name = "{}{}".format(xml_ns, xml_name)
-                            serialized.set(xml_name, new_attr)
+                                xml_name = "{{{}}}{}".format(xml_ns, xml_name)
+                            serialized.set(xml_name, new_attr)  # type: ignore
                             continue
                         if xml_desc.get("text", False):
-                            serialized.text = new_attr
+                            serialized.text = new_attr  # type: ignore
                             continue
                         if isinstance(new_attr, list):
-                            serialized.extend(new_attr)
+                            serialized.extend(new_attr)  # type: ignore
                         elif isinstance(new_attr, ET.Element):
                             # If the down XML has no XML/Name, we MUST replace the tag with the local tag. But keeping the namespaces.
                             if "name" not in getattr(orig_attr, "_xml_map", {}):
                                 splitted_tag = new_attr.tag.split("}")
                                 if len(splitted_tag) == 2:  # Namespace
                                     new_attr.tag = "}".join([splitted_tag[0], xml_name])
                                 else:
                                     new_attr.tag = xml_name
-                            serialized.append(new_attr)
+                            serialized.append(new_attr)  # type: ignore
                         else:  # That's a basic type
                             # Integrate namespace if necessary
                             local_node = _create_xml_node(xml_name, xml_prefix, xml_ns)
                             local_node.text = unicode_str(new_attr)
-                            serialized.append(local_node)
+                            serialized.append(local_node)  # type: ignore
                     else:  # JSON
-                        for k in reversed(keys):
-                            unflattened = {k: new_attr}
-                            new_attr = unflattened
+                        for k in reversed(keys):  # type: ignore
+                            new_attr = {k: new_attr}
 
                         _new_attr = new_attr
                         _serialized = serialized
-                        for k in keys:
+                        for k in keys:  # type: ignore
                             if k not in _serialized:
-                                _serialized.update(_new_attr)
-                            _new_attr = _new_attr[k]
+                                _serialized.update(_new_attr)  # type: ignore
+                            _new_attr = _new_attr[k]  # type: ignore
                             _serialized = _serialized[k]
                 except ValueError:
                     continue
 
         except (AttributeError, KeyError, TypeError) as err:
             msg = "Attribute {} in object {} cannot be serialized.\n{}".format(attr_name, class_name, str(target_obj))
             raise_with_traceback(SerializationError, msg, err)
@@ -655,31 +678,31 @@
         :param str data_type: The type to be serialized from.
         :rtype: dict
         :raises: SerializationError if serialization fails.
         :raises: ValueError if data is None
         """
 
         # Just in case this is a dict
-        internal_data_type = data_type.strip("[]{}")
-        internal_data_type = self.dependencies.get(internal_data_type, None)
+        internal_data_type_str = data_type.strip("[]{}")
+        internal_data_type = self.dependencies.get(internal_data_type_str, None)
         try:
             is_xml_model_serialization = kwargs["is_xml"]
         except KeyError:
             if internal_data_type and issubclass(internal_data_type, Model):
                 is_xml_model_serialization = kwargs.setdefault("is_xml", internal_data_type.is_xml_model())
             else:
                 is_xml_model_serialization = False
         if internal_data_type and not isinstance(internal_data_type, Enum):
             try:
                 deserializer = Deserializer(self.dependencies)
                 # Since it's on serialization, it's almost sure that format is not JSON REST
                 # We're not able to deal with additional properties for now.
                 deserializer.additional_properties_detection = False
                 if is_xml_model_serialization:
-                    deserializer.key_extractors = [
+                    deserializer.key_extractors = [  # type: ignore
                         attribute_key_case_insensitive_extractor,
                     ]
                 else:
                     deserializer.key_extractors = [
                         rest_key_case_insensitive_extractor,
                         attribute_key_case_insensitive_extractor,
                         last_rest_key_case_insensitive_extractor,
@@ -776,14 +799,16 @@
         :raises: ValueError if data is None
         :raises: SerializationError if serialization fails.
         """
         if data is None:
             raise ValueError("No value for given attribute")
 
         try:
+            if data is AzureCoreNull:
+                return None
             if data_type in self.basic_types.values():
                 return self.serialize_basic(data, data_type, **kwargs)
 
             elif data_type in self.serialize_type:
                 return self.serialize_type[data_type](data, **kwargs)
 
             # If dependencies is empty, try with current data class
@@ -839,15 +864,15 @@
         """
         try:  # If I received an enum, return its value
             return data.value
         except AttributeError:
             pass
 
         try:
-            if isinstance(data, unicode):
+            if isinstance(data, unicode):  # type: ignore
                 # Don't change it, JSON and XML ElementTree are totally able
                 # to serialize correctly u'' strings
                 return data
         except NameError:
             return str(data)
         else:
             return str(data)
@@ -997,18 +1022,18 @@
     @staticmethod
     def serialize_enum(attr, enum_obj=None):
         try:
             result = attr.value
         except AttributeError:
             result = attr
         try:
-            enum_obj(result)
+            enum_obj(result)  # type: ignore
             return result
         except ValueError:
-            for enum_value in enum_obj:
+            for enum_value in enum_obj:  # type: ignore
                 if enum_value.value.lower() == str(attr).lower():
                     return enum_value.value
             error = "{!r} is not valid value for enum {!r}"
             raise SerializationError(error.format(attr, enum_obj))
 
     @staticmethod
     def serialize_bytearray(attr, **kwargs):
@@ -1160,15 +1185,16 @@
 
 
 def rest_key_extractor(attr, attr_desc, data):
     key = attr_desc["key"]
     working_data = data
 
     while "." in key:
-        dict_keys = _FLATTEN.split(key)
+        # Need the cast, as for some reasons "split" is typed as list[str | Any]
+        dict_keys = cast(List[str], _FLATTEN.split(key))
         if len(dict_keys) == 1:
             key = _decode_attribute_map_key(dict_keys[0])
             break
         working_key = _decode_attribute_map_key(dict_keys[0])
         working_data = working_data.get(working_key, data)
         if working_data is None:
             # If at any point while following flatten JSON path see None, it means
@@ -1241,15 +1267,15 @@
     :rtype: tuple
     :returns: A tuple XML name + namespace dict
     """
     internal_type_xml_map = getattr(internal_type, "_xml_map", {})
     xml_name = internal_type_xml_map.get("name", internal_type.__name__)
     xml_ns = internal_type_xml_map.get("ns", None)
     if xml_ns:
-        xml_name = "{}{}".format(xml_ns, xml_name)
+        xml_name = "{{{}}}{}".format(xml_ns, xml_name)
     return xml_name
 
 
 def xml_key_extractor(attr, attr_desc, data):
     if isinstance(data, dict):
         return None
 
@@ -1265,15 +1291,15 @@
     is_wrapped = xml_desc.get("wrapped", False)
     internal_type = attr_desc.get("internalType", None)
     internal_type_xml_map = getattr(internal_type, "_xml_map", {})
 
     # Integrate namespace if necessary
     xml_ns = xml_desc.get("ns", internal_type_xml_map.get("ns", None))
     if xml_ns:
-        xml_name = "{}{}".format(xml_ns, xml_name)
+        xml_name = "{{{}}}{}".format(xml_ns, xml_name)
 
     # If it's an attribute, that's simple
     if xml_desc.get("attr", False):
         return data.get(xml_name)
 
     # If it's x-ms-text, that's simple too
     if xml_desc.get("text", False):
@@ -1331,15 +1357,15 @@
     :ivar list key_extractors: Ordered list of extractors to be used by this deserializer.
     """
 
     basic_types = {str: "str", int: "int", bool: "bool", float: "float"}
 
     valid_date = re.compile(r"\d{4}[-]\d{2}[-]\d{2}T\d{2}:\d{2}:\d{2}" r"\.?\d*Z?[-+]?[\d{2}]?:?[\d{2}]?")
 
-    def __init__(self, classes=None):
+    def __init__(self, classes: Optional[Mapping[str, Type[ModelType]]] = None):
         self.deserialize_type = {
             "iso-8601": Deserializer.deserialize_iso,
             "rfc-1123": Deserializer.deserialize_rfc,
             "unix-time": Deserializer.deserialize_unix,
             "duration": Deserializer.deserialize_duration,
             "date": Deserializer.deserialize_date,
             "time": Deserializer.deserialize_time,
@@ -1351,15 +1377,15 @@
             "[]": self.deserialize_iter,
             "{}": self.deserialize_dict,
         }
         self.deserialize_expected_types = {
             "duration": (isodate.Duration, datetime.timedelta),
             "iso-8601": (datetime.datetime),
         }
-        self.dependencies = dict(classes) if classes else {}
+        self.dependencies: Dict[str, Type[ModelType]] = dict(classes) if classes else {}
         self.key_extractors = [rest_key_extractor, xml_key_extractor]
         # Additional properties only works if the "rest_key_extractor" is used to
         # extract the keys. Making it to work whatever the key extractor is too much
         # complicated, with no real scenario for now.
         # So adding a flag to disable additional properties detection. This flag should be
         # used if your expect the deserialization to NOT come from a JSON REST syntax.
         # Otherwise, result are unexpected
@@ -1412,15 +1438,15 @@
             return self.deserialize_data(data, response)
         elif isinstance(response, type) and issubclass(response, Enum):
             return self.deserialize_enum(data, response)
 
         if data is None:
             return data
         try:
-            attributes = response._attribute_map
+            attributes = response._attribute_map  # type: ignore
             d_attrs = {}
             for attr, attr_desc in attributes.items():
                 # Check empty string. If it's not empty, someone has a real "additionalProperties"...
                 if attr == "additional_properties" and attr_desc["key"] == "":
                     continue
                 raw_value = None
                 # Enhance attr_desc with some dynamic data
@@ -1440,15 +1466,15 @@
                             _LOGGER.warning(msg, found_value, key_extractor, attr)
                             continue
                         raw_value = found_value
 
                 value = self.deserialize_data(raw_value, attr_desc["type"])
                 d_attrs[attr] = value
         except (AttributeError, TypeError, KeyError) as err:
-            msg = "Unable to deserialize to object: " + class_name
+            msg = "Unable to deserialize to object: " + class_name  # type: ignore
             raise_with_traceback(DeserializationError, msg, err)
         else:
             additional_properties = self._build_additional_properties(attributes, data)
             return self._instantiate_model(response, d_attrs, additional_properties)
 
     def _build_additional_properties(self, attribute_map, data):
         if not self.additional_properties_detection:
@@ -1470,40 +1496,40 @@
 
     def _classify_target(self, target, data):
         """Check to see whether the deserialization target object can
         be classified into a subclass.
         Once classification has been determined, initialize object.
 
         :param str target: The target object type to deserialize to.
-        :param str/dict data: The response data to deseralize.
+        :param str/dict data: The response data to deserialize.
         """
         if target is None:
             return None, None
 
         if isinstance(target, basestring):
             try:
                 target = self.dependencies[target]
             except KeyError:
                 return target, target
 
         try:
             target = target._classify(data, self.dependencies)
         except AttributeError:
             pass  # Target is not a Model, no classify
-        return target, target.__class__.__name__
+        return target, target.__class__.__name__  # type: ignore
 
     def failsafe_deserialize(self, target_obj, data, content_type=None):
         """Ignores any errors encountered in deserialization,
         and falls back to not deserializing the object. Recommended
         for use in error deserialization, as we want to return the
         HttpResponseError to users, and not have them deal with
         a deserialization error.
 
         :param str target_obj: The target object type to deserialize to.
-        :param str/dict data: The response data to deseralize.
+        :param str/dict data: The response data to deserialize.
         :param str content_type: Swagger "produces" if available.
         """
         try:
             return self(target_obj, data, content_type=content_type)
         except:
             _LOGGER.debug(
                 "Ran into a deserialization error. Ignoring since this is failsafe deserialization", exc_info=True
@@ -1539,15 +1565,15 @@
             return RawDeserializer.deserialize_from_http_generics(raw_data.text(), raw_data.headers)
 
         # Assume this enough to recognize requests.Response without importing it.
         if hasattr(raw_data, "_content_consumed"):
             return RawDeserializer.deserialize_from_http_generics(raw_data.text, raw_data.headers)
 
         if isinstance(raw_data, (basestring, bytes)) or hasattr(raw_data, "read"):
-            return RawDeserializer.deserialize_from_text(raw_data, content_type)
+            return RawDeserializer.deserialize_from_text(raw_data, content_type)  # type: ignore
         return raw_data
 
     def _instantiate_model(self, response, attrs, additional_properties=None):
         """Instantiate a response model passing in deserialized args.
 
         :param response: The response model class.
         :param d_attrs: The deserialized response attributes.
@@ -1561,15 +1587,15 @@
                 response_obj = response(**kwargs)
                 for attr in readonly:
                     setattr(response_obj, attr, attrs.get(attr))
                 if additional_properties:
                     response_obj.additional_properties = additional_properties
                 return response_obj
             except TypeError as err:
-                msg = "Unable to deserialize {} into model {}. ".format(kwargs, response)
+                msg = "Unable to deserialize {} into model {}. ".format(kwargs, response)  # type: ignore
                 raise DeserializationError(msg + str(err))
         else:
             try:
                 for attr, value in attrs.items():
                     setattr(response, attr, value)
                 return response
             except Exception as exp:
@@ -1743,15 +1769,15 @@
         # We might be here because we have an enum modeled as string,
         # and we try to deserialize a partial dict with enum inside
         if isinstance(data, Enum):
             return data
 
         # Consider this is real string
         try:
-            if isinstance(data, unicode):
+            if isinstance(data, unicode):  # type: ignore
                 return data
         except NameError:
             return str(data)
         else:
             return str(data)
 
     @staticmethod
@@ -1794,58 +1820,58 @@
 
         :param str attr: response string to be deserialized.
         :rtype: bytearray
         :raises: TypeError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        return bytearray(b64decode(attr))
+        return bytearray(b64decode(attr))  # type: ignore
 
     @staticmethod
     def deserialize_base64(attr):
         """Deserialize base64 encoded string into string.
 
         :param str attr: response string to be deserialized.
         :rtype: bytearray
         :raises: TypeError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        padding = "=" * (3 - (len(attr) + 3) % 4)
-        attr = attr + padding
+        padding = "=" * (3 - (len(attr) + 3) % 4)  # type: ignore
+        attr = attr + padding  # type: ignore
         encoded = attr.replace("-", "+").replace("_", "/")
         return b64decode(encoded)
 
     @staticmethod
     def deserialize_decimal(attr):
         """Deserialize string into Decimal object.
 
         :param str attr: response string to be deserialized.
         :rtype: Decimal
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            return decimal.Decimal(attr)
+            return decimal.Decimal(attr)  # type: ignore
         except decimal.DecimalException as err:
             msg = "Invalid decimal {}".format(attr)
             raise_with_traceback(DeserializationError, msg, err)
 
     @staticmethod
     def deserialize_long(attr):
         """Deserialize string into long (Py2) or int (Py3).
 
         :param str attr: response string to be deserialized.
         :rtype: long or int
         :raises: ValueError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        return _long_type(attr)
+        return _long_type(attr)  # type: ignore
 
     @staticmethod
     def deserialize_duration(attr):
         """Deserialize ISO-8601 formatted string into TimeDelta object.
 
         :param str attr: response string to be deserialized.
         :rtype: TimeDelta
@@ -1867,45 +1893,45 @@
 
         :param str attr: response string to be deserialized.
         :rtype: Date
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        if re.search(r"[^\W\d_]", attr, re.I + re.U):
+        if re.search(r"[^\W\d_]", attr, re.I + re.U):  # type: ignore
             raise DeserializationError("Date must have only digits and -. Received: %s" % attr)
         # This must NOT use defaultmonth/defaultday. Using None ensure this raises an exception.
         return isodate.parse_date(attr, defaultmonth=None, defaultday=None)
 
     @staticmethod
     def deserialize_time(attr):
         """Deserialize ISO-8601 formatted string into time object.
 
         :param str attr: response string to be deserialized.
         :rtype: datetime.time
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        if re.search(r"[^\W\d_]", attr, re.I + re.U):
+        if re.search(r"[^\W\d_]", attr, re.I + re.U):  # type: ignore
             raise DeserializationError("Date must have only digits and -. Received: %s" % attr)
         return isodate.parse_time(attr)
 
     @staticmethod
     def deserialize_rfc(attr):
         """Deserialize RFC-1123 formatted string into Datetime object.
 
         :param str attr: response string to be deserialized.
         :rtype: Datetime
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            parsed_date = email.utils.parsedate_tz(attr)
+            parsed_date = email.utils.parsedate_tz(attr)  # type: ignore
             date_obj = datetime.datetime(
                 *parsed_date[:6], tzinfo=_FixedOffset(datetime.timedelta(minutes=(parsed_date[9] or 0) / 60))
             )
             if not date_obj.tzinfo:
                 date_obj = date_obj.astimezone(tz=TZ_UTC)
         except ValueError as err:
             msg = "Cannot deserialize to rfc datetime object."
@@ -1920,15 +1946,15 @@
         :param str attr: response string to be deserialized.
         :rtype: Datetime
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            attr = attr.upper()
+            attr = attr.upper()  # type: ignore
             match = Deserializer.valid_date.match(attr)
             if not match:
                 raise ValueError("Invalid datetime string: " + attr)
 
             check_decimal = attr.split(".")
             if len(check_decimal) > 1:
                 decimal_str = ""
@@ -1956,15 +1982,15 @@
         This is represented as seconds.
 
         :param int attr: Object to be serialized.
         :rtype: Datetime
         :raises: DeserializationError if format invalid
         """
         if isinstance(attr, ET.Element):
-            attr = int(attr.text)
+            attr = int(attr.text)  # type: ignore
         try:
             date_obj = datetime.datetime.fromtimestamp(attr, TZ_UTC)
         except ValueError as err:
             msg = "Cannot deserialize to unix datetime object."
             raise_with_traceback(DeserializationError, msg, err)
         else:
             return date_obj
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_configuration.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_configuration.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,58 +1,54 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
-import sys
 from typing import Any
 
 from azure.core.configuration import Configuration
 from azure.core.pipeline import policies
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
-
 VERSION = "unknown"
 
 
-class SearchServiceClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
-    """Configuration for SearchServiceClient.
+class SearchIndexClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
+    """Configuration for SearchIndexClient.
 
     Note that all parameters used to create this instance are saved as instance
     attributes.
 
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :param index_name: The name of the index. Required.
+    :type index_name: str
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
-    def __init__(self, endpoint: str, **kwargs: Any) -> None:
-        super(SearchServiceClientConfiguration, self).__init__(**kwargs)
-        api_version = kwargs.pop("api_version", "2021-04-30-Preview")  # type: Literal["2021-04-30-Preview"]
+    def __init__(self, endpoint: str, index_name: str, **kwargs: Any) -> None:
+        super(SearchIndexClientConfiguration, self).__init__(**kwargs)
+        api_version: str = kwargs.pop("api_version", "2023-07-01-Preview")
 
         if endpoint is None:
             raise ValueError("Parameter 'endpoint' must not be None.")
+        if index_name is None:
+            raise ValueError("Parameter 'index_name' must not be None.")
 
         self.endpoint = endpoint
+        self.index_name = index_name
         self.api_version = api_version
-        kwargs.setdefault("sdk_moniker", "searchserviceclient/{}".format(VERSION))
+        kwargs.setdefault("sdk_moniker", "searchindexclient/{}".format(VERSION))
         self._configure(**kwargs)
 
-    def _configure(
-        self, **kwargs  # type: Any
-    ):
-        # type: (...) -> None
+    def _configure(self, **kwargs: Any) -> None:
         self.user_agent_policy = kwargs.get("user_agent_policy") or policies.UserAgentPolicy(**kwargs)
         self.headers_policy = kwargs.get("headers_policy") or policies.HeadersPolicy(**kwargs)
         self.proxy_policy = kwargs.get("proxy_policy") or policies.ProxyPolicy(**kwargs)
         self.logging_policy = kwargs.get("logging_policy") or policies.NetworkTraceLoggingPolicy(**kwargs)
         self.http_logging_policy = kwargs.get("http_logging_policy") or policies.HttpLoggingPolicy(**kwargs)
-        self.retry_policy = kwargs.get("retry_policy") or policies.RetryPolicy(**kwargs)
+        self.retry_policy = kwargs.get("retry_policy") or policies.AsyncRetryPolicy(**kwargs)
         self.custom_hook_policy = kwargs.get("custom_hook_policy") or policies.CustomHookPolicy(**kwargs)
-        self.redirect_policy = kwargs.get("redirect_policy") or policies.RedirectPolicy(**kwargs)
+        self.redirect_policy = kwargs.get("redirect_policy") or policies.AsyncRedirectPolicy(**kwargs)
         self.authentication_policy = kwargs.get("authentication_policy")
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/_vendor.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_vendor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from abc import ABC
-from typing import TYPE_CHECKING
+from typing import List, TYPE_CHECKING, cast
 
 from azure.core.pipeline.transport import HttpRequest
 
 from ._configuration import SearchServiceClientConfiguration
 
 if TYPE_CHECKING:
     # pylint: disable=unused-import,ungrouped-imports
@@ -27,15 +27,16 @@
 
 def _format_url_section(template, **kwargs):
     components = template.split("/")
     while components:
         try:
             return template.format(**kwargs)
         except KeyError as key:
-            formatted_components = template.split("/")
+            # Need the cast, as for some reasons "split" is typed as list[str | Any]
+            formatted_components = cast(List[str], template.split("/"))
             components = [c for c in formatted_components if "{}".format(key.args[0]) not in c]
             template = "/".join(components)
 
 
 class SearchServiceClientMixinABC(ABC):
     """DO NOT use this class. It is for internal typing use only."""
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -21,18 +21,14 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
@@ -45,27 +41,25 @@
     if_none_match: Optional[str] = None,
     skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/datasources('{dataSourceName}')")
     path_format_arguments = {
         "dataSourceName": _SERIALIZER.url("data_source_name", data_source_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
     if skip_indexer_reset_requirement_for_cache is not None:
         _params["ignoreResetRequirements"] = _SERIALIZER.query(
             "skip_indexer_reset_requirement_for_cache", skip_indexer_reset_requirement_for_cache, "bool"
         )
@@ -92,26 +86,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/datasources('{dataSourceName}')")
     path_format_arguments = {
         "dataSourceName": _SERIALIZER.url("data_source_name", data_source_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -126,26 +118,24 @@
 
 def build_get_request(
     data_source_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/datasources('{dataSourceName}')")
     path_format_arguments = {
         "dataSourceName": _SERIALIZER.url("data_source_name", data_source_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -156,17 +146,15 @@
 
 def build_list_request(
     *, select: Optional[str] = None, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/datasources")
 
     # Construct parameters
     if select is not None:
@@ -181,18 +169,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/datasources")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -238,14 +224,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param data_source: The definition of the datasource to create or update. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource
@@ -281,14 +270,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param data_source: The definition of the datasource to create or update. Required.
         :type data_source: IO
@@ -322,21 +314,24 @@
         if_none_match: Optional[str] = None,
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param data_source: The definition of the datasource to create or update. Is either a model
-         type or a IO type. Required.
+        :param data_source: The definition of the datasource to create or update. Is either a
+         SearchIndexerDataSource type or a IO type. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -360,27 +355,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(data_source, (IO, bytes)):
+        if isinstance(data_source, (IOBase, bytes)):
             _content = data_source
         else:
             _json = self._serialize.body(data_source, "SearchIndexerDataSource")
 
         request = build_create_or_update_request(
             data_source_name=data_source_name,
             prefer=prefer,
@@ -396,18 +389,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -416,31 +410,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         data_source_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Data-Source
+
         :param data_source_name: The name of the datasource to delete. Required.
         :type data_source_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -459,18 +456,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             data_source_name=data_source_name,
@@ -482,38 +477,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    delete.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace
     def get(
         self, data_source_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Retrieves a datasource definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Data-Source
+
         :param data_source_name: The name of the datasource to retrieve. Required.
         :type data_source_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerDataSource or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerDataSource
@@ -526,18 +525,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             data_source_name=data_source_name,
@@ -547,18 +544,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -567,22 +565,25 @@
         deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    get.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListDataSourcesResult:
         """Lists all datasources available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Data-Sources
+
         :param select: Selects which top-level properties of the data sources to retrieve. Specified as
          a comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -597,18 +598,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListDataSourcesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListDataSourcesResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -618,18 +617,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -638,27 +638,30 @@
         deserialized = self._deserialize("ListDataSourcesResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/datasources"}  # type: ignore
+    list.metadata = {"url": "/datasources"}
 
     @overload
     def create(
         self,
         data_source: _models.SearchIndexerDataSource,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
         :param data_source: The definition of the datasource to create. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -675,14 +678,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
         :param data_source: The definition of the datasource to create. Required.
         :type data_source: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -697,16 +703,19 @@
         self,
         data_source: Union[_models.SearchIndexerDataSource, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
-        :param data_source: The definition of the datasource to create. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
+        :param data_source: The definition of the datasource to create. Is either a
+         SearchIndexerDataSource type or a IO type. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -721,27 +730,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(data_source, (IO, bytes)):
+        if isinstance(data_source, (IOBase, bytes)):
             _content = data_source
         else:
             _json = self._serialize.body(data_source, "SearchIndexerDataSource")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -752,18 +759,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -772,8 +780,8 @@
         deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/datasources"}  # type: ignore
+    create.metadata = {"url": "/datasources"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._data_sources_operations import DataSourcesOperations
 from ._indexers_operations import IndexersOperations
 from ._skillsets_operations import SkillsetsOperations
 from ._synonym_maps_operations import SynonymMapsOperations
 from ._indexes_operations import IndexesOperations
 from ._aliases_operations import AliasesOperations
 from ._search_service_client_operations import SearchServiceClientOperationsMixin
 
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "DataSourcesOperations",
     "IndexersOperations",
     "SkillsetsOperations",
     "SynonymMapsOperations",
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_skillsets_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -21,18 +21,14 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
@@ -46,27 +42,25 @@
     skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
     disable_cache_reprocessing_change_detection: Optional[bool] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets('{skillsetName}')")
     path_format_arguments = {
         "skillsetName": _SERIALIZER.url("skillset_name", skillset_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
     if skip_indexer_reset_requirement_for_cache is not None:
         _params["ignoreResetRequirements"] = _SERIALIZER.query(
             "skip_indexer_reset_requirement_for_cache", skip_indexer_reset_requirement_for_cache, "bool"
         )
@@ -97,26 +91,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets('{skillsetName}')")
     path_format_arguments = {
         "skillsetName": _SERIALIZER.url("skillset_name", skillset_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -131,26 +123,24 @@
 
 def build_get_request(
     skillset_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets('{skillsetName}')")
     path_format_arguments = {
         "skillsetName": _SERIALIZER.url("skillset_name", skillset_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -161,17 +151,15 @@
 
 def build_list_request(
     *, select: Optional[str] = None, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets")
 
     # Construct parameters
     if select is not None:
@@ -186,18 +174,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -214,27 +200,25 @@
 
 def build_reset_skills_request(
     skillset_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/skillsets('{skillsetName}')/search.resetskills")
     path_format_arguments = {
         "skillsetName": _SERIALIZER.url("skillset_name", skillset_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -277,14 +261,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
          service. Required.
@@ -325,14 +312,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
          service. Required.
@@ -371,21 +361,24 @@
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         disable_cache_reprocessing_change_detection: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
-         service. Is either a model type or a IO type. Required.
+         service. Is either a SearchIndexerSkillset type or a IO type. Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -412,27 +405,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skillset, (IO, bytes)):
+        if isinstance(skillset, (IOBase, bytes)):
             _content = skillset
         else:
             _json = self._serialize.body(skillset, "SearchIndexerSkillset")
 
         request = build_create_or_update_request(
             skillset_name=skillset_name,
             prefer=prefer,
@@ -449,18 +440,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -469,31 +461,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         skillset_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/delete-skillset
+
         :param skillset_name: The name of the skillset to delete. Required.
         :type skillset_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -512,18 +507,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             skillset_name=skillset_name,
@@ -535,38 +528,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    delete.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace
     def get(
         self, skillset_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Retrieves a skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/get-skillset
+
         :param skillset_name: The name of the skillset to retrieve. Required.
         :type skillset_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerSkillset or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerSkillset
@@ -579,18 +576,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             skillset_name=skillset_name,
@@ -600,18 +595,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -620,22 +616,25 @@
         deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    get.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListSkillsetsResult:
         """List all skillsets in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/list-skillset
+
         :param select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -650,18 +649,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListSkillsetsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListSkillsetsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -671,18 +668,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -691,27 +689,30 @@
         deserialized = self._deserialize("ListSkillsetsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/skillsets"}  # type: ignore
+    list.metadata = {"url": "/skillsets"}
 
     @overload
     def create(
         self,
         skillset: _models.SearchIndexerSkillset,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service.
          Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
@@ -729,14 +730,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service.
          Required.
         :type skillset: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
@@ -752,16 +756,19 @@
         self,
         skillset: Union[_models.SearchIndexerSkillset, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service. Is
-         either a model type or a IO type. Required.
+         either a SearchIndexerSkillset type or a IO type. Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -776,27 +783,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skillset, (IO, bytes)):
+        if isinstance(skillset, (IOBase, bytes)):
             _content = skillset
         else:
             _json = self._serialize.body(skillset, "SearchIndexerSkillset")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -807,18 +812,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -827,28 +833,31 @@
         deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/skillsets"}  # type: ignore
+    create.metadata = {"url": "/skillsets"}
 
     @overload
     def reset_skills(  # pylint: disable=inconsistent-return-statements
         self,
         skillset_name: str,
         skill_names: _models.SkillNames,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
         :param skill_names: The names of skills to reset. Required.
         :type skill_names: ~search_service_client.models.SkillNames
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
@@ -868,14 +877,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
         :param skill_names: The names of skills to reset. Required.
         :type skill_names: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
@@ -893,17 +905,20 @@
         skillset_name: str,
         skill_names: Union[_models.SkillNames, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
-        :param skill_names: The names of skills to reset. Is either a model type or a IO type.
+        :param skill_names: The names of skills to reset. Is either a SkillNames type or a IO type.
          Required.
         :type skill_names: ~search_service_client.models.SkillNames or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -919,27 +934,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skill_names, (IO, bytes)):
+        if isinstance(skill_names, (IOBase, bytes)):
             _content = skill_names
         else:
             _json = self._serialize.body(skill_names, "SkillNames")
 
         request = build_reset_skills_request(
             skillset_name=skillset_name,
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -951,24 +964,25 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset_skills.metadata = {"url": "/skillsets('{skillsetName}')/search.resetskills"}  # type: ignore
+    reset_skills.metadata = {"url": "/skillsets('{skillsetName}')/search.resetskills"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_aliases_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_aliases_operations.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Iterable, Optional, TypeVar, Union, overload
 import urllib.parse
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
@@ -23,33 +23,27 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/aliases")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -64,17 +58,15 @@
     return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_list_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/aliases")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -95,27 +87,25 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/aliases('{aliasName}')")
     path_format_arguments = {
         "aliasName": _SERIALIZER.url("alias_name", alias_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -138,26 +128,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/aliases('{aliasName}')")
     path_format_arguments = {
         "aliasName": _SERIALIZER.url("alias_name", alias_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -170,26 +158,24 @@
     return HttpRequest(method="DELETE", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_get_request(alias_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/aliases('{aliasName}')")
     path_format_arguments = {
         "aliasName": _SERIALIZER.url("alias_name", alias_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -224,14 +210,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
         :param alias: The definition of the alias to create. Required.
         :type alias: ~search_service_client.models.SearchAlias
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -248,14 +237,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
         :param alias: The definition of the alias to create. Required.
         :type alias: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -270,15 +262,18 @@
         self,
         alias: Union[_models.SearchAlias, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
-        :param alias: The definition of the alias to create. Is either a model type or a IO type.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
+        :param alias: The definition of the alias to create. Is either a SearchAlias type or a IO type.
          Required.
         :type alias: ~search_service_client.models.SearchAlias or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -294,27 +289,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(alias, (IO, bytes)):
+        if isinstance(alias, (IOBase, bytes)):
             _content = alias
         else:
             _json = self._serialize.body(alias, "SearchAlias")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -325,18 +318,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -345,36 +339,37 @@
         deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/aliases"}  # type: ignore
+    create.metadata = {"url": "/aliases"}
 
     @distributed_trace
     def list(
         self, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> Iterable["_models.SearchAlias"]:
         """Lists all aliases available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Aliases
+
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: An iterator like instance of either SearchAlias or the result of cls(response)
         :rtype: ~azure.core.paging.ItemPaged[~search_service_client.models.SearchAlias]
         :raises ~azure.core.exceptions.HttpResponseError:
         """
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListAliasesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListAliasesResult] = kwargs.pop("cls", None)
 
         error_map = {
             401: ClientAuthenticationError,
             404: ResourceNotFoundError,
             409: ResourceExistsError,
             304: ResourceNotModifiedError,
         }
@@ -395,15 +390,15 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
 
             else:
                 # make call to next link with the client's api-version
                 _parsed_next_link = urllib.parse.urlparse(next_link)
                 _next_request_params = case_insensitive_dict(
                     {
                         key: [urllib.parse.quote(v) for v in value]
@@ -416,43 +411,44 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
                 request.method = "GET"
             return request
 
         def extract_data(pipeline_response):
             deserialized = self._deserialize("ListAliasesResult", pipeline_response)
             list_of_elem = deserialized.aliases
             if cls:
-                list_of_elem = cls(list_of_elem)
+                list_of_elem = cls(list_of_elem)  # type: ignore
             return None, iter(list_of_elem)
 
         def get_next(next_link=None):
             request = prepare_request(next_link)
 
-            pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-                request, stream=False, **kwargs
+            _stream = False
+            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+                request, stream=_stream, **kwargs
             )
             response = pipeline_response.http_response
 
             if response.status_code not in [200]:
                 map_error(status_code=response.status_code, response=response, error_map=error_map)
                 error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
                 raise HttpResponseError(response=response, model=error)
 
             return pipeline_response
 
         return ItemPaged(get_next, extract_data)
 
-    list.metadata = {"url": "/aliases"}  # type: ignore
+    list.metadata = {"url": "/aliases"}
 
     @overload
     def create_or_update(
         self,
         alias_name: str,
         prefer: Union[str, _models.Enum0],
         alias: _models.SearchAlias,
@@ -461,14 +457,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param alias: The definition of the alias to create or update. Required.
         :type alias: ~search_service_client.models.SearchAlias
@@ -500,14 +499,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param alias: The definition of the alias to create or update. Required.
         :type alias: IO
@@ -537,21 +539,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param alias: The definition of the alias to create or update. Is either a model type or a IO
-         type. Required.
+        :param alias: The definition of the alias to create or update. Is either a SearchAlias type or
+         a IO type. Required.
         :type alias: ~search_service_client.models.SearchAlias or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -572,27 +577,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(alias, (IO, bytes)):
+        if isinstance(alias, (IOBase, bytes)):
             _content = alias
         else:
             _json = self._serialize.body(alias, "SearchAlias")
 
         request = build_create_or_update_request(
             alias_name=alias_name,
             prefer=prefer,
@@ -607,18 +610,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -627,32 +631,35 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/aliases('{aliasName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         alias_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a search alias and its associated mapping to an index. This operation is permanent,
         with no recovery option. The mapped index is untouched by this operation.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Alias
+
         :param alias_name: The name of the alias to delete. Required.
         :type alias_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -671,18 +678,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             alias_name=alias_name,
@@ -694,38 +699,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    delete.metadata = {"url": "/aliases('{aliasName}')"}
 
     @distributed_trace
     def get(
         self, alias_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchAlias:
         """Retrieves an alias definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Alias
+
         :param alias_name: The name of the alias to retrieve. Required.
         :type alias_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchAlias or the result of cls(response)
         :rtype: ~search_service_client.models.SearchAlias
@@ -738,18 +747,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             alias_name=alias_name,
@@ -759,18 +766,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -779,8 +787,8 @@
         deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    get.metadata = {"url": "/aliases('{aliasName}')"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexes_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexes_operations.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Iterable, Optional, TypeVar, Union, overload
 import urllib.parse
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
@@ -23,33 +23,27 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -66,17 +60,15 @@
 
 def build_list_request(
     *, select: Optional[str] = None, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes")
 
     # Construct parameters
     if select is not None:
@@ -100,27 +92,25 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes('{indexName}')")
     path_format_arguments = {
         "indexName": _SERIALIZER.url("index_name", index_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     if allow_index_downtime is not None:
         _params["allowIndexDowntime"] = _SERIALIZER.query("allow_index_downtime", allow_index_downtime, "bool")
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
@@ -145,26 +135,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes('{indexName}')")
     path_format_arguments = {
         "indexName": _SERIALIZER.url("index_name", index_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -177,26 +165,24 @@
     return HttpRequest(method="DELETE", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_get_request(index_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes('{indexName}')")
     path_format_arguments = {
         "indexName": _SERIALIZER.url("index_name", index_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -207,26 +193,24 @@
 
 def build_get_statistics_request(
     index_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes('{indexName}')/search.stats")
     path_format_arguments = {
         "indexName": _SERIALIZER.url("index_name", index_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -237,27 +221,25 @@
 
 def build_analyze_request(
     index_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexes('{indexName}')/search.analyze")
     path_format_arguments = {
         "indexName": _SERIALIZER.url("index_name", index_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -294,14 +276,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
         :param index: The definition of the index to create. Required.
         :type index: ~search_service_client.models.SearchIndex
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -318,14 +303,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
         :param index: The definition of the index to create. Required.
         :type index: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -340,15 +328,18 @@
         self,
         index: Union[_models.SearchIndex, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
-        :param index: The definition of the index to create. Is either a model type or a IO type.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
+        :param index: The definition of the index to create. Is either a SearchIndex type or a IO type.
          Required.
         :type index: ~search_service_client.models.SearchIndex or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -364,27 +355,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(index, (IO, bytes)):
+        if isinstance(index, (IOBase, bytes)):
             _content = index
         else:
             _json = self._serialize.body(index, "SearchIndex")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -395,18 +384,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -415,40 +405,41 @@
         deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/indexes"}  # type: ignore
+    create.metadata = {"url": "/indexes"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> Iterable["_models.SearchIndex"]:
         """Lists all indexes available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Indexes
+
         :param select: Selects which top-level properties of the index definitions to retrieve.
          Specified as a comma-separated list of JSON property names, or '*' for all properties. The
          default is all properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: An iterator like instance of either SearchIndex or the result of cls(response)
         :rtype: ~azure.core.paging.ItemPaged[~search_service_client.models.SearchIndex]
         :raises ~azure.core.exceptions.HttpResponseError:
         """
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListIndexesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListIndexesResult] = kwargs.pop("cls", None)
 
         error_map = {
             401: ClientAuthenticationError,
             404: ResourceNotFoundError,
             409: ResourceExistsError,
             304: ResourceNotModifiedError,
         }
@@ -470,15 +461,15 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
 
             else:
                 # make call to next link with the client's api-version
                 _parsed_next_link = urllib.parse.urlparse(next_link)
                 _next_request_params = case_insensitive_dict(
                     {
                         key: [urllib.parse.quote(v) for v in value]
@@ -491,43 +482,44 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
                 request.method = "GET"
             return request
 
         def extract_data(pipeline_response):
             deserialized = self._deserialize("ListIndexesResult", pipeline_response)
             list_of_elem = deserialized.indexes
             if cls:
-                list_of_elem = cls(list_of_elem)
+                list_of_elem = cls(list_of_elem)  # type: ignore
             return None, iter(list_of_elem)
 
         def get_next(next_link=None):
             request = prepare_request(next_link)
 
-            pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-                request, stream=False, **kwargs
+            _stream = False
+            pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+                request, stream=_stream, **kwargs
             )
             response = pipeline_response.http_response
 
             if response.status_code not in [200]:
                 map_error(status_code=response.status_code, response=response, error_map=error_map)
                 error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
                 raise HttpResponseError(response=response, model=error)
 
             return pipeline_response
 
         return ItemPaged(get_next, extract_data)
 
-    list.metadata = {"url": "/indexes"}  # type: ignore
+    list.metadata = {"url": "/indexes"}
 
     @overload
     def create_or_update(
         self,
         index_name: str,
         prefer: Union[str, _models.Enum0],
         index: _models.SearchIndex,
@@ -537,14 +529,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param index: The definition of the index to create or update. Required.
         :type index: ~search_service_client.models.SearchIndex
@@ -583,14 +578,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param index: The definition of the index to create or update. Required.
         :type index: IO
@@ -627,21 +625,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param index: The definition of the index to create or update. Is either a model type or a IO
-         type. Required.
+        :param index: The definition of the index to create or update. Is either a SearchIndex type or
+         a IO type. Required.
         :type index: ~search_service_client.models.SearchIndex or IO
         :param allow_index_downtime: Allows new analyzers, tokenizers, token filters, or char filters
          to be added to an index by taking the index offline for at least a few seconds. This
          temporarily causes indexing and query requests to fail. Performance and write availability of
          the index can be impaired for several minutes after the index is updated, or longer for very
          large indexes. Default value is None.
         :type allow_index_downtime: bool
@@ -668,27 +669,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(index, (IO, bytes)):
+        if isinstance(index, (IOBase, bytes)):
             _content = index
         else:
             _json = self._serialize.body(index, "SearchIndex")
 
         request = build_create_or_update_request(
             index_name=index_name,
             prefer=prefer,
@@ -704,18 +703,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -724,33 +724,36 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         index_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a search index and all the documents it contains. This operation is permanent, with no
         recovery option. Make sure you have a master copy of your index definition, data ingestion
         code, and a backup of the primary data source in case you need to re-build the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Index
+
         :param index_name: The name of the index to delete. Required.
         :type index_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -769,18 +772,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             index_name=index_name,
@@ -792,38 +793,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    delete.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace
     def get(
         self, index_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndex:
         """Retrieves an index definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Index
+
         :param index_name: The name of the index to retrieve. Required.
         :type index_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndex or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndex
@@ -836,18 +841,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             index_name=index_name,
@@ -857,18 +860,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -877,22 +881,25 @@
         deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    get.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace
     def get_statistics(
         self, index_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.GetIndexStatisticsResult:
         """Returns statistics for the given index, including a document count and storage usage.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Index-Statistics
+
         :param index_name: The name of the index for which to retrieve statistics. Required.
         :type index_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: GetIndexStatisticsResult or the result of cls(response)
         :rtype: ~search_service_client.models.GetIndexStatisticsResult
@@ -905,18 +912,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.GetIndexStatisticsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.GetIndexStatisticsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_statistics_request(
             index_name=index_name,
@@ -926,18 +931,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -946,28 +952,31 @@
         deserialized = self._deserialize("GetIndexStatisticsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_statistics.metadata = {"url": "/indexes('{indexName}')/search.stats"}  # type: ignore
+    get_statistics.metadata = {"url": "/indexes('{indexName}')/search.stats"}
 
     @overload
     def analyze(
         self,
         index_name: str,
         request: _models.AnalyzeRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
         :param request: The text and analyzer or analysis components to test. Required.
         :type request: ~search_service_client.models.AnalyzeRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
@@ -987,14 +996,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
         :param request: The text and analyzer or analysis components to test. Required.
         :type request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
@@ -1012,18 +1024,21 @@
         index_name: str,
         request: Union[_models.AnalyzeRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
-        :param request: The text and analyzer or analysis components to test. Is either a model type or
-         a IO type. Required.
+        :param request: The text and analyzer or analysis components to test. Is either a
+         AnalyzeRequest type or a IO type. Required.
         :type request: ~search_service_client.models.AnalyzeRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1038,27 +1053,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AnalyzeResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.AnalyzeResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(request, (IO, bytes)):
+        if isinstance(request, (IOBase, bytes)):
             _content = request
         else:
             _json = self._serialize.body(request, "AnalyzeRequest")
 
         request = build_analyze_request(
             index_name=index_name,
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -1070,18 +1083,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1090,8 +1104,8 @@
         deserialized = self._deserialize("AnalyzeResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    analyze.metadata = {"url": "/indexes('{indexName}')/search.analyze"}  # type: ignore
+    analyze.metadata = {"url": "/indexes('{indexName}')/search.analyze"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_indexers_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_indexers_operations.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -21,43 +21,37 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
 def build_reset_request(
     indexer_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')/search.reset")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -68,27 +62,25 @@
 
 def build_reset_docs_request(
     indexer_name: str, *, overwrite: bool = False, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')/search.resetdocs")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     if overwrite is not None:
         _params["overwrite"] = _SERIALIZER.query("overwrite", overwrite, "bool")
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
@@ -101,26 +93,24 @@
     return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_run_request(indexer_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')/search.run")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -139,27 +129,25 @@
     skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
     disable_cache_reprocessing_change_detection: Optional[bool] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
     if skip_indexer_reset_requirement_for_cache is not None:
         _params["ignoreResetRequirements"] = _SERIALIZER.query(
             "skip_indexer_reset_requirement_for_cache", skip_indexer_reset_requirement_for_cache, "bool"
         )
@@ -190,26 +178,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -222,26 +208,24 @@
     return HttpRequest(method="DELETE", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_get_request(indexer_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -252,17 +236,15 @@
 
 def build_list_request(
     *, select: Optional[str] = None, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers")
 
     # Construct parameters
     if select is not None:
@@ -277,18 +259,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -305,26 +285,24 @@
 
 def build_get_status_request(
     indexer_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/indexers('{indexerName}')/search.status")
     path_format_arguments = {
         "indexerName": _SERIALIZER.url("indexer_name", indexer_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -354,14 +332,17 @@
 
     @distributed_trace
     def reset(  # pylint: disable=inconsistent-return-statements
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> None:
         """Resets the change tracking state associated with an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Reset-Indexer
+
         :param indexer_name: The name of the indexer to reset. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -374,18 +355,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_reset_request(
             indexer_name=indexer_name,
@@ -395,45 +374,49 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset.metadata = {"url": "/indexers('{indexerName}')/search.reset"}  # type: ignore
+    reset.metadata = {"url": "/indexers('{indexerName}')/search.reset"}
 
     @overload
     def reset_docs(  # pylint: disable=inconsistent-return-statements
         self,
         indexer_name: str,
         overwrite: bool = False,
         request_options: Optional[_models.RequestOptions] = None,
         keys_or_ids: Optional[_models.DocumentKeysOrIds] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
@@ -457,14 +440,17 @@
         keys_or_ids: Optional[IO] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
@@ -486,22 +472,25 @@
         overwrite: bool = False,
         request_options: Optional[_models.RequestOptions] = None,
         keys_or_ids: Optional[Union[_models.DocumentKeysOrIds, IO]] = None,
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
-        :param keys_or_ids: Is either a model type or a IO type. Default value is None.
+        :param keys_or_ids: Is either a DocumentKeysOrIds type or a IO type. Default value is None.
         :type keys_or_ids: ~search_service_client.models.DocumentKeysOrIds or IO
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -514,27 +503,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(keys_or_ids, (IO, bytes)):
+        if isinstance(keys_or_ids, (IOBase, bytes)):
             _content = keys_or_ids
         else:
             if keys_or_ids is not None:
                 _json = self._serialize.body(keys_or_ids, "DocumentKeysOrIds")
             else:
                 _json = None
 
@@ -550,38 +537,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset_docs.metadata = {"url": "/indexers('{indexerName}')/search.resetdocs"}  # type: ignore
+    reset_docs.metadata = {"url": "/indexers('{indexerName}')/search.resetdocs"}
 
     @distributed_trace
     def run(  # pylint: disable=inconsistent-return-statements
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> None:
         """Runs an indexer on-demand.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Run-Indexer
+
         :param indexer_name: The name of the indexer to run. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -594,18 +585,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_run_request(
             indexer_name=indexer_name,
@@ -615,31 +604,32 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [202]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    run.metadata = {"url": "/indexers('{indexerName}')/search.run"}  # type: ignore
+    run.metadata = {"url": "/indexers('{indexerName}')/search.run"}
 
     @overload
     def create_or_update(
         self,
         indexer_name: str,
         prefer: Union[str, _models.Enum0],
         indexer: _models.SearchIndexer,
@@ -650,14 +640,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param indexer: The definition of the indexer to create or update. Required.
         :type indexer: ~search_service_client.models.SearchIndexer
@@ -697,14 +690,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param indexer: The definition of the indexer to create or update. Required.
         :type indexer: IO
@@ -742,21 +738,24 @@
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         disable_cache_reprocessing_change_detection: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param indexer: The definition of the indexer to create or update. Is either a model type or a
-         IO type. Required.
+        :param indexer: The definition of the indexer to create or update. Is either a SearchIndexer
+         type or a IO type. Required.
         :type indexer: ~search_service_client.models.SearchIndexer or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -783,27 +782,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(indexer, (IO, bytes)):
+        if isinstance(indexer, (IOBase, bytes)):
             _content = indexer
         else:
             _json = self._serialize.body(indexer, "SearchIndexer")
 
         request = build_create_or_update_request(
             indexer_name=indexer_name,
             prefer=prefer,
@@ -820,18 +817,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -840,31 +838,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         indexer_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Indexer
+
         :param indexer_name: The name of the indexer to delete. Required.
         :type indexer_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -883,18 +884,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             indexer_name=indexer_name,
@@ -906,38 +905,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    delete.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace
     def get(
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexer:
         """Retrieves an indexer definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Indexer
+
         :param indexer_name: The name of the indexer to retrieve. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexer or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexer
@@ -950,18 +953,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             indexer_name=indexer_name,
@@ -971,18 +972,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -991,22 +993,25 @@
         deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    get.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListIndexersResult:
         """Lists all indexers available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Indexers
+
         :param select: Selects which top-level properties of the indexers to retrieve. Specified as a
          comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1021,18 +1026,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListIndexersResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListIndexersResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -1042,18 +1045,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1062,27 +1066,30 @@
         deserialized = self._deserialize("ListIndexersResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/indexers"}  # type: ignore
+    list.metadata = {"url": "/indexers"}
 
     @overload
     def create(
         self,
         indexer: _models.SearchIndexer,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer: The definition of the indexer to create. Required.
         :type indexer: ~search_service_client.models.SearchIndexer
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1099,14 +1106,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer: The definition of the indexer to create. Required.
         :type indexer: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1121,16 +1131,19 @@
         self,
         indexer: Union[_models.SearchIndexer, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
-        :param indexer: The definition of the indexer to create. Is either a model type or a IO type.
-         Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
+        :param indexer: The definition of the indexer to create. Is either a SearchIndexer type or a IO
+         type. Required.
         :type indexer: ~search_service_client.models.SearchIndexer or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1145,27 +1158,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(indexer, (IO, bytes)):
+        if isinstance(indexer, (IOBase, bytes)):
             _content = indexer
         else:
             _json = self._serialize.body(indexer, "SearchIndexer")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -1176,18 +1187,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1196,22 +1208,25 @@
         deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/indexers"}  # type: ignore
+    create.metadata = {"url": "/indexers"}
 
     @distributed_trace
     def get_status(
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerStatus:
         """Returns the current status and execution history of an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Indexer-Status
+
         :param indexer_name: The name of the indexer for which to retrieve status. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerStatus or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerStatus
@@ -1224,18 +1239,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerStatus]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerStatus] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_status_request(
             indexer_name=indexer_name,
@@ -1245,18 +1258,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1265,8 +1279,8 @@
         deserialized = self._deserialize("SearchIndexerStatus", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_status.metadata = {"url": "/indexers('{indexerName}')/search.status"}  # type: ignore
+    get_status.metadata = {"url": "/indexers('{indexerName}')/search.status"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_search_service_client_operations.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
 from typing import Any, Callable, Dict, Optional, TypeVar
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -21,32 +20,26 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
 def build_get_service_statistics_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/servicestats")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -80,18 +73,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ServiceStatistics]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ServiceStatistics] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_service_statistics_request(
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -100,18 +91,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -120,8 +112,8 @@
         deserialized = self._deserialize("ServiceStatistics", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_service_statistics.metadata = {"url": "/servicestats"}  # type: ignore
+    get_service_statistics.metadata = {"url": "/servicestats"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_synonym_maps_operations.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -21,18 +21,14 @@
 from azure.core.tracing.decorator import distributed_trace
 from azure.core.utils import case_insensitive_dict
 
 from .. import models as _models
 from .._serialization import Serializer
 from .._vendor import SearchServiceClientMixinABC, _convert_request, _format_url_section
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
@@ -44,27 +40,25 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/synonymmaps('{synonymMapName}')")
     path_format_arguments = {
         "synonymMapName": _SERIALIZER.url("synonym_map_name", synonym_map_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -87,26 +81,24 @@
     if_match: Optional[str] = None,
     if_none_match: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/synonymmaps('{synonymMapName}')")
     path_format_arguments = {
         "synonymMapName": _SERIALIZER.url("synonym_map_name", synonym_map_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -121,26 +113,24 @@
 
 def build_get_request(
     synonym_map_name: str, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/synonymmaps('{synonymMapName}')")
     path_format_arguments = {
         "synonymMapName": _SERIALIZER.url("synonym_map_name", synonym_map_name, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
     if x_ms_client_request_id is not None:
         _headers["x-ms-client-request-id"] = _SERIALIZER.header("x_ms_client_request_id", x_ms_client_request_id, "str")
@@ -151,17 +141,15 @@
 
 def build_list_request(
     *, select: Optional[str] = None, x_ms_client_request_id: Optional[str] = None, **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/synonymmaps")
 
     # Construct parameters
     if select is not None:
@@ -176,18 +164,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_create_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/synonymmaps")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -232,14 +218,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param synonym_map: The definition of the synonym map to create or update. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap
@@ -271,14 +260,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param synonym_map: The definition of the synonym map to create or update. Required.
         :type synonym_map: IO
@@ -308,21 +300,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param synonym_map: The definition of the synonym map to create or update. Is either a model
-         type or a IO type. Required.
+        :param synonym_map: The definition of the synonym map to create or update. Is either a
+         SynonymMap type or a IO type. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -343,27 +338,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(synonym_map, (IO, bytes)):
+        if isinstance(synonym_map, (IOBase, bytes)):
             _content = synonym_map
         else:
             _json = self._serialize.body(synonym_map, "SynonymMap")
 
         request = build_create_or_update_request(
             synonym_map_name=synonym_map_name,
             prefer=prefer,
@@ -378,18 +371,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -398,31 +392,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace
     def delete(  # pylint: disable=inconsistent-return-statements
         self,
         synonym_map_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to delete. Required.
         :type synonym_map_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -441,18 +438,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             synonym_map_name=synonym_map_name,
@@ -464,38 +459,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    delete.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace
     def get(
         self, synonym_map_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SynonymMap:
         """Retrieves a synonym map definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to retrieve. Required.
         :type synonym_map_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SynonymMap or the result of cls(response)
         :rtype: ~search_service_client.models.SynonymMap
@@ -508,18 +507,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             synonym_map_name=synonym_map_name,
@@ -529,18 +526,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -549,22 +547,25 @@
         deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    get.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListSynonymMapsResult:
         """Lists all synonym maps available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Synonym-Maps
+
         :param select: Selects which top-level properties of the synonym maps to retrieve. Specified as
          a comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -579,18 +580,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListSynonymMapsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListSynonymMapsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -600,18 +599,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -620,27 +620,30 @@
         deserialized = self._deserialize("ListSynonymMapsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/synonymmaps"}  # type: ignore
+    list.metadata = {"url": "/synonymmaps"}
 
     @overload
     def create(
         self,
         synonym_map: _models.SynonymMap,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
         :param synonym_map: The definition of the synonym map to create. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -657,14 +660,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
         :param synonym_map: The definition of the synonym map to create. Required.
         :type synonym_map: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -679,16 +685,19 @@
         self,
         synonym_map: Union[_models.SynonymMap, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
-        :param synonym_map: The definition of the synonym map to create. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
+        :param synonym_map: The definition of the synonym map to create. Is either a SynonymMap type or
+         a IO type. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -703,27 +712,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(synonym_map, (IO, bytes)):
+        if isinstance(synonym_map, (IOBase, bytes)):
             _content = synonym_map
         else:
             _json = self._serialize.body(synonym_map, "SynonymMap")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -734,18 +741,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -754,8 +762,8 @@
         deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/synonymmaps"}  # type: ignore
+    create.metadata = {"url": "/synonymmaps"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._search_service_client import SearchServiceClient
 
 try:
     from ._patch import __all__ as _patch_all
-    from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+    from ._patch import *  # pylint: disable=unused-wildcard-import
 except ImportError:
     _patch_all = []
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "SearchServiceClient",
 ]
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_search_service_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_search_service_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from copy import deepcopy
 from typing import Any, Awaitable
 
 from azure.core import AsyncPipelineClient
 from azure.core.rest import AsyncHttpResponse, HttpRequest
 
-from .. import models
+from .. import models as _models
 from .._serialization import Deserializer, Serializer
 from ._configuration import SearchServiceClientConfiguration
 from .operations import (
     AliasesOperations,
     DataSourcesOperations,
     IndexersOperations,
     IndexesOperations,
@@ -40,27 +40,27 @@
     :vartype synonym_maps: search_service_client.aio.operations.SynonymMapsOperations
     :ivar indexes: IndexesOperations operations
     :vartype indexes: search_service_client.aio.operations.IndexesOperations
     :ivar aliases: AliasesOperations operations
     :vartype aliases: search_service_client.aio.operations.AliasesOperations
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(  # pylint: disable=missing-client-constructor-parameter-credential
         self, endpoint: str, **kwargs: Any
     ) -> None:
         _endpoint = "{endpoint}"
         self._config = SearchServiceClientConfiguration(endpoint=endpoint, **kwargs)
-        self._client = AsyncPipelineClient(base_url=_endpoint, config=self._config, **kwargs)
+        self._client: AsyncPipelineClient = AsyncPipelineClient(base_url=_endpoint, config=self._config, **kwargs)
 
-        client_models = {k: v for k, v in models.__dict__.items() if isinstance(v, type)}
+        client_models = {k: v for k, v in _models.__dict__.items() if isinstance(v, type)}
         self._serialize = Serializer(client_models)
         self._deserialize = Deserializer(client_models)
         self._serialize.client_side_validation = False
         self.data_sources = DataSourcesOperations(self._client, self._config, self._serialize, self._deserialize)
         self.indexers = IndexersOperations(self._client, self._config, self._serialize, self._deserialize)
         self.skillsets = SkillsetsOperations(self._client, self._config, self._serialize, self._deserialize)
         self.synonym_maps = SynonymMapsOperations(self._client, self._config, self._serialize, self._deserialize)
@@ -96,9 +96,9 @@
     async def close(self) -> None:
         await self._client.close()
 
     async def __aenter__(self) -> "SearchServiceClient":
         await self._client.__aenter__()
         return self
 
-    async def __aexit__(self, *exc_details) -> None:
+    async def __aexit__(self, *exc_details: Any) -> None:
         await self._client.__aexit__(*exc_details)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_configuration.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/_configuration.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,43 +1,37 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
-import sys
 from typing import Any
 
 from azure.core.configuration import Configuration
 from azure.core.pipeline import policies
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
-
 VERSION = "unknown"
 
 
 class SearchServiceClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
     """Configuration for SearchServiceClient.
 
     Note that all parameters used to create this instance are saved as instance
     attributes.
 
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(self, endpoint: str, **kwargs: Any) -> None:
         super(SearchServiceClientConfiguration, self).__init__(**kwargs)
-        api_version = kwargs.pop("api_version", "2021-04-30-Preview")  # type: Literal["2021-04-30-Preview"]
+        api_version: str = kwargs.pop("api_version", "2023-07-01-Preview")
 
         if endpoint is None:
             raise ValueError("Parameter 'endpoint' must not be None.")
 
         self.endpoint = endpoint
         self.api_version = api_version
         kwargs.setdefault("sdk_moniker", "searchserviceclient/{}".format(VERSION))
@@ -45,11 +39,11 @@
 
     def _configure(self, **kwargs: Any) -> None:
         self.user_agent_policy = kwargs.get("user_agent_policy") or policies.UserAgentPolicy(**kwargs)
         self.headers_policy = kwargs.get("headers_policy") or policies.HeadersPolicy(**kwargs)
         self.proxy_policy = kwargs.get("proxy_policy") or policies.ProxyPolicy(**kwargs)
         self.logging_policy = kwargs.get("logging_policy") or policies.NetworkTraceLoggingPolicy(**kwargs)
         self.http_logging_policy = kwargs.get("http_logging_policy") or policies.HttpLoggingPolicy(**kwargs)
-        self.retry_policy = kwargs.get("retry_policy") or policies.AsyncRetryPolicy(**kwargs)
+        self.retry_policy = kwargs.get("retry_policy") or policies.RetryPolicy(**kwargs)
         self.custom_hook_policy = kwargs.get("custom_hook_policy") or policies.CustomHookPolicy(**kwargs)
-        self.redirect_policy = kwargs.get("redirect_policy") or policies.AsyncRedirectPolicy(**kwargs)
+        self.redirect_policy = kwargs.get("redirect_policy") or policies.RedirectPolicy(**kwargs)
         self.authentication_policy = kwargs.get("authentication_policy")
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/_vendor.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_vendor.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from abc import ABC
 from typing import TYPE_CHECKING
 
 from azure.core.pipeline.transport import HttpRequest
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_data_sources_operations.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -28,18 +28,14 @@
     build_create_request,
     build_delete_request,
     build_get_request,
     build_list_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class DataSourcesOperations:
     """
     .. warning::
@@ -71,14 +67,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param data_source: The definition of the datasource to create or update. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource
@@ -114,14 +113,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param data_source: The definition of the datasource to create or update. Required.
         :type data_source: IO
@@ -155,21 +157,24 @@
         if_none_match: Optional[str] = None,
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource or updates a datasource if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Data-Source
+
         :param data_source_name: The name of the datasource to create or update. Required.
         :type data_source_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param data_source: The definition of the datasource to create or update. Is either a model
-         type or a IO type. Required.
+        :param data_source: The definition of the datasource to create or update. Is either a
+         SearchIndexerDataSource type or a IO type. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -193,27 +198,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(data_source, (IO, bytes)):
+        if isinstance(data_source, (IOBase, bytes)):
             _content = data_source
         else:
             _json = self._serialize.body(data_source, "SearchIndexerDataSource")
 
         request = build_create_or_update_request(
             data_source_name=data_source_name,
             prefer=prefer,
@@ -229,18 +232,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -249,31 +253,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         data_source_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Data-Source
+
         :param data_source_name: The name of the datasource to delete. Required.
         :type data_source_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -292,18 +299,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             data_source_name=data_source_name,
@@ -315,38 +320,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    delete.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace_async
     async def get(
         self, data_source_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Retrieves a datasource definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Data-Source
+
         :param data_source_name: The name of the datasource to retrieve. Required.
         :type data_source_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerDataSource or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerDataSource
@@ -359,18 +368,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             data_source_name=data_source_name,
@@ -380,18 +387,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -400,22 +408,25 @@
         deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/datasources('{dataSourceName}')"}  # type: ignore
+    get.metadata = {"url": "/datasources('{dataSourceName}')"}
 
     @distributed_trace_async
     async def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListDataSourcesResult:
         """Lists all datasources available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Data-Sources
+
         :param select: Selects which top-level properties of the data sources to retrieve. Specified as
          a comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -430,18 +441,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListDataSourcesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListDataSourcesResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -451,18 +460,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -471,27 +481,30 @@
         deserialized = self._deserialize("ListDataSourcesResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/datasources"}  # type: ignore
+    list.metadata = {"url": "/datasources"}
 
     @overload
     async def create(
         self,
         data_source: _models.SearchIndexerDataSource,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
         :param data_source: The definition of the datasource to create. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -508,14 +521,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
         :param data_source: The definition of the datasource to create. Required.
         :type data_source: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -530,16 +546,19 @@
         self,
         data_source: Union[_models.SearchIndexerDataSource, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerDataSource:
         """Creates a new datasource.
 
-        :param data_source: The definition of the datasource to create. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Data-Source
+
+        :param data_source: The definition of the datasource to create. Is either a
+         SearchIndexerDataSource type or a IO type. Required.
         :type data_source: ~search_service_client.models.SearchIndexerDataSource or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -554,27 +573,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerDataSource]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerDataSource] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(data_source, (IO, bytes)):
+        if isinstance(data_source, (IOBase, bytes)):
             _content = data_source
         else:
             _json = self._serialize.body(data_source, "SearchIndexerDataSource")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -585,18 +602,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -605,8 +623,8 @@
         deserialized = self._deserialize("SearchIndexerDataSource", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/datasources"}  # type: ignore
+    create.metadata = {"url": "/datasources"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._data_sources_operations import DataSourcesOperations
 from ._indexers_operations import IndexersOperations
 from ._skillsets_operations import SkillsetsOperations
 from ._synonym_maps_operations import SynonymMapsOperations
 from ._indexes_operations import IndexesOperations
 from ._aliases_operations import AliasesOperations
 from ._search_service_client_operations import SearchServiceClientOperationsMixin
 
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "DataSourcesOperations",
     "IndexersOperations",
     "SkillsetsOperations",
     "SynonymMapsOperations",
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_skillsets_operations.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -29,18 +29,14 @@
     build_delete_request,
     build_get_request,
     build_list_request,
     build_reset_skills_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class SkillsetsOperations:
     """
     .. warning::
@@ -73,14 +69,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
          service. Required.
@@ -121,14 +120,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
          service. Required.
@@ -167,21 +169,24 @@
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         disable_cache_reprocessing_change_detection: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service or updates the skillset if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/update-skillset
+
         :param skillset_name: The name of the skillset to create or update. Required.
         :type skillset_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param skillset: The skillset containing one or more skills to create or update in a search
-         service. Is either a model type or a IO type. Required.
+         service. Is either a SearchIndexerSkillset type or a IO type. Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -208,27 +213,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skillset, (IO, bytes)):
+        if isinstance(skillset, (IOBase, bytes)):
             _content = skillset
         else:
             _json = self._serialize.body(skillset, "SearchIndexerSkillset")
 
         request = build_create_or_update_request(
             skillset_name=skillset_name,
             prefer=prefer,
@@ -245,18 +248,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -265,31 +269,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         skillset_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/delete-skillset
+
         :param skillset_name: The name of the skillset to delete. Required.
         :type skillset_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -308,18 +315,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             skillset_name=skillset_name,
@@ -331,38 +336,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    delete.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace_async
     async def get(
         self, skillset_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Retrieves a skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/get-skillset
+
         :param skillset_name: The name of the skillset to retrieve. Required.
         :type skillset_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerSkillset or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerSkillset
@@ -375,18 +384,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             skillset_name=skillset_name,
@@ -396,18 +403,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -416,22 +424,25 @@
         deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/skillsets('{skillsetName}')"}  # type: ignore
+    get.metadata = {"url": "/skillsets('{skillsetName}')"}
 
     @distributed_trace_async
     async def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListSkillsetsResult:
         """List all skillsets in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/list-skillset
+
         :param select: Selects which top-level properties of the skillsets to retrieve. Specified as a
          comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -446,18 +457,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListSkillsetsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListSkillsetsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -467,18 +476,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -487,27 +497,30 @@
         deserialized = self._deserialize("ListSkillsetsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/skillsets"}  # type: ignore
+    list.metadata = {"url": "/skillsets"}
 
     @overload
     async def create(
         self,
         skillset: _models.SearchIndexerSkillset,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service.
          Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
@@ -525,14 +538,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service.
          Required.
         :type skillset: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
@@ -548,16 +564,19 @@
         self,
         skillset: Union[_models.SearchIndexerSkillset, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexerSkillset:
         """Creates a new skillset in a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/create-skillset
+
         :param skillset: The skillset containing one or more skills to create in a search service. Is
-         either a model type or a IO type. Required.
+         either a SearchIndexerSkillset type or a IO type. Required.
         :type skillset: ~search_service_client.models.SearchIndexerSkillset or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -572,27 +591,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerSkillset]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexerSkillset] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skillset, (IO, bytes)):
+        if isinstance(skillset, (IOBase, bytes)):
             _content = skillset
         else:
             _json = self._serialize.body(skillset, "SearchIndexerSkillset")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -603,18 +620,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -623,28 +641,31 @@
         deserialized = self._deserialize("SearchIndexerSkillset", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/skillsets"}  # type: ignore
+    create.metadata = {"url": "/skillsets"}
 
     @overload
     async def reset_skills(  # pylint: disable=inconsistent-return-statements
         self,
         skillset_name: str,
         skill_names: _models.SkillNames,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
         :param skill_names: The names of skills to reset. Required.
         :type skill_names: ~search_service_client.models.SkillNames
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
@@ -664,14 +685,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
         :param skill_names: The names of skills to reset. Required.
         :type skill_names: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
@@ -689,17 +713,20 @@
         skillset_name: str,
         skill_names: Union[_models.SkillNames, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Reset an existing skillset in a search service.
 
+        .. seealso::
+           - https://aka.ms/reset-skills
+
         :param skillset_name: The name of the skillset to reset. Required.
         :type skillset_name: str
-        :param skill_names: The names of skills to reset. Is either a model type or a IO type.
+        :param skill_names: The names of skills to reset. Is either a SkillNames type or a IO type.
          Required.
         :type skill_names: ~search_service_client.models.SkillNames or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -715,27 +742,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(skill_names, (IO, bytes)):
+        if isinstance(skill_names, (IOBase, bytes)):
             _content = skill_names
         else:
             _json = self._serialize.body(skill_names, "SkillNames")
 
         request = build_reset_skills_request(
             skillset_name=skillset_name,
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -747,24 +772,25 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset_skills.metadata = {"url": "/skillsets('{skillsetName}')/search.resetskills"}  # type: ignore
+    reset_skills.metadata = {"url": "/skillsets('{skillsetName}')/search.resetskills"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_aliases_operations.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, AsyncIterable, Callable, Dict, IO, Optional, TypeVar, Union, overload
 import urllib.parse
 
 from azure.core.async_paging import AsyncItemPaged, AsyncList
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
@@ -31,18 +31,14 @@
     build_create_request,
     build_delete_request,
     build_get_request,
     build_list_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class AliasesOperations:
     """
     .. warning::
@@ -69,14 +65,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
         :param alias: The definition of the alias to create. Required.
         :type alias: ~search_service_client.models.SearchAlias
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -93,14 +92,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
         :param alias: The definition of the alias to create. Required.
         :type alias: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -115,15 +117,18 @@
         self,
         alias: Union[_models.SearchAlias, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias.
 
-        :param alias: The definition of the alias to create. Is either a model type or a IO type.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Alias
+
+        :param alias: The definition of the alias to create. Is either a SearchAlias type or a IO type.
          Required.
         :type alias: ~search_service_client.models.SearchAlias or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -139,27 +144,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(alias, (IO, bytes)):
+        if isinstance(alias, (IOBase, bytes)):
             _content = alias
         else:
             _json = self._serialize.body(alias, "SearchAlias")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -170,18 +173,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -190,36 +194,37 @@
         deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/aliases"}  # type: ignore
+    create.metadata = {"url": "/aliases"}
 
     @distributed_trace
     def list(
         self, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> AsyncIterable["_models.SearchAlias"]:
         """Lists all aliases available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Aliases
+
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: An iterator like instance of either SearchAlias or the result of cls(response)
         :rtype: ~azure.core.async_paging.AsyncItemPaged[~search_service_client.models.SearchAlias]
         :raises ~azure.core.exceptions.HttpResponseError:
         """
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListAliasesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListAliasesResult] = kwargs.pop("cls", None)
 
         error_map = {
             401: ClientAuthenticationError,
             404: ResourceNotFoundError,
             409: ResourceExistsError,
             304: ResourceNotModifiedError,
         }
@@ -240,15 +245,15 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
 
             else:
                 # make call to next link with the client's api-version
                 _parsed_next_link = urllib.parse.urlparse(next_link)
                 _next_request_params = case_insensitive_dict(
                     {
                         key: [urllib.parse.quote(v) for v in value]
@@ -261,43 +266,44 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
                 request.method = "GET"
             return request
 
         async def extract_data(pipeline_response):
             deserialized = self._deserialize("ListAliasesResult", pipeline_response)
             list_of_elem = deserialized.aliases
             if cls:
-                list_of_elem = cls(list_of_elem)
+                list_of_elem = cls(list_of_elem)  # type: ignore
             return None, AsyncList(list_of_elem)
 
         async def get_next(next_link=None):
             request = prepare_request(next_link)
 
-            pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-                request, stream=False, **kwargs
+            _stream = False
+            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+                request, stream=_stream, **kwargs
             )
             response = pipeline_response.http_response
 
             if response.status_code not in [200]:
                 map_error(status_code=response.status_code, response=response, error_map=error_map)
                 error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
                 raise HttpResponseError(response=response, model=error)
 
             return pipeline_response
 
         return AsyncItemPaged(get_next, extract_data)
 
-    list.metadata = {"url": "/aliases"}  # type: ignore
+    list.metadata = {"url": "/aliases"}
 
     @overload
     async def create_or_update(
         self,
         alias_name: str,
         prefer: Union[str, _models.Enum0],
         alias: _models.SearchAlias,
@@ -306,14 +312,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param alias: The definition of the alias to create or update. Required.
         :type alias: ~search_service_client.models.SearchAlias
@@ -345,14 +354,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param alias: The definition of the alias to create or update. Required.
         :type alias: IO
@@ -382,21 +394,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchAlias:
         """Creates a new search alias or updates an alias if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Alias
+
         :param alias_name: The definition of the alias to create or update. Required.
         :type alias_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param alias: The definition of the alias to create or update. Is either a model type or a IO
-         type. Required.
+        :param alias: The definition of the alias to create or update. Is either a SearchAlias type or
+         a IO type. Required.
         :type alias: ~search_service_client.models.SearchAlias or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -417,27 +432,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(alias, (IO, bytes)):
+        if isinstance(alias, (IOBase, bytes)):
             _content = alias
         else:
             _json = self._serialize.body(alias, "SearchAlias")
 
         request = build_create_or_update_request(
             alias_name=alias_name,
             prefer=prefer,
@@ -452,18 +465,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -472,32 +486,35 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/aliases('{aliasName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         alias_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a search alias and its associated mapping to an index. This operation is permanent,
         with no recovery option. The mapped index is untouched by this operation.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Alias
+
         :param alias_name: The name of the alias to delete. Required.
         :type alias_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -516,18 +533,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             alias_name=alias_name,
@@ -539,38 +554,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    delete.metadata = {"url": "/aliases('{aliasName}')"}
 
     @distributed_trace_async
     async def get(
         self, alias_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchAlias:
         """Retrieves an alias definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Alias
+
         :param alias_name: The name of the alias to retrieve. Required.
         :type alias_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchAlias or the result of cls(response)
         :rtype: ~search_service_client.models.SearchAlias
@@ -583,18 +602,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchAlias]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchAlias] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             alias_name=alias_name,
@@ -604,18 +621,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -624,8 +642,8 @@
         deserialized = self._deserialize("SearchAlias", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/aliases('{aliasName}')"}  # type: ignore
+    get.metadata = {"url": "/aliases('{aliasName}')"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexes_operations.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, AsyncIterable, Callable, Dict, IO, Optional, TypeVar, Union, overload
 import urllib.parse
 
 from azure.core.async_paging import AsyncItemPaged, AsyncList
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
@@ -33,18 +33,14 @@
     build_delete_request,
     build_get_request,
     build_get_statistics_request,
     build_list_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class IndexesOperations:
     """
     .. warning::
@@ -71,14 +67,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
         :param index: The definition of the index to create. Required.
         :type index: ~search_service_client.models.SearchIndex
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -95,14 +94,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
         :param index: The definition of the index to create. Required.
         :type index: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -117,15 +119,18 @@
         self,
         index: Union[_models.SearchIndex, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index.
 
-        :param index: The definition of the index to create. Is either a model type or a IO type.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Index
+
+        :param index: The definition of the index to create. Is either a SearchIndex type or a IO type.
          Required.
         :type index: ~search_service_client.models.SearchIndex or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
@@ -141,27 +146,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(index, (IO, bytes)):
+        if isinstance(index, (IOBase, bytes)):
             _content = index
         else:
             _json = self._serialize.body(index, "SearchIndex")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -172,18 +175,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -192,40 +196,41 @@
         deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/indexes"}  # type: ignore
+    create.metadata = {"url": "/indexes"}
 
     @distributed_trace
     def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> AsyncIterable["_models.SearchIndex"]:
         """Lists all indexes available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Indexes
+
         :param select: Selects which top-level properties of the index definitions to retrieve.
          Specified as a comma-separated list of JSON property names, or '*' for all properties. The
          default is all properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: An iterator like instance of either SearchIndex or the result of cls(response)
         :rtype: ~azure.core.async_paging.AsyncItemPaged[~search_service_client.models.SearchIndex]
         :raises ~azure.core.exceptions.HttpResponseError:
         """
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListIndexesResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListIndexesResult] = kwargs.pop("cls", None)
 
         error_map = {
             401: ClientAuthenticationError,
             404: ResourceNotFoundError,
             409: ResourceExistsError,
             304: ResourceNotModifiedError,
         }
@@ -247,15 +252,15 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
 
             else:
                 # make call to next link with the client's api-version
                 _parsed_next_link = urllib.parse.urlparse(next_link)
                 _next_request_params = case_insensitive_dict(
                     {
                         key: [urllib.parse.quote(v) for v in value]
@@ -268,43 +273,44 @@
                 )
                 request = _convert_request(request)
                 path_format_arguments = {
                     "endpoint": self._serialize.url(
                         "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                     ),
                 }
-                request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+                request.url = self._client.format_url(request.url, **path_format_arguments)
                 request.method = "GET"
             return request
 
         async def extract_data(pipeline_response):
             deserialized = self._deserialize("ListIndexesResult", pipeline_response)
             list_of_elem = deserialized.indexes
             if cls:
-                list_of_elem = cls(list_of_elem)
+                list_of_elem = cls(list_of_elem)  # type: ignore
             return None, AsyncList(list_of_elem)
 
         async def get_next(next_link=None):
             request = prepare_request(next_link)
 
-            pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-                request, stream=False, **kwargs
+            _stream = False
+            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+                request, stream=_stream, **kwargs
             )
             response = pipeline_response.http_response
 
             if response.status_code not in [200]:
                 map_error(status_code=response.status_code, response=response, error_map=error_map)
                 error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
                 raise HttpResponseError(response=response, model=error)
 
             return pipeline_response
 
         return AsyncItemPaged(get_next, extract_data)
 
-    list.metadata = {"url": "/indexes"}  # type: ignore
+    list.metadata = {"url": "/indexes"}
 
     @overload
     async def create_or_update(
         self,
         index_name: str,
         prefer: Union[str, _models.Enum0],
         index: _models.SearchIndex,
@@ -314,14 +320,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param index: The definition of the index to create or update. Required.
         :type index: ~search_service_client.models.SearchIndex
@@ -360,14 +369,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param index: The definition of the index to create or update. Required.
         :type index: IO
@@ -404,21 +416,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndex:
         """Creates a new search index or updates an index if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Index
+
         :param index_name: The definition of the index to create or update. Required.
         :type index_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param index: The definition of the index to create or update. Is either a model type or a IO
-         type. Required.
+        :param index: The definition of the index to create or update. Is either a SearchIndex type or
+         a IO type. Required.
         :type index: ~search_service_client.models.SearchIndex or IO
         :param allow_index_downtime: Allows new analyzers, tokenizers, token filters, or char filters
          to be added to an index by taking the index offline for at least a few seconds. This
          temporarily causes indexing and query requests to fail. Performance and write availability of
          the index can be impaired for several minutes after the index is updated, or longer for very
          large indexes. Default value is None.
         :type allow_index_downtime: bool
@@ -445,27 +460,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(index, (IO, bytes)):
+        if isinstance(index, (IOBase, bytes)):
             _content = index
         else:
             _json = self._serialize.body(index, "SearchIndex")
 
         request = build_create_or_update_request(
             index_name=index_name,
             prefer=prefer,
@@ -481,18 +494,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -501,33 +515,36 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         index_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a search index and all the documents it contains. This operation is permanent, with no
         recovery option. Make sure you have a master copy of your index definition, data ingestion
         code, and a backup of the primary data source in case you need to re-build the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Index
+
         :param index_name: The name of the index to delete. Required.
         :type index_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -546,18 +563,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             index_name=index_name,
@@ -569,38 +584,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    delete.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace_async
     async def get(
         self, index_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndex:
         """Retrieves an index definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Index
+
         :param index_name: The name of the index to retrieve. Required.
         :type index_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndex or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndex
@@ -613,18 +632,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndex]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndex] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             index_name=index_name,
@@ -634,18 +651,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -654,22 +672,25 @@
         deserialized = self._deserialize("SearchIndex", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/indexes('{indexName}')"}  # type: ignore
+    get.metadata = {"url": "/indexes('{indexName}')"}
 
     @distributed_trace_async
     async def get_statistics(
         self, index_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.GetIndexStatisticsResult:
         """Returns statistics for the given index, including a document count and storage usage.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Index-Statistics
+
         :param index_name: The name of the index for which to retrieve statistics. Required.
         :type index_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: GetIndexStatisticsResult or the result of cls(response)
         :rtype: ~search_service_client.models.GetIndexStatisticsResult
@@ -682,18 +703,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.GetIndexStatisticsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.GetIndexStatisticsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_statistics_request(
             index_name=index_name,
@@ -703,18 +722,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -723,28 +743,31 @@
         deserialized = self._deserialize("GetIndexStatisticsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_statistics.metadata = {"url": "/indexes('{indexName}')/search.stats"}  # type: ignore
+    get_statistics.metadata = {"url": "/indexes('{indexName}')/search.stats"}
 
     @overload
     async def analyze(
         self,
         index_name: str,
         request: _models.AnalyzeRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
         :param request: The text and analyzer or analysis components to test. Required.
         :type request: ~search_service_client.models.AnalyzeRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
@@ -764,14 +787,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
         :param request: The text and analyzer or analysis components to test. Required.
         :type request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
@@ -789,18 +815,21 @@
         index_name: str,
         request: Union[_models.AnalyzeRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.AnalyzeResult:
         """Shows how an analyzer breaks text into tokens.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/test-analyzer
+
         :param index_name: The name of the index for which to test an analyzer. Required.
         :type index_name: str
-        :param request: The text and analyzer or analysis components to test. Is either a model type or
-         a IO type. Required.
+        :param request: The text and analyzer or analysis components to test. Is either a
+         AnalyzeRequest type or a IO type. Required.
         :type request: ~search_service_client.models.AnalyzeRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -815,27 +844,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AnalyzeResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.AnalyzeResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(request, (IO, bytes)):
+        if isinstance(request, (IOBase, bytes)):
             _content = request
         else:
             _json = self._serialize.body(request, "AnalyzeRequest")
 
         request = build_analyze_request(
             index_name=index_name,
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -847,18 +874,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -867,8 +895,8 @@
         deserialized = self._deserialize("AnalyzeResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    analyze.metadata = {"url": "/indexes('{indexName}')/search.analyze"}  # type: ignore
+    analyze.metadata = {"url": "/indexes('{indexName}')/search.analyze"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_indexers_operations.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -32,18 +32,14 @@
     build_list_request,
     build_reset_docs_request,
     build_reset_request,
     build_run_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class IndexersOperations:
     """
     .. warning::
@@ -65,14 +61,17 @@
 
     @distributed_trace_async
     async def reset(  # pylint: disable=inconsistent-return-statements
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> None:
         """Resets the change tracking state associated with an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Reset-Indexer
+
         :param indexer_name: The name of the indexer to reset. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -85,18 +84,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_reset_request(
             indexer_name=indexer_name,
@@ -106,45 +103,49 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset.metadata = {"url": "/indexers('{indexerName}')/search.reset"}  # type: ignore
+    reset.metadata = {"url": "/indexers('{indexerName}')/search.reset"}
 
     @overload
     async def reset_docs(  # pylint: disable=inconsistent-return-statements
         self,
         indexer_name: str,
         overwrite: bool = False,
         request_options: Optional[_models.RequestOptions] = None,
         keys_or_ids: Optional[_models.DocumentKeysOrIds] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
@@ -168,14 +169,17 @@
         keys_or_ids: Optional[IO] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
@@ -197,22 +201,25 @@
         overwrite: bool = False,
         request_options: Optional[_models.RequestOptions] = None,
         keys_or_ids: Optional[Union[_models.DocumentKeysOrIds, IO]] = None,
         **kwargs: Any
     ) -> None:
         """Resets specific documents in the datasource to be selectively re-ingested by the indexer.
 
+        .. seealso::
+           - https://aka.ms/reset-documents
+
         :param indexer_name: The name of the indexer to reset documents for. Required.
         :type indexer_name: str
         :param overwrite: If false, keys or ids will be appended to existing ones. If true, only the
          keys or ids in this payload will be queued to be re-ingested. Default value is False.
         :type overwrite: bool
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
-        :param keys_or_ids: Is either a model type or a IO type. Default value is None.
+        :param keys_or_ids: Is either a DocumentKeysOrIds type or a IO type. Default value is None.
         :type keys_or_ids: ~search_service_client.models.DocumentKeysOrIds or IO
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -225,27 +232,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(keys_or_ids, (IO, bytes)):
+        if isinstance(keys_or_ids, (IOBase, bytes)):
             _content = keys_or_ids
         else:
             if keys_or_ids is not None:
                 _json = self._serialize.body(keys_or_ids, "DocumentKeysOrIds")
             else:
                 _json = None
 
@@ -261,38 +266,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    reset_docs.metadata = {"url": "/indexers('{indexerName}')/search.resetdocs"}  # type: ignore
+    reset_docs.metadata = {"url": "/indexers('{indexerName}')/search.resetdocs"}
 
     @distributed_trace_async
     async def run(  # pylint: disable=inconsistent-return-statements
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> None:
         """Runs an indexer on-demand.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Run-Indexer
+
         :param indexer_name: The name of the indexer to run. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: None or the result of cls(response)
         :rtype: None
@@ -305,18 +314,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_run_request(
             indexer_name=indexer_name,
@@ -326,31 +333,32 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [202]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    run.metadata = {"url": "/indexers('{indexerName}')/search.run"}  # type: ignore
+    run.metadata = {"url": "/indexers('{indexerName}')/search.run"}
 
     @overload
     async def create_or_update(
         self,
         indexer_name: str,
         prefer: Union[str, _models.Enum0],
         indexer: _models.SearchIndexer,
@@ -361,14 +369,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param indexer: The definition of the indexer to create or update. Required.
         :type indexer: ~search_service_client.models.SearchIndexer
@@ -408,14 +419,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param indexer: The definition of the indexer to create or update. Required.
         :type indexer: IO
@@ -453,21 +467,24 @@
         skip_indexer_reset_requirement_for_cache: Optional[bool] = None,
         disable_cache_reprocessing_change_detection: Optional[bool] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer or updates an indexer if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer_name: The name of the indexer to create or update. Required.
         :type indexer_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param indexer: The definition of the indexer to create or update. Is either a model type or a
-         IO type. Required.
+        :param indexer: The definition of the indexer to create or update. Is either a SearchIndexer
+         type or a IO type. Required.
         :type indexer: ~search_service_client.models.SearchIndexer or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -494,27 +511,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(indexer, (IO, bytes)):
+        if isinstance(indexer, (IOBase, bytes)):
             _content = indexer
         else:
             _json = self._serialize.body(indexer, "SearchIndexer")
 
         request = build_create_or_update_request(
             indexer_name=indexer_name,
             prefer=prefer,
@@ -531,18 +546,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -551,31 +567,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         indexer_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Indexer
+
         :param indexer_name: The name of the indexer to delete. Required.
         :type indexer_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -594,18 +613,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             indexer_name=indexer_name,
@@ -617,38 +634,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    delete.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace_async
     async def get(
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexer:
         """Retrieves an indexer definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Indexer
+
         :param indexer_name: The name of the indexer to retrieve. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexer or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexer
@@ -661,18 +682,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             indexer_name=indexer_name,
@@ -682,18 +701,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -702,22 +722,25 @@
         deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/indexers('{indexerName}')"}  # type: ignore
+    get.metadata = {"url": "/indexers('{indexerName}')"}
 
     @distributed_trace_async
     async def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListIndexersResult:
         """Lists all indexers available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Indexers
+
         :param select: Selects which top-level properties of the indexers to retrieve. Specified as a
          comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -732,18 +755,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListIndexersResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListIndexersResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -753,18 +774,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -773,27 +795,30 @@
         deserialized = self._deserialize("ListIndexersResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/indexers"}  # type: ignore
+    list.metadata = {"url": "/indexers"}
 
     @overload
     async def create(
         self,
         indexer: _models.SearchIndexer,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer: The definition of the indexer to create. Required.
         :type indexer: ~search_service_client.models.SearchIndexer
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -810,14 +835,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
         :param indexer: The definition of the indexer to create. Required.
         :type indexer: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -832,16 +860,19 @@
         self,
         indexer: Union[_models.SearchIndexer, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchIndexer:
         """Creates a new indexer.
 
-        :param indexer: The definition of the indexer to create. Is either a model type or a IO type.
-         Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Indexer
+
+        :param indexer: The definition of the indexer to create. Is either a SearchIndexer type or a IO
+         type. Required.
         :type indexer: ~search_service_client.models.SearchIndexer or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -856,27 +887,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexer]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchIndexer] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(indexer, (IO, bytes)):
+        if isinstance(indexer, (IOBase, bytes)):
             _content = indexer
         else:
             _json = self._serialize.body(indexer, "SearchIndexer")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -887,18 +916,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -907,22 +937,25 @@
         deserialized = self._deserialize("SearchIndexer", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/indexers"}  # type: ignore
+    create.metadata = {"url": "/indexers"}
 
     @distributed_trace_async
     async def get_status(
         self, indexer_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SearchIndexerStatus:
         """Returns the current status and execution history of an indexer.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Indexer-Status
+
         :param indexer_name: The name of the indexer for which to retrieve status. Required.
         :type indexer_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SearchIndexerStatus or the result of cls(response)
         :rtype: ~search_service_client.models.SearchIndexerStatus
@@ -935,18 +968,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchIndexerStatus]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchIndexerStatus] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_status_request(
             indexer_name=indexer_name,
@@ -956,18 +987,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -976,8 +1008,8 @@
         deserialized = self._deserialize("SearchIndexerStatus", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_status.metadata = {"url": "/indexers('{indexerName}')/search.status"}  # type: ignore
+    get_status.metadata = {"url": "/indexers('{indexerName}')/search.status"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_search_service_client_operations.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
 from typing import Any, Callable, Dict, Optional, TypeVar
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -22,18 +21,14 @@
 from azure.core.utils import case_insensitive_dict
 
 from ... import models as _models
 from ..._vendor import _convert_request
 from ...operations._search_service_client_operations import build_get_service_statistics_request
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class SearchServiceClientOperationsMixin(SearchServiceClientMixinABC):
     @distributed_trace_async
     async def get_service_statistics(
@@ -55,18 +50,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ServiceStatistics]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ServiceStatistics] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_service_statistics_request(
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -75,18 +68,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -95,8 +89,8 @@
         deserialized = self._deserialize("ServiceStatistics", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get_service_statistics.metadata = {"url": "/servicestats"}  # type: ignore
+    get_service_statistics.metadata = {"url": "/servicestats"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_synonym_maps_operations.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
-import sys
+from io import IOBase
 from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
     ResourceNotFoundError,
@@ -28,18 +28,14 @@
     build_create_request,
     build_delete_request,
     build_get_request,
     build_list_request,
 )
 from .._vendor import SearchServiceClientMixinABC
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class SynonymMapsOperations:
     """
     .. warning::
@@ -70,14 +66,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param synonym_map: The definition of the synonym map to create or update. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap
@@ -109,14 +108,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
         :param synonym_map: The definition of the synonym map to create or update. Required.
         :type synonym_map: IO
@@ -146,21 +148,24 @@
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map or updates a synonym map if it already exists.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Update-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to create or update. Required.
         :type synonym_map_name: str
         :param prefer: For HTTP PUT requests, instructs the service to return the created/updated
          resource on success. "return=representation" Required.
         :type prefer: str or ~search_service_client.models.Enum0
-        :param synonym_map: The definition of the synonym map to create or update. Is either a model
-         type or a IO type. Required.
+        :param synonym_map: The definition of the synonym map to create or update. Is either a
+         SynonymMap type or a IO type. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap or IO
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
         :type if_none_match: str
@@ -181,27 +186,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(synonym_map, (IO, bytes)):
+        if isinstance(synonym_map, (IOBase, bytes)):
             _content = synonym_map
         else:
             _json = self._serialize.body(synonym_map, "SynonymMap")
 
         request = build_create_or_update_request(
             synonym_map_name=synonym_map_name,
             prefer=prefer,
@@ -216,18 +219,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -236,31 +240,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if response.status_code == 201:
             deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    create_or_update.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    create_or_update.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace_async
     async def delete(  # pylint: disable=inconsistent-return-statements
         self,
         synonym_map_name: str,
         if_match: Optional[str] = None,
         if_none_match: Optional[str] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> None:
         """Deletes a synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Delete-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to delete. Required.
         :type synonym_map_name: str
         :param if_match: Defines the If-Match condition. The operation will be performed only if the
          ETag on the server matches this value. Default value is None.
         :type if_match: str
         :param if_none_match: Defines the If-None-Match condition. The operation will be performed only
          if the ETag on the server does not match this value. Default value is None.
@@ -279,18 +286,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[None]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[None] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_delete_request(
             synonym_map_name=synonym_map_name,
@@ -302,38 +307,42 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [204, 404]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
             raise HttpResponseError(response=response, model=error)
 
         if cls:
             return cls(pipeline_response, None, {})
 
-    delete.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    delete.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace_async
     async def get(
         self, synonym_map_name: str, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.SynonymMap:
         """Retrieves a synonym map definition.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Get-Synonym-Map
+
         :param synonym_map_name: The name of the synonym map to retrieve. Required.
         :type synonym_map_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: SynonymMap or the result of cls(response)
         :rtype: ~search_service_client.models.SynonymMap
@@ -346,18 +355,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             synonym_map_name=synonym_map_name,
@@ -367,18 +374,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -387,22 +395,25 @@
         deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/synonymmaps('{synonymMapName}')"}  # type: ignore
+    get.metadata = {"url": "/synonymmaps('{synonymMapName}')"}
 
     @distributed_trace_async
     async def list(
         self, select: Optional[str] = None, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any
     ) -> _models.ListSynonymMapsResult:
         """Lists all synonym maps available for a search service.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/List-Synonym-Maps
+
         :param select: Selects which top-level properties of the synonym maps to retrieve. Specified as
          a comma-separated list of JSON property names, or '*' for all properties. The default is all
          properties. Default value is None.
         :type select: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -417,18 +428,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.ListSynonymMapsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.ListSynonymMapsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_list_request(
             select=select,
@@ -438,18 +447,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -458,27 +468,30 @@
         deserialized = self._deserialize("ListSynonymMapsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    list.metadata = {"url": "/synonymmaps"}  # type: ignore
+    list.metadata = {"url": "/synonymmaps"}
 
     @overload
     async def create(
         self,
         synonym_map: _models.SynonymMap,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
         :param synonym_map: The definition of the synonym map to create. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -495,14 +508,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
         :param synonym_map: The definition of the synonym map to create. Required.
         :type synonym_map: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -517,16 +533,19 @@
         self,
         synonym_map: Union[_models.SynonymMap, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SynonymMap:
         """Creates a new synonym map.
 
-        :param synonym_map: The definition of the synonym map to create. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Create-Synonym-Map
+
+        :param synonym_map: The definition of the synonym map to create. Is either a SynonymMap type or
+         a IO type. Required.
         :type synonym_map: ~search_service_client.models.SynonymMap or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_service_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -541,27 +560,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SynonymMap]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SynonymMap] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(synonym_map, (IO, bytes)):
+        if isinstance(synonym_map, (IOBase, bytes)):
             _content = synonym_map
         else:
             _json = self._serialize.body(synonym_map, "SynonymMap")
 
         request = build_create_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -572,18 +589,19 @@
             headers=_headers,
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [201]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -592,8 +610,8 @@
         deserialized = self._deserialize("SynonymMap", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    create.metadata = {"url": "/synonymmaps"}  # type: ignore
+    create.metadata = {"url": "/synonymmaps"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._models_py3 import AnalyzeRequest
 from ._models_py3 import AnalyzeResult
 from ._models_py3 import AnalyzedTokenInfo
 from ._models_py3 import AsciiFoldingTokenFilter
@@ -43,14 +43,15 @@
 from ._models_py3 import EntityRecognitionSkillV3
 from ._models_py3 import FieldMapping
 from ._models_py3 import FieldMappingFunction
 from ._models_py3 import FreshnessScoringFunction
 from ._models_py3 import FreshnessScoringParameters
 from ._models_py3 import GetIndexStatisticsResult
 from ._models_py3 import HighWaterMarkChangeDetectionPolicy
+from ._models_py3 import HnswParameters
 from ._models_py3 import ImageAnalysisSkill
 from ._models_py3 import IndexerCurrentState
 from ._models_py3 import IndexerExecutionResult
 from ._models_py3 import IndexingParameters
 from ._models_py3 import IndexingParametersConfiguration
 from ._models_py3 import IndexingSchedule
 from ._models_py3 import InputFieldMappingEntry
@@ -150,14 +151,16 @@
 from ._models_py3 import TagScoringParameters
 from ._models_py3 import TextTranslationSkill
 from ._models_py3 import TextWeights
 from ._models_py3 import TokenFilter
 from ._models_py3 import TruncateTokenFilter
 from ._models_py3 import UaxUrlEmailTokenizer
 from ._models_py3 import UniqueTokenFilter
+from ._models_py3 import VectorSearch
+from ._models_py3 import VectorSearchAlgorithmConfiguration
 from ._models_py3 import WebApiSkill
 from ._models_py3 import WordDelimiterTokenFilter
 
 from ._search_service_client_enums import BlobIndexerDataToExtract
 from ._search_service_client_enums import BlobIndexerImageAction
 from ._search_service_client_enums import BlobIndexerPDFTextRotationAlgorithm
 from ._search_service_client_enums import BlobIndexerParsingMode
@@ -195,17 +198,18 @@
 from ._search_service_client_enums import SplitSkillLanguage
 from ._search_service_client_enums import StemmerTokenFilterLanguage
 from ._search_service_client_enums import StopwordsList
 from ._search_service_client_enums import TextSplitMode
 from ._search_service_client_enums import TextTranslationSkillLanguage
 from ._search_service_client_enums import TokenCharacterKind
 from ._search_service_client_enums import TokenFilterName
+from ._search_service_client_enums import VectorSearchAlgorithmMetric
 from ._search_service_client_enums import VisualFeature
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "AnalyzeRequest",
     "AnalyzeResult",
     "AnalyzedTokenInfo",
     "AsciiFoldingTokenFilter",
@@ -244,14 +248,15 @@
     "EntityRecognitionSkillV3",
     "FieldMapping",
     "FieldMappingFunction",
     "FreshnessScoringFunction",
     "FreshnessScoringParameters",
     "GetIndexStatisticsResult",
     "HighWaterMarkChangeDetectionPolicy",
+    "HnswParameters",
     "ImageAnalysisSkill",
     "IndexerCurrentState",
     "IndexerExecutionResult",
     "IndexingParameters",
     "IndexingParametersConfiguration",
     "IndexingSchedule",
     "InputFieldMappingEntry",
@@ -351,14 +356,16 @@
     "TagScoringParameters",
     "TextTranslationSkill",
     "TextWeights",
     "TokenFilter",
     "TruncateTokenFilter",
     "UaxUrlEmailTokenizer",
     "UniqueTokenFilter",
+    "VectorSearch",
+    "VectorSearchAlgorithmConfiguration",
     "WebApiSkill",
     "WordDelimiterTokenFilter",
     "BlobIndexerDataToExtract",
     "BlobIndexerImageAction",
     "BlobIndexerPDFTextRotationAlgorithm",
     "BlobIndexerParsingMode",
     "CharFilterName",
@@ -395,11 +402,12 @@
     "SplitSkillLanguage",
     "StemmerTokenFilterLanguage",
     "StopwordsList",
     "TextSplitMode",
     "TextTranslationSkillLanguage",
     "TokenCharacterKind",
     "TokenFilterName",
+    "VectorSearchAlgorithmMetric",
     "VisualFeature",
 ]
 __all__.extend([p for p in _patch_all if p not in __all__])
 _patch_sdk()
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_models_py3.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/models/_models_py3.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # coding=utf-8
 # pylint: disable=too-many-lines
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 import datetime
 import sys
 from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union
 
@@ -52,15 +52,15 @@
     _attribute_map = {
         "token": {"key": "token", "type": "str"},
         "start_offset": {"key": "startOffset", "type": "int"},
         "end_offset": {"key": "endOffset", "type": "int"},
         "position": {"key": "position", "type": "int"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.token = None
         self.start_offset = None
         self.end_offset = None
         self.position = None
 
@@ -123,16 +123,16 @@
         *,
         text: str,
         analyzer: Optional[Union[str, "_models.LexicalAnalyzerName"]] = None,
         tokenizer: Optional[Union[str, "_models.LexicalTokenizerName"]] = None,
         normalizer: Optional[Union[str, "_models.LexicalNormalizerName"]] = None,
         token_filters: Optional[List[Union[str, "_models.TokenFilterName"]]] = None,
         char_filters: Optional[List[Union[str, "_models.CharFilterName"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword text: The text to break into tokens. Required.
         :paramtype text: str
         :keyword analyzer: The name of the analyzer to use to break the given text. Known values are:
          "ar.microsoft", "ar.lucene", "hy.lucene", "bn.microsoft", "eu.lucene", "bg.microsoft",
          "bg.lucene", "ca.microsoft", "ca.lucene", "zh-Hans.microsoft", "zh-Hans.lucene",
          "zh-Hant.microsoft", "zh-Hant.lucene", "hr.microsoft", "cs.microsoft", "cs.lucene",
@@ -187,15 +187,15 @@
         "tokens": {"required": True},
     }
 
     _attribute_map = {
         "tokens": {"key": "tokens", "type": "[AnalyzedTokenInfo]"},
     }
 
-    def __init__(self, *, tokens: List["_models.AnalyzedTokenInfo"], **kwargs):
+    def __init__(self, *, tokens: List["_models.AnalyzedTokenInfo"], **kwargs: Any) -> None:
         """
         :keyword tokens: The list of tokens returned by the analyzer specified in the request.
          Required.
         :paramtype tokens: list[~search_service_client.models.AnalyzedTokenInfo]
         """
         super().__init__(**kwargs)
         self.tokens = tokens
@@ -259,28 +259,30 @@
             "#Microsoft.Azure.Search.SynonymTokenFilter": "SynonymTokenFilter",
             "#Microsoft.Azure.Search.TruncateTokenFilter": "TruncateTokenFilter",
             "#Microsoft.Azure.Search.UniqueTokenFilter": "UniqueTokenFilter",
             "#Microsoft.Azure.Search.WordDelimiterTokenFilter": "WordDelimiterTokenFilter",
         }
     }
 
-    def __init__(self, *, name: str, **kwargs):
+    def __init__(self, *, name: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
 
 
 class AsciiFoldingTokenFilter(TokenFilter):
-    """Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such equivalents exist. This token filter is implemented using Apache Lucene.
+    """Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127
+    ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
+    equivalents exist. This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -298,31 +300,32 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "preserve_original": {"key": "preserveOriginal", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, preserve_original: bool = False, **kwargs):
+    def __init__(self, *, name: str, preserve_original: bool = False, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword preserve_original: A value indicating whether the original token will be kept. Default
          is false.
         :paramtype preserve_original: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.AsciiFoldingTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.AsciiFoldingTokenFilter"
         self.preserve_original = preserve_original
 
 
 class AzureActiveDirectoryApplicationCredentials(_serialization.Model):
-    """Credentials of a registered application created for your search service, used for authenticated access to the encryption keys stored in Azure Key Vault.
+    """Credentials of a registered application created for your search service, used for authenticated
+    access to the encryption keys stored in Azure Key Vault.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar application_id: An AAD Application ID that was granted the required access permissions to
      the Azure Key Vault that is to be used when encrypting your data at rest. The Application ID
      should not be confused with the Object ID for your AAD Application. Required.
     :vartype application_id: str
@@ -335,15 +338,15 @@
     }
 
     _attribute_map = {
         "application_id": {"key": "applicationId", "type": "str"},
         "application_secret": {"key": "applicationSecret", "type": "str"},
     }
 
-    def __init__(self, *, application_id: str, application_secret: Optional[str] = None, **kwargs):
+    def __init__(self, *, application_id: str, application_secret: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword application_id: An AAD Application ID that was granted the required access permissions
          to the Azure Key Vault that is to be used when encrypting your data at rest. The Application ID
          should not be confused with the Object ID for your AAD Application. Required.
         :paramtype application_id: str
         :keyword application_secret: The authentication key of the specified AAD application.
         :paramtype application_secret: str
@@ -428,16 +431,16 @@
         self,
         *,
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -450,24 +453,26 @@
          of an upstream skill. Required.
         :paramtype inputs: list[~search_service_client.models.InputFieldMappingEntry]
         :keyword outputs: The output of a skill is either a field in a search index, or a value that
          can be consumed as an input by another skill. Required.
         :paramtype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
         self.description = description
         self.context = context
         self.inputs = inputs
         self.outputs = outputs
 
 
 class AzureMachineLearningSkill(SearchIndexerSkill):  # pylint: disable=too-many-instance-attributes
-    """The AML skill allows you to extend AI enrichment with a custom Azure Machine Learning (AML) model. Once an AML model is trained and deployed, an AML skill integrates it into AI enrichment.
+    """The AML skill allows you to extend AI enrichment with a custom Azure Machine Learning (AML)
+    model. Once an AML model is trained and deployed, an AML skill integrates it into AI
+    enrichment.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -539,16 +544,16 @@
         context: Optional[str] = None,
         scoring_uri: Optional[str] = None,
         authentication_key: Optional[str] = None,
         resource_id: Optional[str] = None,
         timeout: Optional[datetime.timedelta] = None,
         region: Optional[str] = None,
         degree_of_parallelism: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -583,25 +588,27 @@
          your endpoint is failing under too high of a request load, or raise it if your endpoint is able
          to accept more requests and you would like an increase in the performance of the indexer. If
          not set, a default value of 5 is used. The degreeOfParallelism can be set to a maximum of 10
          and a minimum of 1.
         :paramtype degree_of_parallelism: int
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Custom.AmlSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Custom.AmlSkill"
         self.scoring_uri = scoring_uri
         self.authentication_key = authentication_key
         self.resource_id = resource_id
         self.timeout = timeout
         self.region = region
         self.degree_of_parallelism = degree_of_parallelism
 
 
 class Similarity(_serialization.Model):
-    """Base type for similarity algorithms. Similarity algorithms are used to calculate scores that tie queries to documents. The higher the score, the more relevant the document is to that specific query. Those scores are used to rank the search results.
+    """Base type for similarity algorithms. Similarity algorithms are used to calculate scores that
+    tie queries to documents. The higher the score, the more relevant the document is to that
+    specific query. Those scores are used to rank the search results.
 
     You probably want to use the sub-classes and not this class directly. Known sub-classes are:
     BM25Similarity, ClassicSimilarity
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Required.
@@ -619,22 +626,24 @@
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.BM25Similarity": "BM25Similarity",
             "#Microsoft.Azure.Search.ClassicSimilarity": "ClassicSimilarity",
         }
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
 
 
 class BM25Similarity(Similarity):
-    """Ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like algorithm that includes length normalization (controlled by the 'b' parameter) as well as term frequency saturation (controlled by the 'k1' parameter).
+    """Ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like algorithm
+    that includes length normalization (controlled by the 'b' parameter) as well as term frequency
+    saturation (controlled by the 'k1' parameter).
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Required.
     :vartype odata_type: str
     :ivar k1: This property controls the scaling function between the term frequency of each
      matching terms and the final relevance score of a document-query pair. By default, a value of
@@ -652,27 +661,27 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "k1": {"key": "k1", "type": "float"},
         "b": {"key": "b", "type": "float"},
     }
 
-    def __init__(self, *, k1: Optional[float] = None, b: Optional[float] = None, **kwargs):
+    def __init__(self, *, k1: Optional[float] = None, b: Optional[float] = None, **kwargs: Any) -> None:
         """
         :keyword k1: This property controls the scaling function between the term frequency of each
          matching terms and the final relevance score of a document-query pair. By default, a value of
          1.2 is used. A value of 0.0 means the score does not scale with an increase in term frequency.
         :paramtype k1: float
         :keyword b: This property controls how the length of a document affects the relevance score. By
          default, a value of 0.75 is used. A value of 0.0 means no length normalization is applied,
          while a value of 1.0 means the score is fully normalized by the length of the document.
         :paramtype b: float
         """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.BM25Similarity"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.BM25Similarity"
         self.k1 = k1
         self.b = b
 
 
 class CharFilter(_serialization.Model):
     """Base type for character filters.
 
@@ -702,28 +711,29 @@
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.MappingCharFilter": "MappingCharFilter",
             "#Microsoft.Azure.Search.PatternReplaceCharFilter": "PatternReplaceCharFilter",
         }
     }
 
-    def __init__(self, *, name: str, **kwargs):
+    def __init__(self, *, name: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the char filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
 
 
 class CjkBigramTokenFilter(TokenFilter):
-    """Forms bigrams of CJK terms that are generated from the standard tokenizer. This token filter is implemented using Apache Lucene.
+    """Forms bigrams of CJK terms that are generated from the standard tokenizer. This token filter is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -750,36 +760,38 @@
 
     def __init__(
         self,
         *,
         name: str,
         ignore_scripts: Optional[List[Union[str, "_models.CjkBigramTokenFilterScripts"]]] = None,
         output_unigrams: bool = False,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword ignore_scripts: The scripts to ignore.
         :paramtype ignore_scripts: list[str or
          ~search_service_client.models.CjkBigramTokenFilterScripts]
         :keyword output_unigrams: A value indicating whether to output both unigrams and bigrams (if
          true), or just bigrams (if false). Default is false.
         :paramtype output_unigrams: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.CjkBigramTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.CjkBigramTokenFilter"
         self.ignore_scripts = ignore_scripts
         self.output_unigrams = output_unigrams
 
 
 class ClassicSimilarity(Similarity):
-    """Legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of TF-IDF. This variation of TF-IDF introduces static document length normalization as well as coordinating factors that penalize documents that only partially match the searched queries.
+    """Legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of TF-IDF.
+    This variation of TF-IDF introduces static document length normalization as well as
+    coordinating factors that penalize documents that only partially match the searched queries.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Required.
     :vartype odata_type: str
     """
 
@@ -787,18 +799,18 @@
         "odata_type": {"required": True},
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.ClassicSimilarity"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.ClassicSimilarity"
 
 
 class LexicalTokenizer(_serialization.Model):
     """Base type for tokenizers.
 
     You probably want to use the sub-classes and not this class directly. Known sub-classes are:
     ClassicTokenizer, EdgeNGramTokenizer, KeywordTokenizer, KeywordTokenizerV2,
@@ -839,28 +851,29 @@
             "#Microsoft.Azure.Search.PatternTokenizer": "PatternTokenizer",
             "#Microsoft.Azure.Search.StandardTokenizer": "LuceneStandardTokenizer",
             "#Microsoft.Azure.Search.StandardTokenizerV2": "LuceneStandardTokenizerV2",
             "#Microsoft.Azure.Search.UaxUrlEmailTokenizer": "UaxUrlEmailTokenizer",
         }
     }
 
-    def __init__(self, *, name: str, **kwargs):
+    def __init__(self, *, name: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
 
 
 class ClassicTokenizer(LexicalTokenizer):
-    """Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer is implemented using Apache Lucene.
+    """Grammar-based tokenizer that is suitable for processing most European-language documents. This
+    tokenizer is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -879,26 +892,26 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs):
+    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 255. Tokens longer than the
          maximum length are split. The maximum token length that can be used is 300 characters.
         :paramtype max_token_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.ClassicTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.ClassicTokenizer"
         self.max_token_length = max_token_length
 
 
 class CognitiveServicesAccount(_serialization.Model):
     """Base type for describing any cognitive service resource attached to a skillset.
 
     You probably want to use the sub-classes and not this class directly. Known sub-classes are:
@@ -925,21 +938,21 @@
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.CognitiveServicesByKey": "CognitiveServicesAccountKey",
             "#Microsoft.Azure.Search.DefaultCognitiveServices": "DefaultCognitiveServicesAccount",
         }
     }
 
-    def __init__(self, *, description: Optional[str] = None, **kwargs):
+    def __init__(self, *, description: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword description: Description of the cognitive service resource attached to a skillset.
         :paramtype description: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.description = description
 
 
 class CognitiveServicesAccountKey(CognitiveServicesAccount):
     """A cognitive service resource provisioned with a key that is attached to a skillset.
 
     All required parameters must be populated in order to send to Azure.
@@ -961,29 +974,30 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "description": {"key": "description", "type": "str"},
         "key": {"key": "key", "type": "str"},
     }
 
-    def __init__(self, *, key: str, description: Optional[str] = None, **kwargs):
+    def __init__(self, *, key: str, description: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword description: Description of the cognitive service resource attached to a skillset.
         :paramtype description: str
         :keyword key: The key used to provision the cognitive service resource attached to a skillset.
          Required.
         :paramtype key: str
         """
         super().__init__(description=description, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.CognitiveServicesByKey"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.CognitiveServicesByKey"
         self.key = key
 
 
 class CommonGramTokenFilter(TokenFilter):
-    """Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
+    """Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed
+    too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -1011,16 +1025,22 @@
         "name": {"key": "name", "type": "str"},
         "common_words": {"key": "commonWords", "type": "[str]"},
         "ignore_case": {"key": "ignoreCase", "type": "bool"},
         "use_query_mode": {"key": "queryMode", "type": "bool"},
     }
 
     def __init__(
-        self, *, name: str, common_words: List[str], ignore_case: bool = False, use_query_mode: bool = False, **kwargs
-    ):
+        self,
+        *,
+        name: str,
+        common_words: List[str],
+        ignore_case: bool = False,
+        use_query_mode: bool = False,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword common_words: The set of common words. Required.
         :paramtype common_words: list[str]
@@ -1029,22 +1049,23 @@
         :paramtype ignore_case: bool
         :keyword use_query_mode: A value that indicates whether the token filter is in query mode. When
          in query mode, the token filter generates bigrams and then removes common words and single
          terms followed by a common word. Default is false.
         :paramtype use_query_mode: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.CommonGramTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.CommonGramTokenFilter"
         self.common_words = common_words
         self.ignore_case = ignore_case
         self.use_query_mode = use_query_mode
 
 
 class ConditionalSkill(SearchIndexerSkill):
-    """A skill that enables scenarios that require a Boolean operation to determine the data to assign to an output.
+    """A skill that enables scenarios that require a Boolean operation to determine the data to assign
+    to an output.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -1083,16 +1104,16 @@
         self,
         *,
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -1105,15 +1126,15 @@
          of an upstream skill. Required.
         :paramtype inputs: list[~search_service_client.models.InputFieldMappingEntry]
         :keyword outputs: The output of a skill is either a field in a search index, or a value that
          can be consumed as an input by another skill. Required.
         :paramtype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Util.ConditionalSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Util.ConditionalSkill"
 
 
 class CorsOptions(_serialization.Model):
     """Defines options to control Cross-Origin Resource Sharing (CORS) for an index.
 
     All required parameters must be populated in order to send to Azure.
 
@@ -1132,15 +1153,15 @@
     }
 
     _attribute_map = {
         "allowed_origins": {"key": "allowedOrigins", "type": "[str]"},
         "max_age_in_seconds": {"key": "maxAgeInSeconds", "type": "int"},
     }
 
-    def __init__(self, *, allowed_origins: List[str], max_age_in_seconds: Optional[int] = None, **kwargs):
+    def __init__(self, *, allowed_origins: List[str], max_age_in_seconds: Optional[int] = None, **kwargs: Any) -> None:
         """
         :keyword allowed_origins: The list of origins from which JavaScript code will be granted access
          to your index. Can contain a list of hosts of the form
          {protocol}://{fully-qualified-domain-name}[:{port#}], or a single '*' to allow all origins (not
          recommended). Required.
         :paramtype allowed_origins: list[str]
         :keyword max_age_in_seconds: The duration for which browsers should cache CORS preflight
@@ -1183,28 +1204,31 @@
             "#Microsoft.Azure.Search.CustomAnalyzer": "CustomAnalyzer",
             "#Microsoft.Azure.Search.PatternAnalyzer": "PatternAnalyzer",
             "#Microsoft.Azure.Search.StandardAnalyzer": "LuceneStandardAnalyzer",
             "#Microsoft.Azure.Search.StopAnalyzer": "StopAnalyzer",
         }
     }
 
-    def __init__(self, *, name: str, **kwargs):
+    def __init__(self, *, name: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the analyzer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
 
 
 class CustomAnalyzer(LexicalAnalyzer):
-    """Allows you to take control over the process of converting text into indexable/searchable tokens. It's a user-defined configuration consisting of a single predefined tokenizer and one or more filters. The tokenizer is responsible for breaking text into tokens, and the filters for modifying tokens emitted by the tokenizer.
+    """Allows you to take control over the process of converting text into indexable/searchable
+    tokens. It's a user-defined configuration consisting of a single predefined tokenizer and one
+    or more filters. The tokenizer is responsible for breaking text into tokens, and the filters
+    for modifying tokens emitted by the tokenizer.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the analyzer. Required.
     :vartype odata_type: str
     :ivar name: The name of the analyzer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -1243,16 +1267,16 @@
     def __init__(
         self,
         *,
         name: str,
         tokenizer: Union[str, "_models.LexicalTokenizerName"],
         token_filters: Optional[List[Union[str, "_models.TokenFilterName"]]] = None,
         char_filters: Optional[List[Union[str, "_models.CharFilterName"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the analyzer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword tokenizer: The name of the tokenizer to use to divide continuous text into a sequence
          of tokens, such as breaking a sentence into words. Required. Known values are: "classic",
@@ -1266,15 +1290,15 @@
         :paramtype token_filters: list[str or ~search_service_client.models.TokenFilterName]
         :keyword char_filters: A list of character filters used to prepare input text before it is
          processed by the tokenizer. For instance, they can replace certain characters or symbols. The
          filters are run in the order in which they are listed.
         :paramtype char_filters: list[str or ~search_service_client.models.CharFilterName]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.CustomAnalyzer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.CustomAnalyzer"
         self.tokenizer = tokenizer
         self.token_filters = token_filters
         self.char_filters = char_filters
 
 
 class CustomEntity(_serialization.Model):  # pylint: disable=too-many-instance-attributes
     """An object that contains information about the matches that were found, and related metadata.
@@ -1358,16 +1382,16 @@
         case_sensitive: Optional[bool] = None,
         accent_sensitive: Optional[bool] = None,
         fuzzy_edit_distance: Optional[int] = None,
         default_case_sensitive: Optional[bool] = None,
         default_accent_sensitive: Optional[bool] = None,
         default_fuzzy_edit_distance: Optional[int] = None,
         aliases: Optional[List["_models.CustomEntityAlias"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The top-level entity descriptor. Matches in the skill output will be grouped by
          this name, and it should represent the "normalized" form of the text being found. Required.
         :paramtype name: str
         :keyword description: This field can be used as a passthrough for custom metadata about the
          matched text(s). The value of this field will appear with every match of its entity in the
          skill output.
@@ -1423,15 +1447,16 @@
         self.default_case_sensitive = default_case_sensitive
         self.default_accent_sensitive = default_accent_sensitive
         self.default_fuzzy_edit_distance = default_fuzzy_edit_distance
         self.aliases = aliases
 
 
 class CustomEntityAlias(_serialization.Model):
-    """A complex object that can be used to specify alternative spellings or synonyms to the root entity name.
+    """A complex object that can be used to specify alternative spellings or synonyms to the root
+    entity name.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar text: The text of the alias. Required.
     :vartype text: str
     :ivar case_sensitive: Determine if the alias is case sensitive.
     :vartype case_sensitive: bool
@@ -1455,16 +1480,16 @@
     def __init__(
         self,
         *,
         text: str,
         case_sensitive: Optional[bool] = None,
         accent_sensitive: Optional[bool] = None,
         fuzzy_edit_distance: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword text: The text of the alias. Required.
         :paramtype text: str
         :keyword case_sensitive: Determine if the alias is case sensitive.
         :paramtype case_sensitive: bool
         :keyword accent_sensitive: Determine if the alias is accent sensitive.
         :paramtype accent_sensitive: bool
@@ -1554,16 +1579,16 @@
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.CustomEntityLookupSkillLanguage"]] = None,
         entities_definition_uri: Optional[str] = None,
         inline_entities_definition: Optional[List["_models.CustomEntity"]] = None,
         global_default_case_sensitive: Optional[bool] = None,
         global_default_accent_sensitive: Optional[bool] = None,
         global_default_fuzzy_edit_distance: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -1596,15 +1621,15 @@
          is not set in CustomEntity, this value will be the default value.
         :paramtype global_default_accent_sensitive: bool
         :keyword global_default_fuzzy_edit_distance: A global flag for FuzzyEditDistance. If
          FuzzyEditDistance is not set in CustomEntity, this value will be the default value.
         :paramtype global_default_fuzzy_edit_distance: int
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.CustomEntityLookupSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.CustomEntityLookupSkill"
         self.default_language_code = default_language_code
         self.entities_definition_uri = entities_definition_uri
         self.inline_entities_definition = inline_entities_definition
         self.global_default_case_sensitive = global_default_case_sensitive
         self.global_default_accent_sensitive = global_default_accent_sensitive
         self.global_default_fuzzy_edit_distance = global_default_fuzzy_edit_distance
 
@@ -1634,29 +1659,31 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
     }
 
     _subtype_map = {"odata_type": {"#Microsoft.Azure.Search.CustomNormalizer": "CustomNormalizer"}}
 
-    def __init__(self, *, name: str, **kwargs):
+    def __init__(self, *, name: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the normalizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. It cannot end in '.microsoft' nor '.lucene', nor be named 'asciifolding',
          'standard', 'lowercase', 'uppercase', or 'elision'. Required.
         :paramtype name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
         self.name = name
 
 
 class CustomNormalizer(LexicalNormalizer):
-    """Allows you to configure normalization for filterable, sortable, and facetable fields, which by default operate with strict matching. This is a user-defined configuration consisting of at least one or more filters, which modify the token that is stored.
+    """Allows you to configure normalization for filterable, sortable, and facetable fields, which by
+    default operate with strict matching. This is a user-defined configuration consisting of at
+    least one or more filters, which modify the token that is stored.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the normalizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the normalizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -1687,16 +1714,16 @@
 
     def __init__(
         self,
         *,
         name: str,
         token_filters: Optional[List[Union[str, "_models.TokenFilterName"]]] = None,
         char_filters: Optional[List[Union[str, "_models.CharFilterName"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the normalizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. It cannot end in '.microsoft' nor '.lucene', nor be named 'asciifolding',
          'standard', 'lowercase', 'uppercase', or 'elision'. Required.
         :paramtype name: str
         :keyword token_filters: A list of token filters used to filter out or modify the input token.
@@ -1705,15 +1732,15 @@
         :paramtype token_filters: list[str or ~search_service_client.models.TokenFilterName]
         :keyword char_filters: A list of character filters used to prepare input text before it is
          processed. For instance, they can replace certain characters or symbols. The filters are run in
          the order in which they are listed.
         :paramtype char_filters: list[str or ~search_service_client.models.CharFilterName]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.CustomNormalizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.CustomNormalizer"
         self.token_filters = token_filters
         self.char_filters = char_filters
 
 
 class DataChangeDetectionPolicy(_serialization.Model):
     """Base type for data change detection policies.
 
@@ -1737,18 +1764,18 @@
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy": "HighWaterMarkChangeDetectionPolicy",
             "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy": "SqlIntegratedChangeTrackingPolicy",
         }
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
 
 
 class DataDeletionDetectionPolicy(_serialization.Model):
     """Base type for data deletion detection policies.
 
     You probably want to use the sub-classes and not this class directly. Known sub-classes are:
     SoftDeleteColumnDeletionDetectionPolicy
@@ -1769,33 +1796,33 @@
 
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy": "SoftDeleteColumnDeletionDetectionPolicy"
         }
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
 
 
 class DataSourceCredentials(_serialization.Model):
     """Represents credentials that can be used to connect to a datasource.
 
     :ivar connection_string: The connection string for the datasource. Set to ':code:`<unchanged>`'
      if you do not want the connection string updated.
     :vartype connection_string: str
     """
 
     _attribute_map = {
         "connection_string": {"key": "connectionString", "type": "str"},
     }
 
-    def __init__(self, *, connection_string: Optional[str] = None, **kwargs):
+    def __init__(self, *, connection_string: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword connection_string: The connection string for the datasource. Set to
          ':code:`<unchanged>`' if you do not want the connection string updated.
         :paramtype connection_string: str
         """
         super().__init__(**kwargs)
         self.connection_string = connection_string
@@ -1818,25 +1845,26 @@
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "description": {"key": "description", "type": "str"},
     }
 
-    def __init__(self, *, description: Optional[str] = None, **kwargs):
+    def __init__(self, *, description: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword description: Description of the cognitive service resource attached to a skillset.
         :paramtype description: str
         """
         super().__init__(description=description, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.DefaultCognitiveServices"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.DefaultCognitiveServices"
 
 
 class DictionaryDecompounderTokenFilter(TokenFilter):
-    """Decomposes compound words found in many Germanic languages. This token filter is implemented using Apache Lucene.
+    """Decomposes compound words found in many Germanic languages. This token filter is implemented
+    using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -1882,16 +1910,16 @@
         *,
         name: str,
         word_list: List[str],
         min_word_size: int = 5,
         min_subword_size: int = 2,
         max_subword_size: int = 15,
         only_longest_match: bool = False,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword word_list: The list of words to match against. Required.
         :paramtype word_list: list[str]
@@ -1905,15 +1933,15 @@
          outputted. Default is 15. Maximum is 300.
         :paramtype max_subword_size: int
         :keyword only_longest_match: A value indicating whether to add only the longest matching
          subword to the output. Default is false.
         :paramtype only_longest_match: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter"
         self.word_list = word_list
         self.min_word_size = min_word_size
         self.min_subword_size = min_subword_size
         self.max_subword_size = max_subword_size
         self.only_longest_match = only_longest_match
 
 
@@ -1963,29 +1991,29 @@
 
     def __init__(
         self,
         *,
         field_name: str,
         boost: float,
         interpolation: Optional[Union[str, "_models.ScoringFunctionInterpolation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword field_name: The name of the field used as input to the scoring function. Required.
         :paramtype field_name: str
         :keyword boost: A multiplier for the raw score. Must be a positive number not equal to 1.0.
          Required.
         :paramtype boost: float
         :keyword interpolation: A value indicating how boosting will be interpolated across document
          scores; defaults to "Linear". Known values are: "linear", "constant", "quadratic", and
          "logarithmic".
         :paramtype interpolation: str or ~search_service_client.models.ScoringFunctionInterpolation
         """
         super().__init__(**kwargs)
-        self.type = None  # type: Optional[str]
+        self.type: Optional[str] = None
         self.field_name = field_name
         self.boost = boost
         self.interpolation = interpolation
 
 
 class DistanceScoringFunction(ScoringFunction):
     """Defines a function that boosts scores based on distance from a geographic location.
@@ -2026,31 +2054,31 @@
     def __init__(
         self,
         *,
         field_name: str,
         boost: float,
         parameters: "_models.DistanceScoringParameters",
         interpolation: Optional[Union[str, "_models.ScoringFunctionInterpolation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword field_name: The name of the field used as input to the scoring function. Required.
         :paramtype field_name: str
         :keyword boost: A multiplier for the raw score. Must be a positive number not equal to 1.0.
          Required.
         :paramtype boost: float
         :keyword interpolation: A value indicating how boosting will be interpolated across document
          scores; defaults to "Linear". Known values are: "linear", "constant", "quadratic", and
          "logarithmic".
         :paramtype interpolation: str or ~search_service_client.models.ScoringFunctionInterpolation
         :keyword parameters: Parameter values for the distance scoring function. Required.
         :paramtype parameters: ~search_service_client.models.DistanceScoringParameters
         """
         super().__init__(field_name=field_name, boost=boost, interpolation=interpolation, **kwargs)
-        self.type = "distance"  # type: str
+        self.type: str = "distance"
         self.parameters = parameters
 
 
 class DistanceScoringParameters(_serialization.Model):
     """Provides parameter values to a distance scoring function.
 
     All required parameters must be populated in order to send to Azure.
@@ -2069,15 +2097,15 @@
     }
 
     _attribute_map = {
         "reference_point_parameter": {"key": "referencePointParameter", "type": "str"},
         "boosting_distance": {"key": "boostingDistance", "type": "float"},
     }
 
-    def __init__(self, *, reference_point_parameter: str, boosting_distance: float, **kwargs):
+    def __init__(self, *, reference_point_parameter: str, boosting_distance: float, **kwargs: Any) -> None:
         """
         :keyword reference_point_parameter: The name of the parameter passed in search queries to
          specify the reference location. Required.
         :paramtype reference_point_parameter: str
         :keyword boosting_distance: The distance in kilometers from the reference location where the
          boosting range ends. Required.
         :paramtype boosting_distance: float
@@ -2144,16 +2172,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         parsing_mode: Optional[str] = None,
         data_to_extract: Optional[str] = None,
         configuration: Optional[Dict[str, JSON]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -2173,15 +2201,15 @@
         :keyword data_to_extract: The type of data to be extracted for the skill. Will be set to
          'contentAndMetadata' if not defined.
         :paramtype data_to_extract: str
         :keyword configuration: A dictionary of configurations for the skill.
         :paramtype configuration: dict[str, JSON]
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Util.DocumentExtractionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Util.DocumentExtractionSkill"
         self.parsing_mode = parsing_mode
         self.data_to_extract = data_to_extract
         self.configuration = configuration
 
 
 class DocumentKeysOrIds(_serialization.Model):
     """DocumentKeysOrIds.
@@ -2198,29 +2226,30 @@
     }
 
     def __init__(
         self,
         *,
         document_keys: Optional[List[str]] = None,
         datasource_document_ids: Optional[List[str]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword document_keys: document keys to be reset.
         :paramtype document_keys: list[str]
         :keyword datasource_document_ids: datasource document identifiers to be reset.
         :paramtype datasource_document_ids: list[str]
         """
         super().__init__(**kwargs)
         self.document_keys = document_keys
         self.datasource_document_ids = datasource_document_ids
 
 
 class EdgeNGramTokenFilter(TokenFilter):
-    """Generates n-grams of the given size(s) starting from the front or the back of an input token. This token filter is implemented using Apache Lucene.
+    """Generates n-grams of the given size(s) starting from the front or the back of an input token.
+    This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -2252,16 +2281,16 @@
     def __init__(
         self,
         *,
         name: str,
         min_gram: int = 1,
         max_gram: int = 2,
         side: Optional[Union[str, "_models.EdgeNGramTokenFilterSide"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Must be less than the value of
          maxGram.
@@ -2269,22 +2298,23 @@
         :keyword max_gram: The maximum n-gram length. Default is 2.
         :paramtype max_gram: int
         :keyword side: Specifies which side of the input the n-gram should be generated from. Default
          is "front". Known values are: "front" and "back".
         :paramtype side: str or ~search_service_client.models.EdgeNGramTokenFilterSide
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.EdgeNGramTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.EdgeNGramTokenFilter"
         self.min_gram = min_gram
         self.max_gram = max_gram
         self.side = side
 
 
 class EdgeNGramTokenFilterV2(TokenFilter):
-    """Generates n-grams of the given size(s) starting from the front or the back of an input token. This token filter is implemented using Apache Lucene.
+    """Generates n-grams of the given size(s) starting from the front or the back of an input token.
+    This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -2318,16 +2348,16 @@
     def __init__(
         self,
         *,
         name: str,
         min_gram: int = 1,
         max_gram: int = 2,
         side: Optional[Union[str, "_models.EdgeNGramTokenFilterSide"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than
          the value of maxGram.
@@ -2335,22 +2365,23 @@
         :keyword max_gram: The maximum n-gram length. Default is 2. Maximum is 300.
         :paramtype max_gram: int
         :keyword side: Specifies which side of the input the n-gram should be generated from. Default
          is "front". Known values are: "front" and "back".
         :paramtype side: str or ~search_service_client.models.EdgeNGramTokenFilterSide
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.EdgeNGramTokenFilterV2"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.EdgeNGramTokenFilterV2"
         self.min_gram = min_gram
         self.max_gram = max_gram
         self.side = side
 
 
 class EdgeNGramTokenizer(LexicalTokenizer):
-    """Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.
+    """Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -2383,38 +2414,39 @@
     def __init__(
         self,
         *,
         name: str,
         min_gram: int = 1,
         max_gram: int = 2,
         token_chars: Optional[List[Union[str, "_models.TokenCharacterKind"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than
          the value of maxGram.
         :paramtype min_gram: int
         :keyword max_gram: The maximum n-gram length. Default is 2. Maximum is 300.
         :paramtype max_gram: int
         :keyword token_chars: Character classes to keep in the tokens.
         :paramtype token_chars: list[str or ~search_service_client.models.TokenCharacterKind]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.EdgeNGramTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.EdgeNGramTokenizer"
         self.min_gram = min_gram
         self.max_gram = max_gram
         self.token_chars = token_chars
 
 
 class ElisionTokenFilter(TokenFilter):
-    """Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). This token filter is implemented using Apache Lucene.
+    """Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). This
+    token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -2431,25 +2463,25 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "articles": {"key": "articles", "type": "[str]"},
     }
 
-    def __init__(self, *, name: str, articles: Optional[List[str]] = None, **kwargs):
+    def __init__(self, *, name: str, articles: Optional[List[str]] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword articles: The set of articles to remove.
         :paramtype articles: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.ElisionTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.ElisionTokenFilter"
         self.articles = articles
 
 
 class EntityLinkingSkill(SearchIndexerSkill):
     """Using the Text Analytics API, extracts linked entities from text.
 
     All required parameters must be populated in order to send to Azure.
@@ -2510,16 +2542,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[str] = None,
         minimum_precision: Optional[float] = None,
         model_version: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -2542,15 +2574,15 @@
         :paramtype minimum_precision: float
         :keyword model_version: The version of the model to use when calling the Text Analytics
          service. It will default to the latest available when not specified. We recommend you do not
          specify this value unless absolutely necessary.
         :paramtype model_version: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.V3.EntityLinkingSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.V3.EntityLinkingSkill"
         self.default_language_code = default_language_code
         self.minimum_precision = minimum_precision
         self.model_version = model_version
 
 
 class EntityRecognitionSkill(SearchIndexerSkill):
     """Text analytics entity recognition.
@@ -2620,16 +2652,16 @@
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         categories: Optional[List[Union[str, "_models.EntityCategory"]]] = None,
         default_language_code: Optional[Union[str, "_models.EntityRecognitionSkillLanguage"]] = None,
         include_typeless_entities: Optional[bool] = None,
         minimum_precision: Optional[float] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -2658,15 +2690,15 @@
         :paramtype include_typeless_entities: bool
         :keyword minimum_precision: A value between 0 and 1 that be used to only include entities whose
          confidence score is greater than the value specified. If not set (default), or if explicitly
          set to null, all entities will be included.
         :paramtype minimum_precision: float
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.EntityRecognitionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.EntityRecognitionSkill"
         self.categories = categories
         self.default_language_code = default_language_code
         self.include_typeless_entities = include_typeless_entities
         self.minimum_precision = minimum_precision
 
 
 class EntityRecognitionSkillV3(SearchIndexerSkill):
@@ -2734,16 +2766,16 @@
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         categories: Optional[List[str]] = None,
         default_language_code: Optional[str] = None,
         minimum_precision: Optional[float] = None,
         model_version: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -2768,15 +2800,15 @@
         :paramtype minimum_precision: float
         :keyword model_version: The version of the model to use when calling the Text Analytics
          service. It will default to the latest available when not specified. We recommend you do not
          specify this value unless absolutely necessary.
         :paramtype model_version: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.V3.EntityRecognitionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.V3.EntityRecognitionSkill"
         self.categories = categories
         self.default_language_code = default_language_code
         self.minimum_precision = minimum_precision
         self.model_version = model_version
 
 
 class FieldMapping(_serialization.Model):
@@ -2805,16 +2837,16 @@
 
     def __init__(
         self,
         *,
         source_field_name: str,
         target_field_name: Optional[str] = None,
         mapping_function: Optional["_models.FieldMappingFunction"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword source_field_name: The name of the field in the data source. Required.
         :paramtype source_field_name: str
         :keyword target_field_name: The name of the target field in the index. Same as the source field
          name by default.
         :paramtype target_field_name: str
         :keyword mapping_function: A function to apply to each source field value before indexing.
@@ -2843,15 +2875,15 @@
     }
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "parameters": {"key": "parameters", "type": "{object}"},
     }
 
-    def __init__(self, *, name: str, parameters: Optional[Dict[str, JSON]] = None, **kwargs):
+    def __init__(self, *, name: str, parameters: Optional[Dict[str, JSON]] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the field mapping function. Required.
         :paramtype name: str
         :keyword parameters: A dictionary of parameter name/value pairs to pass to the function. Each
          value must be of a primitive type.
         :paramtype parameters: dict[str, JSON]
         """
@@ -2899,31 +2931,31 @@
     def __init__(
         self,
         *,
         field_name: str,
         boost: float,
         parameters: "_models.FreshnessScoringParameters",
         interpolation: Optional[Union[str, "_models.ScoringFunctionInterpolation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword field_name: The name of the field used as input to the scoring function. Required.
         :paramtype field_name: str
         :keyword boost: A multiplier for the raw score. Must be a positive number not equal to 1.0.
          Required.
         :paramtype boost: float
         :keyword interpolation: A value indicating how boosting will be interpolated across document
          scores; defaults to "Linear". Known values are: "linear", "constant", "quadratic", and
          "logarithmic".
         :paramtype interpolation: str or ~search_service_client.models.ScoringFunctionInterpolation
         :keyword parameters: Parameter values for the freshness scoring function. Required.
         :paramtype parameters: ~search_service_client.models.FreshnessScoringParameters
         """
         super().__init__(field_name=field_name, boost=boost, interpolation=interpolation, **kwargs)
-        self.type = "freshness"  # type: str
+        self.type: str = "freshness"
         self.parameters = parameters
 
 
 class FreshnessScoringParameters(_serialization.Model):
     """Provides parameter values to a freshness scoring function.
 
     All required parameters must be populated in order to send to Azure.
@@ -2937,26 +2969,27 @@
         "boosting_duration": {"required": True},
     }
 
     _attribute_map = {
         "boosting_duration": {"key": "boostingDuration", "type": "duration"},
     }
 
-    def __init__(self, *, boosting_duration: datetime.timedelta, **kwargs):
+    def __init__(self, *, boosting_duration: datetime.timedelta, **kwargs: Any) -> None:
         """
         :keyword boosting_duration: The expiration period after which boosting will stop for a
          particular document. Required.
         :paramtype boosting_duration: ~datetime.timedelta
         """
         super().__init__(**kwargs)
         self.boosting_duration = boosting_duration
 
 
 class GetIndexStatisticsResult(_serialization.Model):
-    """Statistics for a given index. Statistics are collected periodically and are not guaranteed to always be up-to-date.
+    """Statistics for a given index. Statistics are collected periodically and are not guaranteed to
+    always be up-to-date.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar document_count: The number of documents in the index. Required.
     :vartype document_count: int
@@ -2970,23 +3003,24 @@
     }
 
     _attribute_map = {
         "document_count": {"key": "documentCount", "type": "int"},
         "storage_size": {"key": "storageSize", "type": "int"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.document_count = None
         self.storage_size = None
 
 
 class HighWaterMarkChangeDetectionPolicy(DataChangeDetectionPolicy):
-    """Defines a data change detection policy that captures changes based on the value of a high water mark column.
+    """Defines a data change detection policy that captures changes based on the value of a high water
+    mark column.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the data change detection policy. Required.
     :vartype odata_type: str
     :ivar high_water_mark_column_name: The name of the high water mark column. Required.
     :vartype high_water_mark_column_name: str
@@ -2998,26 +3032,97 @@
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "high_water_mark_column_name": {"key": "highWaterMarkColumnName", "type": "str"},
     }
 
-    def __init__(self, *, high_water_mark_column_name: str, **kwargs):
+    def __init__(self, *, high_water_mark_column_name: str, **kwargs: Any) -> None:
         """
         :keyword high_water_mark_column_name: The name of the high water mark column. Required.
         :paramtype high_water_mark_column_name: str
         """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy"
         self.high_water_mark_column_name = high_water_mark_column_name
 
 
+class HnswParameters(_serialization.Model):
+    """Contains the parameters specific to hnsw algorithm.
+
+    :ivar m: The number of bi-directional links created for every new element during construction.
+     Increasing this parameter value may improve recall and reduce retrieval times for datasets with
+     high intrinsic dimensionality at the expense of increased memory consumption and longer
+     indexing time.
+    :vartype m: int
+    :ivar ef_construction: The size of the dynamic list containing the nearest neighbors, which is
+     used during index time. Increasing this parameter may improve index quality, at the expense of
+     increased indexing time. At a certain point, increasing this parameter leads to diminishing
+     returns.
+    :vartype ef_construction: int
+    :ivar ef_search: The size of the dynamic list containing the nearest neighbors, which is used
+     during search time. Increasing this parameter may improve search results, at the expense of
+     slower search. Increasing this parameter leads to diminishing returns..
+    :vartype ef_search: int
+    :ivar metric: The similarity metric to use for vector comparisons. Known values are: "cosine",
+     "euclidean", and "dotProduct".
+    :vartype metric: str or ~search_service_client.models.VectorSearchAlgorithmMetric
+    """
+
+    _validation = {
+        "m": {"maximum": 10, "minimum": 4},
+        "ef_construction": {"maximum": 1000, "minimum": 100},
+        "ef_search": {"maximum": 1000, "minimum": 100},
+    }
+
+    _attribute_map = {
+        "m": {"key": "m", "type": "int"},
+        "ef_construction": {"key": "efConstruction", "type": "int"},
+        "ef_search": {"key": "efSearch", "type": "int"},
+        "metric": {"key": "metric", "type": "str"},
+    }
+
+    def __init__(
+        self,
+        *,
+        m: int = 4,
+        ef_construction: int = 400,
+        ef_search: int = 500,
+        metric: Optional[Union[str, "_models.VectorSearchAlgorithmMetric"]] = None,
+        **kwargs: Any
+    ) -> None:
+        """
+        :keyword m: The number of bi-directional links created for every new element during
+         construction. Increasing this parameter value may improve recall and reduce retrieval times for
+         datasets with high intrinsic dimensionality at the expense of increased memory consumption and
+         longer indexing time.
+        :paramtype m: int
+        :keyword ef_construction: The size of the dynamic list containing the nearest neighbors, which
+         is used during index time. Increasing this parameter may improve index quality, at the expense
+         of increased indexing time. At a certain point, increasing this parameter leads to diminishing
+         returns.
+        :paramtype ef_construction: int
+        :keyword ef_search: The size of the dynamic list containing the nearest neighbors, which is
+         used during search time. Increasing this parameter may improve search results, at the expense
+         of slower search. Increasing this parameter leads to diminishing returns..
+        :paramtype ef_search: int
+        :keyword metric: The similarity metric to use for vector comparisons. Known values are:
+         "cosine", "euclidean", and "dotProduct".
+        :paramtype metric: str or ~search_service_client.models.VectorSearchAlgorithmMetric
+        """
+        super().__init__(**kwargs)
+        self.m = m
+        self.ef_construction = ef_construction
+        self.ef_search = ef_search
+        self.metric = metric
+
+
 class ImageAnalysisSkill(SearchIndexerSkill):
-    """A skill that analyzes image files. It extracts a rich set of visual features based on the image content.
+    """A skill that analyzes image files. It extracts a rich set of visual features based on the image
+    content.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -3072,16 +3177,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.ImageAnalysisSkillLanguage"]] = None,
         visual_features: Optional[List[Union[str, "_models.VisualFeature"]]] = None,
         details: Optional[List[Union[str, "_models.ImageDetail"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -3105,15 +3210,15 @@
          ~search_service_client.models.ImageAnalysisSkillLanguage
         :keyword visual_features: A list of visual features.
         :paramtype visual_features: list[str or ~search_service_client.models.VisualFeature]
         :keyword details: A string indicating which domain-specific details to return.
         :paramtype details: list[str or ~search_service_client.models.ImageDetail]
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Vision.ImageAnalysisSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Vision.ImageAnalysisSkill"
         self.default_language_code = default_language_code
         self.visual_features = visual_features
         self.details = details
 
 
 class IndexerCurrentState(_serialization.Model):
     """Represents all of the state that defines and dictates the indexer's current execution.
@@ -3161,15 +3266,15 @@
         "all_docs_final_change_tracking_state": {"key": "allDocsFinalChangeTrackingState", "type": "str"},
         "reset_docs_initial_change_tracking_state": {"key": "resetDocsInitialChangeTrackingState", "type": "str"},
         "reset_docs_final_change_tracking_state": {"key": "resetDocsFinalChangeTrackingState", "type": "str"},
         "reset_document_keys": {"key": "resetDocumentKeys", "type": "[str]"},
         "reset_datasource_document_ids": {"key": "resetDatasourceDocumentIds", "type": "[str]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.mode = None
         self.all_docs_initial_change_tracking_state = None
         self.all_docs_final_change_tracking_state = None
         self.reset_docs_initial_change_tracking_state = None
         self.reset_docs_final_change_tracking_state = None
@@ -3241,15 +3346,15 @@
         "warnings": {"key": "warnings", "type": "[SearchIndexerWarning]"},
         "item_count": {"key": "itemsProcessed", "type": "int"},
         "failed_item_count": {"key": "itemsFailed", "type": "int"},
         "initial_tracking_state": {"key": "initialTrackingState", "type": "str"},
         "final_tracking_state": {"key": "finalTrackingState", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.status = None
         self.status_detail = None
         self.current_state = None
         self.error_message = None
         self.start_time = None
@@ -3289,16 +3394,16 @@
     def __init__(
         self,
         *,
         batch_size: Optional[int] = None,
         max_failed_items: int = 0,
         max_failed_items_per_batch: int = 0,
         configuration: Optional["_models.IndexingParametersConfiguration"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword batch_size: The number of items that are read from the data source and indexed as a
          single batch in order to improve performance. The default depends on the data source type.
         :paramtype batch_size: int
         :keyword max_failed_items: The maximum number of items that can fail indexing for indexer
          execution to still be considered successful. -1 means no limit. Default is 0.
         :paramtype max_failed_items: int
@@ -3313,15 +3418,16 @@
         self.batch_size = batch_size
         self.max_failed_items = max_failed_items
         self.max_failed_items_per_batch = max_failed_items_per_batch
         self.configuration = configuration
 
 
 class IndexingParametersConfiguration(_serialization.Model):  # pylint: disable=too-many-instance-attributes
-    """A dictionary of indexer-specific configuration properties. Each name is the name of a specific property. Each value must be of a primitive type.
+    """A dictionary of indexer-specific configuration properties. Each name is the name of a specific
+    property. Each value must be of a primitive type.
 
     :ivar additional_properties: Unmatched properties from the message are deserialized to this
      collection.
     :vartype additional_properties: dict[str, JSON]
     :ivar parsing_mode: Represents the parsing mode for indexing from an Azure blob data source.
      Known values are: "default", "text", "delimitedText", "json", "jsonArray", and "jsonLines".
     :vartype parsing_mode: str or ~search_service_client.models.BlobIndexerParsingMode
@@ -3425,16 +3531,16 @@
         document_root: Optional[str] = None,
         data_to_extract: Union[str, "_models.BlobIndexerDataToExtract"] = "contentAndMetadata",
         image_action: Union[str, "_models.BlobIndexerImageAction"] = "none",
         allow_skillset_to_read_file_data: bool = False,
         pdf_text_rotation_algorithm: Union[str, "_models.BlobIndexerPDFTextRotationAlgorithm"] = "none",
         execution_environment: Union[str, "_models.IndexerExecutionEnvironment"] = "standard",
         query_timeout: str = "00:05:00",
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, JSON]
         :keyword parsing_mode: Represents the parsing mode for indexing from an Azure blob data source.
          Known values are: "default", "text", "delimitedText", "json", "jsonArray", and "jsonLines".
         :paramtype parsing_mode: str or ~search_service_client.models.BlobIndexerParsingMode
@@ -3534,15 +3640,17 @@
     }
 
     _attribute_map = {
         "interval": {"key": "interval", "type": "duration"},
         "start_time": {"key": "startTime", "type": "iso-8601"},
     }
 
-    def __init__(self, *, interval: datetime.timedelta, start_time: Optional[datetime.datetime] = None, **kwargs):
+    def __init__(
+        self, *, interval: datetime.timedelta, start_time: Optional[datetime.datetime] = None, **kwargs: Any
+    ) -> None:
         """
         :keyword interval: The interval of time between indexer executions. Required.
         :paramtype interval: ~datetime.timedelta
         :keyword start_time: The time when an indexer should start running.
         :paramtype start_time: ~datetime.datetime
         """
         super().__init__(**kwargs)
@@ -3579,16 +3687,16 @@
     def __init__(
         self,
         *,
         name: str,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the input. Required.
         :paramtype name: str
         :keyword source: The source of the input.
         :paramtype source: str
         :keyword source_context: The source context used for selecting recursive inputs.
         :paramtype source_context: str
@@ -3599,15 +3707,16 @@
         self.name = name
         self.source = source
         self.source_context = source_context
         self.inputs = inputs
 
 
 class KeepTokenFilter(TokenFilter):
-    """A token filter that only keeps tokens with text contained in a specified list of words. This token filter is implemented using Apache Lucene.
+    """A token filter that only keeps tokens with text contained in a specified list of words. This
+    token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -3629,28 +3738,28 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "keep_words": {"key": "keepWords", "type": "[str]"},
         "lower_case_keep_words": {"key": "keepWordsCase", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, keep_words: List[str], lower_case_keep_words: bool = False, **kwargs):
+    def __init__(self, *, name: str, keep_words: List[str], lower_case_keep_words: bool = False, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword keep_words: The list of words to keep. Required.
         :paramtype keep_words: list[str]
         :keyword lower_case_keep_words: A value indicating whether to lower case all words first.
          Default is false.
         :paramtype lower_case_keep_words: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.KeepTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.KeepTokenFilter"
         self.keep_words = keep_words
         self.lower_case_keep_words = lower_case_keep_words
 
 
 class KeyPhraseExtractionSkill(SearchIndexerSkill):
     """A skill that uses text analytics for key phrase extraction.
 
@@ -3713,16 +3822,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.KeyPhraseExtractionSkillLanguage"]] = None,
         max_key_phrase_count: Optional[int] = None,
         model_version: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -3747,15 +3856,15 @@
         :paramtype max_key_phrase_count: int
         :keyword model_version: The version of the model to use when calling the Text Analytics
          service. It will default to the latest available when not specified. We recommend you do not
          specify this value unless absolutely necessary.
         :paramtype model_version: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.KeyPhraseExtractionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.KeyPhraseExtractionSkill"
         self.default_language_code = default_language_code
         self.max_key_phrase_count = max_key_phrase_count
         self.model_version = model_version
 
 
 class KeywordMarkerTokenFilter(TokenFilter):
     """Marks terms as keywords. This token filter is implemented using Apache Lucene.
@@ -3784,28 +3893,28 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "keywords": {"key": "keywords", "type": "[str]"},
         "ignore_case": {"key": "ignoreCase", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, keywords: List[str], ignore_case: bool = False, **kwargs):
+    def __init__(self, *, name: str, keywords: List[str], ignore_case: bool = False, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword keywords: A list of words to mark as keywords. Required.
         :paramtype keywords: list[str]
         :keyword ignore_case: A value indicating whether to ignore case. If true, all words are
          converted to lower case first. Default is false.
         :paramtype ignore_case: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.KeywordMarkerTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.KeywordMarkerTokenFilter"
         self.keywords = keywords
         self.ignore_case = ignore_case
 
 
 class KeywordTokenizer(LexicalTokenizer):
     """Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
 
@@ -3828,25 +3937,25 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "buffer_size": {"key": "bufferSize", "type": "int"},
     }
 
-    def __init__(self, *, name: str, buffer_size: int = 256, **kwargs):
+    def __init__(self, *, name: str, buffer_size: int = 256, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword buffer_size: The read buffer size in bytes. Default is 256.
         :paramtype buffer_size: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.KeywordTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.KeywordTokenizer"
         self.buffer_size = buffer_size
 
 
 class KeywordTokenizerV2(LexicalTokenizer):
     """Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
@@ -3870,31 +3979,33 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 256, **kwargs):
+    def __init__(self, *, name: str, max_token_length: int = 256, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 256. Tokens longer than the
          maximum length are split. The maximum token length that can be used is 300 characters.
         :paramtype max_token_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.KeywordTokenizerV2"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.KeywordTokenizerV2"
         self.max_token_length = max_token_length
 
 
 class LanguageDetectionSkill(SearchIndexerSkill):
-    """A skill that detects the language of input text and reports a single language code for every document submitted on the request. The language code is paired with a score indicating the confidence of the analysis.
+    """A skill that detects the language of input text and reports a single language code for every
+    document submitted on the request. The language code is paired with a score indicating the
+    confidence of the analysis.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -3944,16 +4055,16 @@
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_country_hint: Optional[str] = None,
         model_version: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -3973,21 +4084,22 @@
         :paramtype default_country_hint: str
         :keyword model_version: The version of the model to use when calling the Text Analytics
          service. It will default to the latest available when not specified. We recommend you do not
          specify this value unless absolutely necessary.
         :paramtype model_version: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.LanguageDetectionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.LanguageDetectionSkill"
         self.default_country_hint = default_country_hint
         self.model_version = model_version
 
 
 class LengthTokenFilter(TokenFilter):
-    """Removes words that are too long or too short. This token filter is implemented using Apache Lucene.
+    """Removes words that are too long or too short. This token filter is implemented using Apache
+    Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4010,34 +4122,35 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "min_length": {"key": "min", "type": "int"},
         "max_length": {"key": "max", "type": "int"},
     }
 
-    def __init__(self, *, name: str, min_length: int = 0, max_length: int = 300, **kwargs):
+    def __init__(self, *, name: str, min_length: int = 0, max_length: int = 300, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword min_length: The minimum length in characters. Default is 0. Maximum is 300. Must be
          less than the value of max.
         :paramtype min_length: int
         :keyword max_length: The maximum length in characters. Default and maximum is 300.
         :paramtype max_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.LengthTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.LengthTokenFilter"
         self.min_length = min_length
         self.max_length = max_length
 
 
 class LimitTokenFilter(TokenFilter):
-    """Limits the number of tokens while indexing. This token filter is implemented using Apache Lucene.
+    """Limits the number of tokens while indexing. This token filter is implemented using Apache
+    Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4058,34 +4171,35 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_count": {"key": "maxTokenCount", "type": "int"},
         "consume_all_tokens": {"key": "consumeAllTokens", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, max_token_count: int = 1, consume_all_tokens: bool = False, **kwargs):
+    def __init__(self, *, name: str, max_token_count: int = 1, consume_all_tokens: bool = False, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword max_token_count: The maximum number of tokens to produce. Default is 1.
         :paramtype max_token_count: int
         :keyword consume_all_tokens: A value indicating whether all tokens from the input must be
          consumed even if maxTokenCount is reached. Default is false.
         :paramtype consume_all_tokens: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.LimitTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.LimitTokenFilter"
         self.max_token_count = max_token_count
         self.consume_all_tokens = consume_all_tokens
 
 
 class ListAliasesResult(_serialization.Model):
-    """Response from a List Aliases request. If successful, it includes the associated index mappings for all aliases.
+    """Response from a List Aliases request. If successful, it includes the associated index mappings
+    for all aliases.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar aliases: The aliases in the Search service. Required.
     :vartype aliases: list[~search_service_client.models.SearchAlias]
@@ -4095,22 +4209,23 @@
         "aliases": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "aliases": {"key": "value", "type": "[SearchAlias]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.aliases = None
 
 
 class ListDataSourcesResult(_serialization.Model):
-    """Response from a List Datasources request. If successful, it includes the full definitions of all datasources.
+    """Response from a List Datasources request. If successful, it includes the full definitions of
+    all datasources.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar data_sources: The datasources in the Search service. Required.
     :vartype data_sources: list[~search_service_client.models.SearchIndexerDataSource]
@@ -4120,22 +4235,23 @@
         "data_sources": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "data_sources": {"key": "value", "type": "[SearchIndexerDataSource]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.data_sources = None
 
 
 class ListIndexersResult(_serialization.Model):
-    """Response from a List Indexers request. If successful, it includes the full definitions of all indexers.
+    """Response from a List Indexers request. If successful, it includes the full definitions of all
+    indexers.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar indexers: The indexers in the Search service. Required.
     :vartype indexers: list[~search_service_client.models.SearchIndexer]
@@ -4145,22 +4261,23 @@
         "indexers": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "indexers": {"key": "value", "type": "[SearchIndexer]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.indexers = None
 
 
 class ListIndexesResult(_serialization.Model):
-    """Response from a List Indexes request. If successful, it includes the full definitions of all indexes.
+    """Response from a List Indexes request. If successful, it includes the full definitions of all
+    indexes.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar indexes: The indexes in the Search service. Required.
     :vartype indexes: list[~search_service_client.models.SearchIndex]
@@ -4170,22 +4287,23 @@
         "indexes": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "indexes": {"key": "value", "type": "[SearchIndex]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.indexes = None
 
 
 class ListSkillsetsResult(_serialization.Model):
-    """Response from a list skillset request. If successful, it includes the full definitions of all skillsets.
+    """Response from a list skillset request. If successful, it includes the full definitions of all
+    skillsets.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar skillsets: The skillsets defined in the Search service. Required.
     :vartype skillsets: list[~search_service_client.models.SearchIndexerSkillset]
@@ -4195,22 +4313,23 @@
         "skillsets": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "skillsets": {"key": "value", "type": "[SearchIndexerSkillset]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.skillsets = None
 
 
 class ListSynonymMapsResult(_serialization.Model):
-    """Response from a List SynonymMaps request. If successful, it includes the full definitions of all synonym maps.
+    """Response from a List SynonymMaps request. If successful, it includes the full definitions of
+    all synonym maps.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar synonym_maps: The synonym maps in the Search service. Required.
     :vartype synonym_maps: list[~search_service_client.models.SynonymMap]
@@ -4220,22 +4339,23 @@
         "synonym_maps": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "synonym_maps": {"key": "value", "type": "[SynonymMap]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.synonym_maps = None
 
 
 class LuceneStandardAnalyzer(LexicalAnalyzer):
-    """Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop filter.
+    """Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop
+    filter.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the analyzer. Required.
     :vartype odata_type: str
     :ivar name: The name of the analyzer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4257,34 +4377,37 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
         "stopwords": {"key": "stopwords", "type": "[str]"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 255, stopwords: Optional[List[str]] = None, **kwargs):
+    def __init__(
+        self, *, name: str, max_token_length: int = 255, stopwords: Optional[List[str]] = None, **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the analyzer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 255. Tokens longer than the
          maximum length are split. The maximum token length that can be used is 300 characters.
         :paramtype max_token_length: int
         :keyword stopwords: A list of stopwords.
         :paramtype stopwords: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StandardAnalyzer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StandardAnalyzer"
         self.max_token_length = max_token_length
         self.stopwords = stopwords
 
 
 class LuceneStandardTokenizer(LexicalTokenizer):
-    """Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using Apache Lucene.
+    """Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4302,31 +4425,32 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs):
+    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 255. Tokens longer than the
          maximum length are split.
         :paramtype max_token_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StandardTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StandardTokenizer"
         self.max_token_length = max_token_length
 
 
 class LuceneStandardTokenizerV2(LexicalTokenizer):
-    """Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using Apache Lucene.
+    """Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4345,26 +4469,26 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs):
+    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 255. Tokens longer than the
          maximum length are split. The maximum token length that can be used is 300 characters.
         :paramtype max_token_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StandardTokenizerV2"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StandardTokenizerV2"
         self.max_token_length = max_token_length
 
 
 class MagnitudeScoringFunction(ScoringFunction):
     """Defines a function that boosts scores based on the magnitude of a numeric field.
 
     All required parameters must be populated in order to send to Azure.
@@ -4403,31 +4527,31 @@
     def __init__(
         self,
         *,
         field_name: str,
         boost: float,
         parameters: "_models.MagnitudeScoringParameters",
         interpolation: Optional[Union[str, "_models.ScoringFunctionInterpolation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword field_name: The name of the field used as input to the scoring function. Required.
         :paramtype field_name: str
         :keyword boost: A multiplier for the raw score. Must be a positive number not equal to 1.0.
          Required.
         :paramtype boost: float
         :keyword interpolation: A value indicating how boosting will be interpolated across document
          scores; defaults to "Linear". Known values are: "linear", "constant", "quadratic", and
          "logarithmic".
         :paramtype interpolation: str or ~search_service_client.models.ScoringFunctionInterpolation
         :keyword parameters: Parameter values for the magnitude scoring function. Required.
         :paramtype parameters: ~search_service_client.models.MagnitudeScoringParameters
         """
         super().__init__(field_name=field_name, boost=boost, interpolation=interpolation, **kwargs)
-        self.type = "magnitude"  # type: str
+        self.type: str = "magnitude"
         self.parameters = parameters
 
 
 class MagnitudeScoringParameters(_serialization.Model):
     """Provides parameter values to a magnitude scoring function.
 
     All required parameters must be populated in order to send to Azure.
@@ -4454,16 +4578,16 @@
 
     def __init__(
         self,
         *,
         boosting_range_start: float,
         boosting_range_end: float,
         should_boost_beyond_range_by_constant: Optional[bool] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword boosting_range_start: The field value at which boosting starts. Required.
         :paramtype boosting_range_start: float
         :keyword boosting_range_end: The field value at which boosting ends. Required.
         :paramtype boosting_range_end: float
         :keyword should_boost_beyond_range_by_constant: A value indicating whether to apply a constant
          boost for field values beyond the range end value; default is false.
@@ -4472,15 +4596,17 @@
         super().__init__(**kwargs)
         self.boosting_range_start = boosting_range_start
         self.boosting_range_end = boosting_range_end
         self.should_boost_beyond_range_by_constant = should_boost_beyond_range_by_constant
 
 
 class MappingCharFilter(CharFilter):
-    """A character filter that applies mappings defined with the mappings option. Matching is greedy (longest pattern matching at a given point wins). Replacement is allowed to be the empty string. This character filter is implemented using Apache Lucene.
+    """A character filter that applies mappings defined with the mappings option. Matching is greedy
+    (longest pattern matching at a given point wins). Replacement is allowed to be the empty
+    string. This character filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the char filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the char filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4499,31 +4625,32 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "mappings": {"key": "mappings", "type": "[str]"},
     }
 
-    def __init__(self, *, name: str, mappings: List[str], **kwargs):
+    def __init__(self, *, name: str, mappings: List[str], **kwargs: Any) -> None:
         """
         :keyword name: The name of the char filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword mappings: A list of mappings of the following format: "a=>b" (all occurrences of the
          character "a" will be replaced with character "b"). Required.
         :paramtype mappings: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.MappingCharFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.MappingCharFilter"
         self.mappings = mappings
 
 
 class MergeSkill(SearchIndexerSkill):
-    """A skill for merging two or more strings into a single unified string, with an optional user-defined delimiter separating each component part.
+    """A skill for merging two or more strings into a single unified string, with an optional
+    user-defined delimiter separating each component part.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -4572,16 +4699,16 @@
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         insert_pre_tag: str = " ",
         insert_post_tag: str = " ",
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -4600,15 +4727,15 @@
          an empty space.
         :paramtype insert_pre_tag: str
         :keyword insert_post_tag: The tag indicates the end of the merged text. By default, the tag is
          an empty space.
         :paramtype insert_post_tag: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.MergeSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.MergeSkill"
         self.insert_pre_tag = insert_pre_tag
         self.insert_post_tag = insert_post_tag
 
 
 class MicrosoftLanguageStemmingTokenizer(LexicalTokenizer):
     """Divides text using language-specific rules and reduces words to their base forms.
 
@@ -4655,16 +4782,16 @@
     def __init__(
         self,
         *,
         name: str,
         max_token_length: int = 255,
         is_search_tokenizer: bool = False,
         language: Optional[Union[str, "_models.MicrosoftStemmingTokenizerLanguage"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Tokens longer than the maximum length are
          split. Maximum token length that can be used is 300 characters. Tokens longer than 300
@@ -4680,15 +4807,15 @@
          "icelandic", "indonesian", "italian", "kannada", "latvian", "lithuanian", "malay", "malayalam",
          "marathi", "norwegianBokmaal", "polish", "portuguese", "portugueseBrazilian", "punjabi",
          "romanian", "russian", "serbianCyrillic", "serbianLatin", "slovak", "slovenian", "spanish",
          "swedish", "tamil", "telugu", "turkish", "ukrainian", and "urdu".
         :paramtype language: str or ~search_service_client.models.MicrosoftStemmingTokenizerLanguage
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer"
         self.max_token_length = max_token_length
         self.is_search_tokenizer = is_search_tokenizer
         self.language = language
 
 
 class MicrosoftLanguageTokenizer(LexicalTokenizer):
     """Divides text using language-specific rules.
@@ -4736,16 +4863,16 @@
     def __init__(
         self,
         *,
         name: str,
         max_token_length: int = 255,
         is_search_tokenizer: bool = False,
         language: Optional[Union[str, "_models.MicrosoftTokenizerLanguage"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Tokens longer than the maximum length are
          split. Maximum token length that can be used is 300 characters. Tokens longer than 300
@@ -4761,15 +4888,15 @@
          "indonesian", "italian", "japanese", "kannada", "korean", "malay", "malayalam", "marathi",
          "norwegianBokmaal", "polish", "portuguese", "portugueseBrazilian", "punjabi", "romanian",
          "russian", "serbianCyrillic", "serbianLatin", "slovenian", "spanish", "swedish", "tamil",
          "telugu", "thai", "ukrainian", "urdu", and "vietnamese".
         :paramtype language: str or ~search_service_client.models.MicrosoftTokenizerLanguage
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer"
         self.max_token_length = max_token_length
         self.is_search_tokenizer = is_search_tokenizer
         self.language = language
 
 
 class NGramTokenFilter(TokenFilter):
     """Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
@@ -4797,28 +4924,28 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "min_gram": {"key": "minGram", "type": "int"},
         "max_gram": {"key": "maxGram", "type": "int"},
     }
 
-    def __init__(self, *, name: str, min_gram: int = 1, max_gram: int = 2, **kwargs):
+    def __init__(self, *, name: str, min_gram: int = 1, max_gram: int = 2, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Must be less than the value of
          maxGram.
         :paramtype min_gram: int
         :keyword max_gram: The maximum n-gram length. Default is 2.
         :paramtype max_gram: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.NGramTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.NGramTokenFilter"
         self.min_gram = min_gram
         self.max_gram = max_gram
 
 
 class NGramTokenFilterV2(TokenFilter):
     """Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
 
@@ -4847,34 +4974,35 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "min_gram": {"key": "minGram", "type": "int"},
         "max_gram": {"key": "maxGram", "type": "int"},
     }
 
-    def __init__(self, *, name: str, min_gram: int = 1, max_gram: int = 2, **kwargs):
+    def __init__(self, *, name: str, min_gram: int = 1, max_gram: int = 2, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than
          the value of maxGram.
         :paramtype min_gram: int
         :keyword max_gram: The maximum n-gram length. Default is 2. Maximum is 300.
         :paramtype max_gram: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.NGramTokenFilterV2"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.NGramTokenFilterV2"
         self.min_gram = min_gram
         self.max_gram = max_gram
 
 
 class NGramTokenizer(LexicalTokenizer):
-    """Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.
+    """Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -4907,31 +5035,31 @@
     def __init__(
         self,
         *,
         name: str,
         min_gram: int = 1,
         max_gram: int = 2,
         token_chars: Optional[List[Union[str, "_models.TokenCharacterKind"]]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword min_gram: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than
          the value of maxGram.
         :paramtype min_gram: int
         :keyword max_gram: The maximum n-gram length. Default is 2. Maximum is 300.
         :paramtype max_gram: int
         :keyword token_chars: Character classes to keep in the tokens.
         :paramtype token_chars: list[str or ~search_service_client.models.TokenCharacterKind]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.NGramTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.NGramTokenizer"
         self.min_gram = min_gram
         self.max_gram = max_gram
         self.token_chars = token_chars
 
 
 class OcrSkill(SearchIndexerSkill):
     """A skill that extracts text from image files.
@@ -4965,15 +5093,15 @@
      "ia", "iu", "ga", "it", "ja", "Jns", "jv", "kea", "kac", "xnr", "krc", "kaa-cyrl", "kaa",
      "csb", "kk-cyrl", "kk-latn", "klr", "kha", "quc", "ko", "kfq", "kpy", "kos", "kum", "ku-arab",
      "ku-latn", "kru", "ky", "lkt", "la", "lt", "dsb", "smj", "lb", "bfz", "ms", "mt", "kmj", "gv",
      "mi", "mr", "mn", "cnr-cyrl", "cnr-latn", "nap", "ne", "niu", "nog", "sme", "nb", "no", "oc",
      "os", "ps", "fa", "pl", "pt", "pa", "ksh", "ro", "rm", "ru", "sck", "sm", "sa", "sat", "sco",
      "gd", "sr", "sr-Cyrl", "sr-Latn", "xsr", "srx", "sms", "sk", "sl", "so", "sma", "es", "sw",
      "sv", "tg", "tt", "tet", "thf", "to", "tr", "tk", "tyv", "hsb", "ur", "ug", "uz-arab",
-     "uz-cyrl", "uz", "vo", "wae", "cy", "fy", "yua", "za", "zu", and "unk".
+     "uz-cyrl", "uz", "vo", "wae", "cy", "fy", "yua", "za", "zu", "unk", and "is".
     :vartype default_language_code: str or ~search_service_client.models.OcrSkillLanguage
     :ivar should_detect_orientation: A value indicating to turn orientation detection on or not.
      Default is false.
     :vartype should_detect_orientation: bool
     :ivar line_ending: Defines the sequence of characters to use between the lines of text
      recognized by the OCR skill. The default value is "space". Known values are: "space",
      "carriageReturn", "lineFeed", and "carriageReturnLineFeed".
@@ -5005,16 +5133,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.OcrSkillLanguage"]] = None,
         should_detect_orientation: bool = False,
         line_ending: Optional[Union[str, "_models.LineEnding"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -5038,26 +5166,26 @@
          "ia", "iu", "ga", "it", "ja", "Jns", "jv", "kea", "kac", "xnr", "krc", "kaa-cyrl", "kaa",
          "csb", "kk-cyrl", "kk-latn", "klr", "kha", "quc", "ko", "kfq", "kpy", "kos", "kum", "ku-arab",
          "ku-latn", "kru", "ky", "lkt", "la", "lt", "dsb", "smj", "lb", "bfz", "ms", "mt", "kmj", "gv",
          "mi", "mr", "mn", "cnr-cyrl", "cnr-latn", "nap", "ne", "niu", "nog", "sme", "nb", "no", "oc",
          "os", "ps", "fa", "pl", "pt", "pa", "ksh", "ro", "rm", "ru", "sck", "sm", "sa", "sat", "sco",
          "gd", "sr", "sr-Cyrl", "sr-Latn", "xsr", "srx", "sms", "sk", "sl", "so", "sma", "es", "sw",
          "sv", "tg", "tt", "tet", "thf", "to", "tr", "tk", "tyv", "hsb", "ur", "ug", "uz-arab",
-         "uz-cyrl", "uz", "vo", "wae", "cy", "fy", "yua", "za", "zu", and "unk".
+         "uz-cyrl", "uz", "vo", "wae", "cy", "fy", "yua", "za", "zu", "unk", and "is".
         :paramtype default_language_code: str or ~search_service_client.models.OcrSkillLanguage
         :keyword should_detect_orientation: A value indicating to turn orientation detection on or not.
          Default is false.
         :paramtype should_detect_orientation: bool
         :keyword line_ending: Defines the sequence of characters to use between the lines of text
          recognized by the OCR skill. The default value is "space". Known values are: "space",
          "carriageReturn", "lineFeed", and "carriageReturnLineFeed".
         :paramtype line_ending: str or ~search_service_client.models.LineEnding
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Vision.OcrSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Vision.OcrSkill"
         self.default_language_code = default_language_code
         self.should_detect_orientation = should_detect_orientation
         self.line_ending = line_ending
 
 
 class OutputFieldMappingEntry(_serialization.Model):
     """Output field mapping for a skill.
@@ -5075,15 +5203,15 @@
     }
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "target_name": {"key": "targetName", "type": "str"},
     }
 
-    def __init__(self, *, name: str, target_name: Optional[str] = None, **kwargs):
+    def __init__(self, *, name: str, target_name: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the output defined by the skill. Required.
         :paramtype name: str
         :keyword target_name: The target name of the output. It is optional and default to name.
         :paramtype target_name: str
         """
         super().__init__(**kwargs)
@@ -5136,16 +5264,16 @@
         *,
         name: str,
         delimiter: str = "/",
         replacement: str = "/",
         max_token_length: int = 300,
         reverse_token_order: bool = False,
         number_of_tokens_to_skip: int = 0,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword delimiter: The delimiter character to use. Default is "/".
         :paramtype delimiter: str
@@ -5156,24 +5284,25 @@
         :keyword reverse_token_order: A value indicating whether to generate tokens in reverse order.
          Default is false.
         :paramtype reverse_token_order: bool
         :keyword number_of_tokens_to_skip: The number of initial tokens to skip. Default is 0.
         :paramtype number_of_tokens_to_skip: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PathHierarchyTokenizerV2"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PathHierarchyTokenizerV2"
         self.delimiter = delimiter
         self.replacement = replacement
         self.max_token_length = max_token_length
         self.reverse_token_order = reverse_token_order
         self.number_of_tokens_to_skip = number_of_tokens_to_skip
 
 
 class PatternAnalyzer(LexicalAnalyzer):
-    """Flexibly separates text into terms via a regular expression pattern. This analyzer is implemented using Apache Lucene.
+    """Flexibly separates text into terms via a regular expression pattern. This analyzer is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the analyzer. Required.
     :vartype odata_type: str
     :ivar name: The name of the analyzer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -5210,16 +5339,16 @@
         self,
         *,
         name: str,
         lower_case_terms: bool = True,
         pattern: str = "\W+",
         flags: Optional[Union[str, "_models.RegexFlags"]] = None,
         stopwords: Optional[List[str]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the analyzer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword lower_case_terms: A value indicating whether terms should be lower-cased. Default is
          true.
@@ -5230,23 +5359,24 @@
         :keyword flags: Regular expression flags. Known values are: "CANON_EQ", "CASE_INSENSITIVE",
          "COMMENTS", "DOTALL", "LITERAL", "MULTILINE", "UNICODE_CASE", and "UNIX_LINES".
         :paramtype flags: str or ~search_service_client.models.RegexFlags
         :keyword stopwords: A list of stopwords.
         :paramtype stopwords: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PatternAnalyzer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PatternAnalyzer"
         self.lower_case_terms = lower_case_terms
         self.pattern = pattern
         self.flags = flags
         self.stopwords = stopwords
 
 
 class PatternCaptureTokenFilter(TokenFilter):
-    """Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns. This token filter is implemented using Apache Lucene.
+    """Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns.
+    This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -5268,34 +5398,38 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "patterns": {"key": "patterns", "type": "[str]"},
         "preserve_original": {"key": "preserveOriginal", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, patterns: List[str], preserve_original: bool = True, **kwargs):
+    def __init__(self, *, name: str, patterns: List[str], preserve_original: bool = True, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword patterns: A list of patterns to match against each token. Required.
         :paramtype patterns: list[str]
         :keyword preserve_original: A value indicating whether to return the original token even if one
          of the patterns matches. Default is true.
         :paramtype preserve_original: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PatternCaptureTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PatternCaptureTokenFilter"
         self.patterns = patterns
         self.preserve_original = preserve_original
 
 
 class PatternReplaceCharFilter(CharFilter):
-    """A character filter that replaces characters in the input string. It uses a regular expression to identify character sequences to preserve and a replacement pattern to identify characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented using Apache Lucene.
+    """A character filter that replaces characters in the input string. It uses a regular expression
+    to identify character sequences to preserve and a replacement pattern to identify characters to
+    replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
+    replacement "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented
+    using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the char filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the char filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -5317,33 +5451,37 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "pattern": {"key": "pattern", "type": "str"},
         "replacement": {"key": "replacement", "type": "str"},
     }
 
-    def __init__(self, *, name: str, pattern: str, replacement: str, **kwargs):
+    def __init__(self, *, name: str, pattern: str, replacement: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the char filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword pattern: A regular expression pattern. Required.
         :paramtype pattern: str
         :keyword replacement: The replacement text. Required.
         :paramtype replacement: str
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PatternReplaceCharFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PatternReplaceCharFilter"
         self.pattern = pattern
         self.replacement = replacement
 
 
 class PatternReplaceTokenFilter(TokenFilter):
-    """A character filter that replaces characters in the input string. It uses a regular expression to identify character sequences to preserve and a replacement pattern to identify characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using Apache Lucene.
+    """A character filter that replaces characters in the input string. It uses a regular expression
+    to identify character sequences to preserve and a replacement pattern to identify characters to
+    replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
+    replacement "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -5365,33 +5503,34 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "pattern": {"key": "pattern", "type": "str"},
         "replacement": {"key": "replacement", "type": "str"},
     }
 
-    def __init__(self, *, name: str, pattern: str, replacement: str, **kwargs):
+    def __init__(self, *, name: str, pattern: str, replacement: str, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword pattern: A regular expression pattern. Required.
         :paramtype pattern: str
         :keyword replacement: The replacement text. Required.
         :paramtype replacement: str
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PatternReplaceTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PatternReplaceTokenFilter"
         self.pattern = pattern
         self.replacement = replacement
 
 
 class PatternTokenizer(LexicalTokenizer):
-    """Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is implemented using Apache Lucene.
+    """Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the tokenizer. Required.
     :vartype odata_type: str
     :ivar name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -5425,16 +5564,16 @@
     def __init__(
         self,
         *,
         name: str,
         pattern: str = "\W+",
         flags: Optional[Union[str, "_models.RegexFlags"]] = None,
         group: int = -1,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword pattern: A regular expression pattern to match token separators. Default is an
          expression that matches one or more non-word characters.
@@ -5444,15 +5583,15 @@
         :paramtype flags: str or ~search_service_client.models.RegexFlags
         :keyword group: The zero-based ordinal of the matching group in the regular expression pattern
          to extract into tokens. Use -1 if you want to use the entire pattern to split the input into
          tokens, irrespective of matching groups. Default is -1.
         :paramtype group: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PatternTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PatternTokenizer"
         self.pattern = pattern
         self.flags = flags
         self.group = group
 
 
 class PhoneticTokenFilter(TokenFilter):
     """Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.
@@ -5488,37 +5627,38 @@
 
     def __init__(
         self,
         *,
         name: str,
         encoder: Optional[Union[str, "_models.PhoneticEncoder"]] = None,
         replace_original_tokens: bool = True,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword encoder: The phonetic encoder to use. Default is "metaphone". Known values are:
          "metaphone", "doubleMetaphone", "soundex", "refinedSoundex", "caverphone1", "caverphone2",
          "cologne", "nysiis", "koelnerPhonetik", "haasePhonetik", and "beiderMorse".
         :paramtype encoder: str or ~search_service_client.models.PhoneticEncoder
         :keyword replace_original_tokens: A value indicating whether encoded tokens should replace
          original tokens. If false, encoded tokens are added as synonyms. Default is true.
         :paramtype replace_original_tokens: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.PhoneticTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.PhoneticTokenFilter"
         self.encoder = encoder
         self.replace_original_tokens = replace_original_tokens
 
 
 class PIIDetectionSkill(SearchIndexerSkill):  # pylint: disable=too-many-instance-attributes
-    """Using the Text Analytics API, extracts personal information from an input text and gives you the option of masking it.
+    """Using the Text Analytics API, extracts personal information from an input text and gives you
+    the option of masking it.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -5594,16 +5734,16 @@
         default_language_code: Optional[str] = None,
         minimum_precision: Optional[float] = None,
         masking_mode: Optional[Union[str, "_models.PIIDetectionSkillMaskingMode"]] = None,
         masking_character: Optional[str] = None,
         model_version: Optional[str] = None,
         pii_categories: Optional[List[str]] = None,
         domain: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -5637,26 +5777,27 @@
         :keyword pii_categories: A list of PII entity categories that should be extracted and masked.
         :paramtype pii_categories: list[str]
         :keyword domain: If specified, will set the PII domain to include only a subset of the entity
          categories. Possible values include: 'phi', 'none'. Default is 'none'.
         :paramtype domain: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.PIIDetectionSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.PIIDetectionSkill"
         self.default_language_code = default_language_code
         self.minimum_precision = minimum_precision
         self.masking_mode = masking_mode
         self.masking_character = masking_character
         self.model_version = model_version
         self.pii_categories = pii_categories
         self.domain = domain
 
 
 class PrioritizedFields(_serialization.Model):
-    """Describes the title, content, and keywords fields to be used for semantic ranking, captions, highlights, and answers.
+    """Describes the title, content, and keywords fields to be used for semantic ranking, captions,
+    highlights, and answers.
 
     :ivar title_field: Defines the title field to be used for semantic ranking, captions,
      highlights, and answers. If you don't have a title field in your index, leave this blank.
     :vartype title_field: ~search_service_client.models.SemanticField
     :ivar prioritized_content_fields: Defines the content fields to be used for semantic ranking,
      captions, highlights, and answers. For the best result, the selected fields should contain text
      in natural language form. The order of the fields in the array represents their priority.
@@ -5677,16 +5818,16 @@
 
     def __init__(
         self,
         *,
         title_field: Optional["_models.SemanticField"] = None,
         prioritized_content_fields: Optional[List["_models.SemanticField"]] = None,
         prioritized_keywords_fields: Optional[List["_models.SemanticField"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword title_field: Defines the title field to be used for semantic ranking, captions,
          highlights, and answers. If you don't have a title field in your index, leave this blank.
         :paramtype title_field: ~search_service_client.models.SemanticField
         :keyword prioritized_content_fields: Defines the content fields to be used for semantic
          ranking, captions, highlights, and answers. For the best result, the selected fields should
          contain text in natural language form. The order of the fields in the array represents their
@@ -5711,15 +5852,15 @@
     :vartype x_ms_client_request_id: str
     """
 
     _attribute_map = {
         "x_ms_client_request_id": {"key": "x-ms-client-request-id", "type": "str"},
     }
 
-    def __init__(self, *, x_ms_client_request_id: Optional[str] = None, **kwargs):
+    def __init__(self, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword x_ms_client_request_id: The tracking ID sent with the request to help with debugging.
         :paramtype x_ms_client_request_id: str
         """
         super().__init__(**kwargs)
         self.x_ms_client_request_id = x_ms_client_request_id
 
@@ -5740,15 +5881,15 @@
     }
 
     _attribute_map = {
         "usage": {"key": "usage", "type": "int"},
         "quota": {"key": "quota", "type": "int"},
     }
 
-    def __init__(self, *, usage: int, quota: Optional[int] = None, **kwargs):
+    def __init__(self, *, usage: int, quota: Optional[int] = None, **kwargs: Any) -> None:
         """
         :keyword usage: The resource usage amount. Required.
         :paramtype usage: int
         :keyword quota: The resource amount quota.
         :paramtype quota: int
         """
         super().__init__(**kwargs)
@@ -5788,16 +5929,16 @@
     def __init__(
         self,
         *,
         name: str,
         text_weights: Optional["_models.TextWeights"] = None,
         functions: Optional[List["_models.ScoringFunction"]] = None,
         function_aggregation: Optional[Union[str, "_models.ScoringFunctionAggregation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the scoring profile. Required.
         :paramtype name: str
         :keyword text_weights: Parameters that boost scoring based on text matches in certain index
          fields.
         :paramtype text_weights: ~search_service_client.models.TextWeights
         :keyword functions: The collection of functions that influence the scoring of documents.
@@ -5812,15 +5953,16 @@
         self.name = name
         self.text_weights = text_weights
         self.functions = functions
         self.function_aggregation = function_aggregation
 
 
 class SearchAlias(_serialization.Model):
-    """Represents an index alias, which describes a mapping from the alias name to an index. The alias name can be used in place of the index name for supported operations.
+    """Represents an index alias, which describes a mapping from the alias name to an index. The alias
+    name can be used in place of the index name for supported operations.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar name: The name of the alias. Required.
     :vartype name: str
     :ivar indexes: The name of the index this alias maps to. Only one index name may be specified.
      Required.
@@ -5836,15 +5978,15 @@
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "indexes": {"key": "indexes", "type": "[str]"},
         "e_tag": {"key": "@odata\\.etag", "type": "str"},
     }
 
-    def __init__(self, *, name: str, indexes: List[str], e_tag: Optional[str] = None, **kwargs):
+    def __init__(self, *, name: str, indexes: List[str], e_tag: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the alias. Required.
         :paramtype name: str
         :keyword indexes: The name of the index this alias maps to. Only one index name may be
          specified. Required.
         :paramtype indexes: list[str]
         :keyword e_tag: The ETag of the alias.
@@ -5879,33 +6021,34 @@
 
     _attribute_map = {
         "code": {"key": "code", "type": "str"},
         "message": {"key": "message", "type": "str"},
         "details": {"key": "details", "type": "[SearchError]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.code = None
         self.message = None
         self.details = None
 
 
 class SearchField(_serialization.Model):  # pylint: disable=too-many-instance-attributes
-    """Represents a field in an index definition, which describes the name, data type, and search behavior of a field.
+    """Represents a field in an index definition, which describes the name, data type, and search
+    behavior of a field.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar name: The name of the field, which must be unique within the fields collection of the
      index or parent field. Required.
     :vartype name: str
     :ivar type: The data type of the field. Required. Known values are: "Edm.String", "Edm.Int32",
-     "Edm.Int64", "Edm.Double", "Edm.Boolean", "Edm.DateTimeOffset", "Edm.GeographyPoint", and
-     "Edm.ComplexType".
+     "Edm.Int64", "Edm.Double", "Edm.Boolean", "Edm.DateTimeOffset", "Edm.GeographyPoint",
+     "Edm.ComplexType", and "Edm.Single".
     :vartype type: str or ~search_service_client.models.SearchFieldDataType
     :ivar key: A value indicating whether the field uniquely identifies documents in the index.
      Exactly one top-level field in each index must be chosen as the key field and it must be of
      type Edm.String. Key fields can be used to look up documents directly and update or delete
      specific documents. Default is false for simple fields and null for complex fields.
     :vartype key: bool
     :ivar retrievable: A value indicating whether the field can be returned in a search result. You
@@ -6018,28 +6161,34 @@
      "whitespace".
     :vartype index_analyzer: str or ~search_service_client.models.LexicalAnalyzerName
     :ivar normalizer: The name of the normalizer to use for the field. This option can be used only
      with fields with filterable, sortable, or facetable enabled. Once the normalizer is chosen, it
      cannot be changed for the field. Must be null for complex fields. Known values are:
      "asciifolding", "elision", "lowercase", "standard", and "uppercase".
     :vartype normalizer: str or ~search_service_client.models.LexicalNormalizerName
+    :ivar dimensions: The dimensionality of the vector field.
+    :vartype dimensions: int
+    :ivar vector_search_configuration: The name of the vector search algorithm configuration that
+     specifies the algorithm and optional parameters for searching the vector field.
+    :vartype vector_search_configuration: str
     :ivar synonym_maps: A list of the names of synonym maps to associate with this field. This
      option can be used only with searchable fields. Currently only one synonym map per field is
      supported. Assigning a synonym map to a field ensures that query terms targeting that field are
      expanded at query-time using the rules in the synonym map. This attribute can be changed on
      existing fields. Must be null or an empty collection for complex fields.
     :vartype synonym_maps: list[str]
     :ivar fields: A list of sub-fields if this is a field of type Edm.ComplexType or
      Collection(Edm.ComplexType). Must be null or empty for simple fields.
     :vartype fields: list[~search_service_client.models.SearchField]
     """
 
     _validation = {
         "name": {"required": True},
         "type": {"required": True},
+        "dimensions": {"maximum": 2048, "minimum": 2},
     }
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "type": {"key": "type", "type": "str"},
         "key": {"key": "key", "type": "bool"},
         "retrievable": {"key": "retrievable", "type": "bool"},
@@ -6047,14 +6196,16 @@
         "filterable": {"key": "filterable", "type": "bool"},
         "sortable": {"key": "sortable", "type": "bool"},
         "facetable": {"key": "facetable", "type": "bool"},
         "analyzer": {"key": "analyzer", "type": "str"},
         "search_analyzer": {"key": "searchAnalyzer", "type": "str"},
         "index_analyzer": {"key": "indexAnalyzer", "type": "str"},
         "normalizer": {"key": "normalizer", "type": "str"},
+        "dimensions": {"key": "dimensions", "type": "int"},
+        "vector_search_configuration": {"key": "vectorSearchConfiguration", "type": "str"},
         "synonym_maps": {"key": "synonymMaps", "type": "[str]"},
         "fields": {"key": "fields", "type": "[SearchField]"},
     }
 
     def __init__(
         self,
         *,
@@ -6066,25 +6217,27 @@
         filterable: Optional[bool] = None,
         sortable: Optional[bool] = None,
         facetable: Optional[bool] = None,
         analyzer: Optional[Union[str, "_models.LexicalAnalyzerName"]] = None,
         search_analyzer: Optional[Union[str, "_models.LexicalAnalyzerName"]] = None,
         index_analyzer: Optional[Union[str, "_models.LexicalAnalyzerName"]] = None,
         normalizer: Optional[Union[str, "_models.LexicalNormalizerName"]] = None,
+        dimensions: Optional[int] = None,
+        vector_search_configuration: Optional[str] = None,
         synonym_maps: Optional[List[str]] = None,
         fields: Optional[List["_models.SearchField"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the field, which must be unique within the fields collection of the
          index or parent field. Required.
         :paramtype name: str
         :keyword type: The data type of the field. Required. Known values are: "Edm.String",
          "Edm.Int32", "Edm.Int64", "Edm.Double", "Edm.Boolean", "Edm.DateTimeOffset",
-         "Edm.GeographyPoint", and "Edm.ComplexType".
+         "Edm.GeographyPoint", "Edm.ComplexType", and "Edm.Single".
         :paramtype type: str or ~search_service_client.models.SearchFieldDataType
         :keyword key: A value indicating whether the field uniquely identifies documents in the index.
          Exactly one top-level field in each index must be chosen as the key field and it must be of
          type Edm.String. Key fields can be used to look up documents directly and update or delete
          specific documents. Default is false for simple fields and null for complex fields.
         :paramtype key: bool
         :keyword retrievable: A value indicating whether the field can be returned in a search result.
@@ -6197,14 +6350,19 @@
          "whitespace".
         :paramtype index_analyzer: str or ~search_service_client.models.LexicalAnalyzerName
         :keyword normalizer: The name of the normalizer to use for the field. This option can be used
          only with fields with filterable, sortable, or facetable enabled. Once the normalizer is
          chosen, it cannot be changed for the field. Must be null for complex fields. Known values are:
          "asciifolding", "elision", "lowercase", "standard", and "uppercase".
         :paramtype normalizer: str or ~search_service_client.models.LexicalNormalizerName
+        :keyword dimensions: The dimensionality of the vector field.
+        :paramtype dimensions: int
+        :keyword vector_search_configuration: The name of the vector search algorithm configuration
+         that specifies the algorithm and optional parameters for searching the vector field.
+        :paramtype vector_search_configuration: str
         :keyword synonym_maps: A list of the names of synonym maps to associate with this field. This
          option can be used only with searchable fields. Currently only one synonym map per field is
          supported. Assigning a synonym map to a field ensures that query terms targeting that field are
          expanded at query-time using the rules in the synonym map. This attribute can be changed on
          existing fields. Must be null or an empty collection for complex fields.
         :paramtype synonym_maps: list[str]
         :keyword fields: A list of sub-fields if this is a field of type Edm.ComplexType or
@@ -6220,20 +6378,23 @@
         self.filterable = filterable
         self.sortable = sortable
         self.facetable = facetable
         self.analyzer = analyzer
         self.search_analyzer = search_analyzer
         self.index_analyzer = index_analyzer
         self.normalizer = normalizer
+        self.dimensions = dimensions
+        self.vector_search_configuration = vector_search_configuration
         self.synonym_maps = synonym_maps
         self.fields = fields
 
 
 class SearchIndex(_serialization.Model):  # pylint: disable=too-many-instance-attributes
-    """Represents a search index definition, which describes the fields and search behavior of an index.
+    """Represents a search index definition, which describes the fields and search behavior of an
+    index.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar name: The name of the index. Required.
     :vartype name: str
     :ivar fields: The fields of the index. Required.
     :vartype fields: list[~search_service_client.models.SearchField]
@@ -6270,14 +6431,16 @@
      documents matching a search query. The similarity algorithm can only be defined at index
      creation time and cannot be modified on existing indexes. If null, the ClassicSimilarity
      algorithm is used.
     :vartype similarity: ~search_service_client.models.Similarity
     :ivar semantic_settings: Defines parameters for a search index that influence semantic
      capabilities.
     :vartype semantic_settings: ~search_service_client.models.SemanticSettings
+    :ivar vector_search: Contains configuration options related to vector search.
+    :vartype vector_search: ~search_service_client.models.VectorSearch
     :ivar e_tag: The ETag of the index.
     :vartype e_tag: str
     """
 
     _validation = {
         "name": {"required": True},
         "fields": {"required": True},
@@ -6294,14 +6457,15 @@
         "tokenizers": {"key": "tokenizers", "type": "[LexicalTokenizer]"},
         "token_filters": {"key": "tokenFilters", "type": "[TokenFilter]"},
         "char_filters": {"key": "charFilters", "type": "[CharFilter]"},
         "normalizers": {"key": "normalizers", "type": "[LexicalNormalizer]"},
         "encryption_key": {"key": "encryptionKey", "type": "SearchResourceEncryptionKey"},
         "similarity": {"key": "similarity", "type": "Similarity"},
         "semantic_settings": {"key": "semantic", "type": "SemanticSettings"},
+        "vector_search": {"key": "vectorSearch", "type": "VectorSearch"},
         "e_tag": {"key": "@odata\\.etag", "type": "str"},
     }
 
     def __init__(
         self,
         *,
         name: str,
@@ -6314,17 +6478,18 @@
         tokenizers: Optional[List["_models.LexicalTokenizer"]] = None,
         token_filters: Optional[List["_models.TokenFilter"]] = None,
         char_filters: Optional[List["_models.CharFilter"]] = None,
         normalizers: Optional[List["_models.LexicalNormalizer"]] = None,
         encryption_key: Optional["_models.SearchResourceEncryptionKey"] = None,
         similarity: Optional["_models.Similarity"] = None,
         semantic_settings: Optional["_models.SemanticSettings"] = None,
+        vector_search: Optional["_models.VectorSearch"] = None,
         e_tag: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the index. Required.
         :paramtype name: str
         :keyword fields: The fields of the index. Required.
         :paramtype fields: list[~search_service_client.models.SearchField]
         :keyword scoring_profiles: The scoring profiles for the index.
         :paramtype scoring_profiles: list[~search_service_client.models.ScoringProfile]
@@ -6359,14 +6524,16 @@
          documents matching a search query. The similarity algorithm can only be defined at index
          creation time and cannot be modified on existing indexes. If null, the ClassicSimilarity
          algorithm is used.
         :paramtype similarity: ~search_service_client.models.Similarity
         :keyword semantic_settings: Defines parameters for a search index that influence semantic
          capabilities.
         :paramtype semantic_settings: ~search_service_client.models.SemanticSettings
+        :keyword vector_search: Contains configuration options related to vector search.
+        :paramtype vector_search: ~search_service_client.models.VectorSearch
         :keyword e_tag: The ETag of the index.
         :paramtype e_tag: str
         """
         super().__init__(**kwargs)
         self.name = name
         self.fields = fields
         self.scoring_profiles = scoring_profiles
@@ -6377,14 +6544,15 @@
         self.tokenizers = tokenizers
         self.token_filters = token_filters
         self.char_filters = char_filters
         self.normalizers = normalizers
         self.encryption_key = encryption_key
         self.similarity = similarity
         self.semantic_settings = semantic_settings
+        self.vector_search = vector_search
         self.e_tag = e_tag
 
 
 class SearchIndexer(_serialization.Model):  # pylint: disable=too-many-instance-attributes
     """Represents an indexer.
 
     All required parameters must be populated in order to send to Azure.
@@ -6463,16 +6631,16 @@
         parameters: Optional["_models.IndexingParameters"] = None,
         field_mappings: Optional[List["_models.FieldMapping"]] = None,
         output_field_mappings: Optional[List["_models.FieldMapping"]] = None,
         is_disabled: bool = False,
         e_tag: Optional[str] = None,
         encryption_key: Optional["_models.SearchResourceEncryptionKey"] = None,
         cache: Optional["_models.SearchIndexerCache"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the indexer. Required.
         :paramtype name: str
         :keyword description: The description of the indexer.
         :paramtype description: str
         :keyword data_source_name: The name of the datasource from which this indexer reads data.
          Required.
@@ -6529,38 +6697,58 @@
     """SearchIndexerCache.
 
     :ivar storage_connection_string: The connection string to the storage account where the cache
      data will be persisted.
     :vartype storage_connection_string: str
     :ivar enable_reprocessing: Specifies whether incremental reprocessing is enabled.
     :vartype enable_reprocessing: bool
+    :ivar identity: The user-assigned managed identity used for connections to the enrichment
+     cache.  If the connection string indicates an identity (ResourceId) and it's not specified, the
+     system-assigned managed identity is used. On updates to the indexer, if the identity is
+     unspecified, the value remains unchanged. If set to "none", the value of this property is
+     cleared.
+    :vartype identity: ~search_service_client.models.SearchIndexerDataIdentity
     """
 
     _attribute_map = {
         "storage_connection_string": {"key": "storageConnectionString", "type": "str"},
         "enable_reprocessing": {"key": "enableReprocessing", "type": "bool"},
+        "identity": {"key": "identity", "type": "SearchIndexerDataIdentity"},
     }
 
     def __init__(
-        self, *, storage_connection_string: Optional[str] = None, enable_reprocessing: Optional[bool] = None, **kwargs
-    ):
+        self,
+        *,
+        storage_connection_string: Optional[str] = None,
+        enable_reprocessing: Optional[bool] = None,
+        identity: Optional["_models.SearchIndexerDataIdentity"] = None,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword storage_connection_string: The connection string to the storage account where the
          cache data will be persisted.
         :paramtype storage_connection_string: str
         :keyword enable_reprocessing: Specifies whether incremental reprocessing is enabled.
         :paramtype enable_reprocessing: bool
+        :keyword identity: The user-assigned managed identity used for connections to the enrichment
+         cache.  If the connection string indicates an identity (ResourceId) and it's not specified, the
+         system-assigned managed identity is used. On updates to the indexer, if the identity is
+         unspecified, the value remains unchanged. If set to "none", the value of this property is
+         cleared.
+        :paramtype identity: ~search_service_client.models.SearchIndexerDataIdentity
         """
         super().__init__(**kwargs)
         self.storage_connection_string = storage_connection_string
         self.enable_reprocessing = enable_reprocessing
+        self.identity = identity
 
 
 class SearchIndexerDataContainer(_serialization.Model):
-    """Represents information about the entity (such as Azure SQL table or CosmosDB collection) that will be indexed.
+    """Represents information about the entity (such as Azure SQL table or CosmosDB collection) that
+    will be indexed.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar name: The name of the table or view (for Azure SQL data source) or collection (for
      CosmosDB data source) that will be indexed. Required.
     :vartype name: str
     :ivar query: A query that is applied to this data container. The syntax and meaning of this
@@ -6573,15 +6761,15 @@
     }
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "query": {"key": "query", "type": "str"},
     }
 
-    def __init__(self, *, name: str, query: Optional[str] = None, **kwargs):
+    def __init__(self, *, name: str, query: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the table or view (for Azure SQL data source) or collection (for
          CosmosDB data source) that will be indexed. Required.
         :paramtype name: str
         :keyword query: A query that is applied to this data container. The syntax and meaning of this
          parameter is datasource-specific. Not supported by Azure SQL datasources.
         :paramtype query: str
@@ -6614,18 +6802,18 @@
     _subtype_map = {
         "odata_type": {
             "#Microsoft.Azure.Search.DataNoneIdentity": "SearchIndexerDataNoneIdentity",
             "#Microsoft.Azure.Search.DataUserAssignedIdentity": "SearchIndexerDataUserAssignedIdentity",
         }
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = None  # type: Optional[str]
+        self.odata_type: Optional[str] = None
 
 
 class SearchIndexerDataNoneIdentity(SearchIndexerDataIdentity):
     """Clears the identity property of a datasource.
 
     All required parameters must be populated in order to send to Azure.
 
@@ -6637,18 +6825,18 @@
         "odata_type": {"required": True},
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.DataNoneIdentity"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.DataNoneIdentity"
 
 
 class SearchIndexerDataSource(_serialization.Model):
     """Represents a datasource definition, which can be used to configure an indexer.
 
     All required parameters must be populated in order to send to Azure.
 
@@ -6716,16 +6904,16 @@
         container: "_models.SearchIndexerDataContainer",
         description: Optional[str] = None,
         identity: Optional["_models.SearchIndexerDataIdentity"] = None,
         data_change_detection_policy: Optional["_models.DataChangeDetectionPolicy"] = None,
         data_deletion_detection_policy: Optional["_models.DataDeletionDetectionPolicy"] = None,
         e_tag: Optional[str] = None,
         encryption_key: Optional["_models.SearchResourceEncryptionKey"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the datasource. Required.
         :paramtype name: str
         :keyword description: The description of the datasource.
         :paramtype description: str
         :keyword type: The type of the datasource. Required. Known values are: "azuresql", "cosmosdb",
          "azureblob", "azuretable", "mysql", and "adlsgen2".
@@ -6791,24 +6979,24 @@
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "user_assigned_identity": {"key": "userAssignedIdentity", "type": "str"},
     }
 
-    def __init__(self, *, user_assigned_identity: str, **kwargs):
+    def __init__(self, *, user_assigned_identity: str, **kwargs: Any) -> None:
         """
         :keyword user_assigned_identity: The fully qualified Azure resource Id of a user assigned
          managed identity typically in the form
          "/subscriptions/12345678-1234-1234-1234-1234567890ab/resourceGroups/rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myId"
          that should have been assigned to the search service. Required.
         :paramtype user_assigned_identity: str
         """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.DataUserAssignedIdentity"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.DataUserAssignedIdentity"
         self.user_assigned_identity = user_assigned_identity
 
 
 class SearchIndexerError(_serialization.Model):
     """Represents an item- or document-level indexing error.
 
     Variables are only populated by the server, and will be ignored when sending a request.
@@ -6850,15 +7038,15 @@
         "error_message": {"key": "errorMessage", "type": "str"},
         "status_code": {"key": "statusCode", "type": "int"},
         "name": {"key": "name", "type": "str"},
         "details": {"key": "details", "type": "str"},
         "documentation_link": {"key": "documentationLink", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.key = None
         self.error_message = None
         self.status_code = None
         self.name = None
         self.details = None
@@ -6871,44 +7059,59 @@
     All required parameters must be populated in order to send to Azure.
 
     :ivar storage_connection_string: The connection string to the storage account projections will
      be stored in. Required.
     :vartype storage_connection_string: str
     :ivar projections: A list of additional projections to perform during indexing. Required.
     :vartype projections: list[~search_service_client.models.SearchIndexerKnowledgeStoreProjection]
+    :ivar identity: The user-assigned managed identity used for connections to Azure Storage when
+     writing knowledge store projections. If the connection string indicates an identity
+     (ResourceId) and it's not specified, the system-assigned managed identity is used. On updates
+     to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none",
+     the value of this property is cleared.
+    :vartype identity: ~search_service_client.models.SearchIndexerDataIdentity
     """
 
     _validation = {
         "storage_connection_string": {"required": True},
         "projections": {"required": True},
     }
 
     _attribute_map = {
         "storage_connection_string": {"key": "storageConnectionString", "type": "str"},
         "projections": {"key": "projections", "type": "[SearchIndexerKnowledgeStoreProjection]"},
+        "identity": {"key": "identity", "type": "SearchIndexerDataIdentity"},
     }
 
     def __init__(
         self,
         *,
         storage_connection_string: str,
         projections: List["_models.SearchIndexerKnowledgeStoreProjection"],
-        **kwargs
-    ):
+        identity: Optional["_models.SearchIndexerDataIdentity"] = None,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword storage_connection_string: The connection string to the storage account projections
          will be stored in. Required.
         :paramtype storage_connection_string: str
         :keyword projections: A list of additional projections to perform during indexing. Required.
         :paramtype projections:
          list[~search_service_client.models.SearchIndexerKnowledgeStoreProjection]
+        :keyword identity: The user-assigned managed identity used for connections to Azure Storage
+         when writing knowledge store projections. If the connection string indicates an identity
+         (ResourceId) and it's not specified, the system-assigned managed identity is used. On updates
+         to the indexer, if the identity is unspecified, the value remains unchanged. If set to "none",
+         the value of this property is cleared.
+        :paramtype identity: ~search_service_client.models.SearchIndexerDataIdentity
         """
         super().__init__(**kwargs)
         self.storage_connection_string = storage_connection_string
         self.projections = projections
+        self.identity = identity
 
 
 class SearchIndexerKnowledgeStoreProjectionSelector(_serialization.Model):
     """Abstract class to share properties between concrete selectors.
 
     :ivar reference_key_name: Name of reference key to different projection.
     :vartype reference_key_name: str
@@ -6934,16 +7137,16 @@
         self,
         *,
         reference_key_name: Optional[str] = None,
         generated_key_name: Optional[str] = None,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword reference_key_name: Name of reference key to different projection.
         :paramtype reference_key_name: str
         :keyword generated_key_name: Name of generated key to store projection under.
         :paramtype generated_key_name: str
         :keyword source: Source data to project.
         :paramtype source: str
@@ -6997,16 +7200,16 @@
         *,
         storage_container: str,
         reference_key_name: Optional[str] = None,
         generated_key_name: Optional[str] = None,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword reference_key_name: Name of reference key to different projection.
         :paramtype reference_key_name: str
         :keyword generated_key_name: Name of generated key to store projection under.
         :paramtype generated_key_name: str
         :keyword source: Source data to project.
         :paramtype source: str
@@ -7065,16 +7268,16 @@
         *,
         storage_container: str,
         reference_key_name: Optional[str] = None,
         generated_key_name: Optional[str] = None,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword reference_key_name: Name of reference key to different projection.
         :paramtype reference_key_name: str
         :keyword generated_key_name: Name of generated key to store projection under.
         :paramtype generated_key_name: str
         :keyword source: Source data to project.
         :paramtype source: str
@@ -7133,16 +7336,16 @@
         *,
         storage_container: str,
         reference_key_name: Optional[str] = None,
         generated_key_name: Optional[str] = None,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword reference_key_name: Name of reference key to different projection.
         :paramtype reference_key_name: str
         :keyword generated_key_name: Name of generated key to store projection under.
         :paramtype generated_key_name: str
         :keyword source: Source data to project.
         :paramtype source: str
@@ -7186,16 +7389,16 @@
 
     def __init__(
         self,
         *,
         tables: Optional[List["_models.SearchIndexerKnowledgeStoreTableProjectionSelector"]] = None,
         objects: Optional[List["_models.SearchIndexerKnowledgeStoreObjectProjectionSelector"]] = None,
         files: Optional[List["_models.SearchIndexerKnowledgeStoreFileProjectionSelector"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword tables: Projections to Azure Table storage.
         :paramtype tables:
          list[~search_service_client.models.SearchIndexerKnowledgeStoreTableProjectionSelector]
         :keyword objects: Projections to Azure Blob storage.
         :paramtype objects:
          list[~search_service_client.models.SearchIndexerKnowledgeStoreObjectProjectionSelector]
@@ -7246,16 +7449,16 @@
         *,
         table_name: str,
         reference_key_name: Optional[str] = None,
         generated_key_name: Optional[str] = None,
         source: Optional[str] = None,
         source_context: Optional[str] = None,
         inputs: Optional[List["_models.InputFieldMappingEntry"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword reference_key_name: Name of reference key to different projection.
         :paramtype reference_key_name: str
         :keyword generated_key_name: Name of generated key to store projection under.
         :paramtype generated_key_name: str
         :keyword source: Source data to project.
         :paramtype source: str
@@ -7301,15 +7504,15 @@
 
     _attribute_map = {
         "max_run_time": {"key": "maxRunTime", "type": "duration"},
         "max_document_extraction_size": {"key": "maxDocumentExtractionSize", "type": "int"},
         "max_document_content_characters_to_extract": {"key": "maxDocumentContentCharactersToExtract", "type": "int"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.max_run_time = None
         self.max_document_extraction_size = None
         self.max_document_content_characters_to_extract = None
 
 
@@ -7365,16 +7568,16 @@
         name: str,
         skills: List["_models.SearchIndexerSkill"],
         description: Optional[str] = None,
         cognitive_services_account: Optional["_models.CognitiveServicesAccount"] = None,
         knowledge_store: Optional["_models.SearchIndexerKnowledgeStore"] = None,
         e_tag: Optional[str] = None,
         encryption_key: Optional["_models.SearchResourceEncryptionKey"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skillset. Required.
         :paramtype name: str
         :keyword description: The description of the skillset.
         :paramtype description: str
         :keyword skills: A list of skills in the skillset. Required.
         :paramtype skills: list[~search_service_client.models.SearchIndexerSkill]
@@ -7436,15 +7639,15 @@
     _attribute_map = {
         "status": {"key": "status", "type": "str"},
         "last_result": {"key": "lastResult", "type": "IndexerExecutionResult"},
         "execution_history": {"key": "executionHistory", "type": "[IndexerExecutionResult]"},
         "limits": {"key": "limits", "type": "SearchIndexerLimits"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.status = None
         self.last_result = None
         self.execution_history = None
         self.limits = None
 
@@ -7484,26 +7687,28 @@
         "key": {"key": "key", "type": "str"},
         "message": {"key": "message", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "details": {"key": "details", "type": "str"},
         "documentation_link": {"key": "documentationLink", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.key = None
         self.message = None
         self.name = None
         self.details = None
         self.documentation_link = None
 
 
 class SearchResourceEncryptionKey(_serialization.Model):
-    """A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be used to encrypt or decrypt data-at-rest in Azure Cognitive Search, such as indexes and synonym maps.
+    """A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be
+    used to encrypt or decrypt data-at-rest in Azure Cognitive Search, such as indexes and synonym
+    maps.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar key_name: The name of your Azure Key Vault key to be used to encrypt your data at rest.
      Required.
     :vartype key_name: str
     :ivar key_version: The version of your Azure Key Vault key to be used to encrypt your data at
@@ -7542,16 +7747,16 @@
         self,
         *,
         key_name: str,
         key_version: str,
         vault_uri: str,
         access_credentials: Optional["_models.AzureActiveDirectoryApplicationCredentials"] = None,
         identity: Optional["_models.SearchIndexerDataIdentity"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword key_name: The name of your Azure Key Vault key to be used to encrypt your data at
          rest. Required.
         :paramtype key_name: str
         :keyword key_version: The version of your Azure Key Vault key to be used to encrypt your data
          at rest. Required.
         :paramtype key_version: str
@@ -7596,15 +7801,15 @@
     }
 
     _attribute_map = {
         "name": {"key": "name", "type": "str"},
         "prioritized_fields": {"key": "prioritizedFields", "type": "PrioritizedFields"},
     }
 
-    def __init__(self, *, name: str, prioritized_fields: "_models.PrioritizedFields", **kwargs):
+    def __init__(self, *, name: str, prioritized_fields: "_models.PrioritizedFields", **kwargs: Any) -> None:
         """
         :keyword name: The name of the semantic configuration. Required.
         :paramtype name: str
         :keyword prioritized_fields: Describes the title, content, and keyword fields to be used for
          semantic ranking, captions, highlights, and answers. At least one of the three sub properties
          (titleField, prioritizedKeywordsFields and prioritizedContentFields) need to be set. Required.
         :paramtype prioritized_fields: ~search_service_client.models.PrioritizedFields
@@ -7621,45 +7826,60 @@
     :vartype field_name: str
     """
 
     _attribute_map = {
         "field_name": {"key": "fieldName", "type": "str"},
     }
 
-    def __init__(self, *, field_name: Optional[str] = None, **kwargs):
+    def __init__(self, *, field_name: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword field_name:
         :paramtype field_name: str
         """
         super().__init__(**kwargs)
         self.field_name = field_name
 
 
 class SemanticSettings(_serialization.Model):
     """Defines parameters for a search index that influence semantic capabilities.
 
+    :ivar default_configuration: Allows you to set a default semantic configuration in your index,
+     making it optional to pass it on as a query parameter every time.
+    :vartype default_configuration: str
     :ivar configurations: The semantic configurations for the index.
     :vartype configurations: list[~search_service_client.models.SemanticConfiguration]
     """
 
     _attribute_map = {
+        "default_configuration": {"key": "defaultConfiguration", "type": "str"},
         "configurations": {"key": "configurations", "type": "[SemanticConfiguration]"},
     }
 
-    def __init__(self, *, configurations: Optional[List["_models.SemanticConfiguration"]] = None, **kwargs):
-        """
+    def __init__(
+        self,
+        *,
+        default_configuration: Optional[str] = None,
+        configurations: Optional[List["_models.SemanticConfiguration"]] = None,
+        **kwargs: Any
+    ) -> None:
+        """
+        :keyword default_configuration: Allows you to set a default semantic configuration in your
+         index, making it optional to pass it on as a query parameter every time.
+        :paramtype default_configuration: str
         :keyword configurations: The semantic configurations for the index.
         :paramtype configurations: list[~search_service_client.models.SemanticConfiguration]
         """
         super().__init__(**kwargs)
+        self.default_configuration = default_configuration
         self.configurations = configurations
 
 
 class SentimentSkill(SearchIndexerSkill):
-    """Text analytics positive-negative sentiment analysis, scored as a floating point value in a range of zero to 1.
+    """Text analytics positive-negative sentiment analysis, scored as a floating point value in a
+    range of zero to 1.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -7704,16 +7924,16 @@
         *,
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.SentimentSkillLanguage"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -7730,20 +7950,22 @@
         :paramtype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
         :keyword default_language_code: A value indicating which language code to use. Default is en.
          Known values are: "da", "nl", "en", "fi", "fr", "de", "el", "it", "no", "pl", "pt-PT", "ru",
          "es", "sv", and "tr".
         :paramtype default_language_code: str or ~search_service_client.models.SentimentSkillLanguage
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.SentimentSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.SentimentSkill"
         self.default_language_code = default_language_code
 
 
 class SentimentSkillV3(SearchIndexerSkill):
-    """Using the Text Analytics API, evaluates unstructured text and for each record, provides sentiment labels (such as "negative", "neutral" and "positive") based on the highest confidence score found by the service at a sentence and document-level.
+    """Using the Text Analytics API, evaluates unstructured text and for each record, provides
+    sentiment labels (such as "negative", "neutral" and "positive") based on the highest confidence
+    score found by the service at a sentence and document-level.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -7798,16 +8020,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[str] = None,
         include_opinion_mining: bool = False,
         model_version: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -7830,15 +8052,15 @@
         :paramtype include_opinion_mining: bool
         :keyword model_version: The version of the model to use when calling the Text Analytics
          service. It will default to the latest available when not specified. We recommend you do not
          specify this value unless absolutely necessary.
         :paramtype model_version: str
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.V3.SentimentSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.V3.SentimentSkill"
         self.default_language_code = default_language_code
         self.include_opinion_mining = include_opinion_mining
         self.model_version = model_version
 
 
 class ServiceCounters(_serialization.Model):
     """Represents service-level resource counters and quotas.
@@ -7890,16 +8112,16 @@
         index_counter: "_models.ResourceCounter",
         indexer_counter: "_models.ResourceCounter",
         data_source_counter: "_models.ResourceCounter",
         storage_size_counter: "_models.ResourceCounter",
         synonym_map_counter: "_models.ResourceCounter",
         alias_counter: Optional["_models.ResourceCounter"] = None,
         skillset_counter: Optional["_models.ResourceCounter"] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword alias_counter: Total number of aliases.
         :paramtype alias_counter: ~search_service_client.models.ResourceCounter
         :keyword document_counter: Total number of documents across all indexes in the service.
          Required.
         :paramtype document_counter: ~search_service_client.models.ResourceCounter
         :keyword index_counter: Total number of indexes. Required.
@@ -7955,16 +8177,16 @@
     def __init__(
         self,
         *,
         max_fields_per_index: Optional[int] = None,
         max_field_nesting_depth_per_index: Optional[int] = None,
         max_complex_collection_fields_per_index: Optional[int] = None,
         max_complex_objects_in_collections_per_document: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword max_fields_per_index: The maximum allowed fields per index.
         :paramtype max_fields_per_index: int
         :keyword max_field_nesting_depth_per_index: The maximum depth which you can nest sub-fields in
          an index, including the top-level complex field. For example, a/b/c has a nesting depth of 3.
         :paramtype max_field_nesting_depth_per_index: int
         :keyword max_complex_collection_fields_per_index: The maximum number of fields of type
@@ -7978,15 +8200,16 @@
         self.max_fields_per_index = max_fields_per_index
         self.max_field_nesting_depth_per_index = max_field_nesting_depth_per_index
         self.max_complex_collection_fields_per_index = max_complex_collection_fields_per_index
         self.max_complex_objects_in_collections_per_document = max_complex_objects_in_collections_per_document
 
 
 class ServiceStatistics(_serialization.Model):
-    """Response from a get service statistics request. If successful, it includes service level counters and limits.
+    """Response from a get service statistics request. If successful, it includes service level
+    counters and limits.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar counters: Service level resource counters. Required.
     :vartype counters: ~search_service_client.models.ServiceCounters
     :ivar limits: Service level general limits. Required.
     :vartype limits: ~search_service_client.models.ServiceLimits
@@ -7998,28 +8221,29 @@
     }
 
     _attribute_map = {
         "counters": {"key": "counters", "type": "ServiceCounters"},
         "limits": {"key": "limits", "type": "ServiceLimits"},
     }
 
-    def __init__(self, *, counters: "_models.ServiceCounters", limits: "_models.ServiceLimits", **kwargs):
+    def __init__(self, *, counters: "_models.ServiceCounters", limits: "_models.ServiceLimits", **kwargs: Any) -> None:
         """
         :keyword counters: Service level resource counters. Required.
         :paramtype counters: ~search_service_client.models.ServiceCounters
         :keyword limits: Service level general limits. Required.
         :paramtype limits: ~search_service_client.models.ServiceLimits
         """
         super().__init__(**kwargs)
         self.counters = counters
         self.limits = limits
 
 
 class ShaperSkill(SearchIndexerSkill):
-    """A skill for reshaping the outputs. It creates a complex type to support composite fields (also known as multipart fields).
+    """A skill for reshaping the outputs. It creates a complex type to support composite fields (also
+    known as multipart fields).
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -8058,16 +8282,16 @@
         self,
         *,
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -8080,19 +8304,20 @@
          of an upstream skill. Required.
         :paramtype inputs: list[~search_service_client.models.InputFieldMappingEntry]
         :keyword outputs: The output of a skill is either a field in a search index, or a value that
          can be consumed as an input by another skill. Required.
         :paramtype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Util.ShaperSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Util.ShaperSkill"
 
 
 class ShingleTokenFilter(TokenFilter):
-    """Creates combinations of tokens as a single token. This token filter is implemented using Apache Lucene.
+    """Creates combinations of tokens as a single token. This token filter is implemented using Apache
+    Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -8142,16 +8367,16 @@
         name: str,
         max_shingle_size: int = 2,
         min_shingle_size: int = 2,
         output_unigrams: bool = True,
         output_unigrams_if_no_shingles: bool = False,
         token_separator: str = " ",
         filter_token: str = "_",
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword max_shingle_size: The maximum shingle size. Default and minimum value is 2.
         :paramtype max_shingle_size: int
@@ -8169,15 +8394,15 @@
          Default is a single space (" ").
         :paramtype token_separator: str
         :keyword filter_token: The string to insert for each position at which there is no token.
          Default is an underscore ("_").
         :paramtype filter_token: str
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.ShingleTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.ShingleTokenFilter"
         self.max_shingle_size = max_shingle_size
         self.min_shingle_size = min_shingle_size
         self.output_unigrams = output_unigrams
         self.output_unigrams_if_no_shingles = output_unigrams_if_no_shingles
         self.token_separator = token_separator
         self.filter_token = filter_token
 
@@ -8189,25 +8414,26 @@
     :vartype skill_names: list[str]
     """
 
     _attribute_map = {
         "skill_names": {"key": "skillNames", "type": "[str]"},
     }
 
-    def __init__(self, *, skill_names: Optional[List[str]] = None, **kwargs):
+    def __init__(self, *, skill_names: Optional[List[str]] = None, **kwargs: Any) -> None:
         """
         :keyword skill_names: the names of skills to be reset.
         :paramtype skill_names: list[str]
         """
         super().__init__(**kwargs)
         self.skill_names = skill_names
 
 
 class SnowballTokenFilter(TokenFilter):
-    """A filter that stems words using a Snowball-generated stemmer. This token filter is implemented using Apache Lucene.
+    """A filter that stems words using a Snowball-generated stemmer. This token filter is implemented
+    using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -8228,33 +8454,37 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "language": {"key": "language", "type": "str"},
     }
 
-    def __init__(self, *, name: str, language: Union[str, "_models.SnowballTokenFilterLanguage"], **kwargs):
+    def __init__(
+        self, *, name: str, language: Union[str, "_models.SnowballTokenFilterLanguage"], **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword language: The language to use. Required. Known values are: "armenian", "basque",
          "catalan", "danish", "dutch", "english", "finnish", "french", "german", "german2", "hungarian",
          "italian", "kp", "lovins", "norwegian", "porter", "portuguese", "romanian", "russian",
          "spanish", "swedish", and "turkish".
         :paramtype language: str or ~search_service_client.models.SnowballTokenFilterLanguage
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.SnowballTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.SnowballTokenFilter"
         self.language = language
 
 
 class SoftDeleteColumnDeletionDetectionPolicy(DataDeletionDetectionPolicy):
-    """Defines a data deletion detection policy that implements a soft-deletion strategy. It determines whether an item should be deleted based on the value of a designated 'soft delete' column.
+    """Defines a data deletion detection policy that implements a soft-deletion strategy. It
+    determines whether an item should be deleted based on the value of a designated 'soft delete'
+    column.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the data deletion detection policy. Required.
     :vartype odata_type: str
     :ivar soft_delete_column_name: The name of the column to use for soft-deletion detection.
     :vartype soft_delete_column_name: str
@@ -8269,24 +8499,28 @@
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "soft_delete_column_name": {"key": "softDeleteColumnName", "type": "str"},
         "soft_delete_marker_value": {"key": "softDeleteMarkerValue", "type": "str"},
     }
 
     def __init__(
-        self, *, soft_delete_column_name: Optional[str] = None, soft_delete_marker_value: Optional[str] = None, **kwargs
-    ):
+        self,
+        *,
+        soft_delete_column_name: Optional[str] = None,
+        soft_delete_marker_value: Optional[str] = None,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword soft_delete_column_name: The name of the column to use for soft-deletion detection.
         :paramtype soft_delete_column_name: str
         :keyword soft_delete_marker_value: The marker value that identifies an item as deleted.
         :paramtype soft_delete_marker_value: str
         """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy"
         self.soft_delete_column_name = soft_delete_column_name
         self.soft_delete_marker_value = soft_delete_marker_value
 
 
 class SplitSkill(SearchIndexerSkill):
     """A skill to split a string into chunks of text.
 
@@ -8345,16 +8579,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_language_code: Optional[Union[str, "_models.SplitSkillLanguage"]] = None,
         text_split_mode: Optional[Union[str, "_models.TextSplitMode"]] = None,
         maximum_page_length: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -8375,22 +8609,23 @@
         :keyword text_split_mode: A value indicating which split mode to perform. Known values are:
          "pages" and "sentences".
         :paramtype text_split_mode: str or ~search_service_client.models.TextSplitMode
         :keyword maximum_page_length: The desired maximum page length. Default is 10000.
         :paramtype maximum_page_length: int
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.SplitSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.SplitSkill"
         self.default_language_code = default_language_code
         self.text_split_mode = text_split_mode
         self.maximum_page_length = maximum_page_length
 
 
 class SqlIntegratedChangeTrackingPolicy(DataChangeDetectionPolicy):
-    """Defines a data change detection policy that captures changes using the Integrated Change Tracking feature of Azure SQL Database.
+    """Defines a data change detection policy that captures changes using the Integrated Change
+    Tracking feature of Azure SQL Database.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the data change detection policy. Required.
     :vartype odata_type: str
     """
 
@@ -8398,22 +8633,25 @@
         "odata_type": {"required": True},
     }
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy"
 
 
 class StemmerOverrideTokenFilter(TokenFilter):
-    """Provides the ability to override other stemming filters with custom dictionary-based stemming. Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with stemmers down the chain. Must be placed before any stemming filters. This token filter is implemented using Apache Lucene.
+    """Provides the ability to override other stemming filters with custom dictionary-based stemming.
+    Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with
+    stemmers down the chain. Must be placed before any stemming filters. This token filter is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -8432,26 +8670,26 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "rules": {"key": "rules", "type": "[str]"},
     }
 
-    def __init__(self, *, name: str, rules: List[str], **kwargs):
+    def __init__(self, *, name: str, rules: List[str], **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword rules: A list of stemming rules in the following format: "word => stem", for example:
          "ran => run". Required.
         :paramtype rules: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StemmerOverrideTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StemmerOverrideTokenFilter"
         self.rules = rules
 
 
 class StemmerTokenFilter(TokenFilter):
     """Language specific stemming filter. This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
@@ -8482,15 +8720,15 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "language": {"key": "language", "type": "str"},
     }
 
-    def __init__(self, *, name: str, language: Union[str, "_models.StemmerTokenFilterLanguage"], **kwargs):
+    def __init__(self, *, name: str, language: Union[str, "_models.StemmerTokenFilterLanguage"], **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword language: The language to use. Required. Known values are: "arabic", "armenian",
          "basque", "brazilian", "bulgarian", "catalan", "czech", "danish", "dutch", "dutchKp",
@@ -8500,20 +8738,21 @@
          "hungarian", "lightHungarian", "indonesian", "irish", "italian", "lightItalian", "sorani",
          "latvian", "norwegian", "lightNorwegian", "minimalNorwegian", "lightNynorsk", "minimalNynorsk",
          "portuguese", "lightPortuguese", "minimalPortuguese", "portugueseRslp", "romanian", "russian",
          "lightRussian", "spanish", "lightSpanish", "swedish", "lightSwedish", and "turkish".
         :paramtype language: str or ~search_service_client.models.StemmerTokenFilterLanguage
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StemmerTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StemmerTokenFilter"
         self.language = language
 
 
 class StopAnalyzer(LexicalAnalyzer):
-    """Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is implemented using Apache Lucene.
+    """Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is
+    implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the analyzer. Required.
     :vartype odata_type: str
     :ivar name: The name of the analyzer. It must only contain letters, digits, spaces, dashes or
      underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -8530,25 +8769,25 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "stopwords": {"key": "stopwords", "type": "[str]"},
     }
 
-    def __init__(self, *, name: str, stopwords: Optional[List[str]] = None, **kwargs):
+    def __init__(self, *, name: str, stopwords: Optional[List[str]] = None, **kwargs: Any) -> None:
         """
         :keyword name: The name of the analyzer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword stopwords: A list of stopwords.
         :paramtype stopwords: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StopAnalyzer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StopAnalyzer"
         self.stopwords = stopwords
 
 
 class StopwordsTokenFilter(TokenFilter):
     """Removes stop words from a token stream. This token filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
@@ -8595,16 +8834,16 @@
         self,
         *,
         name: str,
         stopwords: Optional[List[str]] = None,
         stopwords_list: Optional[Union[str, "_models.StopwordsList"]] = None,
         ignore_case: bool = False,
         remove_trailing_stop_words: bool = True,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword stopwords: The list of stopwords. This property and the stopwords list property cannot
          both be set.
@@ -8620,15 +8859,15 @@
          converted to lower case first. Default is false.
         :paramtype ignore_case: bool
         :keyword remove_trailing_stop_words: A value indicating whether to ignore the last search term
          if it's a stop word. Default is true.
         :paramtype remove_trailing_stop_words: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.StopwordsTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.StopwordsTokenFilter"
         self.stopwords = stopwords
         self.stopwords_list = stopwords_list
         self.ignore_case = ignore_case
         self.remove_trailing_stop_words = remove_trailing_stop_words
 
 
 class Suggester(_serialization.Model):
@@ -8658,15 +8897,15 @@
         "name": {"key": "name", "type": "str"},
         "search_mode": {"key": "searchMode", "type": "str"},
         "source_fields": {"key": "sourceFields", "type": "[str]"},
     }
 
     search_mode = "analyzingInfixMatching"
 
-    def __init__(self, *, name: str, source_fields: List[str], **kwargs):
+    def __init__(self, *, name: str, source_fields: List[str], **kwargs: Any) -> None:
         """
         :keyword name: The name of the suggester. Required.
         :paramtype name: str
         :keyword source_fields: The list of field names to which the suggester applies. Each field must
          be searchable. Required.
         :paramtype source_fields: list[str]
         """
@@ -8722,16 +8961,16 @@
     def __init__(
         self,
         *,
         name: str,
         synonyms: str,
         encryption_key: Optional["_models.SearchResourceEncryptionKey"] = None,
         e_tag: Optional[str] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the synonym map. Required.
         :paramtype name: str
         :keyword synonyms: A series of synonym rules in the specified synonym map format. The rules
          must be separated by newlines. Required.
         :paramtype synonyms: str
         :keyword encryption_key: A description of an encryption key that you create in Azure Key Vault.
@@ -8750,15 +8989,16 @@
         self.name = name
         self.synonyms = synonyms
         self.encryption_key = encryption_key
         self.e_tag = e_tag
 
 
 class SynonymTokenFilter(TokenFilter):
-    """Matches single or multi-word synonyms in a token stream. This token filter is implemented using Apache Lucene.
+    """Matches single or multi-word synonyms in a token stream. This token filter is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -8793,15 +9033,17 @@
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "synonyms": {"key": "synonyms", "type": "[str]"},
         "ignore_case": {"key": "ignoreCase", "type": "bool"},
         "expand": {"key": "expand", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, synonyms: List[str], ignore_case: bool = False, expand: bool = True, **kwargs):
+    def __init__(
+        self, *, name: str, synonyms: List[str], ignore_case: bool = False, expand: bool = True, **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword synonyms: A list of synonyms in following one of two formats: 1. incredible,
          unbelievable, fabulous => amazing - all terms on the left side of => symbol will be replaced
@@ -8818,22 +9060,23 @@
          fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible,
          unbelievable, fabulous, amazing. If false, the following list: incredible, unbelievable,
          fabulous, amazing will be equivalent to: incredible, unbelievable, fabulous, amazing =>
          incredible. Default is true.
         :paramtype expand: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.SynonymTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.SynonymTokenFilter"
         self.synonyms = synonyms
         self.ignore_case = ignore_case
         self.expand = expand
 
 
 class TagScoringFunction(ScoringFunction):
-    """Defines a function that boosts scores of documents with string values matching a given list of tags.
+    """Defines a function that boosts scores of documents with string values matching a given list of
+    tags.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar type: Indicates the type of function to use. Valid values include magnitude, freshness,
      distance, and tag. The function type must be lower case. Required.
     :vartype type: str
     :ivar field_name: The name of the field used as input to the scoring function. Required.
@@ -8867,31 +9110,31 @@
     def __init__(
         self,
         *,
         field_name: str,
         boost: float,
         parameters: "_models.TagScoringParameters",
         interpolation: Optional[Union[str, "_models.ScoringFunctionInterpolation"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword field_name: The name of the field used as input to the scoring function. Required.
         :paramtype field_name: str
         :keyword boost: A multiplier for the raw score. Must be a positive number not equal to 1.0.
          Required.
         :paramtype boost: float
         :keyword interpolation: A value indicating how boosting will be interpolated across document
          scores; defaults to "Linear". Known values are: "linear", "constant", "quadratic", and
          "logarithmic".
         :paramtype interpolation: str or ~search_service_client.models.ScoringFunctionInterpolation
         :keyword parameters: Parameter values for the tag scoring function. Required.
         :paramtype parameters: ~search_service_client.models.TagScoringParameters
         """
         super().__init__(field_name=field_name, boost=boost, interpolation=interpolation, **kwargs)
-        self.type = "tag"  # type: str
+        self.type: str = "tag"
         self.parameters = parameters
 
 
 class TagScoringParameters(_serialization.Model):
     """Provides parameter values to a tag scoring function.
 
     All required parameters must be populated in order to send to Azure.
@@ -8905,15 +9148,15 @@
         "tags_parameter": {"required": True},
     }
 
     _attribute_map = {
         "tags_parameter": {"key": "tagsParameter", "type": "str"},
     }
 
-    def __init__(self, *, tags_parameter: str, **kwargs):
+    def __init__(self, *, tags_parameter: str, **kwargs: Any) -> None:
         """
         :keyword tags_parameter: The name of the parameter passed in search queries to specify the list
          of tags to compare against the target field. Required.
         :paramtype tags_parameter: str
         """
         super().__init__(**kwargs)
         self.tags_parameter = tags_parameter
@@ -8944,34 +9187,35 @@
     :vartype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
     :ivar default_to_language_code: The language code to translate documents into for documents
      that don't specify the to language explicitly. Required. Known values are: "af", "ar", "bn",
      "bs", "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj", "fil",
      "fi", "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw", "tlh",
      "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt", "pt-br",
      "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty", "ta",
-     "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+     "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and "is".
     :vartype default_to_language_code: str or
      ~search_service_client.models.TextTranslationSkillLanguage
     :ivar default_from_language_code: The language code to translate documents from for documents
      that don't specify the from language explicitly. Known values are: "af", "ar", "bn", "bs",
      "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj", "fil", "fi",
      "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw", "tlh",
      "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt", "pt-br",
      "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty", "ta",
-     "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+     "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and "is".
     :vartype default_from_language_code: str or
      ~search_service_client.models.TextTranslationSkillLanguage
     :ivar suggested_from: The language code to translate documents from when neither the
      fromLanguageCode input nor the defaultFromLanguageCode parameter are provided, and the
      automatic language detection is unsuccessful. Default is en. Known values are: "af", "ar",
      "bn", "bs", "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj",
      "fil", "fi", "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw",
      "tlh", "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt",
      "pt-br", "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty",
-     "ta", "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+     "ta", "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and
+     "is".
     :vartype suggested_from: str or ~search_service_client.models.TextTranslationSkillLanguage
     """
 
     _validation = {
         "odata_type": {"required": True},
         "inputs": {"required": True},
         "outputs": {"required": True},
@@ -8997,16 +9241,16 @@
         outputs: List["_models.OutputFieldMappingEntry"],
         default_to_language_code: Union[str, "_models.TextTranslationSkillLanguage"],
         name: Optional[str] = None,
         description: Optional[str] = None,
         context: Optional[str] = None,
         default_from_language_code: Optional[Union[str, "_models.TextTranslationSkillLanguage"]] = None,
         suggested_from: Optional[Union[str, "_models.TextTranslationSkillLanguage"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -9023,38 +9267,39 @@
         :paramtype outputs: list[~search_service_client.models.OutputFieldMappingEntry]
         :keyword default_to_language_code: The language code to translate documents into for documents
          that don't specify the to language explicitly. Required. Known values are: "af", "ar", "bn",
          "bs", "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj", "fil",
          "fi", "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw", "tlh",
          "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt", "pt-br",
          "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty", "ta",
-         "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+         "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and "is".
         :paramtype default_to_language_code: str or
          ~search_service_client.models.TextTranslationSkillLanguage
         :keyword default_from_language_code: The language code to translate documents from for
          documents that don't specify the from language explicitly. Known values are: "af", "ar", "bn",
          "bs", "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj", "fil",
          "fi", "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw", "tlh",
          "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt", "pt-br",
          "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty", "ta",
-         "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+         "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and "is".
         :paramtype default_from_language_code: str or
          ~search_service_client.models.TextTranslationSkillLanguage
         :keyword suggested_from: The language code to translate documents from when neither the
          fromLanguageCode input nor the defaultFromLanguageCode parameter are provided, and the
          automatic language detection is unsuccessful. Default is en. Known values are: "af", "ar",
          "bn", "bs", "bg", "yue", "ca", "zh-Hans", "zh-Hant", "hr", "cs", "da", "nl", "en", "et", "fj",
          "fil", "fi", "fr", "de", "el", "ht", "he", "hi", "mww", "hu", "is", "id", "it", "ja", "sw",
          "tlh", "tlh-Latn", "tlh-Piqd", "ko", "lv", "lt", "mg", "ms", "mt", "nb", "fa", "pl", "pt",
          "pt-br", "pt-PT", "otq", "ro", "ru", "sm", "sr-Cyrl", "sr-Latn", "sk", "sl", "es", "sv", "ty",
-         "ta", "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", and "pa".
+         "ta", "te", "th", "to", "tr", "uk", "ur", "vi", "cy", "yua", "ga", "kn", "mi", "ml", "pa", and
+         "is".
         :paramtype suggested_from: str or ~search_service_client.models.TextTranslationSkillLanguage
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Text.TranslationSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Text.TranslationSkill"
         self.default_to_language_code = default_to_language_code
         self.default_from_language_code = default_from_language_code
         self.suggested_from = suggested_from
 
 
 class TextWeights(_serialization.Model):
     """Defines weights on index fields for which matches should boost scoring in search queries.
@@ -9070,15 +9315,15 @@
         "weights": {"required": True},
     }
 
     _attribute_map = {
         "weights": {"key": "weights", "type": "{float}"},
     }
 
-    def __init__(self, *, weights: Dict[str, float], **kwargs):
+    def __init__(self, *, weights: Dict[str, float], **kwargs: Any) -> None:
         """
         :keyword weights: The dictionary of per-field weights to boost document scoring. The keys are
          field names and the values are the weights for each field. Required.
         :paramtype weights: dict[str, float]
         """
         super().__init__(**kwargs)
         self.weights = weights
@@ -9107,25 +9352,25 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "length": {"key": "length", "type": "int"},
     }
 
-    def __init__(self, *, name: str, length: int = 300, **kwargs):
+    def __init__(self, *, name: str, length: int = 300, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword length: The length at which terms will be truncated. Default and maximum is 300.
         :paramtype length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.TruncateTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.TruncateTokenFilter"
         self.length = length
 
 
 class UaxUrlEmailTokenizer(LexicalTokenizer):
     """Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
@@ -9149,31 +9394,32 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "max_token_length": {"key": "maxTokenLength", "type": "int"},
     }
 
-    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs):
+    def __init__(self, *, name: str, max_token_length: int = 255, **kwargs: Any) -> None:
         """
         :keyword name: The name of the tokenizer. It must only contain letters, digits, spaces, dashes
          or underscores, can only start and end with alphanumeric characters, and is limited to 128
          characters. Required.
         :paramtype name: str
         :keyword max_token_length: The maximum token length. Default is 255. Tokens longer than the
          maximum length are split. The maximum token length that can be used is 300 characters.
         :paramtype max_token_length: int
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.UaxUrlEmailTokenizer"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.UaxUrlEmailTokenizer"
         self.max_token_length = max_token_length
 
 
 class UniqueTokenFilter(TokenFilter):
-    """Filters out tokens with same text as the previous token. This token filter is implemented using Apache Lucene.
+    """Filters out tokens with same text as the previous token. This token filter is implemented using
+    Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -9191,31 +9437,104 @@
 
     _attribute_map = {
         "odata_type": {"key": "@odata\\.type", "type": "str"},
         "name": {"key": "name", "type": "str"},
         "only_on_same_position": {"key": "onlyOnSamePosition", "type": "bool"},
     }
 
-    def __init__(self, *, name: str, only_on_same_position: bool = False, **kwargs):
+    def __init__(self, *, name: str, only_on_same_position: bool = False, **kwargs: Any) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword only_on_same_position: A value indicating whether to remove duplicates only at the
          same position. Default is false.
         :paramtype only_on_same_position: bool
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.UniqueTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.UniqueTokenFilter"
         self.only_on_same_position = only_on_same_position
 
 
+class VectorSearch(_serialization.Model):
+    """Contains configuration options related to vector search.
+
+    :ivar algorithm_configurations: Contains configuration options specific to the algorithm used
+     during indexing time.
+    :vartype algorithm_configurations:
+     list[~search_service_client.models.VectorSearchAlgorithmConfiguration]
+    """
+
+    _attribute_map = {
+        "algorithm_configurations": {"key": "algorithmConfigurations", "type": "[VectorSearchAlgorithmConfiguration]"},
+    }
+
+    def __init__(
+        self,
+        *,
+        algorithm_configurations: Optional[List["_models.VectorSearchAlgorithmConfiguration"]] = None,
+        **kwargs: Any
+    ) -> None:
+        """
+        :keyword algorithm_configurations: Contains configuration options specific to the algorithm
+         used during indexing time.
+        :paramtype algorithm_configurations:
+         list[~search_service_client.models.VectorSearchAlgorithmConfiguration]
+        """
+        super().__init__(**kwargs)
+        self.algorithm_configurations = algorithm_configurations
+
+
+class VectorSearchAlgorithmConfiguration(_serialization.Model):
+    """Contains configuration options specific to the algorithm used during indexing time.
+
+    All required parameters must be populated in order to send to Azure.
+
+    :ivar name: The name to associate with this particular configuration. Required.
+    :vartype name: str
+    :ivar kind: The name of the kind of algorithm being configured for use with vector search.
+     Required.
+    :vartype kind: str
+    :ivar hnsw_parameters: Contains the parameters specific to hnsw algorithm.
+    :vartype hnsw_parameters: ~search_service_client.models.HnswParameters
+    """
+
+    _validation = {
+        "name": {"required": True},
+        "kind": {"required": True},
+    }
+
+    _attribute_map = {
+        "name": {"key": "name", "type": "str"},
+        "kind": {"key": "kind", "type": "str"},
+        "hnsw_parameters": {"key": "hnswParameters", "type": "HnswParameters"},
+    }
+
+    def __init__(
+        self, *, name: str, kind: str, hnsw_parameters: Optional["_models.HnswParameters"] = None, **kwargs: Any
+    ) -> None:
+        """
+        :keyword name: The name to associate with this particular configuration. Required.
+        :paramtype name: str
+        :keyword kind: The name of the kind of algorithm being configured for use with vector search.
+         Required.
+        :paramtype kind: str
+        :keyword hnsw_parameters: Contains the parameters specific to hnsw algorithm.
+        :paramtype hnsw_parameters: ~search_service_client.models.HnswParameters
+        """
+        super().__init__(**kwargs)
+        self.name = name
+        self.kind = kind
+        self.hnsw_parameters = hnsw_parameters
+
+
 class WebApiSkill(SearchIndexerSkill):  # pylint: disable=too-many-instance-attributes
-    """A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call your custom code.
+    """A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call
+    your custom code.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the skill. Required.
     :vartype odata_type: str
     :ivar name: The name of the skill which uniquely identifies it within the skillset. A skill
      with no name defined will be given a default name of its 1-based index in the skills array,
@@ -9242,14 +9561,26 @@
     :ivar timeout: The desired timeout for the request. Default is 30 seconds.
     :vartype timeout: ~datetime.timedelta
     :ivar batch_size: The desired batch size which indicates number of documents.
     :vartype batch_size: int
     :ivar degree_of_parallelism: If set, the number of parallel calls that can be made to the Web
      API.
     :vartype degree_of_parallelism: int
+    :ivar auth_resource_id: Applies to custom skills that connect to external code in an Azure
+     function or some other application that provides the transformations. This value should be the
+     application ID created for the function or app when it was registered with Azure Active
+     Directory. When specified, the custom skill connects to the function or app using a managed ID
+     (either system or user-assigned) of the search service and the access token of the function or
+     app, using this value as the resource id for creating the scope of the access token.
+    :vartype auth_resource_id: str
+    :ivar auth_identity: The user-assigned managed identity used for outbound connections. If an
+     authResourceId is provided and it's not specified, the system-assigned managed identity is
+     used. On updates to the indexer, if the identity is unspecified, the value remains unchanged.
+     If set to "none", the value of this property is cleared.
+    :vartype auth_identity: ~search_service_client.models.SearchIndexerDataIdentity
     """
 
     _validation = {
         "odata_type": {"required": True},
         "inputs": {"required": True},
         "outputs": {"required": True},
         "uri": {"required": True},
@@ -9264,14 +9595,16 @@
         "outputs": {"key": "outputs", "type": "[OutputFieldMappingEntry]"},
         "uri": {"key": "uri", "type": "str"},
         "http_headers": {"key": "httpHeaders", "type": "{str}"},
         "http_method": {"key": "httpMethod", "type": "str"},
         "timeout": {"key": "timeout", "type": "duration"},
         "batch_size": {"key": "batchSize", "type": "int"},
         "degree_of_parallelism": {"key": "degreeOfParallelism", "type": "int"},
+        "auth_resource_id": {"key": "authResourceId", "type": "str"},
+        "auth_identity": {"key": "authIdentity", "type": "SearchIndexerDataIdentity"},
     }
 
     def __init__(
         self,
         *,
         inputs: List["_models.InputFieldMappingEntry"],
         outputs: List["_models.OutputFieldMappingEntry"],
@@ -9280,16 +9613,18 @@
         description: Optional[str] = None,
         context: Optional[str] = None,
         http_headers: Optional[Dict[str, str]] = None,
         http_method: Optional[str] = None,
         timeout: Optional[datetime.timedelta] = None,
         batch_size: Optional[int] = None,
         degree_of_parallelism: Optional[int] = None,
-        **kwargs
-    ):
+        auth_resource_id: Optional[str] = None,
+        auth_identity: Optional["_models.SearchIndexerDataIdentity"] = None,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the skill which uniquely identifies it within the skillset. A skill
          with no name defined will be given a default name of its 1-based index in the skills array,
          prefixed with the character '#'.
         :paramtype name: str
         :keyword description: The description of the skill which describes the inputs, outputs, and
          usage of the skill.
@@ -9313,27 +9648,42 @@
         :keyword timeout: The desired timeout for the request. Default is 30 seconds.
         :paramtype timeout: ~datetime.timedelta
         :keyword batch_size: The desired batch size which indicates number of documents.
         :paramtype batch_size: int
         :keyword degree_of_parallelism: If set, the number of parallel calls that can be made to the
          Web API.
         :paramtype degree_of_parallelism: int
+        :keyword auth_resource_id: Applies to custom skills that connect to external code in an Azure
+         function or some other application that provides the transformations. This value should be the
+         application ID created for the function or app when it was registered with Azure Active
+         Directory. When specified, the custom skill connects to the function or app using a managed ID
+         (either system or user-assigned) of the search service and the access token of the function or
+         app, using this value as the resource id for creating the scope of the access token.
+        :paramtype auth_resource_id: str
+        :keyword auth_identity: The user-assigned managed identity used for outbound connections. If an
+         authResourceId is provided and it's not specified, the system-assigned managed identity is
+         used. On updates to the indexer, if the identity is unspecified, the value remains unchanged.
+         If set to "none", the value of this property is cleared.
+        :paramtype auth_identity: ~search_service_client.models.SearchIndexerDataIdentity
         """
         super().__init__(name=name, description=description, context=context, inputs=inputs, outputs=outputs, **kwargs)
-        self.odata_type = "#Microsoft.Skills.Custom.WebApiSkill"  # type: str
+        self.odata_type: str = "#Microsoft.Skills.Custom.WebApiSkill"
         self.uri = uri
         self.http_headers = http_headers
         self.http_method = http_method
         self.timeout = timeout
         self.batch_size = batch_size
         self.degree_of_parallelism = degree_of_parallelism
+        self.auth_resource_id = auth_resource_id
+        self.auth_identity = auth_identity
 
 
 class WordDelimiterTokenFilter(TokenFilter):  # pylint: disable=too-many-instance-attributes
-    """Splits words into subwords and performs optional transformations on subword groups. This token filter is implemented using Apache Lucene.
+    """Splits words into subwords and performs optional transformations on subword groups. This token
+    filter is implemented using Apache Lucene.
 
     All required parameters must be populated in order to send to Azure.
 
     :ivar odata_type: Identifies the concrete type of the token filter. Required.
     :vartype odata_type: str
     :ivar name: The name of the token filter. It must only contain letters, digits, spaces, dashes
      or underscores, can only start and end with alphanumeric characters, and is limited to 128
@@ -9401,16 +9751,16 @@
         catenate_numbers: bool = False,
         catenate_all: bool = False,
         split_on_case_change: bool = True,
         preserve_original: bool = False,
         split_on_numerics: bool = True,
         stem_english_possessive: bool = True,
         protected_words: Optional[List[str]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword name: The name of the token filter. It must only contain letters, digits, spaces,
          dashes or underscores, can only start and end with alphanumeric characters, and is limited to
          128 characters. Required.
         :paramtype name: str
         :keyword generate_word_parts: A value indicating whether to generate part words. If set, causes
          parts of words to be generated; for example "AzureSearch" becomes "Azure" "Search". Default is
@@ -9441,15 +9791,15 @@
         :keyword stem_english_possessive: A value indicating whether to remove trailing "'s" for each
          subword. Default is true.
         :paramtype stem_english_possessive: bool
         :keyword protected_words: A list of tokens to protect from being delimited.
         :paramtype protected_words: list[str]
         """
         super().__init__(name=name, **kwargs)
-        self.odata_type = "#Microsoft.Azure.Search.WordDelimiterTokenFilter"  # type: str
+        self.odata_type: str = "#Microsoft.Azure.Search.WordDelimiterTokenFilter"
         self.generate_word_parts = generate_word_parts
         self.generate_number_parts = generate_number_parts
         self.catenate_words = catenate_words
         self.catenate_numbers = catenate_numbers
         self.catenate_all = catenate_all
         self.split_on_case_change = split_on_case_change
         self.preserve_original = preserve_original
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/indexes/_generated/models/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/__init__.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/_paging.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/_paging.py`

 * *Files 11% similar despite different names*

```diff
@@ -37,46 +37,56 @@
     def _first_iterator_instance(self):
         if self._first_page_iterator_instance is None:
             self._page_iterator = self.by_page()
             self._first_page_iterator_instance = self._page_iterator
         return self._first_page_iterator_instance
 
     async def get_facets(self) -> Optional[Dict]:
-        """Return any facet results if faceting was requested."""
+        """Return any facet results if faceting was requested.
+
+        :return: Facet results.
+        :rtype: dict
+        """
         return await self._first_iterator_instance().get_facets()
 
     async def get_coverage(self) -> float:
         """Return the coverage percentage, if `minimum_coverage` was
         specificied for the query.
 
+        :return: Coverage percentage.
+        :rtype: float
         """
         return await self._first_iterator_instance().get_coverage()
 
     async def get_count(self) -> int:
         """Return the count of results if `include_total_count` was
         set for the query.
 
+        :return: Count of results.
+        :rtype: int
         """
         return await self._first_iterator_instance().get_count()
 
     async def get_answers(self) -> Optional[List[AnswerResult]]:
-        """Return answers."""
+        """Return answers.
+
+        :return: Answers.
+        :rtype: list[~azure.search.documents.AnswerResult]
+        """
         return await self._first_iterator_instance().get_answers()
 
 
 # The pylint error silenced below seems spurious, as the inner wrapper does, in
 # fact, become a method of the class when it is applied.
 def _ensure_response(f):
     # pylint:disable=protected-access
     async def wrapper(self, *args, **kw):
         if self._current_page is None:
             self._response = await self._get_next(self.continuation_token)
-            self.continuation_token, self._current_page = await self._extract_data(
-                self._response
-            )
+            self.continuation_token, self._current_page = await self._extract_data(self._response)
         return await f(self, *args, **kw)
 
     return wrapper
 
 
 class AsyncSearchPageIterator(AsyncPageIterator[ReturnType]):
     def __init__(self, client, initial_query, kwargs, continuation_token=None) -> None:
@@ -89,28 +99,22 @@
         self._initial_query = initial_query
         self._kwargs = kwargs
         self._facets = None
         self._api_version = kwargs.pop("api_version", "2020-06-30")
 
     async def _get_next_cb(self, continuation_token):
         if continuation_token is None:
-            return await self._client.documents.search_post(
-                search_request=self._initial_query.request, **self._kwargs
-            )
+            return await self._client.documents.search_post(search_request=self._initial_query.request, **self._kwargs)
 
         _next_link, next_page_request = unpack_continuation_token(continuation_token)
 
-        return await self._client.documents.search_post(
-            search_request=next_page_request, **self._kwargs
-        )
+        return await self._client.documents.search_post(search_request=next_page_request, **self._kwargs)
 
-    async def _extract_data_cb(self, response):  # pylint:disable=no-self-use
-        continuation_token = pack_continuation_token(
-            response, api_version=self._api_version
-        )
+    async def _extract_data_cb(self, response):
+        continuation_token = pack_continuation_token(response, api_version=self._api_version)
         results = [convert_search_result(r) for r in response.results]
         return continuation_token, results
 
     @_ensure_response
     async def get_facets(self):
         self.continuation_token = None
         facets = self._response.facets
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_client_async.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_client_async.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,14 +19,17 @@
     QueryAnswerType,
     QueryCaptionType,
     QueryLanguage,
     QuerySpellerType,
     QueryType,
     SearchMode,
     ScoringStatistics,
+    Vector,
+    SemanticErrorHandling,
+    QueryDebugMode,
 )
 from .._search_documents_error import RequestEntityTooLargeError
 from .._index_documents_batch import IndexDocumentsBatch
 from .._queries import AutocompleteQuery, SearchQuery, SuggestQuery
 from .._api_versions import DEFAULT_VERSION
 from .._headers_mixin import HeadersMixin
 from .._version import SDK_MONIKER
@@ -55,19 +58,15 @@
             :dedent: 4
             :caption: Creating the SearchClient with an API key.
     """
 
     _ODATA_ACCEPT: str = "application/json;odata.metadata=none"
 
     def __init__(
-        self,
-        endpoint: str,
-        index_name: str,
-        credential: Union[AzureKeyCredential, AsyncTokenCredential],
-        **kwargs
+        self, endpoint: str, index_name: str, credential: Union[AzureKeyCredential, AsyncTokenCredential], **kwargs: Any
     ) -> None:
         self._api_version = kwargs.pop("api_version", DEFAULT_VERSION)
         self._index_documents_batch = IndexDocumentsBatch()
         self._endpoint: str = endpoint
         self._index_name: str = index_name
         self._credential = credential
         audience = kwargs.pop("audience", None)
@@ -89,128 +88,137 @@
                 authentication_policy=authentication_policy,
                 sdk_moniker=SDK_MONIKER,
                 api_version=self._api_version,
                 **kwargs
             )
 
     def __repr__(self) -> str:
-        return "<SearchClient [endpoint={}, index={}]>".format(
-            repr(self._endpoint), repr(self._index_name)
-        )[:1024]
+        return "<SearchClient [endpoint={}, index={}]>".format(repr(self._endpoint), repr(self._index_name))[:1024]
 
     async def close(self) -> None:
-        """Close the :class:`~azure.search.documents.aio.SearchClient` session."""
+        """Close the :class:`~azure.search.documents.aio.SearchClient` session.
+
+        :return: None
+        :rtype: None
+        """
         return await self._client.close()
 
     @distributed_trace_async
     async def get_document_count(self, **kwargs: Any) -> int:
         """Return the number of documents in the Azure search index.
 
+        :return: The count of documents in the index
         :rtype: int
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         return int(await self._client.documents.count(**kwargs))
 
     @distributed_trace_async
     async def get_document(self, key: str, selected_fields: Optional[List[str]] = None, **kwargs: Any) -> Dict:
         """Retrieve a document from the Azure search index by its key.
 
         :param key: The primary key value for the document to retrieve
         :type key: str
-        :param selected_fields: a allowlist of fields to include in the results
-        :type selected_fields: List[str]
-        :rtype:  Dict
+        :param selected_fields: an allow-list of fields to include in the results
+        :type selected_fields: list[str]
+        :return: The document that matches the specified key
+        :rtype:  dict
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_get_document_async.py
                 :start-after: [START get_document_async]
                 :end-before: [END get_document_async]
                 :language: python
                 :dedent: 4
                 :caption: Get a specific document from the search index.
         """
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        result = await self._client.documents.get(
-            key=key, selected_fields=selected_fields, **kwargs
-        )
+        result = await self._client.documents.get(key=key, selected_fields=selected_fields, **kwargs)
         return cast(dict, result)
 
-
     @distributed_trace_async
     async def search(
-            self,
-            search_text: str,
-            *,
-            include_total_count: Optional[bool] = None,
-            facets: Optional[List[str]] = None,
-            filter: Optional[str] = None,
-            highlight_fields: Optional[str] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            order_by: Optional[List[str]] = None,
-            query_type: Optional[Union[str, QueryType]] = None,
-            scoring_parameters: Optional[List[str]] = None,
-            scoring_profile: Optional[str] = None,
-            search_fields: Optional[List[str]] = None,
-            search_mode: Optional[Union[str, SearchMode]] = None,
-            query_language: Optional[Union[str, QueryLanguage]] = None,
-            query_speller: Optional[Union[str, QuerySpellerType]] = None,
-            query_answer: Optional[Union[str, QueryAnswerType]] = None,
-            query_answer_count: Optional[int] = None,
-            query_caption: Optional[Union[str, QueryCaptionType]] = None,
-            query_caption_highlight: Optional[bool] = None,
-            semantic_fields: Optional[List[str]] = None,
-            semantic_configuration_name: Optional[str] = None,
-            select: Optional[List[str]] = None,
-            skip: Optional[int] = None,
-            top: Optional[int] = None,
-            scoring_statistics: Optional[Union[str, ScoringStatistics]] = None,
-            session_id: Optional[str] = None,
-            **kwargs) -> AsyncSearchItemPaged[Dict]:
+        self,
+        search_text: str,
+        *,
+        include_total_count: Optional[bool] = None,
+        facets: Optional[List[str]] = None,
+        filter: Optional[str] = None,
+        highlight_fields: Optional[str] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        order_by: Optional[List[str]] = None,
+        query_type: Optional[Union[str, QueryType]] = None,
+        scoring_parameters: Optional[List[str]] = None,
+        scoring_profile: Optional[str] = None,
+        search_fields: Optional[List[str]] = None,
+        search_mode: Optional[Union[str, SearchMode]] = None,
+        query_language: Optional[Union[str, QueryLanguage]] = None,
+        query_speller: Optional[Union[str, QuerySpellerType]] = None,
+        query_answer: Optional[Union[str, QueryAnswerType]] = None,
+        query_answer_count: Optional[int] = None,
+        query_answer_threshold: Optional[float] = None,
+        query_caption: Optional[Union[str, QueryCaptionType]] = None,
+        query_caption_highlight: Optional[bool] = None,
+        semantic_fields: Optional[List[str]] = None,
+        semantic_configuration_name: Optional[str] = None,
+        select: Optional[List[str]] = None,
+        skip: Optional[int] = None,
+        top: Optional[int] = None,
+        scoring_statistics: Optional[Union[str, ScoringStatistics]] = None,
+        session_id: Optional[str] = None,
+        vector: Optional[List[float]] = None,
+        top_k: Optional[int] = None,
+        vector_fields: Optional[str] = None,
+        semantic_error_handling: Optional[Union[str, SemanticErrorHandling]] = None,
+        semantic_max_wait_in_milliseconds: Optional[int] = None,
+        debug: Optional[Union[str, QueryDebugMode]] = None,
+        **kwargs
+    ) -> AsyncSearchItemPaged[Dict]:
         # pylint:disable=too-many-locals, disable=redefined-builtin
         """Search the Azure search index for documents.
 
         :param str search_text: A full-text search query expression; Use "*" or omit this parameter to
          match all documents.
         :keyword bool include_total_count: A value that specifies whether to fetch the total count of
          results. Default is false. Setting this value to true may have a performance impact. Note that
          the count returned is an approximation.
-        :keyword List[str] facets: The list of facet expressions to apply to the search query. Each facet
+        :keyword list[str] facets: The list of facet expressions to apply to the search query. Each facet
          expression contains a field name, optionally followed by a comma-separated list of name:value
          pairs.
         :keyword str filter: The OData $filter expression to apply to the search query.
         :keyword str highlight_fields: The comma-separated list of field names to use for hit highlights.
          Only searchable fields can be used for hit highlighting.
         :keyword str highlight_post_tag: A string tag that is appended to hit highlights. Must be set with
          highlightPreTag. Default is </em>.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. Default is <em>.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by a search query in order for the query to be reported as a success. This
          parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 100.
-        :keyword List[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
+        :keyword list[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
          expression can be either a field name or a call to either the geo.distance() or the
          search.score() functions. Each expression can be followed by asc to indicate ascending, and
          desc to indicate descending. The default is ascending order. Ties will be broken by the match
          scores of documents. If no OrderBy is specified, the default sort order is descending by
          document match score. There can be at most 32 $orderby clauses.
         :keyword query_type: A value that specifies the syntax of the search query. The default is
          'simple'. Use 'full' if your query uses the Lucene query syntax. Possible values include:
          'simple', 'full', "semantic".
         :paramtype query_type: str or ~azure.search.documents.models.QueryType
-        :keyword List[str] scoring_parameters: The list of parameter values to be used in scoring functions (for
+        :keyword list[str] scoring_parameters: The list of parameter values to be used in scoring functions (for
          example, referencePointParameter) using the format name-values. For example, if the scoring
          profile defines a function with a parameter called 'mylocation' the parameter string would be
          "mylocation--122.2,44.8" (without the quotes).
         :keyword str scoring_profile: The name of a scoring profile to evaluate match scores for matching
          documents in order to sort the results.
-        :keyword List[str] search_fields: The list of field names to which to scope the full-text search. When
+        :keyword list[str] search_fields: The list of field names to which to scope the full-text search. When
          using fielded search (fieldName:searchExpression) in a full Lucene query, the field names of
          each fielded search expression take precedence over any field names listed in this parameter.
         :keyword search_mode: A value that specifies whether any or all of the search terms must be
          matched in order to count the document as a match. Possible values include: 'any', 'all'.
         :paramtype search_mode: str or ~azure.search.documents.models.SearchMode
         :keyword query_language: The language of the search query. Possible values include: "none", "en-us",
          "en-gb", "en-in", "en-ca", "en-au", "fr-fr", "fr-ca", "de-de", "es-es", "es-mx", "zh-cn",
@@ -227,48 +235,66 @@
         :keyword query_answer: This parameter is only valid if the query type is 'semantic'. If set,
          the query returns answers extracted from key passages in the highest ranked documents.
          Possible values include: "none", "extractive".
         :paramtype query_answer: str or ~azure.search.documents.models.QueryAnswerType
         :keyword int query_answer_count: This parameter is only valid if the query type is 'semantic' and
          query answer is 'extractive'.
          Configures the number of answers returned. Default count is 1.
+        :keyword float query_answer_threshold: This parameter is only valid if the query type is 'semantic' and
+         query answer is 'extractive'. Configures the number of confidence threshold. Default count is 0.7.
         :keyword query_caption: This parameter is only valid if the query type is 'semantic'. If set, the
          query returns captions extracted from key passages in the highest ranked documents.
          Defaults to 'None'. Possible values include: "none", "extractive".
         :paramtype query_caption: str or ~azure.search.documents.models.QueryCaptionType
         :keyword bool query_caption_highlight: This parameter is only valid if the query type is 'semantic' when
          query caption is set to 'extractive'. Determines whether highlighting is enabled.
          Defaults to 'true'.
-        :keyword List[str] semantic_fields: The list of field names used for semantic search.
+        :keyword list[str] semantic_fields: The list of field names used for semantic search.
         :keyword semantic_configuration_name: The name of the semantic configuration that will be used when
          processing documents for queries of type semantic.
         :paramtype semantic_configuration_name: str
-        :keyword List[str] select: The list of fields to retrieve. If unspecified, all fields marked as retrievable
+        :keyword list[str] select: The list of fields to retrieve. If unspecified, all fields marked as retrievable
          in the schema are included.
         :keyword int skip: The number of search results to skip. This value cannot be greater than 100,000.
          If you need to scan documents in sequence, but cannot use $skip due to this limitation,
          consider using $orderby on a totally-ordered key and $filter with a range query instead.
         :keyword int top: The number of search results to retrieve. This can be used in conjunction with
          $skip to implement client-side paging of search results. If results are truncated due to
          server-side paging, the response will include a continuation token that can be used to issue
          another Search request for the next page of results.
         :keyword scoring_statistics: A value that specifies whether we want to calculate scoring
-        statistics (such as document frequency) globally for more consistent scoring, or locally, for
-        lower latency. The default is 'local'. Use 'global' to aggregate scoring statistics globally
-        before scoring. Using global scoring statistics can increase latency of search queries.
-        Possible values include: "local", "global".
+         statistics (such as document frequency) globally for more consistent scoring, or locally, for
+         lower latency. The default is 'local'. Use 'global' to aggregate scoring statistics globally
+         before scoring. Using global scoring statistics can increase latency of search queries.
+         Possible values include: "local", "global".
         :paramtype scoring_statistics: str or ~azure.search.documents.models.ScoringStatistics
-        :keyword session_id: A value to be used to create a sticky session, which can help getting more
-        consistent results. As long as the same sessionId is used, a best-effort attempt will be made
-        to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
-        interfere with the load balancing of the requests across replicas and adversely affect the
-        performance of the search service. The value used as sessionId cannot start with a '_'
-        character.
-        :paramtype session_id: str
-        :rtype:  AsyncSearchItemPaged[Dict]
+        :keyword str session_id: A value to be used to create a sticky session, which can help getting more
+         consistent results. As long as the same sessionId is used, a best-effort attempt will be made
+         to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
+         interfere with the load balancing of the requests across replicas and adversely affect the
+         performance of the search service. The value used as sessionId cannot start with a '_'
+         character.
+        :keyword semantic_error_handling: Allows the user to choose whether a semantic call should fail
+         completely (default / current behavior), or to return partial results. Known values are:
+         "partial" and "fail".
+        :paramtype semantic_error_handling: str or ~azure.search.documents.models.SemanticErrorHandling
+        :keyword int semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount of
+         time it takes for semantic enrichment to finish processing before the request fails.
+        :keyword debug: Enables a debugging tool that can be used to further explore your Semantic search
+         results. Known values are: "disabled", "speller", "semantic", and "all".
+        :paramtype debug: str or ~azure.search.documents.models.QueryDebugMode
+        :keyword vector: The vector representation of a search query.
+        :paramtype vector: list[float]
+        :keyword top_k: Number of nearest neighbors to return as top hits.
+        :paramtype top_k: int
+        :keyword vector_fields: Vector Fields of type Collection(Edm.Single) to be included in the vector
+          searched.
+        :paramtype vector_fields: str
+        :return: A list of documents (dicts) matching the specified search criteria.
+        :rtype:  AsyncSearchItemPaged[dict]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_simple_query_async.py
                 :start-after: [START simple_query_async]
                 :end-before: [END simple_query_async]
                 :language: python
@@ -292,21 +318,25 @@
                 :language: python
                 :dedent: 4
                 :caption: Get search result facets.
         """
         include_total_result_count = include_total_count
         filter_arg = filter
         search_fields_str = ",".join(search_fields) if search_fields else None
-        answers = query_answer if not query_answer_count else '{}|count-{}'.format(
-            query_answer, query_answer_count
-        )
-        captions = query_caption if not query_caption_highlight else '{}|highlight-{}'.format(
-            query_caption, query_caption_highlight
+        answers = query_answer if not query_answer_count else "{}|count-{}".format(query_answer, query_answer_count)
+        answers = answers if not query_answer_threshold else "{}|threshold-{}".format(answers, query_answer_threshold)
+        captions = (
+            query_caption
+            if not query_caption_highlight
+            else "{}|highlight-{}".format(query_caption, query_caption_highlight)
         )
         semantic_configuration = semantic_configuration_name
+        vector_option = None
+        if vector or top_k or vector_fields:
+            vector_option = Vector(value=vector, top_k=top_k, fields=vector_fields)
 
         query = SearchQuery(
             search_text=search_text,
             include_total_result_count=include_total_result_count,
             facets=facets,
             filter=filter_arg,
             highlight_fields=highlight_fields,
@@ -325,39 +355,42 @@
             captions=captions,
             semantic_fields=",".join(semantic_fields) if semantic_fields else None,
             semantic_configuration=semantic_configuration,
             select=select if isinstance(select, str) else None,
             skip=skip,
             top=top,
             session_id=session_id,
-            scoring_statistics=scoring_statistics
+            scoring_statistics=scoring_statistics,
+            vector=vector_option,
+            semantic_error_handling=semantic_error_handling,
+            semantic_max_wait_in_milliseconds=semantic_max_wait_in_milliseconds,
+            debug=debug,
         )
         if isinstance(select, list):
             query.select(select)
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         kwargs["api_version"] = self._api_version
-        return AsyncSearchItemPaged(
-            self._client, query, kwargs, page_iterator_class=AsyncSearchPageIterator
-        )
+        return AsyncSearchItemPaged(self._client, query, kwargs, page_iterator_class=AsyncSearchPageIterator)
 
     @distributed_trace_async
     async def suggest(
-            self,
-            search_text: str,
-            suggester_name: str,
-            *,
-            use_fuzzy_matching: Optional[bool] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            order_by: Optional[List[str]] = None,
-            search_fields: Optional[List[str]] = None,
-            select: Optional[List[str]] = None,
-            top: Optional[int] = None,
-            **kwargs) -> List[Dict]:
+        self,
+        search_text: str,
+        suggester_name: str,
+        *,
+        use_fuzzy_matching: Optional[bool] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        order_by: Optional[List[str]] = None,
+        search_fields: Optional[List[str]] = None,
+        select: Optional[List[str]] = None,
+        top: Optional[int] = None,
+        **kwargs
+    ) -> List[Dict]:
         """Get search suggestion results from the Azure search index.
 
         :param str search_text: Required. The search text to use to suggest documents. Must be at least 1
         character, and no more than 100 characters.
         :param str suggester_name: Required. The name of the suggester as specified in the suggesters
         collection that's part of the index definition.
         :keyword str filter: An OData expression that filters the documents considered for suggestions.
@@ -370,27 +403,28 @@
          highlightPreTag. If omitted, hit highlighting of suggestions is disabled.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. If omitted, hit highlighting of suggestions is disabled.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by a suggestions query in order for the query to be reported as a success. This
          parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 80.
-        :keyword List[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
+        :keyword list[str] order_by: The list of OData $orderby expressions by which to sort the results. Each
          expression can be either a field name or a call to either the geo.distance() or the
          search.score() functions. Each expression can be followed by asc to indicate ascending, or desc
          to indicate descending. The default is ascending order. Ties will be broken by the match scores
          of documents. If no $orderby is specified, the default sort order is descending by document
          match score. There can be at most 32 $orderby clauses.
-        :keyword List[str] search_fields: The list of field names to search for the specified search text. Target
+        :keyword list[str] search_fields: The list of field names to search for the specified search text. Target
          fields must be included in the specified suggester.
-        :keyword List[str] select: The list of fields to retrieve. If unspecified, only the key field will be
+        :keyword list[str] select: The list of fields to retrieve. If unspecified, only the key field will be
          included in the results.
         :keyword int top: The number of suggestions to retrieve. The value must be a number between 1 and
          100. The default is 5.
-        :rtype:  List[Dict]
+        :return: List of documents.
+        :rtype:  list[dict]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_suggestions_async.py
                 :start-after: [START suggest_query_async]
                 :end-before: [END suggest_query_async]
                 :language: python
@@ -411,34 +445,33 @@
             search_fields=search_fields_str,
             select=select if isinstance(select, str) else None,
             top=top,
         )
         if isinstance(select, list):
             query.select(select)
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        response = await self._client.documents.suggest_post(
-            suggest_request=query.request, **kwargs
-        )
+        response = await self._client.documents.suggest_post(suggest_request=query.request, **kwargs)
         results = [r.as_dict() for r in response.results]
         return results
 
     @distributed_trace_async
     async def autocomplete(
-            self,
-            search_text: str,
-            suggester_name: str,
-            *,
-            mode: Optional[Union[str, AutocompleteMode]] = None,
-            use_fuzzy_matching: Optional[bool] = None,
-            highlight_post_tag: Optional[str] = None,
-            highlight_pre_tag: Optional[str] = None,
-            minimum_coverage: Optional[float] = None,
-            search_fields: Optional[List[str]] = None,
-            top: Optional[int] = None,
-            **kwargs) -> List[Dict]:
+        self,
+        search_text: str,
+        suggester_name: str,
+        *,
+        mode: Optional[Union[str, AutocompleteMode]] = None,
+        use_fuzzy_matching: Optional[bool] = None,
+        highlight_post_tag: Optional[str] = None,
+        highlight_pre_tag: Optional[str] = None,
+        minimum_coverage: Optional[float] = None,
+        search_fields: Optional[List[str]] = None,
+        top: Optional[int] = None,
+        **kwargs
+    ) -> List[Dict]:
         """Get search auto-completion results from the Azure search index.
 
         :param str search_text: The search text on which to base autocomplete results.
         :param str suggester_name: The name of the suggester as specified in the suggesters
         collection that's part of the index definition.
         :keyword mode: Specifies the mode for Autocomplete. The default is 'oneTerm'. Use
          'twoTerms' to get shingles and 'oneTermWithContext' to use the current context while producing
@@ -455,19 +488,19 @@
          highlightPreTag. If omitted, hit highlighting is disabled.
         :keyword str highlight_pre_tag: A string tag that is prepended to hit highlights. Must be set with
          highlightPostTag. If omitted, hit highlighting is disabled.
         :keyword float minimum_coverage: A number between 0 and 100 indicating the percentage of the index that
          must be covered by an autocomplete query in order for the query to be reported as a success.
          This parameter can be useful for ensuring search availability even for services with only one
          replica. The default is 80.
-        :keyword List[str] search_fields: The list of field names to consider when querying for auto-completed
+        :keyword list[str] search_fields: The list of field names to consider when querying for auto-completed
          terms. Target fields must be included in the specified suggester.
         :keyword int top: The number of auto-completed terms to retrieve. This must be a value between 1 and
          100. The default is 5.
-        :rtype:  List[Dict]
+        :rtype:  list[Dict]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_autocomplete_async.py
                 :start-after: [START autocomplete_query_async]
                 :end-before: [END autocomplete_query_async]
                 :language: python
@@ -487,30 +520,30 @@
             highlight_pre_tag=highlight_pre_tag,
             minimum_coverage=minimum_coverage,
             search_fields=search_fields_str,
             top=top,
         )
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
-        response = await self._client.documents.autocomplete_post(
-            autocomplete_request=query.request, **kwargs
-        )
+        response = await self._client.documents.autocomplete_post(autocomplete_request=query.request, **kwargs)
         results = [r.as_dict() for r in response.results]
         return results
 
+    # pylint:disable=client-method-missing-tracing-decorator-async
     async def upload_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Upload documents to the Azure search index.
 
         An upload action is similar to an "upsert" where the document will be
         inserted if it is new and updated/replaced if it exists. All fields are
         replaced in the update case.
 
         :param documents: A list of documents to upload.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_crud_operations_async.py
                 :start-after: [START upload_document_async]
                 :end-before: [END upload_document_async]
                 :language: python
@@ -520,29 +553,31 @@
         batch = IndexDocumentsBatch()
         batch.add_upload_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = await self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator-async, delete-operation-wrong-return-type
     async def delete_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Delete documents from the Azure search index
 
         Delete removes the specified document from the index. Any field you
         specify in a delete operation, other than the key field, will be
         ignored. If you want to remove an individual field from a document, use
         `merge_documents` instead and set the field explicitly to None.
 
         Delete operations are idempotent. That is, even if a document key does
         not exist in the index, attempting a delete operation with that key will
         result in a 200 status code.
 
         :param documents: A list of documents to delete.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_crud_operations_async.py
                 :start-after: [START delete_document_async]
                 :end-before: [END delete_document_async]
                 :language: python
@@ -552,25 +587,27 @@
         batch = IndexDocumentsBatch()
         batch.add_delete_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = await self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator-async
     async def merge_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Merge documents in to existing documents in the Azure search index.
 
         Merge updates an existing document with the specified fields. If the
         document doesn't exist, the merge will fail. Any field you specify in a
         merge will replace the existing field in the document. This also applies
         to collections of primitive and complex types.
 
         :param documents: A list of documents to merge.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
 
         .. admonition:: Example:
 
             .. literalinclude:: ../samples/async_samples/sample_crud_operations_async.py
                 :start-after: [START merge_document_async]
                 :end-before: [END merge_document_async]
                 :language: python
@@ -580,78 +617,74 @@
         batch = IndexDocumentsBatch()
         batch.add_merge_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = await self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
+    # pylint:disable=client-method-missing-tracing-decorator-async
     async def merge_or_upload_documents(self, documents: List[Dict], **kwargs: Any) -> List[IndexingResult]:
         """Merge documents in to existing documents in the Azure search index,
         or upload them if they do not yet exist.
 
         This action behaves like `merge_documents` if a document with the given
         key already exists in the index. If the document does not exist, it
         behaves like `upload_documents` with a new document.
 
         :param documents: A list of documents to merge or upload.
-        :type documents: List[Dict]
-        :rtype:  List[IndexingResult]
+        :type documents: list[dict]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
         """
         batch = IndexDocumentsBatch()
         batch.add_merge_or_upload_actions(documents)
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         results = await self.index_documents(batch, **kwargs)
         return cast(List[IndexingResult], results)
 
     @distributed_trace_async
     async def index_documents(self, batch: IndexDocumentsBatch, **kwargs: Any) -> List[IndexingResult]:
         """Specify a document operations to perform as a batch.
 
         :param batch: A batch of document operations to perform.
         :type batch: IndexDocumentsBatch
-        :rtype:  List[IndexingResult]
+        :return: List of IndexingResult
+        :rtype:  list[IndexingResult]
         :raises :class:`~azure.search.documents.RequestEntityTooLargeError`
         """
         return await self._index_documents_actions(actions=batch.actions, **kwargs)
 
-    @distributed_trace_async
     async def _index_documents_actions(self, actions: List[IndexAction], **kwargs: Any) -> List[IndexingResult]:
         error_map = {413: RequestEntityTooLargeError}
 
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         batch = IndexBatch(actions=actions)
         try:
-            batch_response = await self._client.documents.index(
-                batch=batch, error_map=error_map, **kwargs
-            )
+            batch_response = await self._client.documents.index(batch=batch, error_map=error_map, **kwargs)
             return cast(List[IndexingResult], batch_response.results)
         except RequestEntityTooLargeError:
             if len(actions) == 1:
                 raise
             pos = round(len(actions) / 2)
             batch_response_first_half = await self._index_documents_actions(
                 actions=actions[:pos], error_map=error_map, **kwargs
             )
             if batch_response_first_half:
-                result_first_half = cast(
-                    List[IndexingResult], batch_response_first_half.results
-                )
+                result_first_half = cast(List[IndexingResult], batch_response_first_half.results)
             else:
                 result_first_half = []
             batch_response_second_half = await self._index_documents_actions(
                 actions=actions[pos:], error_map=error_map, **kwargs
             )
             if batch_response_second_half:
-                result_second_half = cast(
-                    List[IndexingResult], batch_response_second_half.results
-                )
+                result_second_half = cast(List[IndexingResult], batch_response_second_half.results)
             else:
                 result_second_half = []
             return result_first_half.extend(result_second_half)
 
-    async def __aenter__(self):
+    async def __aenter__(self) -> "SearchClient":
         await self._client.__aenter__()  # pylint: disable=no-member
         return self
 
-    async def __aexit__(self, *args):
+    async def __aexit__(self, *args) -> None:
         await self._client.__aexit__(*args)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/_timer.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/_timer.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/_index_documents_batch_async.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/_index_documents_batch_async.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 
 
 def _flatten_args(args: Union[List[Dict], List[List[Dict]]]) -> List[Dict]:
     if len(args) == 1 and isinstance(args[0], (List, Tuple)):
         return args[0]
     return args
 
+
 class IndexDocumentsBatch:
     """Represent a batch of update operations for documents in an Azure
     Search index.
 
     Index operations are performed in the order in which they are added
     to the batch.
 
@@ -37,15 +38,15 @@
         inserted if it is new and updated/replaced if it exists. All fields are
         replaced in the update case.
 
         :param documents: Documents to upload to an Azure search index. May be
          a single list of documents, or documents as individual parameters.
         :type documents: Dict or List[Dict]
         :return: the added actions
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return await self._extend_batch(_flatten_args(documents), "upload")
 
     async def add_delete_actions(self, *documents: Union[List[Dict], List[List[Dict]]]) -> List[IndexAction]:
         """Add documents to delete to the Azure search index.
 
         Delete removes the specified document from the index. Any field you
@@ -57,15 +58,15 @@
         not exist in the index, attempting a delete operation with that key will
         result in a 200 status code.
 
         :param documents: Documents to delete from an Azure search index. May be
          a single list of documents, or documents as individual parameters.
         :type documents: Dict or List[Dict]
         :return: the added actions
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return await self._extend_batch(_flatten_args(documents), "delete")
 
     async def add_merge_actions(self, *documents: Union[List[Dict], List[List[Dict]]]) -> List[IndexAction]:
         """Add documents to merge in to existing documents in the Azure search
         index.
 
@@ -74,63 +75,65 @@
         merge will replace the existing field in the document. This also applies
         to collections of primitive and complex types.
 
         :param documents: Documents to merge into an Azure search index. May be
          a single list of documents, or documents as individual parameters.
         :type documents: Dict or List[Dict]
         :return: the added actions
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return await self._extend_batch(_flatten_args(documents), "merge")
 
     async def add_merge_or_upload_actions(self, *documents: Union[List[Dict], List[List[Dict]]]) -> List[IndexAction]:
         """Add documents to merge in to existing documents in the Azure search
         index, or upload if they do not yet exist.
 
         This action behaves like *merge* if a document with the given key
         already exists in the index. If the document does not exist, it behaves
         like *upload* with a new document.
 
         :param documents: Documents to merge or upload into an Azure search
          index. May be a single list of documents, or documents as individual
          parameters.
-        :type documents: Dict or List[Dict]
+        :type documents: dict or list[dict]
         :return: the added actions
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return await self._extend_batch(_flatten_args(documents), "mergeOrUpload")
 
     @property
     def actions(self) -> List[IndexAction]:
         """The list of currently index actions to index.
 
-        :rtype: List[IndexAction]
+        :rtype: list[IndexAction]
         """
         return list(self._actions)
 
     async def dequeue_actions(self) -> List[IndexAction]:
         """Get the list of currently configured index actions and clear it.
 
-        :rtype: List[IndexAction]
+        :return: the list of currently configured index actions
+        :rtype: list[IndexAction]
         """
         async with self._lock:
             result = list(self._actions)
             self._actions = []
         return result
 
     async def enqueue_actions(self, new_actions: Union[IndexAction, List[IndexAction]]) -> None:
-        """Enqueue a list of index actions to index."""
+        """Enqueue a list of index actions to index.
+
+        :param new_actions: the list of index actions to enqueue
+        :type new_actions: list[IndexAction]
+        """
         if isinstance(new_actions, IndexAction):
             async with self._lock:
                 self._actions.append(new_actions)
         else:
             async with self._lock:
                 self._actions.extend(new_actions)
 
     async def _extend_batch(self, documents: List[Dict], action_type: str) -> List[IndexAction]:
-        new_actions = [
-            IndexAction(additional_properties=document, action_type=action_type)
-            for document in documents
-        ]
+        new_actions = [IndexAction(additional_properties=document, action_type=action_type) for document in documents]
         async with self._lock:
             self._actions.extend(new_actions)
         return new_actions
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/aio/_search_indexing_buffered_sender_async.py` & `azure-search-documents-11.4.0b4/azure/search/documents/aio/_search_indexing_buffered_sender_async.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,19 +48,15 @@
      audience is not considered when using a shared key. If audience is not provided, the public cloud audience
      will be assumed.
     """
 
     # pylint: disable=too-many-instance-attributes
 
     def __init__(
-        self,
-        endpoint: str,
-        index_name: str,
-        credential: Union[AzureKeyCredential, AsyncTokenCredential],
-        **kwargs
+        self, endpoint: str, index_name: str, credential: Union[AzureKeyCredential, AsyncTokenCredential], **kwargs: Any
     ) -> None:
         super(SearchIndexingBufferedSender, self).__init__(
             endpoint=endpoint, index_name=index_name, credential=credential, **kwargs
         )
         self._index_documents_batch = IndexDocumentsBatch()
         audience = kwargs.pop("audience", None)
         if isinstance(credential, AzureKeyCredential):
@@ -100,21 +96,25 @@
         return "<SearchIndexingBufferedSender [endpoint={}, index={}]>".format(
             repr(self._endpoint), repr(self._index_name)
         )[:1024]
 
     @property
     def actions(self) -> List[IndexAction]:
         """The list of currently index actions in queue to index.
-        :rtype: List[IndexAction]
+        :return: The list of currently index actions in queue to index.
+        :rtype: list[IndexAction]
         """
         return self._index_documents_batch.actions
 
     @distributed_trace_async
-    async def close(self, **kwargs) -> None:  # pylint: disable=unused-argument
-        """Close the :class:`~azure.search.documents.aio.SearchClient` session."""
+    async def close(self, **kwargs: Any) -> None:  # pylint: disable=unused-argument
+        """Close the :class:`~azure.search.documents.aio.SearchClient` session.
+        :return: None
+        :rtype: None
+        """
         await self._cleanup(flush=True)
         return await self._client.close()
 
     @distributed_trace_async
     async def flush(self, timeout: int = 86400, **kwargs) -> bool:  # pylint:disable=unused-argument
         """Flush the batch.
         :param int timeout: time out setting. Default is 86400s (one day)
@@ -155,24 +155,18 @@
                             break
             except Exception:  # pylint: disable=broad-except
                 pass
 
         self._reset_timer()
 
         try:
-            results = await self._index_documents_actions(
-                actions=actions, timeout=timeout
-            )
+            results = await self._index_documents_actions(actions=actions, timeout=timeout)
             for result in results:
                 try:
-                    action = next(
-                        x
-                        for x in actions
-                        if x.additional_properties.get(self._index_key) == result.key
-                    )
+                    action = next(x for x in actions if x.additional_properties.get(self._index_key) == result.key)
                     if result.succeeded:
                         await self._callback_succeed(action)
                     elif is_retryable_status_code(result.status_code):
                         await self._retry_action(action)
                         has_error = True
                     else:
                         await self._callback_fail(action)
@@ -212,117 +206,108 @@
         if self._auto_flush:
             self._timer = Timer(self._auto_flush_interval, self._process)
 
     @distributed_trace_async
     async def upload_documents(self, documents: List[Dict], **kwargs: Any) -> None:  # pylint: disable=unused-argument
         """Queue upload documents actions.
         :param documents: A list of documents to upload.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = await self._index_documents_batch.add_upload_actions(documents)
         await self._callback_new(actions)
         await self._process_if_needed()
 
     @distributed_trace_async
     async def delete_documents(self, documents: List[Dict], **kwargs: Any) -> None:  # pylint: disable=unused-argument
         """Queue delete documents actions
         :param documents: A list of documents to delete.
-        :type documents: List[Dict]
+        :type documents: list[Dict]
         """
         actions = await self._index_documents_batch.add_delete_actions(documents)
         await self._callback_new(actions)
         await self._process_if_needed()
 
     @distributed_trace_async
     async def merge_documents(self, documents: List[Dict], **kwargs: Any) -> None:  # pylint: disable=unused-argument
         """Queue merge documents actions
         :param documents: A list of documents to merge.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
         actions = await self._index_documents_batch.add_merge_actions(documents)
         await self._callback_new(actions)
         await self._process_if_needed()
 
     @distributed_trace_async
-    async def merge_or_upload_documents(
-        self, documents: List[Dict], **kwargs: Any
-    ) -> None:
+    async def merge_or_upload_documents(self, documents: List[Dict], **kwargs: Any) -> None:
         # pylint: disable=unused-argument
         """Queue merge documents or upload documents actions
         :param documents: A list of documents to merge or upload.
-        :type documents: List[Dict]
+        :type documents: list[dict]
         """
-        actions = await self._index_documents_batch.add_merge_or_upload_actions(
-            documents
-        )
+        actions = await self._index_documents_batch.add_merge_or_upload_actions(documents)
         await self._callback_new(actions)
         await self._process_if_needed()
 
     @distributed_trace_async
     async def index_documents(self, batch: IndexDocumentsBatch, **kwargs: Any) -> List[IndexingResult]:
         """Specify a document operations to perform as a batch.
 
         :param batch: A batch of document operations to perform.
         :type batch: IndexDocumentsBatch
-        :rtype:  List[IndexingResult]
+        :return: Indexing result for each action in the batch.
+        :rtype:  list[IndexingResult]
         :raises :class:`~azure.search.documents.RequestEntityTooLargeError`
         """
         return await self._index_documents_actions(actions=batch.actions, **kwargs)
 
     async def _index_documents_actions(self, actions: List[IndexAction], **kwargs: Any) -> List[IndexingResult]:
         error_map = {413: RequestEntityTooLargeError}
 
         timeout = kwargs.pop("timeout", 86400)
         begin_time = int(time.time())
         kwargs["headers"] = self._merge_client_headers(kwargs.get("headers"))
         batch = IndexBatch(actions=actions)
         try:
-            batch_response = await self._client.documents.index(
-                batch=batch, error_map=error_map, **kwargs
-            )
+            batch_response = await self._client.documents.index(batch=batch, error_map=error_map, **kwargs)
             return cast(List[IndexingResult], batch_response.results)
-        except RequestEntityTooLargeError:
+        except RequestEntityTooLargeError as ex:
             if len(actions) == 1:
                 raise
             pos = round(len(actions) / 2)
             if pos < self._batch_action_count:
                 self._index_documents_batch = pos
             now = int(time.time())
             remaining = timeout - (now - begin_time)
             if remaining < 0:
-                raise ServiceResponseTimeoutError("Service response time out")
+                raise ServiceResponseTimeoutError("Service response time out") from ex
             batch_response_first_half = await self._index_documents_actions(
                 actions=actions[:pos], error_map=error_map, **kwargs
             )
             if len(batch_response_first_half) > 0:
-                result_first_half = cast(
-                    List[IndexingResult], batch_response_first_half.results
-                )
+                result_first_half = cast(List[IndexingResult], batch_response_first_half.results)
             else:
                 result_first_half = []
             now = int(time.time())
             remaining = timeout - (now - begin_time)
             if remaining < 0:
-                raise ServiceResponseTimeoutError("Service response time out")
+                raise ServiceResponseTimeoutError("Service response time out") from ex
             batch_response_second_half = await self._index_documents_actions(
                 actions=actions[pos:], error_map=error_map, **kwargs
             )
             if len(batch_response_second_half) > 0:
-                result_second_half = cast(
-                    List[IndexingResult], batch_response_second_half.results
-                )
+                result_second_half = cast(List[IndexingResult], batch_response_second_half.results)
             else:
                 result_second_half = []
             return result_first_half.extend(result_second_half)
 
-    async def __aenter__(self):
+    async def __aenter__(self) -> "SearchIndexingBufferedSender":
         await self._client.__aenter__()  # pylint: disable=no-member
         return self
 
-    async def __aexit__(self, *args):
+    async def __aexit__(self, *args) -> None:
         await self.close()
         await self._client.__aexit__(*args)
 
     async def _retry_action(self, action: IndexAction) -> None:
         if not self._index_key:
             await self._callback_fail(action)
             return
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/models/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/models/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -32,15 +32,17 @@
     IndexingResult,
     QueryAnswerType,
     QueryCaptionType,
     QueryLanguage,
     QuerySpellerType,
     QueryType,
     ScoringStatistics,
-    SearchMode
+    SearchMode,
+    SemanticErrorHandling,
+    QueryDebugMode,
 )
 from .._utils import odata
 
 
 __all__ = (
     "AnswerResult",
     "AutocompleteMode",
@@ -51,8 +53,10 @@
     "QueryAnswerType",
     "QueryCaptionType",
     "QueryLanguage",
     "QuerySpellerType",
     "QueryType",
     "ScoringStatistics",
     "SearchMode",
+    "SemanticErrorHandling",
+    "QueryDebugMode",
 )
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._search_index_client import SearchIndexClient
 
 try:
     from ._patch import __all__ as _patch_all
-    from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+    from ._patch import *  # pylint: disable=unused-wildcard-import
 except ImportError:
     _patch_all = []
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "SearchIndexClient",
 ]
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/_search_index_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/_search_index_client.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,47 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from copy import deepcopy
 from typing import Any
 
 from azure.core import PipelineClient
 from azure.core.rest import HttpRequest, HttpResponse
 
-from . import models
+from . import models as _models
 from ._configuration import SearchIndexClientConfiguration
 from ._serialization import Deserializer, Serializer
 from .operations import DocumentsOperations
 
 
 class SearchIndexClient:  # pylint: disable=client-accepts-api-version-keyword
     """Client that can be used to query an index and upload, merge, or delete documents.
 
     :ivar documents: DocumentsOperations operations
     :vartype documents: search_index_client.operations.DocumentsOperations
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
     :param index_name: The name of the index. Required.
     :type index_name: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(  # pylint: disable=missing-client-constructor-parameter-credential
         self, endpoint: str, index_name: str, **kwargs: Any
     ) -> None:
         _endpoint = "{endpoint}/indexes('{indexName}')"
         self._config = SearchIndexClientConfiguration(endpoint=endpoint, index_name=index_name, **kwargs)
-        self._client = PipelineClient(base_url=_endpoint, config=self._config, **kwargs)
+        self._client: PipelineClient = PipelineClient(base_url=_endpoint, config=self._config, **kwargs)
 
-        client_models = {k: v for k, v in models.__dict__.items() if isinstance(v, type)}
+        client_models = {k: v for k, v in _models.__dict__.items() if isinstance(v, type)}
         self._serialize = Serializer(client_models)
         self._deserialize = Deserializer(client_models)
         self._serialize.client_side_validation = False
         self.documents = DocumentsOperations(self._client, self._config, self._serialize, self._deserialize)
 
     def _send_request(self, request: HttpRequest, **kwargs: Any) -> HttpResponse:
         """Runs the network request through the client's chained policies.
@@ -66,19 +66,16 @@
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
 
         request_copy.url = self._client.format_url(request_copy.url, **path_format_arguments)
         return self._client.send_request(request_copy, **kwargs)
 
-    def close(self):
-        # type: () -> None
+    def close(self) -> None:
         self._client.close()
 
-    def __enter__(self):
-        # type: () -> SearchIndexClient
+    def __enter__(self) -> "SearchIndexClient":
         self._client.__enter__()
         return self
 
-    def __exit__(self, *exc_details):
-        # type: (Any) -> None
+    def __exit__(self, *exc_details: Any) -> None:
         self._client.__exit__(*exc_details)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/_serialization.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/_serialization.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,56 +21,71 @@
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 # IN THE SOFTWARE.
 #
 # --------------------------------------------------------------------------
 
 # pylint: skip-file
+# pyright: reportUnnecessaryTypeIgnoreComment=false
 
 from base64 import b64decode, b64encode
 import calendar
 import datetime
 import decimal
 import email
 from enum import Enum
 import json
 import logging
 import re
 import sys
 import codecs
+from typing import (
+    Dict,
+    Any,
+    cast,
+    Optional,
+    Union,
+    AnyStr,
+    IO,
+    Mapping,
+    Callable,
+    TypeVar,
+    MutableMapping,
+    Type,
+    List,
+    Mapping,
+)
 
 try:
     from urllib import quote  # type: ignore
 except ImportError:
-    from urllib.parse import quote  # type: ignore
+    from urllib.parse import quote
 import xml.etree.ElementTree as ET
 
-import isodate
-
-from typing import Dict, Any, cast, TYPE_CHECKING
+import isodate  # type: ignore
 
 from azure.core.exceptions import DeserializationError, SerializationError, raise_with_traceback
+from azure.core.serialization import NULL as AzureCoreNull
 
 _BOM = codecs.BOM_UTF8.decode(encoding="utf-8")
 
-if TYPE_CHECKING:
-    from typing import Optional, Union, AnyStr, IO, Mapping
+ModelType = TypeVar("ModelType", bound="Model")
+JSON = MutableMapping[str, Any]
 
 
 class RawDeserializer:
 
     # Accept "text" because we're open minded people...
     JSON_REGEXP = re.compile(r"^(application|text)/([a-z+.]+\+)?json$")
 
     # Name used in context
     CONTEXT_NAME = "deserialized_data"
 
     @classmethod
-    def deserialize_from_text(cls, data, content_type=None):
-        # type: (Optional[Union[AnyStr, IO]], Optional[str]) -> Any
+    def deserialize_from_text(cls, data: Optional[Union[AnyStr, IO]], content_type: Optional[str] = None) -> Any:
         """Decode data according to content-type.
 
         Accept a stream of data as well, but will be load at once in memory for now.
 
         If no content-type, will return the string version (not bytes, not stream)
 
         :param data: Input, could be bytes or stream (will be decoded with UTF8) or text
@@ -128,16 +143,15 @@
                 # The function hack is because Py2.7 messes up with exception
                 # context otherwise.
                 _LOGGER.critical("Wasn't XML not JSON, failing")
                 raise_with_traceback(DeserializationError, "XML is invalid")
         raise DeserializationError("Cannot deserialize content-type: {}".format(content_type))
 
     @classmethod
-    def deserialize_from_http_generics(cls, body_bytes, headers):
-        # type: (Optional[Union[AnyStr, IO]], Mapping) -> Any
+    def deserialize_from_http_generics(cls, body_bytes: Optional[Union[AnyStr, IO]], headers: Mapping) -> Any:
         """Deserialize from HTTP response.
 
         Use bytes and headers to NOT use any requests/aiohttp or whatever
         specific implementation.
         Headers will tested for "content-type"
         """
         # Try to use content-type from headers if available
@@ -156,16 +170,16 @@
         return None
 
 
 try:
     basestring  # type: ignore
     unicode_str = unicode  # type: ignore
 except NameError:
-    basestring = str  # type: ignore
-    unicode_str = str  # type: ignore
+    basestring = str
+    unicode_str = str
 
 _LOGGER = logging.getLogger(__name__)
 
 try:
     _long_type = long  # type: ignore
 except NameError:
     _long_type = int
@@ -184,15 +198,15 @@
 
     def dst(self, dt):
         """No daylight saving for UTC."""
         return datetime.timedelta(hours=1)
 
 
 try:
-    from datetime import timezone as _FixedOffset
+    from datetime import timezone as _FixedOffset  # type: ignore
 except ImportError:  # Python 2.7
 
     class _FixedOffset(datetime.tzinfo):  # type: ignore
         """Fixed offset in minutes east from UTC.
         Copy/pasted from Python doc
         :param datetime.timedelta offset: offset in timedelta format
         """
@@ -215,15 +229,15 @@
         def __getinitargs__(self):
             return (self.__offset,)
 
 
 try:
     from datetime import timezone
 
-    TZ_UTC = timezone.utc  # type: ignore
+    TZ_UTC = timezone.utc
 except ImportError:
     TZ_UTC = UTC()  # type: ignore
 
 _FLATTEN = re.compile(r"(?<!\\)\.")
 
 
 def attribute_transformer(key, attr_desc, value):
@@ -272,79 +286,84 @@
 
 
 class Model(object):
     """Mixin for all client request body/response body models to support
     serialization and deserialization.
     """
 
-    _subtype_map = {}  # type: Dict[str, Dict[str, Any]]
-    _attribute_map = {}  # type: Dict[str, Dict[str, Any]]
-    _validation = {}  # type: Dict[str, Dict[str, Any]]
+    _subtype_map: Dict[str, Dict[str, Any]] = {}
+    _attribute_map: Dict[str, Dict[str, Any]] = {}
+    _validation: Dict[str, Dict[str, Any]] = {}
 
-    def __init__(self, **kwargs):
-        self.additional_properties = {}
+    def __init__(self, **kwargs: Any) -> None:
+        self.additional_properties: Dict[str, Any] = {}
         for k in kwargs:
             if k not in self._attribute_map:
                 _LOGGER.warning("%s is not a known attribute of class %s and will be ignored", k, self.__class__)
             elif k in self._validation and self._validation[k].get("readonly", False):
                 _LOGGER.warning("Readonly attribute %s will be ignored in class %s", k, self.__class__)
             else:
                 setattr(self, k, kwargs[k])
 
-    def __eq__(self, other):
+    def __eq__(self, other: Any) -> bool:
         """Compare objects by comparing all attributes."""
         if isinstance(other, self.__class__):
             return self.__dict__ == other.__dict__
         return False
 
-    def __ne__(self, other):
+    def __ne__(self, other: Any) -> bool:
         """Compare objects by comparing all attributes."""
         return not self.__eq__(other)
 
-    def __str__(self):
+    def __str__(self) -> str:
         return str(self.__dict__)
 
     @classmethod
-    def enable_additional_properties_sending(cls):
+    def enable_additional_properties_sending(cls) -> None:
         cls._attribute_map["additional_properties"] = {"key": "", "type": "{object}"}
 
     @classmethod
-    def is_xml_model(cls):
+    def is_xml_model(cls) -> bool:
         try:
-            cls._xml_map
+            cls._xml_map  # type: ignore
         except AttributeError:
             return False
         return True
 
     @classmethod
     def _create_xml_node(cls):
         """Create XML node."""
         try:
-            xml_map = cls._xml_map
+            xml_map = cls._xml_map  # type: ignore
         except AttributeError:
             xml_map = {}
 
         return _create_xml_node(xml_map.get("name", cls.__name__), xml_map.get("prefix", None), xml_map.get("ns", None))
 
-    def serialize(self, keep_readonly=False, **kwargs):
+    def serialize(self, keep_readonly: bool = False, **kwargs: Any) -> JSON:
         """Return the JSON that would be sent to azure from this model.
 
         This is an alias to `as_dict(full_restapi_key_transformer, keep_readonly=False)`.
 
         If you want XML serialization, you can pass the kwargs is_xml=True.
 
         :param bool keep_readonly: If you want to serialize the readonly attributes
         :returns: A dict JSON compatible object
         :rtype: dict
         """
         serializer = Serializer(self._infer_class_models())
         return serializer._serialize(self, keep_readonly=keep_readonly, **kwargs)
 
-    def as_dict(self, keep_readonly=True, key_transformer=attribute_transformer, **kwargs):
-        """Return a dict that can be JSONify using json.dump.
+    def as_dict(
+        self,
+        keep_readonly: bool = True,
+        key_transformer: Callable[[str, Dict[str, Any], Any], Any] = attribute_transformer,
+        **kwargs: Any
+    ) -> JSON:
+        """Return a dict that can be serialized using json.dump.
 
         Advanced usage might optionally use a callback as parameter:
 
         .. code::python
 
             def my_key_transformer(key, attr_desc, value):
                 return key
@@ -383,41 +402,46 @@
                 raise ValueError("Not Autorest generated code")
         except Exception:
             # Assume it's not Autorest generated (tests?). Add ourselves as dependencies.
             client_models = {cls.__name__: cls}
         return client_models
 
     @classmethod
-    def deserialize(cls, data, content_type=None):
+    def deserialize(cls: Type[ModelType], data: Any, content_type: Optional[str] = None) -> ModelType:
         """Parse a str using the RestAPI syntax and return a model.
 
         :param str data: A str using RestAPI structure. JSON by default.
         :param str content_type: JSON by default, set application/xml if XML.
         :returns: An instance of this model
         :raises: DeserializationError if something went wrong
         """
         deserializer = Deserializer(cls._infer_class_models())
         return deserializer(cls.__name__, data, content_type=content_type)
 
     @classmethod
-    def from_dict(cls, data, key_extractors=None, content_type=None):
+    def from_dict(
+        cls: Type[ModelType],
+        data: Any,
+        key_extractors: Optional[Callable[[str, Dict[str, Any], Any], Any]] = None,
+        content_type: Optional[str] = None,
+    ) -> ModelType:
         """Parse a dict using given key extractor return a model.
 
         By default consider key
         extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor
         and last_rest_key_case_insensitive_extractor)
 
         :param dict data: A dict using RestAPI structure
         :param str content_type: JSON by default, set application/xml if XML.
         :returns: An instance of this model
         :raises: DeserializationError if something went wrong
         """
         deserializer = Deserializer(cls._infer_class_models())
-        deserializer.key_extractors = (
-            [
+        deserializer.key_extractors = (  # type: ignore
+            [  # type: ignore
                 attribute_key_case_insensitive_extractor,
                 rest_key_case_insensitive_extractor,
                 last_rest_key_case_insensitive_extractor,
             ]
             if key_extractors is None
             else key_extractors
         )
@@ -449,15 +473,15 @@
             if subtype_value:
                 # Try to match base class. Can be class name only
                 # (bug to fix in Autorest to support x-ms-discriminator-name)
                 if cls.__name__ == subtype_value:
                     return cls
                 flatten_mapping_type = cls._flatten_subtype(subtype_key, objects)
                 try:
-                    return objects[flatten_mapping_type[subtype_value]]
+                    return objects[flatten_mapping_type[subtype_value]]  # type: ignore
                 except KeyError:
                     _LOGGER.warning(
                         "Subtype value %s has no mapping, use base class %s.",
                         subtype_value,
                         cls.__name__,
                     )
                     break
@@ -517,15 +541,15 @@
         "min_items": lambda x, y: len(x) < y,
         "max_items": lambda x, y: len(x) > y,
         "pattern": lambda x, y: not re.match(y, x, re.UNICODE),
         "unique": lambda x, y: len(x) != len(set(x)),
         "multiple": lambda x, y: x % y != 0,
     }
 
-    def __init__(self, classes=None):
+    def __init__(self, classes: Optional[Mapping[str, Type[ModelType]]] = None):
         self.serialize_type = {
             "iso-8601": Serializer.serialize_iso,
             "rfc-1123": Serializer.serialize_rfc,
             "unix-time": Serializer.serialize_unix,
             "duration": Serializer.serialize_duration,
             "date": Serializer.serialize_date,
             "time": Serializer.serialize_time,
@@ -533,15 +557,15 @@
             "long": Serializer.serialize_long,
             "bytearray": Serializer.serialize_bytearray,
             "base64": Serializer.serialize_base64,
             "object": self.serialize_object,
             "[]": self.serialize_iter,
             "{}": self.serialize_dict,
         }
-        self.dependencies = dict(classes) if classes else {}
+        self.dependencies: Dict[str, Type[ModelType]] = dict(classes) if classes else {}
         self.key_transformer = full_restapi_key_transformer
         self.client_side_validation = True
 
     def _serialize(self, target_obj, data_type=None, **kwargs):
         """Serialize data into a string according to type.
 
         :param target_obj: The data to be serialized.
@@ -601,47 +625,46 @@
                         xml_desc = attr_desc.get("xml", {})
                         xml_name = xml_desc.get("name", attr_desc["key"])
                         xml_prefix = xml_desc.get("prefix", None)
                         xml_ns = xml_desc.get("ns", None)
                         if xml_desc.get("attr", False):
                             if xml_ns:
                                 ET.register_namespace(xml_prefix, xml_ns)
-                                xml_name = "{}{}".format(xml_ns, xml_name)
-                            serialized.set(xml_name, new_attr)
+                                xml_name = "{{{}}}{}".format(xml_ns, xml_name)
+                            serialized.set(xml_name, new_attr)  # type: ignore
                             continue
                         if xml_desc.get("text", False):
-                            serialized.text = new_attr
+                            serialized.text = new_attr  # type: ignore
                             continue
                         if isinstance(new_attr, list):
-                            serialized.extend(new_attr)
+                            serialized.extend(new_attr)  # type: ignore
                         elif isinstance(new_attr, ET.Element):
                             # If the down XML has no XML/Name, we MUST replace the tag with the local tag. But keeping the namespaces.
                             if "name" not in getattr(orig_attr, "_xml_map", {}):
                                 splitted_tag = new_attr.tag.split("}")
                                 if len(splitted_tag) == 2:  # Namespace
                                     new_attr.tag = "}".join([splitted_tag[0], xml_name])
                                 else:
                                     new_attr.tag = xml_name
-                            serialized.append(new_attr)
+                            serialized.append(new_attr)  # type: ignore
                         else:  # That's a basic type
                             # Integrate namespace if necessary
                             local_node = _create_xml_node(xml_name, xml_prefix, xml_ns)
                             local_node.text = unicode_str(new_attr)
-                            serialized.append(local_node)
+                            serialized.append(local_node)  # type: ignore
                     else:  # JSON
-                        for k in reversed(keys):
-                            unflattened = {k: new_attr}
-                            new_attr = unflattened
+                        for k in reversed(keys):  # type: ignore
+                            new_attr = {k: new_attr}
 
                         _new_attr = new_attr
                         _serialized = serialized
-                        for k in keys:
+                        for k in keys:  # type: ignore
                             if k not in _serialized:
-                                _serialized.update(_new_attr)
-                            _new_attr = _new_attr[k]
+                                _serialized.update(_new_attr)  # type: ignore
+                            _new_attr = _new_attr[k]  # type: ignore
                             _serialized = _serialized[k]
                 except ValueError:
                     continue
 
         except (AttributeError, KeyError, TypeError) as err:
             msg = "Attribute {} in object {} cannot be serialized.\n{}".format(attr_name, class_name, str(target_obj))
             raise_with_traceback(SerializationError, msg, err)
@@ -655,31 +678,31 @@
         :param str data_type: The type to be serialized from.
         :rtype: dict
         :raises: SerializationError if serialization fails.
         :raises: ValueError if data is None
         """
 
         # Just in case this is a dict
-        internal_data_type = data_type.strip("[]{}")
-        internal_data_type = self.dependencies.get(internal_data_type, None)
+        internal_data_type_str = data_type.strip("[]{}")
+        internal_data_type = self.dependencies.get(internal_data_type_str, None)
         try:
             is_xml_model_serialization = kwargs["is_xml"]
         except KeyError:
             if internal_data_type and issubclass(internal_data_type, Model):
                 is_xml_model_serialization = kwargs.setdefault("is_xml", internal_data_type.is_xml_model())
             else:
                 is_xml_model_serialization = False
         if internal_data_type and not isinstance(internal_data_type, Enum):
             try:
                 deserializer = Deserializer(self.dependencies)
                 # Since it's on serialization, it's almost sure that format is not JSON REST
                 # We're not able to deal with additional properties for now.
                 deserializer.additional_properties_detection = False
                 if is_xml_model_serialization:
-                    deserializer.key_extractors = [
+                    deserializer.key_extractors = [  # type: ignore
                         attribute_key_case_insensitive_extractor,
                     ]
                 else:
                     deserializer.key_extractors = [
                         rest_key_case_insensitive_extractor,
                         attribute_key_case_insensitive_extractor,
                         last_rest_key_case_insensitive_extractor,
@@ -776,14 +799,16 @@
         :raises: ValueError if data is None
         :raises: SerializationError if serialization fails.
         """
         if data is None:
             raise ValueError("No value for given attribute")
 
         try:
+            if data is AzureCoreNull:
+                return None
             if data_type in self.basic_types.values():
                 return self.serialize_basic(data, data_type, **kwargs)
 
             elif data_type in self.serialize_type:
                 return self.serialize_type[data_type](data, **kwargs)
 
             # If dependencies is empty, try with current data class
@@ -839,15 +864,15 @@
         """
         try:  # If I received an enum, return its value
             return data.value
         except AttributeError:
             pass
 
         try:
-            if isinstance(data, unicode):
+            if isinstance(data, unicode):  # type: ignore
                 # Don't change it, JSON and XML ElementTree are totally able
                 # to serialize correctly u'' strings
                 return data
         except NameError:
             return str(data)
         else:
             return str(data)
@@ -997,18 +1022,18 @@
     @staticmethod
     def serialize_enum(attr, enum_obj=None):
         try:
             result = attr.value
         except AttributeError:
             result = attr
         try:
-            enum_obj(result)
+            enum_obj(result)  # type: ignore
             return result
         except ValueError:
-            for enum_value in enum_obj:
+            for enum_value in enum_obj:  # type: ignore
                 if enum_value.value.lower() == str(attr).lower():
                     return enum_value.value
             error = "{!r} is not valid value for enum {!r}"
             raise SerializationError(error.format(attr, enum_obj))
 
     @staticmethod
     def serialize_bytearray(attr, **kwargs):
@@ -1160,15 +1185,16 @@
 
 
 def rest_key_extractor(attr, attr_desc, data):
     key = attr_desc["key"]
     working_data = data
 
     while "." in key:
-        dict_keys = _FLATTEN.split(key)
+        # Need the cast, as for some reasons "split" is typed as list[str | Any]
+        dict_keys = cast(List[str], _FLATTEN.split(key))
         if len(dict_keys) == 1:
             key = _decode_attribute_map_key(dict_keys[0])
             break
         working_key = _decode_attribute_map_key(dict_keys[0])
         working_data = working_data.get(working_key, data)
         if working_data is None:
             # If at any point while following flatten JSON path see None, it means
@@ -1241,15 +1267,15 @@
     :rtype: tuple
     :returns: A tuple XML name + namespace dict
     """
     internal_type_xml_map = getattr(internal_type, "_xml_map", {})
     xml_name = internal_type_xml_map.get("name", internal_type.__name__)
     xml_ns = internal_type_xml_map.get("ns", None)
     if xml_ns:
-        xml_name = "{}{}".format(xml_ns, xml_name)
+        xml_name = "{{{}}}{}".format(xml_ns, xml_name)
     return xml_name
 
 
 def xml_key_extractor(attr, attr_desc, data):
     if isinstance(data, dict):
         return None
 
@@ -1265,15 +1291,15 @@
     is_wrapped = xml_desc.get("wrapped", False)
     internal_type = attr_desc.get("internalType", None)
     internal_type_xml_map = getattr(internal_type, "_xml_map", {})
 
     # Integrate namespace if necessary
     xml_ns = xml_desc.get("ns", internal_type_xml_map.get("ns", None))
     if xml_ns:
-        xml_name = "{}{}".format(xml_ns, xml_name)
+        xml_name = "{{{}}}{}".format(xml_ns, xml_name)
 
     # If it's an attribute, that's simple
     if xml_desc.get("attr", False):
         return data.get(xml_name)
 
     # If it's x-ms-text, that's simple too
     if xml_desc.get("text", False):
@@ -1331,15 +1357,15 @@
     :ivar list key_extractors: Ordered list of extractors to be used by this deserializer.
     """
 
     basic_types = {str: "str", int: "int", bool: "bool", float: "float"}
 
     valid_date = re.compile(r"\d{4}[-]\d{2}[-]\d{2}T\d{2}:\d{2}:\d{2}" r"\.?\d*Z?[-+]?[\d{2}]?:?[\d{2}]?")
 
-    def __init__(self, classes=None):
+    def __init__(self, classes: Optional[Mapping[str, Type[ModelType]]] = None):
         self.deserialize_type = {
             "iso-8601": Deserializer.deserialize_iso,
             "rfc-1123": Deserializer.deserialize_rfc,
             "unix-time": Deserializer.deserialize_unix,
             "duration": Deserializer.deserialize_duration,
             "date": Deserializer.deserialize_date,
             "time": Deserializer.deserialize_time,
@@ -1351,15 +1377,15 @@
             "[]": self.deserialize_iter,
             "{}": self.deserialize_dict,
         }
         self.deserialize_expected_types = {
             "duration": (isodate.Duration, datetime.timedelta),
             "iso-8601": (datetime.datetime),
         }
-        self.dependencies = dict(classes) if classes else {}
+        self.dependencies: Dict[str, Type[ModelType]] = dict(classes) if classes else {}
         self.key_extractors = [rest_key_extractor, xml_key_extractor]
         # Additional properties only works if the "rest_key_extractor" is used to
         # extract the keys. Making it to work whatever the key extractor is too much
         # complicated, with no real scenario for now.
         # So adding a flag to disable additional properties detection. This flag should be
         # used if your expect the deserialization to NOT come from a JSON REST syntax.
         # Otherwise, result are unexpected
@@ -1412,15 +1438,15 @@
             return self.deserialize_data(data, response)
         elif isinstance(response, type) and issubclass(response, Enum):
             return self.deserialize_enum(data, response)
 
         if data is None:
             return data
         try:
-            attributes = response._attribute_map
+            attributes = response._attribute_map  # type: ignore
             d_attrs = {}
             for attr, attr_desc in attributes.items():
                 # Check empty string. If it's not empty, someone has a real "additionalProperties"...
                 if attr == "additional_properties" and attr_desc["key"] == "":
                     continue
                 raw_value = None
                 # Enhance attr_desc with some dynamic data
@@ -1440,15 +1466,15 @@
                             _LOGGER.warning(msg, found_value, key_extractor, attr)
                             continue
                         raw_value = found_value
 
                 value = self.deserialize_data(raw_value, attr_desc["type"])
                 d_attrs[attr] = value
         except (AttributeError, TypeError, KeyError) as err:
-            msg = "Unable to deserialize to object: " + class_name
+            msg = "Unable to deserialize to object: " + class_name  # type: ignore
             raise_with_traceback(DeserializationError, msg, err)
         else:
             additional_properties = self._build_additional_properties(attributes, data)
             return self._instantiate_model(response, d_attrs, additional_properties)
 
     def _build_additional_properties(self, attribute_map, data):
         if not self.additional_properties_detection:
@@ -1470,40 +1496,40 @@
 
     def _classify_target(self, target, data):
         """Check to see whether the deserialization target object can
         be classified into a subclass.
         Once classification has been determined, initialize object.
 
         :param str target: The target object type to deserialize to.
-        :param str/dict data: The response data to deseralize.
+        :param str/dict data: The response data to deserialize.
         """
         if target is None:
             return None, None
 
         if isinstance(target, basestring):
             try:
                 target = self.dependencies[target]
             except KeyError:
                 return target, target
 
         try:
             target = target._classify(data, self.dependencies)
         except AttributeError:
             pass  # Target is not a Model, no classify
-        return target, target.__class__.__name__
+        return target, target.__class__.__name__  # type: ignore
 
     def failsafe_deserialize(self, target_obj, data, content_type=None):
         """Ignores any errors encountered in deserialization,
         and falls back to not deserializing the object. Recommended
         for use in error deserialization, as we want to return the
         HttpResponseError to users, and not have them deal with
         a deserialization error.
 
         :param str target_obj: The target object type to deserialize to.
-        :param str/dict data: The response data to deseralize.
+        :param str/dict data: The response data to deserialize.
         :param str content_type: Swagger "produces" if available.
         """
         try:
             return self(target_obj, data, content_type=content_type)
         except:
             _LOGGER.debug(
                 "Ran into a deserialization error. Ignoring since this is failsafe deserialization", exc_info=True
@@ -1539,15 +1565,15 @@
             return RawDeserializer.deserialize_from_http_generics(raw_data.text(), raw_data.headers)
 
         # Assume this enough to recognize requests.Response without importing it.
         if hasattr(raw_data, "_content_consumed"):
             return RawDeserializer.deserialize_from_http_generics(raw_data.text, raw_data.headers)
 
         if isinstance(raw_data, (basestring, bytes)) or hasattr(raw_data, "read"):
-            return RawDeserializer.deserialize_from_text(raw_data, content_type)
+            return RawDeserializer.deserialize_from_text(raw_data, content_type)  # type: ignore
         return raw_data
 
     def _instantiate_model(self, response, attrs, additional_properties=None):
         """Instantiate a response model passing in deserialized args.
 
         :param response: The response model class.
         :param d_attrs: The deserialized response attributes.
@@ -1561,15 +1587,15 @@
                 response_obj = response(**kwargs)
                 for attr in readonly:
                     setattr(response_obj, attr, attrs.get(attr))
                 if additional_properties:
                     response_obj.additional_properties = additional_properties
                 return response_obj
             except TypeError as err:
-                msg = "Unable to deserialize {} into model {}. ".format(kwargs, response)
+                msg = "Unable to deserialize {} into model {}. ".format(kwargs, response)  # type: ignore
                 raise DeserializationError(msg + str(err))
         else:
             try:
                 for attr, value in attrs.items():
                     setattr(response, attr, value)
                 return response
             except Exception as exp:
@@ -1743,15 +1769,15 @@
         # We might be here because we have an enum modeled as string,
         # and we try to deserialize a partial dict with enum inside
         if isinstance(data, Enum):
             return data
 
         # Consider this is real string
         try:
-            if isinstance(data, unicode):
+            if isinstance(data, unicode):  # type: ignore
                 return data
         except NameError:
             return str(data)
         else:
             return str(data)
 
     @staticmethod
@@ -1794,58 +1820,58 @@
 
         :param str attr: response string to be deserialized.
         :rtype: bytearray
         :raises: TypeError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        return bytearray(b64decode(attr))
+        return bytearray(b64decode(attr))  # type: ignore
 
     @staticmethod
     def deserialize_base64(attr):
         """Deserialize base64 encoded string into string.
 
         :param str attr: response string to be deserialized.
         :rtype: bytearray
         :raises: TypeError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        padding = "=" * (3 - (len(attr) + 3) % 4)
-        attr = attr + padding
+        padding = "=" * (3 - (len(attr) + 3) % 4)  # type: ignore
+        attr = attr + padding  # type: ignore
         encoded = attr.replace("-", "+").replace("_", "/")
         return b64decode(encoded)
 
     @staticmethod
     def deserialize_decimal(attr):
         """Deserialize string into Decimal object.
 
         :param str attr: response string to be deserialized.
         :rtype: Decimal
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            return decimal.Decimal(attr)
+            return decimal.Decimal(attr)  # type: ignore
         except decimal.DecimalException as err:
             msg = "Invalid decimal {}".format(attr)
             raise_with_traceback(DeserializationError, msg, err)
 
     @staticmethod
     def deserialize_long(attr):
         """Deserialize string into long (Py2) or int (Py3).
 
         :param str attr: response string to be deserialized.
         :rtype: long or int
         :raises: ValueError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        return _long_type(attr)
+        return _long_type(attr)  # type: ignore
 
     @staticmethod
     def deserialize_duration(attr):
         """Deserialize ISO-8601 formatted string into TimeDelta object.
 
         :param str attr: response string to be deserialized.
         :rtype: TimeDelta
@@ -1867,45 +1893,45 @@
 
         :param str attr: response string to be deserialized.
         :rtype: Date
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        if re.search(r"[^\W\d_]", attr, re.I + re.U):
+        if re.search(r"[^\W\d_]", attr, re.I + re.U):  # type: ignore
             raise DeserializationError("Date must have only digits and -. Received: %s" % attr)
         # This must NOT use defaultmonth/defaultday. Using None ensure this raises an exception.
         return isodate.parse_date(attr, defaultmonth=None, defaultday=None)
 
     @staticmethod
     def deserialize_time(attr):
         """Deserialize ISO-8601 formatted string into time object.
 
         :param str attr: response string to be deserialized.
         :rtype: datetime.time
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
-        if re.search(r"[^\W\d_]", attr, re.I + re.U):
+        if re.search(r"[^\W\d_]", attr, re.I + re.U):  # type: ignore
             raise DeserializationError("Date must have only digits and -. Received: %s" % attr)
         return isodate.parse_time(attr)
 
     @staticmethod
     def deserialize_rfc(attr):
         """Deserialize RFC-1123 formatted string into Datetime object.
 
         :param str attr: response string to be deserialized.
         :rtype: Datetime
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            parsed_date = email.utils.parsedate_tz(attr)
+            parsed_date = email.utils.parsedate_tz(attr)  # type: ignore
             date_obj = datetime.datetime(
                 *parsed_date[:6], tzinfo=_FixedOffset(datetime.timedelta(minutes=(parsed_date[9] or 0) / 60))
             )
             if not date_obj.tzinfo:
                 date_obj = date_obj.astimezone(tz=TZ_UTC)
         except ValueError as err:
             msg = "Cannot deserialize to rfc datetime object."
@@ -1920,15 +1946,15 @@
         :param str attr: response string to be deserialized.
         :rtype: Datetime
         :raises: DeserializationError if string format invalid.
         """
         if isinstance(attr, ET.Element):
             attr = attr.text
         try:
-            attr = attr.upper()
+            attr = attr.upper()  # type: ignore
             match = Deserializer.valid_date.match(attr)
             if not match:
                 raise ValueError("Invalid datetime string: " + attr)
 
             check_decimal = attr.split(".")
             if len(check_decimal) > 1:
                 decimal_str = ""
@@ -1956,15 +1982,15 @@
         This is represented as seconds.
 
         :param int attr: Object to be serialized.
         :rtype: Datetime
         :raises: DeserializationError if format invalid
         """
         if isinstance(attr, ET.Element):
-            attr = int(attr.text)
+            attr = int(attr.text)  # type: ignore
         try:
             date_obj = datetime.datetime.fromtimestamp(attr, TZ_UTC)
         except ValueError as err:
             msg = "Cannot deserialize to unix datetime object."
             raise_with_traceback(DeserializationError, msg, err)
         else:
             return date_obj
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/_configuration.py` & `azure-search-documents-11.4.0b4/azure/search/documents/indexes/_generated/aio/_configuration.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,63 +1,49 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
-import sys
 from typing import Any
 
 from azure.core.configuration import Configuration
 from azure.core.pipeline import policies
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
-
 VERSION = "unknown"
 
 
-class SearchIndexClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
-    """Configuration for SearchIndexClient.
+class SearchServiceClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
+    """Configuration for SearchServiceClient.
 
     Note that all parameters used to create this instance are saved as instance
     attributes.
 
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
-    :param index_name: The name of the index. Required.
-    :type index_name: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
-    def __init__(self, endpoint: str, index_name: str, **kwargs: Any) -> None:
-        super(SearchIndexClientConfiguration, self).__init__(**kwargs)
-        api_version = kwargs.pop("api_version", "2021-04-30-Preview")  # type: Literal["2021-04-30-Preview"]
+    def __init__(self, endpoint: str, **kwargs: Any) -> None:
+        super(SearchServiceClientConfiguration, self).__init__(**kwargs)
+        api_version: str = kwargs.pop("api_version", "2023-07-01-Preview")
 
         if endpoint is None:
             raise ValueError("Parameter 'endpoint' must not be None.")
-        if index_name is None:
-            raise ValueError("Parameter 'index_name' must not be None.")
 
         self.endpoint = endpoint
-        self.index_name = index_name
         self.api_version = api_version
-        kwargs.setdefault("sdk_moniker", "searchindexclient/{}".format(VERSION))
+        kwargs.setdefault("sdk_moniker", "searchserviceclient/{}".format(VERSION))
         self._configure(**kwargs)
 
-    def _configure(
-        self, **kwargs  # type: Any
-    ):
-        # type: (...) -> None
+    def _configure(self, **kwargs: Any) -> None:
         self.user_agent_policy = kwargs.get("user_agent_policy") or policies.UserAgentPolicy(**kwargs)
         self.headers_policy = kwargs.get("headers_policy") or policies.HeadersPolicy(**kwargs)
         self.proxy_policy = kwargs.get("proxy_policy") or policies.ProxyPolicy(**kwargs)
         self.logging_policy = kwargs.get("logging_policy") or policies.NetworkTraceLoggingPolicy(**kwargs)
         self.http_logging_policy = kwargs.get("http_logging_policy") or policies.HttpLoggingPolicy(**kwargs)
-        self.retry_policy = kwargs.get("retry_policy") or policies.RetryPolicy(**kwargs)
+        self.retry_policy = kwargs.get("retry_policy") or policies.AsyncRetryPolicy(**kwargs)
         self.custom_hook_policy = kwargs.get("custom_hook_policy") or policies.CustomHookPolicy(**kwargs)
-        self.redirect_policy = kwargs.get("redirect_policy") or policies.RedirectPolicy(**kwargs)
+        self.redirect_policy = kwargs.get("redirect_policy") or policies.AsyncRedirectPolicy(**kwargs)
         self.authentication_policy = kwargs.get("authentication_policy")
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/_vendor.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/_vendor.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
+from typing import List, cast
+
 from azure.core.pipeline.transport import HttpRequest
 
 
 def _convert_request(request, files=None):
     data = request.content if not files else None
     request = HttpRequest(method=request.method, url=request.url, headers=request.headers, data=data)
     if files:
@@ -16,10 +18,11 @@
 
 def _format_url_section(template, **kwargs):
     components = template.split("/")
     while components:
         try:
             return template.format(**kwargs)
         except KeyError as key:
-            formatted_components = template.split("/")
+            # Need the cast, as for some reasons "split" is typed as list[str | Any]
+            formatted_components = cast(List[str], template.split("/"))
             components = [c for c in formatted_components if "{}".format(key.args[0]) not in c]
             template = "/".join(components)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._documents_operations import DocumentsOperations
 
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "DocumentsOperations",
 ]
 __all__.extend([p for p in _patch_all if p not in __all__])
 _patch_sdk()
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_documents_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_documents_operations.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
+from io import IOBase
 import sys
 from typing import Any, Callable, Dict, IO, List, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
@@ -25,33 +26,27 @@
 from .._serialization import Serializer
 from .._vendor import _convert_request, _format_url_section
 
 if sys.version_info >= (3, 9):
     from collections.abc import MutableMapping
 else:
     from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, HttpResponse], T, Dict[str, Any]], Any]]
 
 _SERIALIZER = Serializer()
 _SERIALIZER.client_side_validation = False
 
 
 def build_count_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/$count")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -75,14 +70,17 @@
     highlight_pre_tag: Optional[str] = None,
     minimum_coverage: Optional[float] = None,
     order_by: Optional[List[str]] = None,
     query_type: Optional[Union[str, _models.QueryType]] = None,
     scoring_parameters: Optional[List[str]] = None,
     scoring_profile: Optional[str] = None,
     semantic_configuration: Optional[str] = None,
+    semantic_error_handling: Optional[Union[str, _models.SemanticErrorHandling]] = None,
+    semantic_max_wait_in_milliseconds: Optional[int] = None,
+    debug: Optional[Union[str, _models.QueryDebugMode]] = None,
     search_fields: Optional[List[str]] = None,
     query_language: Optional[Union[str, _models.QueryLanguage]] = None,
     speller: Optional[Union[str, _models.Speller]] = None,
     answers: Optional[Union[str, _models.Answers]] = None,
     search_mode: Optional[Union[str, _models.SearchMode]] = None,
     scoring_statistics: Optional[Union[str, _models.ScoringStatistics]] = None,
     session_id: Optional[str] = None,
@@ -93,17 +91,15 @@
     semantic_fields: Optional[List[str]] = None,
     x_ms_client_request_id: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs")
 
     # Construct parameters
     if search_text is not None:
@@ -130,14 +126,22 @@
         _params["scoringParameter"] = [
             _SERIALIZER.query("scoring_parameters", q, "str") if q is not None else "" for q in scoring_parameters
         ]
     if scoring_profile is not None:
         _params["scoringProfile"] = _SERIALIZER.query("scoring_profile", scoring_profile, "str")
     if semantic_configuration is not None:
         _params["semanticConfiguration"] = _SERIALIZER.query("semantic_configuration", semantic_configuration, "str")
+    if semantic_error_handling is not None:
+        _params["semanticErrorHandling"] = _SERIALIZER.query("semantic_error_handling", semantic_error_handling, "str")
+    if semantic_max_wait_in_milliseconds is not None:
+        _params["semanticMaxWaitInMilliseconds"] = _SERIALIZER.query(
+            "semantic_max_wait_in_milliseconds", semantic_max_wait_in_milliseconds, "int", minimum=700
+        )
+    if debug is not None:
+        _params["debug"] = _SERIALIZER.query("debug", debug, "str")
     if search_fields is not None:
         _params["searchFields"] = _SERIALIZER.query("search_fields", search_fields, "[str]", div=",")
     if query_language is not None:
         _params["queryLanguage"] = _SERIALIZER.query("query_language", query_language, "str")
     if speller is not None:
         _params["speller"] = _SERIALIZER.query("speller", speller, "str")
     if answers is not None:
@@ -168,18 +172,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_search_post_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.post.search")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -200,26 +202,24 @@
     selected_fields: Optional[List[str]] = None,
     x_ms_client_request_id: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs('{key}')")
     path_format_arguments = {
         "key": _SERIALIZER.url("key", key, "str"),
     }
 
-    _url = _format_url_section(_url, **path_format_arguments)
+    _url: str = _format_url_section(_url, **path_format_arguments)  # type: ignore
 
     # Construct parameters
     if selected_fields is not None:
         _params["$select"] = _SERIALIZER.query("selected_fields", selected_fields, "[str]", div=",")
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
 
     # Construct headers
@@ -245,17 +245,15 @@
     top: Optional[int] = None,
     x_ms_client_request_id: Optional[str] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.suggest")
 
     # Construct parameters
     _params["search"] = _SERIALIZER.query("search_text", search_text, "str")
@@ -288,18 +286,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_suggest_post_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.post.suggest")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -314,18 +310,16 @@
     return HttpRequest(method="POST", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_index_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.index")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -354,17 +348,15 @@
     search_fields: Optional[List[str]] = None,
     top: Optional[int] = None,
     **kwargs: Any
 ) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.autocomplete")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -395,18 +387,16 @@
     return HttpRequest(method="GET", url=_url, params=_params, headers=_headers, **kwargs)
 
 
 def build_autocomplete_post_request(*, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> HttpRequest:
     _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
     _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-    api_version = kwargs.pop(
-        "api_version", _params.pop("api-version", "2021-04-30-Preview")
-    )  # type: Literal["2021-04-30-Preview"]
-    content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
+    api_version: str = kwargs.pop("api_version", _params.pop("api-version", "2023-07-01-Preview"))
+    content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
     accept = _headers.pop("Accept", "application/json")
 
     # Construct URL
     _url = kwargs.pop("template_url", "/docs/search.post.autocomplete")
 
     # Construct parameters
     _params["api-version"] = _SERIALIZER.query("api_version", api_version, "str")
@@ -440,14 +430,17 @@
         self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
         self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")
 
     @distributed_trace
     def count(self, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any) -> int:
         """Queries the number of documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Count-Documents
+
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: int or the result of cls(response)
         :rtype: int
         :raises ~azure.core.exceptions.HttpResponseError:
         """
@@ -458,18 +451,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[int]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[int] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_count_request(
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -479,18 +470,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -499,26 +491,29 @@
         deserialized = self._deserialize("int", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    count.metadata = {"url": "/docs/$count"}  # type: ignore
+    count.metadata = {"url": "/docs/$count"}
 
     @distributed_trace
     def search_get(
         self,
         search_text: Optional[str] = None,
         search_options: Optional[_models.SearchOptions] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_text: A full-text search query expression; Use "*" or omit this parameter to
          match all documents. Default value is None.
         :type search_text: str
         :param search_options: Parameter group. Default value is None.
         :type search_options: ~search_index_client.models.SearchOptions
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -534,31 +529,32 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchDocumentsResult] = kwargs.pop("cls", None)
 
         _include_total_result_count = None
         _facets = None
         _filter = None
         _highlight_fields = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
         _minimum_coverage = None
         _order_by = None
         _query_type = None
         _scoring_parameters = None
         _scoring_profile = None
         _semantic_configuration = None
+        _semantic_error_handling = None
+        _semantic_max_wait_in_milliseconds = None
+        _debug = None
         _search_fields = None
         _query_language = None
         _speller = None
         _answers = None
         _search_mode = None
         _scoring_statistics = None
         _session_id = None
@@ -567,14 +563,15 @@
         _top = None
         _captions = None
         _semantic_fields = None
         _x_ms_client_request_id = None
         if search_options is not None:
             _answers = search_options.answers
             _captions = search_options.captions
+            _debug = search_options.debug
             _facets = search_options.facets
             _filter = search_options.filter
             _highlight_fields = search_options.highlight_fields
             _highlight_post_tag = search_options.highlight_post_tag
             _highlight_pre_tag = search_options.highlight_pre_tag
             _include_total_result_count = search_options.include_total_result_count
             _minimum_coverage = search_options.minimum_coverage
@@ -584,15 +581,17 @@
             _scoring_parameters = search_options.scoring_parameters
             _scoring_profile = search_options.scoring_profile
             _scoring_statistics = search_options.scoring_statistics
             _search_fields = search_options.search_fields
             _search_mode = search_options.search_mode
             _select = search_options.select
             _semantic_configuration = search_options.semantic_configuration
+            _semantic_error_handling = search_options.semantic_error_handling
             _semantic_fields = search_options.semantic_fields
+            _semantic_max_wait_in_milliseconds = search_options.semantic_max_wait_in_milliseconds
             _session_id = search_options.session_id
             _skip = search_options.skip
             _speller = search_options.speller
             _top = search_options.top
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
@@ -606,14 +605,17 @@
             highlight_pre_tag=_highlight_pre_tag,
             minimum_coverage=_minimum_coverage,
             order_by=_order_by,
             query_type=_query_type,
             scoring_parameters=_scoring_parameters,
             scoring_profile=_scoring_profile,
             semantic_configuration=_semantic_configuration,
+            semantic_error_handling=_semantic_error_handling,
+            semantic_max_wait_in_milliseconds=_semantic_max_wait_in_milliseconds,
+            debug=_debug,
             search_fields=_search_fields,
             query_language=_query_language,
             speller=_speller,
             answers=_answers,
             search_mode=_search_mode,
             scoring_statistics=_scoring_statistics,
             session_id=_session_id,
@@ -629,18 +631,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -649,27 +652,30 @@
         deserialized = self._deserialize("SearchDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    search_get.metadata = {"url": "/docs"}  # type: ignore
+    search_get.metadata = {"url": "/docs"}
 
     @overload
     def search_post(
         self,
         search_request: _models.SearchRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_request: The definition of the Search request. Required.
         :type search_request: ~search_index_client.models.SearchRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -686,14 +692,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_request: The definition of the Search request. Required.
         :type search_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -708,16 +717,19 @@
         self,
         search_request: Union[_models.SearchRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
-        :param search_request: The definition of the Search request. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
+        :param search_request: The definition of the Search request. Is either a SearchRequest type or
+         a IO type. Required.
         :type search_request: ~search_index_client.models.SearchRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -732,27 +744,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(search_request, (IO, bytes)):
+        if isinstance(search_request, (IOBase, bytes)):
             _content = search_request
         else:
             _json = self._serialize.body(search_request, "SearchRequest")
 
         request = build_search_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -764,18 +774,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -784,26 +795,29 @@
         deserialized = self._deserialize("SearchDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    search_post.metadata = {"url": "/docs/search.post.search"}  # type: ignore
+    search_post.metadata = {"url": "/docs/search.post.search"}
 
     @distributed_trace
     def get(
         self,
         key: str,
         selected_fields: Optional[List[str]] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> JSON:
         """Retrieves a document from the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/lookup-document
+
         :param key: The key of the document to retrieve. Required.
         :type key: str
         :param selected_fields: List of field names to retrieve for the document; Any field not
          retrieved will be missing from the returned document. Default value is None.
         :type selected_fields: list[str]
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -819,18 +833,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[JSON]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[JSON] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             key=key,
@@ -842,18 +854,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -862,27 +875,30 @@
         deserialized = self._deserialize("object", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/docs('{key}')"}  # type: ignore
+    get.metadata = {"url": "/docs('{key}')"}
 
     @distributed_trace
     def suggest_get(
         self,
         search_text: str,
         suggester_name: str,
         suggest_options: Optional[_models.SuggestOptions] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param search_text: The search text to use to suggest documents. Must be at least 1 character,
          and no more than 100 characters. Required.
         :type search_text: str
         :param suggester_name: The name of the suggester as specified in the suggesters collection
          that's part of the index definition. Required.
         :type suggester_name: str
         :param suggest_options: Parameter group. Default value is None.
@@ -901,18 +917,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SuggestDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SuggestDocumentsResult] = kwargs.pop("cls", None)
 
         _filter = None
         _use_fuzzy_matching = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
         _minimum_coverage = None
         _order_by = None
@@ -952,18 +966,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -972,27 +987,30 @@
         deserialized = self._deserialize("SuggestDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    suggest_get.metadata = {"url": "/docs/search.suggest"}  # type: ignore
+    suggest_get.metadata = {"url": "/docs/search.suggest"}
 
     @overload
     def suggest_post(
         self,
         suggest_request: _models.SuggestRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param suggest_request: The Suggest request. Required.
         :type suggest_request: ~search_index_client.models.SuggestRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1009,14 +1027,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param suggest_request: The Suggest request. Required.
         :type suggest_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1031,15 +1052,19 @@
         self,
         suggest_request: Union[_models.SuggestRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
-        :param suggest_request: The Suggest request. Is either a model type or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
+        :param suggest_request: The Suggest request. Is either a SuggestRequest type or a IO type.
+         Required.
         :type suggest_request: ~search_index_client.models.SuggestRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1054,27 +1079,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SuggestDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SuggestDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(suggest_request, (IO, bytes)):
+        if isinstance(suggest_request, (IOBase, bytes)):
             _content = suggest_request
         else:
             _json = self._serialize.body(suggest_request, "SuggestRequest")
 
         request = build_suggest_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -1086,18 +1109,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1106,27 +1130,30 @@
         deserialized = self._deserialize("SuggestDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    suggest_post.metadata = {"url": "/docs/search.post.suggest"}  # type: ignore
+    suggest_post.metadata = {"url": "/docs/search.post.suggest"}
 
     @overload
     def index(
         self,
         batch: _models.IndexBatch,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
         :param batch: The batch of index actions. Required.
         :type batch: ~search_index_client.models.IndexBatch
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1143,14 +1170,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
         :param batch: The batch of index actions. Required.
         :type batch: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1165,15 +1195,18 @@
         self,
         batch: Union[_models.IndexBatch, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
-        :param batch: The batch of index actions. Is either a model type or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
+        :param batch: The batch of index actions. Is either a IndexBatch type or a IO type. Required.
         :type batch: ~search_index_client.models.IndexBatch or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1188,27 +1221,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.IndexDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.IndexDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(batch, (IO, bytes)):
+        if isinstance(batch, (IOBase, bytes)):
             _content = batch
         else:
             _json = self._serialize.body(batch, "IndexBatch")
 
         request = build_index_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -1220,18 +1251,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 207]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1240,31 +1272,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("IndexDocumentsResult", pipeline_response)
 
         if response.status_code == 207:
             deserialized = self._deserialize("IndexDocumentsResult", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    index.metadata = {"url": "/docs/search.index"}  # type: ignore
+    index.metadata = {"url": "/docs/search.index"}
 
     @distributed_trace
     def autocomplete_get(
         self,
         search_text: str,
         suggester_name: str,
         request_options: Optional[_models.RequestOptions] = None,
         autocomplete_options: Optional[_models.AutocompleteOptions] = None,
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param search_text: The incomplete term which should be auto-completed. Required.
         :type search_text: str
         :param suggester_name: The name of the suggester as specified in the suggesters collection
          that's part of the index definition. Required.
         :type suggester_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -1282,18 +1317,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AutocompleteResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.AutocompleteResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         _autocomplete_mode = None
         _filter = None
         _use_fuzzy_matching = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
@@ -1330,18 +1363,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1350,27 +1384,30 @@
         deserialized = self._deserialize("AutocompleteResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    autocomplete_get.metadata = {"url": "/docs/search.autocomplete"}  # type: ignore
+    autocomplete_get.metadata = {"url": "/docs/search.autocomplete"}
 
     @overload
     def autocomplete_post(
         self,
         autocomplete_request: _models.AutocompleteRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param autocomplete_request: The definition of the Autocomplete request. Required.
         :type autocomplete_request: ~search_index_client.models.AutocompleteRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1387,14 +1424,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param autocomplete_request: The definition of the Autocomplete request. Required.
         :type autocomplete_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1409,16 +1449,19 @@
         self,
         autocomplete_request: Union[_models.AutocompleteRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
-        :param autocomplete_request: The definition of the Autocomplete request. Is either a model type
-         or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
+        :param autocomplete_request: The definition of the Autocomplete request. Is either a
+         AutocompleteRequest type or a IO type. Required.
         :type autocomplete_request: ~search_index_client.models.AutocompleteRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1433,27 +1476,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AutocompleteResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.AutocompleteResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(autocomplete_request, (IO, bytes)):
+        if isinstance(autocomplete_request, (IOBase, bytes)):
             _content = autocomplete_request
         else:
             _json = self._serialize.body(autocomplete_request, "AutocompleteRequest")
 
         request = build_autocomplete_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -1465,18 +1506,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1485,8 +1527,8 @@
         deserialized = self._deserialize("AutocompleteResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    autocomplete_post.metadata = {"url": "/docs/search.post.autocomplete"}  # type: ignore
+    autocomplete_post.metadata = {"url": "/docs/search.post.autocomplete"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/operations/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._search_index_client import SearchIndexClient
 
 try:
     from ._patch import __all__ as _patch_all
-    from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+    from ._patch import *  # pylint: disable=unused-wildcard-import
 except ImportError:
     _patch_all = []
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "SearchIndexClient",
 ]
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_search_index_client.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_search_index_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,47 +1,47 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from copy import deepcopy
 from typing import Any, Awaitable
 
 from azure.core import AsyncPipelineClient
 from azure.core.rest import AsyncHttpResponse, HttpRequest
 
-from .. import models
+from .. import models as _models
 from .._serialization import Deserializer, Serializer
 from ._configuration import SearchIndexClientConfiguration
 from .operations import DocumentsOperations
 
 
 class SearchIndexClient:  # pylint: disable=client-accepts-api-version-keyword
     """Client that can be used to query an index and upload, merge, or delete documents.
 
     :ivar documents: DocumentsOperations operations
     :vartype documents: search_index_client.aio.operations.DocumentsOperations
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
     :param index_name: The name of the index. Required.
     :type index_name: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(  # pylint: disable=missing-client-constructor-parameter-credential
         self, endpoint: str, index_name: str, **kwargs: Any
     ) -> None:
         _endpoint = "{endpoint}/indexes('{indexName}')"
         self._config = SearchIndexClientConfiguration(endpoint=endpoint, index_name=index_name, **kwargs)
-        self._client = AsyncPipelineClient(base_url=_endpoint, config=self._config, **kwargs)
+        self._client: AsyncPipelineClient = AsyncPipelineClient(base_url=_endpoint, config=self._config, **kwargs)
 
-        client_models = {k: v for k, v in models.__dict__.items() if isinstance(v, type)}
+        client_models = {k: v for k, v in _models.__dict__.items() if isinstance(v, type)}
         self._serialize = Serializer(client_models)
         self._deserialize = Deserializer(client_models)
         self._serialize.client_side_validation = False
         self.documents = DocumentsOperations(self._client, self._config, self._serialize, self._deserialize)
 
     def _send_request(self, request: HttpRequest, **kwargs: Any) -> Awaitable[AsyncHttpResponse]:
         """Runs the network request through the client's chained policies.
@@ -73,9 +73,9 @@
     async def close(self) -> None:
         await self._client.close()
 
     async def __aenter__(self) -> "SearchIndexClient":
         await self._client.__aenter__()
         return self
 
-    async def __aexit__(self, *exc_details) -> None:
+    async def __aexit__(self, *exc_details: Any) -> None:
         await self._client.__aexit__(*exc_details)
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/operations/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/_configuration.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/_configuration.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,45 +1,39 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
-import sys
 from typing import Any
 
 from azure.core.configuration import Configuration
 from azure.core.pipeline import policies
 
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
-
 VERSION = "unknown"
 
 
 class SearchIndexClientConfiguration(Configuration):  # pylint: disable=too-many-instance-attributes
     """Configuration for SearchIndexClient.
 
     Note that all parameters used to create this instance are saved as instance
     attributes.
 
     :param endpoint: The endpoint URL of the search service. Required.
     :type endpoint: str
     :param index_name: The name of the index. Required.
     :type index_name: str
-    :keyword api_version: Api Version. Default value is "2021-04-30-Preview". Note that overriding
+    :keyword api_version: Api Version. Default value is "2023-07-01-Preview". Note that overriding
      this default value may result in unsupported behavior.
     :paramtype api_version: str
     """
 
     def __init__(self, endpoint: str, index_name: str, **kwargs: Any) -> None:
         super(SearchIndexClientConfiguration, self).__init__(**kwargs)
-        api_version = kwargs.pop("api_version", "2021-04-30-Preview")  # type: Literal["2021-04-30-Preview"]
+        api_version: str = kwargs.pop("api_version", "2023-07-01-Preview")
 
         if endpoint is None:
             raise ValueError("Parameter 'endpoint' must not be None.")
         if index_name is None:
             raise ValueError("Parameter 'index_name' must not be None.")
 
         self.endpoint = endpoint
@@ -50,11 +44,11 @@
 
     def _configure(self, **kwargs: Any) -> None:
         self.user_agent_policy = kwargs.get("user_agent_policy") or policies.UserAgentPolicy(**kwargs)
         self.headers_policy = kwargs.get("headers_policy") or policies.HeadersPolicy(**kwargs)
         self.proxy_policy = kwargs.get("proxy_policy") or policies.ProxyPolicy(**kwargs)
         self.logging_policy = kwargs.get("logging_policy") or policies.NetworkTraceLoggingPolicy(**kwargs)
         self.http_logging_policy = kwargs.get("http_logging_policy") or policies.HttpLoggingPolicy(**kwargs)
-        self.retry_policy = kwargs.get("retry_policy") or policies.AsyncRetryPolicy(**kwargs)
+        self.retry_policy = kwargs.get("retry_policy") or policies.RetryPolicy(**kwargs)
         self.custom_hook_policy = kwargs.get("custom_hook_policy") or policies.CustomHookPolicy(**kwargs)
-        self.redirect_policy = kwargs.get("redirect_policy") or policies.AsyncRedirectPolicy(**kwargs)
+        self.redirect_policy = kwargs.get("redirect_policy") or policies.RedirectPolicy(**kwargs)
         self.authentication_policy = kwargs.get("authentication_policy")
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._documents_operations import DocumentsOperations
 
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "DocumentsOperations",
 ]
 __all__.extend([p for p in _patch_all if p not in __all__])
 _patch_sdk()
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_documents_operations.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_documents_operations.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # pylint: disable=too-many-lines
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
+from io import IOBase
 import sys
 from typing import Any, Callable, Dict, IO, List, Optional, TypeVar, Union, overload
 
 from azure.core.exceptions import (
     ClientAuthenticationError,
     HttpResponseError,
     ResourceExistsError,
@@ -35,18 +36,14 @@
     build_suggest_post_request,
 )
 
 if sys.version_info >= (3, 9):
     from collections.abc import MutableMapping
 else:
     from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
-if sys.version_info >= (3, 8):
-    from typing import Literal  # pylint: disable=no-name-in-module, ungrouped-imports
-else:
-    from typing_extensions import Literal  # type: ignore  # pylint: disable=ungrouped-imports
 JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
 T = TypeVar("T")
 ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]
 
 
 class DocumentsOperations:
     """
@@ -67,14 +64,17 @@
         self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
         self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")
 
     @distributed_trace_async
     async def count(self, request_options: Optional[_models.RequestOptions] = None, **kwargs: Any) -> int:
         """Queries the number of documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Count-Documents
+
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword callable cls: A custom type or function that will be passed the direct response
         :return: int or the result of cls(response)
         :rtype: int
         :raises ~azure.core.exceptions.HttpResponseError:
         """
@@ -85,18 +85,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[int]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[int] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_count_request(
             x_ms_client_request_id=_x_ms_client_request_id,
@@ -106,18 +104,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -126,26 +125,29 @@
         deserialized = self._deserialize("int", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    count.metadata = {"url": "/docs/$count"}  # type: ignore
+    count.metadata = {"url": "/docs/$count"}
 
     @distributed_trace_async
     async def search_get(
         self,
         search_text: Optional[str] = None,
         search_options: Optional[_models.SearchOptions] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_text: A full-text search query expression; Use "*" or omit this parameter to
          match all documents. Default value is None.
         :type search_text: str
         :param search_options: Parameter group. Default value is None.
         :type search_options: ~search_index_client.models.SearchOptions
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -161,31 +163,32 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SearchDocumentsResult] = kwargs.pop("cls", None)
 
         _include_total_result_count = None
         _facets = None
         _filter = None
         _highlight_fields = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
         _minimum_coverage = None
         _order_by = None
         _query_type = None
         _scoring_parameters = None
         _scoring_profile = None
         _semantic_configuration = None
+        _semantic_error_handling = None
+        _semantic_max_wait_in_milliseconds = None
+        _debug = None
         _search_fields = None
         _query_language = None
         _speller = None
         _answers = None
         _search_mode = None
         _scoring_statistics = None
         _session_id = None
@@ -194,14 +197,15 @@
         _top = None
         _captions = None
         _semantic_fields = None
         _x_ms_client_request_id = None
         if search_options is not None:
             _answers = search_options.answers
             _captions = search_options.captions
+            _debug = search_options.debug
             _facets = search_options.facets
             _filter = search_options.filter
             _highlight_fields = search_options.highlight_fields
             _highlight_post_tag = search_options.highlight_post_tag
             _highlight_pre_tag = search_options.highlight_pre_tag
             _include_total_result_count = search_options.include_total_result_count
             _minimum_coverage = search_options.minimum_coverage
@@ -211,15 +215,17 @@
             _scoring_parameters = search_options.scoring_parameters
             _scoring_profile = search_options.scoring_profile
             _scoring_statistics = search_options.scoring_statistics
             _search_fields = search_options.search_fields
             _search_mode = search_options.search_mode
             _select = search_options.select
             _semantic_configuration = search_options.semantic_configuration
+            _semantic_error_handling = search_options.semantic_error_handling
             _semantic_fields = search_options.semantic_fields
+            _semantic_max_wait_in_milliseconds = search_options.semantic_max_wait_in_milliseconds
             _session_id = search_options.session_id
             _skip = search_options.skip
             _speller = search_options.speller
             _top = search_options.top
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
@@ -233,14 +239,17 @@
             highlight_pre_tag=_highlight_pre_tag,
             minimum_coverage=_minimum_coverage,
             order_by=_order_by,
             query_type=_query_type,
             scoring_parameters=_scoring_parameters,
             scoring_profile=_scoring_profile,
             semantic_configuration=_semantic_configuration,
+            semantic_error_handling=_semantic_error_handling,
+            semantic_max_wait_in_milliseconds=_semantic_max_wait_in_milliseconds,
+            debug=_debug,
             search_fields=_search_fields,
             query_language=_query_language,
             speller=_speller,
             answers=_answers,
             search_mode=_search_mode,
             scoring_statistics=_scoring_statistics,
             session_id=_session_id,
@@ -256,18 +265,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -276,27 +286,30 @@
         deserialized = self._deserialize("SearchDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    search_get.metadata = {"url": "/docs"}  # type: ignore
+    search_get.metadata = {"url": "/docs"}
 
     @overload
     async def search_post(
         self,
         search_request: _models.SearchRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_request: The definition of the Search request. Required.
         :type search_request: ~search_index_client.models.SearchRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -313,14 +326,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
         :param search_request: The definition of the Search request. Required.
         :type search_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -335,16 +351,19 @@
         self,
         search_request: Union[_models.SearchRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SearchDocumentsResult:
         """Searches for documents in the index.
 
-        :param search_request: The definition of the Search request. Is either a model type or a IO
-         type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/Search-Documents
+
+        :param search_request: The definition of the Search request. Is either a SearchRequest type or
+         a IO type. Required.
         :type search_request: ~search_index_client.models.SearchRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -359,27 +378,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SearchDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SearchDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(search_request, (IO, bytes)):
+        if isinstance(search_request, (IOBase, bytes)):
             _content = search_request
         else:
             _json = self._serialize.body(search_request, "SearchRequest")
 
         request = build_search_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -391,18 +408,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -411,26 +429,29 @@
         deserialized = self._deserialize("SearchDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    search_post.metadata = {"url": "/docs/search.post.search"}  # type: ignore
+    search_post.metadata = {"url": "/docs/search.post.search"}
 
     @distributed_trace_async
     async def get(
         self,
         key: str,
         selected_fields: Optional[List[str]] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> JSON:
         """Retrieves a document from the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/lookup-document
+
         :param key: The key of the document to retrieve. Required.
         :type key: str
         :param selected_fields: List of field names to retrieve for the document; Any field not
          retrieved will be missing from the returned document. Default value is None.
         :type selected_fields: list[str]
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -446,18 +467,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[JSON]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[JSON] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
 
         request = build_get_request(
             key=key,
@@ -469,18 +488,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -489,27 +509,30 @@
         deserialized = self._deserialize("object", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    get.metadata = {"url": "/docs('{key}')"}  # type: ignore
+    get.metadata = {"url": "/docs('{key}')"}
 
     @distributed_trace_async
     async def suggest_get(
         self,
         search_text: str,
         suggester_name: str,
         suggest_options: Optional[_models.SuggestOptions] = None,
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param search_text: The search text to use to suggest documents. Must be at least 1 character,
          and no more than 100 characters. Required.
         :type search_text: str
         :param suggester_name: The name of the suggester as specified in the suggesters collection
          that's part of the index definition. Required.
         :type suggester_name: str
         :param suggest_options: Parameter group. Default value is None.
@@ -528,18 +551,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SuggestDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.SuggestDocumentsResult] = kwargs.pop("cls", None)
 
         _filter = None
         _use_fuzzy_matching = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
         _minimum_coverage = None
         _order_by = None
@@ -579,18 +600,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -599,27 +621,30 @@
         deserialized = self._deserialize("SuggestDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    suggest_get.metadata = {"url": "/docs/search.suggest"}  # type: ignore
+    suggest_get.metadata = {"url": "/docs/search.suggest"}
 
     @overload
     async def suggest_post(
         self,
         suggest_request: _models.SuggestRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param suggest_request: The Suggest request. Required.
         :type suggest_request: ~search_index_client.models.SuggestRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -636,14 +661,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
         :param suggest_request: The Suggest request. Required.
         :type suggest_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -658,15 +686,19 @@
         self,
         suggest_request: Union[_models.SuggestRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.SuggestDocumentsResult:
         """Suggests documents in the index that match the given partial query text.
 
-        :param suggest_request: The Suggest request. Is either a model type or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/suggestions
+
+        :param suggest_request: The Suggest request. Is either a SuggestRequest type or a IO type.
+         Required.
         :type suggest_request: ~search_index_client.models.SuggestRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -681,27 +713,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.SuggestDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.SuggestDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(suggest_request, (IO, bytes)):
+        if isinstance(suggest_request, (IOBase, bytes)):
             _content = suggest_request
         else:
             _json = self._serialize.body(suggest_request, "SuggestRequest")
 
         request = build_suggest_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -713,18 +743,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -733,27 +764,30 @@
         deserialized = self._deserialize("SuggestDocumentsResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    suggest_post.metadata = {"url": "/docs/search.post.suggest"}  # type: ignore
+    suggest_post.metadata = {"url": "/docs/search.post.suggest"}
 
     @overload
     async def index(
         self,
         batch: _models.IndexBatch,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
         :param batch: The batch of index actions. Required.
         :type batch: ~search_index_client.models.IndexBatch
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -770,14 +804,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
         :param batch: The batch of index actions. Required.
         :type batch: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -792,15 +829,18 @@
         self,
         batch: Union[_models.IndexBatch, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.IndexDocumentsResult:
         """Sends a batch of document write actions to the index.
 
-        :param batch: The batch of index actions. Is either a model type or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/addupdate-or-delete-documents
+
+        :param batch: The batch of index actions. Is either a IndexBatch type or a IO type. Required.
         :type batch: ~search_index_client.models.IndexBatch or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -815,27 +855,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.IndexDocumentsResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.IndexDocumentsResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(batch, (IO, bytes)):
+        if isinstance(batch, (IOBase, bytes)):
             _content = batch
         else:
             _json = self._serialize.body(batch, "IndexBatch")
 
         request = build_index_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -847,18 +885,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200, 207]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -867,31 +906,34 @@
         if response.status_code == 200:
             deserialized = self._deserialize("IndexDocumentsResult", pipeline_response)
 
         if response.status_code == 207:
             deserialized = self._deserialize("IndexDocumentsResult", pipeline_response)
 
         if cls:
-            return cls(pipeline_response, deserialized, {})
+            return cls(pipeline_response, deserialized, {})  # type: ignore
 
-        return deserialized
+        return deserialized  # type: ignore
 
-    index.metadata = {"url": "/docs/search.index"}  # type: ignore
+    index.metadata = {"url": "/docs/search.index"}
 
     @distributed_trace_async
     async def autocomplete_get(
         self,
         search_text: str,
         suggester_name: str,
         request_options: Optional[_models.RequestOptions] = None,
         autocomplete_options: Optional[_models.AutocompleteOptions] = None,
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param search_text: The incomplete term which should be auto-completed. Required.
         :type search_text: str
         :param suggester_name: The name of the suggester as specified in the suggesters collection
          that's part of the index definition. Required.
         :type suggester_name: str
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
@@ -909,18 +951,16 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = kwargs.pop("headers", {}) or {}
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AutocompleteResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        cls: ClsType[_models.AutocompleteResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         _autocomplete_mode = None
         _filter = None
         _use_fuzzy_matching = None
         _highlight_post_tag = None
         _highlight_pre_tag = None
@@ -957,18 +997,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -977,27 +1018,30 @@
         deserialized = self._deserialize("AutocompleteResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    autocomplete_get.metadata = {"url": "/docs/search.autocomplete"}  # type: ignore
+    autocomplete_get.metadata = {"url": "/docs/search.autocomplete"}
 
     @overload
     async def autocomplete_post(
         self,
         autocomplete_request: _models.AutocompleteRequest,
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param autocomplete_request: The definition of the Autocomplete request. Required.
         :type autocomplete_request: ~search_index_client.models.AutocompleteRequest
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1014,14 +1058,17 @@
         request_options: Optional[_models.RequestOptions] = None,
         *,
         content_type: str = "application/json",
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
         :param autocomplete_request: The definition of the Autocomplete request. Required.
         :type autocomplete_request: IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
          Default value is "application/json".
         :paramtype content_type: str
@@ -1036,16 +1083,19 @@
         self,
         autocomplete_request: Union[_models.AutocompleteRequest, IO],
         request_options: Optional[_models.RequestOptions] = None,
         **kwargs: Any
     ) -> _models.AutocompleteResult:
         """Autocompletes incomplete query terms based on input text and matching terms in the index.
 
-        :param autocomplete_request: The definition of the Autocomplete request. Is either a model type
-         or a IO type. Required.
+        .. seealso::
+           - https://docs.microsoft.com/rest/api/searchservice/autocomplete
+
+        :param autocomplete_request: The definition of the Autocomplete request. Is either a
+         AutocompleteRequest type or a IO type. Required.
         :type autocomplete_request: ~search_index_client.models.AutocompleteRequest or IO
         :param request_options: Parameter group. Default value is None.
         :type request_options: ~search_index_client.models.RequestOptions
         :keyword content_type: Body Parameter content-type. Known values are: 'application/json'.
          Default value is None.
         :paramtype content_type: str
         :keyword callable cls: A custom type or function that will be passed the direct response
@@ -1060,27 +1110,25 @@
             304: ResourceNotModifiedError,
         }
         error_map.update(kwargs.pop("error_map", {}) or {})
 
         _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
         _params = case_insensitive_dict(kwargs.pop("params", {}) or {})
 
-        api_version = kwargs.pop(
-            "api_version", _params.pop("api-version", self._config.api_version)
-        )  # type: Literal["2021-04-30-Preview"]
-        content_type = kwargs.pop("content_type", _headers.pop("Content-Type", None))  # type: Optional[str]
-        cls = kwargs.pop("cls", None)  # type: ClsType[_models.AutocompleteResult]
+        api_version: str = kwargs.pop("api_version", _params.pop("api-version", self._config.api_version))
+        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
+        cls: ClsType[_models.AutocompleteResult] = kwargs.pop("cls", None)
 
         _x_ms_client_request_id = None
         if request_options is not None:
             _x_ms_client_request_id = request_options.x_ms_client_request_id
         content_type = content_type or "application/json"
         _json = None
         _content = None
-        if isinstance(autocomplete_request, (IO, bytes)):
+        if isinstance(autocomplete_request, (IOBase, bytes)):
             _content = autocomplete_request
         else:
             _json = self._serialize.body(autocomplete_request, "AutocompleteRequest")
 
         request = build_autocomplete_post_request(
             x_ms_client_request_id=_x_ms_client_request_id,
             api_version=api_version,
@@ -1092,18 +1140,19 @@
             params=_params,
         )
         request = _convert_request(request)
         path_format_arguments = {
             "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
             "indexName": self._serialize.url("self._config.index_name", self._config.index_name, "str"),
         }
-        request.url = self._client.format_url(request.url, **path_format_arguments)  # type: ignore
+        request.url = self._client.format_url(request.url, **path_format_arguments)
 
-        pipeline_response = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
-            request, stream=False, **kwargs
+        _stream = False
+        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
+            request, stream=_stream, **kwargs
         )
 
         response = pipeline_response.http_response
 
         if response.status_code not in [200]:
             map_error(status_code=response.status_code, response=response, error_map=error_map)
             error = self._deserialize.failsafe_deserialize(_models.SearchError, pipeline_response)
@@ -1112,8 +1161,8 @@
         deserialized = self._deserialize("AutocompleteResult", pipeline_response)
 
         if cls:
             return cls(pipeline_response, deserialized, {})
 
         return deserialized
 
-    autocomplete_post.metadata = {"url": "/docs/search.post.autocomplete"}  # type: ignore
+    autocomplete_post.metadata = {"url": "/docs/search.post.autocomplete"}
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/aio/operations/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/_patch.py`

 * *Files identical despite different names*

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/__init__.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,81 +1,101 @@
 # coding=utf-8
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from ._models_py3 import AnswerResult
 from ._models_py3 import AutocompleteItem
 from ._models_py3 import AutocompleteOptions
 from ._models_py3 import AutocompleteRequest
 from ._models_py3 import AutocompleteResult
 from ._models_py3 import CaptionResult
+from ._models_py3 import DocumentDebugInfo
 from ._models_py3 import FacetResult
 from ._models_py3 import IndexAction
 from ._models_py3 import IndexBatch
 from ._models_py3 import IndexDocumentsResult
 from ._models_py3 import IndexingResult
+from ._models_py3 import QueryResultDocumentRerankerInput
+from ._models_py3 import QueryResultDocumentSemanticField
 from ._models_py3 import RequestOptions
 from ._models_py3 import SearchDocumentsResult
 from ._models_py3 import SearchError
 from ._models_py3 import SearchOptions
 from ._models_py3 import SearchRequest
 from ._models_py3 import SearchResult
+from ._models_py3 import SemanticDebugInfo
 from ._models_py3 import SuggestDocumentsResult
 from ._models_py3 import SuggestOptions
 from ._models_py3 import SuggestRequest
 from ._models_py3 import SuggestResult
+from ._models_py3 import Vector
 
 from ._search_index_client_enums import Answers
 from ._search_index_client_enums import AutocompleteMode
 from ._search_index_client_enums import Captions
 from ._search_index_client_enums import IndexActionType
 from ._search_index_client_enums import QueryAnswerType
 from ._search_index_client_enums import QueryCaptionType
+from ._search_index_client_enums import QueryDebugMode
 from ._search_index_client_enums import QueryLanguage
+from ._search_index_client_enums import QueryResultDocumentSemanticFieldState
 from ._search_index_client_enums import QuerySpellerType
 from ._search_index_client_enums import QueryType
 from ._search_index_client_enums import ScoringStatistics
 from ._search_index_client_enums import SearchMode
+from ._search_index_client_enums import SemanticErrorHandling
+from ._search_index_client_enums import SemanticPartialResponseReason
+from ._search_index_client_enums import SemanticPartialResponseType
 from ._search_index_client_enums import Speller
 from ._patch import __all__ as _patch_all
-from ._patch import *  # type: ignore # pylint: disable=unused-wildcard-import
+from ._patch import *  # pylint: disable=unused-wildcard-import
 from ._patch import patch_sdk as _patch_sdk
 
 __all__ = [
     "AnswerResult",
     "AutocompleteItem",
     "AutocompleteOptions",
     "AutocompleteRequest",
     "AutocompleteResult",
     "CaptionResult",
+    "DocumentDebugInfo",
     "FacetResult",
     "IndexAction",
     "IndexBatch",
     "IndexDocumentsResult",
     "IndexingResult",
+    "QueryResultDocumentRerankerInput",
+    "QueryResultDocumentSemanticField",
     "RequestOptions",
     "SearchDocumentsResult",
     "SearchError",
     "SearchOptions",
     "SearchRequest",
     "SearchResult",
+    "SemanticDebugInfo",
     "SuggestDocumentsResult",
     "SuggestOptions",
     "SuggestRequest",
     "SuggestResult",
+    "Vector",
     "Answers",
     "AutocompleteMode",
     "Captions",
     "IndexActionType",
     "QueryAnswerType",
     "QueryCaptionType",
+    "QueryDebugMode",
     "QueryLanguage",
+    "QueryResultDocumentSemanticFieldState",
     "QuerySpellerType",
     "QueryType",
     "ScoringStatistics",
     "SearchMode",
+    "SemanticErrorHandling",
+    "SemanticPartialResponseReason",
+    "SemanticPartialResponseType",
     "Speller",
 ]
 __all__.extend([p for p in _patch_all if p not in __all__])
 _patch_sdk()
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_models_py3.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/models/_models_py3.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 # coding=utf-8
 # pylint: disable=too-many-lines
 # --------------------------------------------------------------------------
-# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.3, generator: @autorest/python@6.2.3)
+# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.11)
 # Changes may cause incorrect behavior and will be lost if the code is regenerated.
 # --------------------------------------------------------------------------
 
 from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union
 
 from .. import _serialization
 
 if TYPE_CHECKING:
     # pylint: disable=unused-import,ungrouped-imports
     from .. import models as _models
 
 
 class AnswerResult(_serialization.Model):
-    """An answer is a text passage extracted from the contents of the most relevant documents that matched the query. Answers are extracted from the top search results. Answer candidates are scored and the top answers are selected.
+    """An answer is a text passage extracted from the contents of the most relevant documents that
+    matched the query. Answers are extracted from the top search results. Answer candidates are
+    scored and the top answers are selected.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     :ivar additional_properties: Unmatched properties from the message are deserialized to this
      collection.
     :vartype additional_properties: dict[str, any]
     :ivar score: The score value represents how relevant the answer is to the query relative to
@@ -45,15 +47,15 @@
         "additional_properties": {"key": "", "type": "{object}"},
         "score": {"key": "score", "type": "float"},
         "key": {"key": "key", "type": "str"},
         "text": {"key": "text", "type": "str"},
         "highlights": {"key": "highlights", "type": "str"},
     }
 
-    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs):
+    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         """
         super().__init__(**kwargs)
         self.additional_properties = additional_properties
@@ -82,15 +84,15 @@
     }
 
     _attribute_map = {
         "text": {"key": "text", "type": "str"},
         "query_plus_text": {"key": "queryPlusText", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.text = None
         self.query_plus_text = None
 
 
 class AutocompleteOptions(_serialization.Model):
@@ -146,16 +148,16 @@
         filter: Optional[str] = None,  # pylint: disable=redefined-builtin
         use_fuzzy_matching: Optional[bool] = None,
         highlight_post_tag: Optional[str] = None,
         highlight_pre_tag: Optional[str] = None,
         minimum_coverage: Optional[float] = None,
         search_fields: Optional[List[str]] = None,
         top: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword autocomplete_mode: Specifies the mode for Autocomplete. The default is 'oneTerm'. Use
          'twoTerms' to get shingles and 'oneTermWithContext' to use the current context while producing
          auto-completed terms. Known values are: "oneTerm", "twoTerms", and "oneTermWithContext".
         :paramtype autocomplete_mode: str or ~search_index_client.models.AutocompleteMode
         :keyword filter: An OData expression that filters the documents used to produce completed terms
          for the Autocomplete result.
@@ -264,16 +266,16 @@
         filter: Optional[str] = None,  # pylint: disable=redefined-builtin
         use_fuzzy_matching: Optional[bool] = None,
         highlight_post_tag: Optional[str] = None,
         highlight_pre_tag: Optional[str] = None,
         minimum_coverage: Optional[float] = None,
         search_fields: Optional[str] = None,
         top: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword search_text: The search text on which to base autocomplete results. Required.
         :paramtype search_text: str
         :keyword autocomplete_mode: Specifies the mode for Autocomplete. The default is 'oneTerm'. Use
          'twoTerms' to get shingles and 'oneTermWithContext' to use the current context while producing
          auto-completed terms. Known values are: "oneTerm", "twoTerms", and "oneTermWithContext".
         :paramtype autocomplete_mode: str or ~search_index_client.models.AutocompleteMode
@@ -340,23 +342,25 @@
     }
 
     _attribute_map = {
         "coverage": {"key": "@search\\.coverage", "type": "float"},
         "results": {"key": "value", "type": "[AutocompleteItem]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.coverage = None
         self.results = None
 
 
 class CaptionResult(_serialization.Model):
-    """Captions are the most representative passages from the document relatively to the search query. They are often used as document summary. Captions are only returned for queries of type 'semantic'..
+    """Captions are the most representative passages from the document relatively to the search query.
+    They are often used as document summary. Captions are only returned for queries of type
+    'semantic'..
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     :ivar additional_properties: Unmatched properties from the message are deserialized to this
      collection.
     :vartype additional_properties: dict[str, any]
     :ivar text: A representative text passage extracted from the document most relevant to the
@@ -374,28 +378,52 @@
 
     _attribute_map = {
         "additional_properties": {"key": "", "type": "{object}"},
         "text": {"key": "text", "type": "str"},
         "highlights": {"key": "highlights", "type": "str"},
     }
 
-    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs):
+    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         """
         super().__init__(**kwargs)
         self.additional_properties = additional_properties
         self.text = None
         self.highlights = None
 
 
+class DocumentDebugInfo(_serialization.Model):
+    """Contains debugging information that can be used to further explore your search results.
+
+    Variables are only populated by the server, and will be ignored when sending a request.
+
+    :ivar semantic: Contains debugging information specific to semantic search queries.
+    :vartype semantic: ~search_index_client.models.SemanticDebugInfo
+    """
+
+    _validation = {
+        "semantic": {"readonly": True},
+    }
+
+    _attribute_map = {
+        "semantic": {"key": "semantic", "type": "SemanticDebugInfo"},
+    }
+
+    def __init__(self, **kwargs: Any) -> None:
+        """ """
+        super().__init__(**kwargs)
+        self.semantic = None
+
+
 class FacetResult(_serialization.Model):
-    """A single bucket of a facet query result. Reports the number of documents with a field value falling within a particular range or having a particular value or interval.
+    """A single bucket of a facet query result. Reports the number of documents with a field value
+    falling within a particular range or having a particular value or interval.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
     :ivar additional_properties: Unmatched properties from the message are deserialized to this
      collection.
     :vartype additional_properties: dict[str, any]
     :ivar count: The approximate count of documents falling within the bucket described by this
@@ -408,15 +436,15 @@
     }
 
     _attribute_map = {
         "additional_properties": {"key": "", "type": "{object}"},
         "count": {"key": "count", "type": "int"},
     }
 
-    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs):
+    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         """
         super().__init__(**kwargs)
         self.additional_properties = additional_properties
@@ -440,16 +468,16 @@
     }
 
     def __init__(
         self,
         *,
         additional_properties: Optional[Dict[str, Any]] = None,
         action_type: Optional[Union[str, "_models.IndexActionType"]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         :keyword action_type: The operation to perform on a document in an indexing batch. Known values
          are: "upload", "merge", "mergeOrUpload", and "delete".
         :paramtype action_type: str or ~search_index_client.models.IndexActionType
@@ -472,15 +500,15 @@
         "actions": {"required": True},
     }
 
     _attribute_map = {
         "actions": {"key": "value", "type": "[IndexAction]"},
     }
 
-    def __init__(self, *, actions: List["_models.IndexAction"], **kwargs):
+    def __init__(self, *, actions: List["_models.IndexAction"], **kwargs: Any) -> None:
         """
         :keyword actions: The actions in the batch. Required.
         :paramtype actions: list[~search_index_client.models.IndexAction]
         """
         super().__init__(**kwargs)
         self.actions = actions
 
@@ -501,15 +529,15 @@
         "results": {"required": True, "readonly": True},
     }
 
     _attribute_map = {
         "results": {"key": "value", "type": "[IndexingResult]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.results = None
 
 
 class IndexingResult(_serialization.Model):
     """Status of an indexing operation for a single document.
@@ -543,35 +571,100 @@
     _attribute_map = {
         "key": {"key": "key", "type": "str"},
         "error_message": {"key": "errorMessage", "type": "str"},
         "succeeded": {"key": "status", "type": "bool"},
         "status_code": {"key": "statusCode", "type": "int"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.key = None
         self.error_message = None
         self.succeeded = None
         self.status_code = None
 
 
+class QueryResultDocumentRerankerInput(_serialization.Model):
+    """The raw concatenated strings that were sent to the semantic enrichment process.
+
+    Variables are only populated by the server, and will be ignored when sending a request.
+
+    :ivar title: The raw string for the title field that was used for semantic enrichment.
+    :vartype title: str
+    :ivar content: The raw concatenated strings for the content fields that were used for semantic
+     enrichment.
+    :vartype content: str
+    :ivar keywords: The raw concatenated strings for the keyword fields that were used for semantic
+     enrichment.
+    :vartype keywords: str
+    """
+
+    _validation = {
+        "title": {"readonly": True},
+        "content": {"readonly": True},
+        "keywords": {"readonly": True},
+    }
+
+    _attribute_map = {
+        "title": {"key": "title", "type": "str"},
+        "content": {"key": "content", "type": "str"},
+        "keywords": {"key": "keywords", "type": "str"},
+    }
+
+    def __init__(self, **kwargs: Any) -> None:
+        """ """
+        super().__init__(**kwargs)
+        self.title = None
+        self.content = None
+        self.keywords = None
+
+
+class QueryResultDocumentSemanticField(_serialization.Model):
+    """Description of fields that were sent to the semantic enrichment process, as well as how they
+    were used.
+
+    Variables are only populated by the server, and will be ignored when sending a request.
+
+    :ivar name: The name of the field that was sent to the semantic enrichment process.
+    :vartype name: str
+    :ivar state: The way the field was used for the semantic enrichment process (fully used,
+     partially used, or unused). Known values are: "used", "unused", and "partial".
+    :vartype state: str or ~search_index_client.models.QueryResultDocumentSemanticFieldState
+    """
+
+    _validation = {
+        "name": {"readonly": True},
+        "state": {"readonly": True},
+    }
+
+    _attribute_map = {
+        "name": {"key": "name", "type": "str"},
+        "state": {"key": "state", "type": "str"},
+    }
+
+    def __init__(self, **kwargs: Any) -> None:
+        """ """
+        super().__init__(**kwargs)
+        self.name = None
+        self.state = None
+
+
 class RequestOptions(_serialization.Model):
     """Parameter group.
 
     :ivar x_ms_client_request_id: The tracking ID sent with the request to help with debugging.
     :vartype x_ms_client_request_id: str
     """
 
     _attribute_map = {
         "x_ms_client_request_id": {"key": "x-ms-client-request-id", "type": "str"},
     }
 
-    def __init__(self, *, x_ms_client_request_id: Optional[str] = None, **kwargs):
+    def __init__(self, *, x_ms_client_request_id: Optional[str] = None, **kwargs: Any) -> None:
         """
         :keyword x_ms_client_request_id: The tracking ID sent with the request to help with debugging.
         :paramtype x_ms_client_request_id: str
         """
         super().__init__(**kwargs)
         self.x_ms_client_request_id = x_ms_client_request_id
 
@@ -598,14 +691,23 @@
      parameter was not specified or set to 'none'.
     :vartype answers: list[~search_index_client.models.AnswerResult]
     :ivar next_page_parameters: Continuation JSON payload returned when Azure Cognitive Search
      can't return all the requested results in a single Search response. You can use this JSON along
      with @odata.nextLink to formulate another POST Search request to get the next part of the
      search response.
     :vartype next_page_parameters: ~search_index_client.models.SearchRequest
+    :ivar semantic_partial_response_reason: Reason that a partial response was returned for a
+     semantic search request. Known values are: "maxWaitExceeded", "capacityOverloaded", and
+     "transient".
+    :vartype semantic_partial_response_reason: str or
+     ~search_index_client.models.SemanticPartialResponseReason
+    :ivar semantic_partial_response_type: Type of partial response that was returned for a semantic
+     search request. Known values are: "baseResults" and "rerankedResults".
+    :vartype semantic_partial_response_type: str or
+     ~search_index_client.models.SemanticPartialResponseType
     :ivar results: The sequence of results returned by the query. Required.
     :vartype results: list[~search_index_client.models.SearchResult]
     :ivar next_link: Continuation URL returned when Azure Cognitive Search can't return all the
      requested results in a single Search response. You can use this URL to formulate another GET or
      POST Search request to get the next part of the search response. Make sure to use the same verb
      (GET or POST) as the request that produced this response.
     :vartype next_link: str
@@ -613,36 +715,42 @@
 
     _validation = {
         "count": {"readonly": True},
         "coverage": {"readonly": True},
         "facets": {"readonly": True},
         "answers": {"readonly": True},
         "next_page_parameters": {"readonly": True},
+        "semantic_partial_response_reason": {"readonly": True},
+        "semantic_partial_response_type": {"readonly": True},
         "results": {"required": True, "readonly": True},
         "next_link": {"readonly": True},
     }
 
     _attribute_map = {
         "count": {"key": "@odata\\.count", "type": "int"},
         "coverage": {"key": "@search\\.coverage", "type": "float"},
         "facets": {"key": "@search\\.facets", "type": "{[FacetResult]}"},
         "answers": {"key": "@search\\.answers", "type": "[AnswerResult]"},
         "next_page_parameters": {"key": "@search\\.nextPageParameters", "type": "SearchRequest"},
+        "semantic_partial_response_reason": {"key": "@search\\.semanticPartialResponseReason", "type": "str"},
+        "semantic_partial_response_type": {"key": "@search\\.semanticPartialResponseType", "type": "str"},
         "results": {"key": "value", "type": "[SearchResult]"},
         "next_link": {"key": "@odata\\.nextLink", "type": "str"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.count = None
         self.coverage = None
         self.facets = None
         self.answers = None
         self.next_page_parameters = None
+        self.semantic_partial_response_reason = None
+        self.semantic_partial_response_type = None
         self.results = None
         self.next_link = None
 
 
 class SearchError(_serialization.Model):
     """Describes an error condition for the Azure Cognitive Search API.
 
@@ -666,15 +774,15 @@
 
     _attribute_map = {
         "code": {"key": "code", "type": "str"},
         "message": {"key": "message", "type": "str"},
         "details": {"key": "details", "type": "[SearchError]"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.code = None
         self.message = None
         self.details = None
 
 
@@ -722,14 +830,23 @@
     :vartype scoring_parameters: list[str]
     :ivar scoring_profile: The name of a scoring profile to evaluate match scores for matching
      documents in order to sort the results.
     :vartype scoring_profile: str
     :ivar semantic_configuration: The name of the semantic configuration that lists which fields
      should be used for semantic ranking, captions, highlights, and answers.
     :vartype semantic_configuration: str
+    :ivar semantic_error_handling: Allows the user to choose whether a semantic call should fail
+     completely, or to return partial results. Known values are: "partial" and "fail".
+    :vartype semantic_error_handling: str or ~search_index_client.models.SemanticErrorHandling
+    :ivar semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount of
+     time it takes for semantic enrichment to finish processing before the request fails.
+    :vartype semantic_max_wait_in_milliseconds: int
+    :ivar debug: Enables a debugging tool that can be used to further explore your search results.
+     Known values are: "disabled" and "semantic".
+    :vartype debug: str or ~search_index_client.models.QueryDebugMode
     :ivar search_fields: The list of field names to which to scope the full-text search. When using
      fielded search (fieldName:searchExpression) in a full Lucene query, the field names of each
      fielded search expression take precedence over any field names listed in this parameter.
     :vartype search_fields: list[str]
     :ivar query_language: The language of the query. Known values are: "none", "en-us", "en-gb",
      "en-in", "en-ca", "en-au", "fr-fr", "fr-ca", "de-de", "es-es", "es-mx", "zh-cn", "zh-tw",
      "pt-br", "pt-pt", "it-it", "ja-jp", "ko-kr", "ru-ru", "cs-cz", "nl-be", "nl-nl", "hu-hu",
@@ -742,22 +859,25 @@
     :ivar speller: Improve search recall by spell-correcting individual search query terms. Known
      values are: "none" and "lexicon".
     :vartype speller: str or ~search_index_client.models.Speller
     :ivar answers: This parameter is only valid if the query type is 'semantic'. If set, the query
      returns answers extracted from key passages in the highest ranked documents. The number of
      answers returned can be configured by appending the pipe character '|' followed by the
      'count-:code:`<number of answers>`' option after the answers parameter value, such as
-     'extractive|count-3'. Default count is 1. Known values are: "none" and "extractive".
+     'extractive|count-3'. Default count is 1. The confidence threshold can be configured by
+     appending the pipe character '|' followed by the 'threshold-:code:`<confidence threshold>`'
+     option after the answers parameter value, such as 'extractive|threshold-0.9'. Default threshold
+     is 0.7. Known values are: "none" and "extractive".
     :vartype answers: str or ~search_index_client.models.Answers
     :ivar search_mode: A value that specifies whether any or all of the search terms must be
      matched in order to count the document as a match. Known values are: "any" and "all".
     :vartype search_mode: str or ~search_index_client.models.SearchMode
     :ivar scoring_statistics: A value that specifies whether we want to calculate scoring
      statistics (such as document frequency) globally for more consistent scoring, or locally, for
-     lower latency. Known values are: "local" and "global".
+     lower latency. Known values are: "local", "global", and "global".
     :vartype scoring_statistics: str or ~search_index_client.models.ScoringStatistics
     :ivar session_id: A value to be used to create a sticky session, which can help to get more
      consistent results. As long as the same sessionId is used, a best-effort attempt will be made
      to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
      interfere with the load balancing of the requests across replicas and adversely affect the
      performance of the search service. The value used as sessionId cannot start with a '_'
      character.
@@ -780,27 +900,34 @@
      pipe character '|' followed by the 'highlight-<true/false>' option, such as
      'extractive|highlight-true'. Defaults to 'None'. Known values are: "none" and "extractive".
     :vartype captions: str or ~search_index_client.models.Captions
     :ivar semantic_fields: The list of field names used for semantic search.
     :vartype semantic_fields: list[str]
     """
 
+    _validation = {
+        "semantic_max_wait_in_milliseconds": {"minimum": 700},
+    }
+
     _attribute_map = {
         "include_total_result_count": {"key": "IncludeTotalResultCount", "type": "bool"},
         "facets": {"key": "Facets", "type": "[str]"},
         "filter": {"key": "$filter", "type": "str"},
         "highlight_fields": {"key": "HighlightFields", "type": "[str]"},
         "highlight_post_tag": {"key": "highlightPostTag", "type": "str"},
         "highlight_pre_tag": {"key": "highlightPreTag", "type": "str"},
         "minimum_coverage": {"key": "minimumCoverage", "type": "float"},
         "order_by": {"key": "OrderBy", "type": "[str]"},
         "query_type": {"key": "queryType", "type": "str"},
         "scoring_parameters": {"key": "ScoringParameters", "type": "[str]"},
         "scoring_profile": {"key": "scoringProfile", "type": "str"},
         "semantic_configuration": {"key": "semanticConfiguration", "type": "str"},
+        "semantic_error_handling": {"key": "semanticErrorHandling", "type": "str"},
+        "semantic_max_wait_in_milliseconds": {"key": "semanticMaxWaitInMilliseconds", "type": "int"},
+        "debug": {"key": "debug", "type": "str"},
         "search_fields": {"key": "searchFields", "type": "[str]"},
         "query_language": {"key": "queryLanguage", "type": "str"},
         "speller": {"key": "speller", "type": "str"},
         "answers": {"key": "answers", "type": "str"},
         "search_mode": {"key": "searchMode", "type": "str"},
         "scoring_statistics": {"key": "scoringStatistics", "type": "str"},
         "session_id": {"key": "sessionId", "type": "str"},
@@ -822,28 +949,31 @@
         highlight_pre_tag: Optional[str] = None,
         minimum_coverage: Optional[float] = None,
         order_by: Optional[List[str]] = None,
         query_type: Optional[Union[str, "_models.QueryType"]] = None,
         scoring_parameters: Optional[List[str]] = None,
         scoring_profile: Optional[str] = None,
         semantic_configuration: Optional[str] = None,
+        semantic_error_handling: Optional[Union[str, "_models.SemanticErrorHandling"]] = None,
+        semantic_max_wait_in_milliseconds: Optional[int] = None,
+        debug: Optional[Union[str, "_models.QueryDebugMode"]] = None,
         search_fields: Optional[List[str]] = None,
         query_language: Optional[Union[str, "_models.QueryLanguage"]] = None,
         speller: Optional[Union[str, "_models.Speller"]] = None,
         answers: Optional[Union[str, "_models.Answers"]] = None,
         search_mode: Optional[Union[str, "_models.SearchMode"]] = None,
         scoring_statistics: Optional[Union[str, "_models.ScoringStatistics"]] = None,
         session_id: Optional[str] = None,
         select: Optional[List[str]] = None,
         skip: Optional[int] = None,
         top: Optional[int] = None,
         captions: Optional[Union[str, "_models.Captions"]] = None,
         semantic_fields: Optional[List[str]] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword include_total_result_count: A value that specifies whether to fetch the total count of
          results. Default is false. Setting this value to true may have a performance impact. Note that
          the count returned is an approximation.
         :paramtype include_total_result_count: bool
         :keyword facets: The list of facet expressions to apply to the search query. Each facet
          expression contains a field name, optionally followed by a comma-separated list of name:value
@@ -883,14 +1013,23 @@
         :paramtype scoring_parameters: list[str]
         :keyword scoring_profile: The name of a scoring profile to evaluate match scores for matching
          documents in order to sort the results.
         :paramtype scoring_profile: str
         :keyword semantic_configuration: The name of the semantic configuration that lists which fields
          should be used for semantic ranking, captions, highlights, and answers.
         :paramtype semantic_configuration: str
+        :keyword semantic_error_handling: Allows the user to choose whether a semantic call should fail
+         completely, or to return partial results. Known values are: "partial" and "fail".
+        :paramtype semantic_error_handling: str or ~search_index_client.models.SemanticErrorHandling
+        :keyword semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount
+         of time it takes for semantic enrichment to finish processing before the request fails.
+        :paramtype semantic_max_wait_in_milliseconds: int
+        :keyword debug: Enables a debugging tool that can be used to further explore your search
+         results. Known values are: "disabled" and "semantic".
+        :paramtype debug: str or ~search_index_client.models.QueryDebugMode
         :keyword search_fields: The list of field names to which to scope the full-text search. When
          using fielded search (fieldName:searchExpression) in a full Lucene query, the field names of
          each fielded search expression take precedence over any field names listed in this parameter.
         :paramtype search_fields: list[str]
         :keyword query_language: The language of the query. Known values are: "none", "en-us", "en-gb",
          "en-in", "en-ca", "en-au", "fr-fr", "fr-ca", "de-de", "es-es", "es-mx", "zh-cn", "zh-tw",
          "pt-br", "pt-pt", "it-it", "ja-jp", "ko-kr", "ru-ru", "cs-cz", "nl-be", "nl-nl", "hu-hu",
@@ -903,22 +1042,25 @@
         :keyword speller: Improve search recall by spell-correcting individual search query terms.
          Known values are: "none" and "lexicon".
         :paramtype speller: str or ~search_index_client.models.Speller
         :keyword answers: This parameter is only valid if the query type is 'semantic'. If set, the
          query returns answers extracted from key passages in the highest ranked documents. The number
          of answers returned can be configured by appending the pipe character '|' followed by the
          'count-:code:`<number of answers>`' option after the answers parameter value, such as
-         'extractive|count-3'. Default count is 1. Known values are: "none" and "extractive".
+         'extractive|count-3'. Default count is 1. The confidence threshold can be configured by
+         appending the pipe character '|' followed by the 'threshold-:code:`<confidence threshold>`'
+         option after the answers parameter value, such as 'extractive|threshold-0.9'. Default threshold
+         is 0.7. Known values are: "none" and "extractive".
         :paramtype answers: str or ~search_index_client.models.Answers
         :keyword search_mode: A value that specifies whether any or all of the search terms must be
          matched in order to count the document as a match. Known values are: "any" and "all".
         :paramtype search_mode: str or ~search_index_client.models.SearchMode
         :keyword scoring_statistics: A value that specifies whether we want to calculate scoring
          statistics (such as document frequency) globally for more consistent scoring, or locally, for
-         lower latency. Known values are: "local" and "global".
+         lower latency. Known values are: "local", "global", and "global".
         :paramtype scoring_statistics: str or ~search_index_client.models.ScoringStatistics
         :keyword session_id: A value to be used to create a sticky session, which can help to get more
          consistent results. As long as the same sessionId is used, a best-effort attempt will be made
          to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
          interfere with the load balancing of the requests across replicas and adversely affect the
          performance of the search service. The value used as sessionId cannot start with a '_'
          character.
@@ -953,14 +1095,17 @@
         self.highlight_pre_tag = highlight_pre_tag
         self.minimum_coverage = minimum_coverage
         self.order_by = order_by
         self.query_type = query_type
         self.scoring_parameters = scoring_parameters
         self.scoring_profile = scoring_profile
         self.semantic_configuration = semantic_configuration
+        self.semantic_error_handling = semantic_error_handling
+        self.semantic_max_wait_in_milliseconds = semantic_max_wait_in_milliseconds
+        self.debug = debug
         self.search_fields = search_fields
         self.query_language = query_language
         self.speller = speller
         self.answers = answers
         self.search_mode = search_mode
         self.scoring_statistics = scoring_statistics
         self.session_id = session_id
@@ -1008,15 +1153,15 @@
      'simple'. Use 'full' if your query uses the Lucene query syntax. Known values are: "simple",
      "full", and "semantic".
     :vartype query_type: str or ~search_index_client.models.QueryType
     :ivar scoring_statistics: A value that specifies whether we want to calculate scoring
      statistics (such as document frequency) globally for more consistent scoring, or locally, for
      lower latency. The default is 'local'. Use 'global' to aggregate scoring statistics globally
      before scoring. Using global scoring statistics can increase latency of search queries. Known
-     values are: "local" and "global".
+     values are: "local", "global", and "global".
     :vartype scoring_statistics: str or ~search_index_client.models.ScoringStatistics
     :ivar session_id: A value to be used to create a sticky session, which can help getting more
      consistent results. As long as the same sessionId is used, a best-effort attempt will be made
      to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
      interfere with the load balancing of the requests across replicas and adversely affect the
      performance of the search service. The value used as sessionId cannot start with a '_'
      character.
@@ -1028,14 +1173,24 @@
     :vartype scoring_parameters: list[str]
     :ivar scoring_profile: The name of a scoring profile to evaluate match scores for matching
      documents in order to sort the results.
     :vartype scoring_profile: str
     :ivar semantic_configuration: The name of a semantic configuration that will be used when
      processing documents for queries of type semantic.
     :vartype semantic_configuration: str
+    :ivar semantic_error_handling: Allows the user to choose whether a semantic call should fail
+     completely (default / current behavior), or to return partial results. Known values are:
+     "partial" and "fail".
+    :vartype semantic_error_handling: str or ~search_index_client.models.SemanticErrorHandling
+    :ivar semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount of
+     time it takes for semantic enrichment to finish processing before the request fails.
+    :vartype semantic_max_wait_in_milliseconds: int
+    :ivar debug: Enables a debugging tool that can be used to further explore your Semantic search
+     results. Known values are: "disabled" and "semantic".
+    :vartype debug: str or ~search_index_client.models.QueryDebugMode
     :ivar search_text: A full-text search query expression; Use "*" or omit this parameter to match
      all documents.
     :vartype search_text: str
     :ivar search_fields: The comma-separated list of field names to which to scope the full-text
      search. When using fielded search (fieldName:searchExpression) in a full Lucene query, the
      field names of each fielded search expression take precedence over any field names listed in
      this parameter.
@@ -1072,16 +1227,22 @@
      request for the next page of results.
     :vartype top: int
     :ivar captions: A value that specifies whether captions should be returned as part of the
      search response. Known values are: "none" and "extractive".
     :vartype captions: str or ~search_index_client.models.QueryCaptionType
     :ivar semantic_fields: The comma-separated list of field names used for semantic search.
     :vartype semantic_fields: str
+    :ivar vector: The query parameters for vector and hybrid search queries.
+    :vartype vector: ~search_index_client.models.Vector
     """
 
+    _validation = {
+        "semantic_max_wait_in_milliseconds": {"minimum": 700},
+    }
+
     _attribute_map = {
         "include_total_result_count": {"key": "count", "type": "bool"},
         "facets": {"key": "facets", "type": "[str]"},
         "filter": {"key": "filter", "type": "str"},
         "highlight_fields": {"key": "highlight", "type": "str"},
         "highlight_post_tag": {"key": "highlightPostTag", "type": "str"},
         "highlight_pre_tag": {"key": "highlightPreTag", "type": "str"},
@@ -1089,25 +1250,29 @@
         "order_by": {"key": "orderby", "type": "str"},
         "query_type": {"key": "queryType", "type": "str"},
         "scoring_statistics": {"key": "scoringStatistics", "type": "str"},
         "session_id": {"key": "sessionId", "type": "str"},
         "scoring_parameters": {"key": "scoringParameters", "type": "[str]"},
         "scoring_profile": {"key": "scoringProfile", "type": "str"},
         "semantic_configuration": {"key": "semanticConfiguration", "type": "str"},
+        "semantic_error_handling": {"key": "semanticErrorHandling", "type": "str"},
+        "semantic_max_wait_in_milliseconds": {"key": "semanticMaxWaitInMilliseconds", "type": "int"},
+        "debug": {"key": "debug", "type": "str"},
         "search_text": {"key": "search", "type": "str"},
         "search_fields": {"key": "searchFields", "type": "str"},
         "search_mode": {"key": "searchMode", "type": "str"},
         "query_language": {"key": "queryLanguage", "type": "str"},
         "speller": {"key": "speller", "type": "str"},
         "answers": {"key": "answers", "type": "str"},
         "select": {"key": "select", "type": "str"},
         "skip": {"key": "skip", "type": "int"},
         "top": {"key": "top", "type": "int"},
         "captions": {"key": "captions", "type": "str"},
         "semantic_fields": {"key": "semanticFields", "type": "str"},
+        "vector": {"key": "vector", "type": "Vector"},
     }
 
     def __init__(  # pylint: disable=too-many-locals
         self,
         *,
         include_total_result_count: Optional[bool] = None,
         facets: Optional[List[str]] = None,
@@ -1119,27 +1284,31 @@
         order_by: Optional[str] = None,
         query_type: Optional[Union[str, "_models.QueryType"]] = None,
         scoring_statistics: Optional[Union[str, "_models.ScoringStatistics"]] = None,
         session_id: Optional[str] = None,
         scoring_parameters: Optional[List[str]] = None,
         scoring_profile: Optional[str] = None,
         semantic_configuration: Optional[str] = None,
+        semantic_error_handling: Optional[Union[str, "_models.SemanticErrorHandling"]] = None,
+        semantic_max_wait_in_milliseconds: Optional[int] = None,
+        debug: Optional[Union[str, "_models.QueryDebugMode"]] = None,
         search_text: Optional[str] = None,
         search_fields: Optional[str] = None,
         search_mode: Optional[Union[str, "_models.SearchMode"]] = None,
         query_language: Optional[Union[str, "_models.QueryLanguage"]] = None,
         speller: Optional[Union[str, "_models.QuerySpellerType"]] = None,
         answers: Optional[Union[str, "_models.QueryAnswerType"]] = None,
         select: Optional[str] = None,
         skip: Optional[int] = None,
         top: Optional[int] = None,
         captions: Optional[Union[str, "_models.QueryCaptionType"]] = None,
         semantic_fields: Optional[str] = None,
-        **kwargs
-    ):
+        vector: Optional["_models.Vector"] = None,
+        **kwargs: Any
+    ) -> None:
         """
         :keyword include_total_result_count: A value that specifies whether to fetch the total count of
          results. Default is false. Setting this value to true may have a performance impact. Note that
          the count returned is an approximation.
         :paramtype include_total_result_count: bool
         :keyword facets: The list of facet expressions to apply to the search query. Each facet
          expression contains a field name, optionally followed by a comma-separated list of name:value
@@ -1172,15 +1341,15 @@
          'simple'. Use 'full' if your query uses the Lucene query syntax. Known values are: "simple",
          "full", and "semantic".
         :paramtype query_type: str or ~search_index_client.models.QueryType
         :keyword scoring_statistics: A value that specifies whether we want to calculate scoring
          statistics (such as document frequency) globally for more consistent scoring, or locally, for
          lower latency. The default is 'local'. Use 'global' to aggregate scoring statistics globally
          before scoring. Using global scoring statistics can increase latency of search queries. Known
-         values are: "local" and "global".
+         values are: "local", "global", and "global".
         :paramtype scoring_statistics: str or ~search_index_client.models.ScoringStatistics
         :keyword session_id: A value to be used to create a sticky session, which can help getting more
          consistent results. As long as the same sessionId is used, a best-effort attempt will be made
          to target the same replica set. Be wary that reusing the same sessionID values repeatedly can
          interfere with the load balancing of the requests across replicas and adversely affect the
          performance of the search service. The value used as sessionId cannot start with a '_'
          character.
@@ -1192,14 +1361,24 @@
         :paramtype scoring_parameters: list[str]
         :keyword scoring_profile: The name of a scoring profile to evaluate match scores for matching
          documents in order to sort the results.
         :paramtype scoring_profile: str
         :keyword semantic_configuration: The name of a semantic configuration that will be used when
          processing documents for queries of type semantic.
         :paramtype semantic_configuration: str
+        :keyword semantic_error_handling: Allows the user to choose whether a semantic call should fail
+         completely (default / current behavior), or to return partial results. Known values are:
+         "partial" and "fail".
+        :paramtype semantic_error_handling: str or ~search_index_client.models.SemanticErrorHandling
+        :keyword semantic_max_wait_in_milliseconds: Allows the user to set an upper bound on the amount
+         of time it takes for semantic enrichment to finish processing before the request fails.
+        :paramtype semantic_max_wait_in_milliseconds: int
+        :keyword debug: Enables a debugging tool that can be used to further explore your Semantic
+         search results. Known values are: "disabled" and "semantic".
+        :paramtype debug: str or ~search_index_client.models.QueryDebugMode
         :keyword search_text: A full-text search query expression; Use "*" or omit this parameter to
          match all documents.
         :paramtype search_text: str
         :keyword search_fields: The comma-separated list of field names to which to scope the full-text
          search. When using fielded search (fieldName:searchExpression) in a full Lucene query, the
          field names of each fielded search expression take precedence over any field names listed in
          this parameter.
@@ -1236,14 +1415,16 @@
          another Search request for the next page of results.
         :paramtype top: int
         :keyword captions: A value that specifies whether captions should be returned as part of the
          search response. Known values are: "none" and "extractive".
         :paramtype captions: str or ~search_index_client.models.QueryCaptionType
         :keyword semantic_fields: The comma-separated list of field names used for semantic search.
         :paramtype semantic_fields: str
+        :keyword vector: The query parameters for vector and hybrid search queries.
+        :paramtype vector: ~search_index_client.models.Vector
         """
         super().__init__(**kwargs)
         self.include_total_result_count = include_total_result_count
         self.facets = facets
         self.filter = filter
         self.highlight_fields = highlight_fields
         self.highlight_post_tag = highlight_post_tag
@@ -1252,25 +1433,29 @@
         self.order_by = order_by
         self.query_type = query_type
         self.scoring_statistics = scoring_statistics
         self.session_id = session_id
         self.scoring_parameters = scoring_parameters
         self.scoring_profile = scoring_profile
         self.semantic_configuration = semantic_configuration
+        self.semantic_error_handling = semantic_error_handling
+        self.semantic_max_wait_in_milliseconds = semantic_max_wait_in_milliseconds
+        self.debug = debug
         self.search_text = search_text
         self.search_fields = search_fields
         self.search_mode = search_mode
         self.query_language = query_language
         self.speller = speller
         self.answers = answers
         self.select = select
         self.skip = skip
         self.top = top
         self.captions = captions
         self.semantic_fields = semantic_fields
+        self.vector = vector
 
 
 class SearchResult(_serialization.Model):
     """Contains a document found by a search query, plus associated metadata.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
@@ -1289,43 +1474,91 @@
     :ivar highlights: Text fragments from the document that indicate the matching search terms,
      organized by each applicable field; null if hit highlighting was not enabled for the query.
     :vartype highlights: dict[str, list[str]]
     :ivar captions: Captions are the most representative passages from the document relatively to
      the search query. They are often used as document summary. Captions are only returned for
      queries of type 'semantic'.
     :vartype captions: list[~search_index_client.models.CaptionResult]
+    :ivar document_debug_info: Contains debugging information that can be used to further explore
+     your search results.
+    :vartype document_debug_info: list[~search_index_client.models.DocumentDebugInfo]
     """
 
     _validation = {
         "score": {"required": True, "readonly": True},
         "reranker_score": {"readonly": True},
         "highlights": {"readonly": True},
         "captions": {"readonly": True},
+        "document_debug_info": {"readonly": True},
     }
 
     _attribute_map = {
         "additional_properties": {"key": "", "type": "{object}"},
         "score": {"key": "@search\\.score", "type": "float"},
         "reranker_score": {"key": "@search\\.rerankerScore", "type": "float"},
         "highlights": {"key": "@search\\.highlights", "type": "{[str]}"},
         "captions": {"key": "@search\\.captions", "type": "[CaptionResult]"},
+        "document_debug_info": {"key": "@search\\.documentDebugInfo", "type": "[DocumentDebugInfo]"},
     }
 
-    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs):
+    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         """
         super().__init__(**kwargs)
         self.additional_properties = additional_properties
         self.score = None
         self.reranker_score = None
         self.highlights = None
         self.captions = None
+        self.document_debug_info = None
+
+
+class SemanticDebugInfo(_serialization.Model):
+    """SemanticDebugInfo.
+
+    Variables are only populated by the server, and will be ignored when sending a request.
+
+    :ivar title_field: The title field that was sent to the semantic enrichment process, as well as
+     how it was used.
+    :vartype title_field: ~search_index_client.models.QueryResultDocumentSemanticField
+    :ivar content_fields: The content fields that were sent to the semantic enrichment process, as
+     well as how they were used.
+    :vartype content_fields: list[~search_index_client.models.QueryResultDocumentSemanticField]
+    :ivar keyword_fields: The keyword fields that were sent to the semantic enrichment process, as
+     well as how they were used.
+    :vartype keyword_fields: list[~search_index_client.models.QueryResultDocumentSemanticField]
+    :ivar reranker_input: The raw concatenated strings that were sent to the semantic enrichment
+     process.
+    :vartype reranker_input: ~search_index_client.models.QueryResultDocumentRerankerInput
+    """
+
+    _validation = {
+        "title_field": {"readonly": True},
+        "content_fields": {"readonly": True},
+        "keyword_fields": {"readonly": True},
+        "reranker_input": {"readonly": True},
+    }
+
+    _attribute_map = {
+        "title_field": {"key": "titleField", "type": "QueryResultDocumentSemanticField"},
+        "content_fields": {"key": "contentFields", "type": "[QueryResultDocumentSemanticField]"},
+        "keyword_fields": {"key": "keywordFields", "type": "[QueryResultDocumentSemanticField]"},
+        "reranker_input": {"key": "rerankerInput", "type": "QueryResultDocumentRerankerInput"},
+    }
+
+    def __init__(self, **kwargs: Any) -> None:
+        """ """
+        super().__init__(**kwargs)
+        self.title_field = None
+        self.content_fields = None
+        self.keyword_fields = None
+        self.reranker_input = None
 
 
 class SuggestDocumentsResult(_serialization.Model):
     """Response containing suggestion query results from an index.
 
     Variables are only populated by the server, and will be ignored when sending a request.
 
@@ -1344,15 +1577,15 @@
     }
 
     _attribute_map = {
         "results": {"key": "value", "type": "[SuggestResult]"},
         "coverage": {"key": "@search\\.coverage", "type": "float"},
     }
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs: Any) -> None:
         """ """
         super().__init__(**kwargs)
         self.results = None
         self.coverage = None
 
 
 class SuggestOptions(_serialization.Model):
@@ -1415,16 +1648,16 @@
         highlight_post_tag: Optional[str] = None,
         highlight_pre_tag: Optional[str] = None,
         minimum_coverage: Optional[float] = None,
         order_by: Optional[List[str]] = None,
         search_fields: Optional[List[str]] = None,
         select: Optional[List[str]] = None,
         top: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword filter: An OData expression that filters the documents considered for suggestions.
         :paramtype filter: str
         :keyword use_fuzzy_matching: A value indicating whether to use fuzzy matching for the
          suggestions query. Default is false. When set to true, the query will find terms even if
          there's a substituted or missing character in the search text. While this provides a better
          experience in some scenarios, it comes at a performance cost as fuzzy suggestions queries are
@@ -1547,16 +1780,16 @@
         highlight_post_tag: Optional[str] = None,
         highlight_pre_tag: Optional[str] = None,
         minimum_coverage: Optional[float] = None,
         order_by: Optional[str] = None,
         search_fields: Optional[str] = None,
         select: Optional[str] = None,
         top: Optional[int] = None,
-        **kwargs
-    ):
+        **kwargs: Any
+    ) -> None:
         """
         :keyword filter: An OData expression that filters the documents considered for suggestions.
         :paramtype filter: str
         :keyword use_fuzzy_matching: A value indicating whether to use fuzzy matching for the
          suggestion query. Default is false. When set to true, the query will find suggestions even if
          there's a substituted or missing character in the search text. While this provides a better
          experience in some scenarios, it comes at a performance cost as fuzzy suggestion searches are
@@ -1629,16 +1862,57 @@
     }
 
     _attribute_map = {
         "additional_properties": {"key": "", "type": "{object}"},
         "text": {"key": "@search\\.text", "type": "str"},
     }
 
-    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs):
+    def __init__(self, *, additional_properties: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:
         """
         :keyword additional_properties: Unmatched properties from the message are deserialized to this
          collection.
         :paramtype additional_properties: dict[str, any]
         """
         super().__init__(**kwargs)
         self.additional_properties = additional_properties
         self.text = None
+
+
+class Vector(_serialization.Model):
+    """The query parameters for vector and hybrid search queries.
+
+    :ivar value: The vector representation of a search query.
+    :vartype value: list[float]
+    :ivar k: Number of nearest neighbors to return as top hits.
+    :vartype k: int
+    :ivar fields: Vector Fields of type Collection(Edm.Single) to be included in the vector
+     searched.
+    :vartype fields: str
+    """
+
+    _attribute_map = {
+        "value": {"key": "value", "type": "[float]"},
+        "k": {"key": "k", "type": "int"},
+        "fields": {"key": "fields", "type": "str"},
+    }
+
+    def __init__(
+        self,
+        *,
+        value: Optional[List[float]] = None,
+        k: Optional[int] = None,
+        fields: Optional[str] = None,
+        **kwargs: Any
+    ) -> None:
+        """
+        :keyword value: The vector representation of a search query.
+        :paramtype value: list[float]
+        :keyword k: Number of nearest neighbors to return as top hits.
+        :paramtype k: int
+        :keyword fields: Vector Fields of type Collection(Edm.Single) to be included in the vector
+         searched.
+        :paramtype fields: str
+        """
+        super().__init__(**kwargs)
+        self.value = value
+        self.k = k
+        self.fields = fields
```

## Comparing `azure-search-documents-11.4.0b3/azure/search/documents/_generated/models/_patch.py` & `azure-search-documents-11.4.0b4/azure/search/documents/_generated/aio/operations/_patch.py`

 * *Files identical despite different names*

